{"meta":{"title":"Fan()","subtitle":null,"description":null,"author":"大范","url":"http://fuxkdb.com"},"pages":[{"title":"tags","date":"2018-06-15T09:34:58.000Z","updated":"2018-06-15T09:35:25.000Z","comments":true,"path":"tags/index.html","permalink":"http://fuxkdb.com/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2017-08-04T14:40:02.000Z","updated":"2019-06-22T17:07:58.000Z","comments":true,"path":"about/index.html","permalink":"http://fuxkdb.com/about/index.html","excerpt":"","text":"About Me一个菜🐓dba, 目前就职于马蜂窝CSDN博客MY BADGES"},{"title":"categories","date":"2018-06-15T09:29:28.000Z","updated":"2018-06-15T09:30:12.000Z","comments":true,"path":"categories/index.html","permalink":"http://fuxkdb.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Orchestrator Failover过程源码分析-II","slug":"2022-05-06-Orchestrator-Failover过程源码分析-II","date":"2022-04-30T10:06:00.000Z","updated":"2022-05-15T08:47:00.066Z","comments":true,"path":"2022/04/30/2022-05-06-Orchestrator-Failover过程源码分析-II/","link":"","permalink":"http://fuxkdb.com/2022/04/30/2022-05-06-Orchestrator-Failover%E8%BF%87%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-II/","excerpt":"Orchestrator Failover过程源码分析-II书接上文Orchestrator Failover过程源码分析-I DeadMaster恢复流程首先通过getCheckAndRecoverFunction获取”checkAndRecoverFunction”123456789101112func getCheckAndRecoverFunction(analysisCode inst.AnalysisCode, analyzedInstanceKey *inst.InstanceKey) ( checkAndRecoverFunction func(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error), isActionableRecovery bool,) &#123; switch analysisCode &#123; // master case inst.DeadMaster, inst.DeadMasterAndSomeReplicas: // 如果analysisCode是DeadMaster 或 DeadMasterAndSomeReplicas if isInEmergencyOperationGracefulPeriod(analyzedInstanceKey) &#123; // 首先判断是否处于 EmergencyOperationGracefulPeriod return checkAndRecoverGenericProblem, false // 如果处于EmergencyOperationGracefulPeriod, 则又相当于啥也没干, 等下一轮recoverTick &#125; else &#123; return checkAndRecoverDeadMaster, true &#125;这里先判断isInEmergencyOperationGracefulPeriod1234func isInEmergencyOperationGracefulPeriod(instanceKey *inst.InstanceKey) bool &#123; _, found := emergencyOperationGracefulPeriodMap.Get(instanceKey.StringCode()) // emergencyOperationGracefulPeriodMap 是一个cache, 有过期时间的&quot;缓存&quot; return found&#125;实际是尝试去emergencyOperationGracefulPeriodMap这个cache找有没有这个instance. 那么问题来了, 是谁在什么时候向这个cache放这个instance呢?其实是在主库处于UnreachableMaster状态下, executeCheckAndRecoverFunction中调用runEmergentOperations时","text":"Orchestrator Failover过程源码分析-II书接上文Orchestrator Failover过程源码分析-I DeadMaster恢复流程首先通过getCheckAndRecoverFunction获取”checkAndRecoverFunction”123456789101112func getCheckAndRecoverFunction(analysisCode inst.AnalysisCode, analyzedInstanceKey *inst.InstanceKey) ( checkAndRecoverFunction func(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error), isActionableRecovery bool,) &#123; switch analysisCode &#123; // master case inst.DeadMaster, inst.DeadMasterAndSomeReplicas: // 如果analysisCode是DeadMaster 或 DeadMasterAndSomeReplicas if isInEmergencyOperationGracefulPeriod(analyzedInstanceKey) &#123; // 首先判断是否处于 EmergencyOperationGracefulPeriod return checkAndRecoverGenericProblem, false // 如果处于EmergencyOperationGracefulPeriod, 则又相当于啥也没干, 等下一轮recoverTick &#125; else &#123; return checkAndRecoverDeadMaster, true &#125;这里先判断isInEmergencyOperationGracefulPeriod1234func isInEmergencyOperationGracefulPeriod(instanceKey *inst.InstanceKey) bool &#123; _, found := emergencyOperationGracefulPeriodMap.Get(instanceKey.StringCode()) // emergencyOperationGracefulPeriodMap 是一个cache, 有过期时间的&quot;缓存&quot; return found&#125;实际是尝试去emergencyOperationGracefulPeriodMap这个cache找有没有这个instance. 那么问题来了, 是谁在什么时候向这个cache放这个instance呢?其实是在主库处于UnreachableMaster状态下, executeCheckAndRecoverFunction中调用runEmergentOperations时 123456789// executeCheckAndRecoverFunction will choose the correct check &amp; recovery function based on analysis.// It executes the function synchronuouslyfunc executeCheckAndRecoverFunction(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error) &#123; atomic.AddInt64(&amp;countPendingRecoveries, 1) defer atomic.AddInt64(&amp;countPendingRecoveries, -1) checkAndRecoverFunction, isActionableRecovery := getCheckAndRecoverFunction(analysisEntry.Analysis, &amp;analysisEntry.AnalyzedInstanceKey) // 最初处于UnreachableMaster时, 这里拿到的是checkAndRecoverGenericProblem analysisEntry.IsActionableRecovery = isActionableRecovery runEmergentOperations(&amp;analysisEntry) 可以看到, UnreachableMaster时, 会运行emergentlyReadTopologyInstance123456func runEmergentOperations(analysisEntry *inst.ReplicationAnalysis) &#123; switch analysisEntry.Analysis &#123; ...省略部分代码 case inst.UnreachableMaster: // 实际目的是强制重新读一下该实例和其所有从库的&quot;信息&quot; go emergentlyReadTopologyInstance(&amp;analysisEntry.AnalyzedInstanceKey, analysisEntry.Analysis) go emergentlyReadTopologyInstanceReplicas(&amp;analysisEntry.AnalyzedInstanceKey, analysisEntry.Analysis)而在emergentlyReadTopologyInstance中会先尝试向emergencyReadTopologyInstanceMap中Add1234567891011// Force a re-read of a topology instance; this is done because we need to substantiate a suspicion// that we may have a failover scenario. we want to speed up reading the complete picture.func emergentlyReadTopologyInstance(instanceKey *inst.InstanceKey, analysisCode inst.AnalysisCode) (instance *inst.Instance, err error) &#123; if existsInCacheError := emergencyReadTopologyInstanceMap.Add(instanceKey.StringCode(), true, cache.DefaultExpiration); existsInCacheError != nil &#123; // Just recently attempted return nil, nil &#125; instance, err = inst.ReadTopologyInstance(instanceKey) // 会调用 ReadTopologyInstanceBufferable inst.AuditOperation(&quot;emergently-read-topology-instance&quot;, instanceKey, string(analysisCode)) return instance, err&#125; ReadTopologyInstance 实际调用 ReadTopologyInstanceBufferable 这个cache是在logic包init时创建的1emergencyOperationGracefulPeriodMap = cache.New(time.Second*5, time.Millisecond*500) // 过期时间5s isInEmergencyOperationGracefulPeriod 如果返回true, 表示尚处于EmergentOperations运行窗口期内, 所以需要等若返回false, 表示已经可以开始”恢复”了, 就会返回checkAndRecoverDeadMaster, 并且isActionableRecovery也为true executeCheckAndRecoverFunction12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// It executes the function synchronuouslyfunc executeCheckAndRecoverFunction(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error) &#123; atomic.AddInt64(&amp;countPendingRecoveries, 1) defer atomic.AddInt64(&amp;countPendingRecoveries, -1) checkAndRecoverFunction, isActionableRecovery := getCheckAndRecoverFunction(analysisEntry.Analysis, &amp;analysisEntry.AnalyzedInstanceKey) analysisEntry.IsActionableRecovery = isActionableRecovery runEmergentOperations(&amp;analysisEntry) ...省略部分代码 // Initiate detection: // 向backend db topology_failure_detection表insert ignore into一条数据 // 这里 skipProcesses = false, 所以会调用 OnFailureDetectionProcesses 钩子 // 到这里我才明白, 原来 skipProcesses 意思是 是否跳过 钩子的执行 registrationSuccess, _, err := checkAndExecuteFailureDetectionProcesses(analysisEntry, skipProcesses) if registrationSuccess &#123; if orcraft.IsRaftEnabled() &#123; _, err := orcraft.PublishCommand(&quot;register-failure-detection&quot;, analysisEntry) log.Errore(err) &#125; &#125; ...省略部分代码 // We don&#x27;t mind whether detection really executed the processes or not // (it may have been silenced due to previous detection). We only care there&#x27;s no error. // We&#x27;re about to embark on recovery shortly... // Check for recovery being disabled globally // 看是否全局禁用了恢复, 如通过API disable-global-recoveries 去禁用全局恢复 if recoveryDisabledGlobally, err := IsRecoveryDisabled(); err != nil &#123; // Unexpected. Shouldn&#x27;t get this log.Errorf(&quot;Unable to determine if recovery is disabled globally: %v&quot;, err) &#125; else if recoveryDisabledGlobally &#123; // 这里checkAndRecover传的是false if !forceInstanceRecovery &#123; // 所以如果全局禁用了recover, recoverTick是不会进行恢复的, 因为forceInstanceRecovery也是false. 至此退出 log.Infof(&quot;CheckAndRecover: Analysis: %+v, InstanceKey: %+v, candidateInstanceKey: %+v, &quot;+ &quot;skipProcesses: %v: NOT Recovering host (disabled globally)&quot;, analysisEntry.Analysis, analysisEntry.AnalyzedInstanceKey, candidateInstanceKey, skipProcesses) return false, nil, err &#125; log.Infof(&quot;CheckAndRecover: Analysis: %+v, InstanceKey: %+v, candidateInstanceKey: %+v, &quot;+ &quot;skipProcesses: %v: recoveries disabled globally but forcing this recovery&quot;, analysisEntry.Analysis, analysisEntry.AnalyzedInstanceKey, candidateInstanceKey, skipProcesses) &#125; // 开始运行checkAndRecoverFunction. 本例中是运行checkAndRecoverDeadMaster // 我们先不看checkAndRecoverDeadMaster具体干了啥, 先往下看 recoveryAttempted, topologyRecovery, err = checkAndRecoverFunction(analysisEntry, candidateInstanceKey, forceInstanceRecovery, skipProcesses) // 实参为analysisEntry, nil, false, false if !recoveryAttempted &#123; return recoveryAttempted, topologyRecovery, err &#125; if topologyRecovery == nil &#123; return recoveryAttempted, topologyRecovery, err &#125; if b, err := json.Marshal(topologyRecovery); err == nil &#123; log.Infof(&quot;Topology recovery: %+v&quot;, string(b)) &#125; else &#123; log.Infof(&quot;Topology recovery: %+v&quot;, *topologyRecovery) &#125; // skipProcesses=false if !skipProcesses &#123; // 没恢复成功, 调用 PostUnsuccessfulFailoverProcesses if topologyRecovery.SuccessorKey == nil &#123; // Execute general unsuccessful post failover processes executeProcesses(config.Config.PostUnsuccessfulFailoverProcesses, &quot;PostUnsuccessfulFailoverProcesses&quot;, topologyRecovery, false) &#125; else &#123; // 否则调用 PostFailoverProcesses // Execute general post failover processes inst.EndDowntime(topologyRecovery.SuccessorKey) executeProcesses(config.Config.PostFailoverProcesses, &quot;PostFailoverProcesses&quot;, topologyRecovery, false) &#125; &#125; ...省略部分代码 // 代码只要能走到这里, 就带表恢复成功了? recoveryAttempted肯定是true // 具体的恢复操作, 都在checkAndRecoverFunction里 return recoveryAttempted, topologyRecovery, err&#125; 对于本文场景, 当主库被认定处于DeadMaster状态后:executeCheckAndRecoverFunction会先调用getCheckAndRecoverFunction获取: checkAndRecoverFunction: checkAndRecoverDeadMaster isActionableRecovery: true 然后检查是否”禁用”了全局故障恢复功能(如通过API disable-global-recoveries), 如果是, 则直接return 接下来, 开始执行checkAndRecoverFunction, 即checkAndRecoverDeadMaster 我们先不看checkAndRecoverDeadMaster具体逻辑, 继续往下看. 下面的代码就是根据checkAndRecoverFunction的执行情况(是否成功恢复), 调用对应的钩子, 最后将恢复结果return回去, 其实到这里, 本轮恢复就算做完了. 具体恢复是否成功, recoverTick也不关心. 具体如何恢复的, 还是要看checkAndRecoverFunction(本例中是checkAndRecoverDeadMaster) 关于钩子, 见:OnFailureDetectionProcessesPostUnsuccessfulFailoverProcessesPostFailoverProcesses checkAndRecoverDeadMaster123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153// checkAndRecoverDeadMaster checks a given analysis, decides whether to take action, and possibly takes action// Returns true when action was taken.func checkAndRecoverDeadMaster(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error) &#123; // forceInstanceRecovery=false // HasAutomatedMasterRecovery 是在GetReplicationAnalysis中会运行 a.ClusterDetails.ReadRecoveryInfo() // ReadRecoveryInfo中会执行 this.HasAutomatedMasterRecovery = this.filtersMatchCluster(config.Config.RecoverMasterClusterFilters) // RecoverMasterClusterFilters 我们配置的是 .* 所以这里为true // false || true = true // 所以这对于recoverTick来说, 目的是检查一下RecoverMasterClusterFilters配置, 看这个集群到底配没配自动故障恢复 if !(forceInstanceRecovery || analysisEntry.ClusterDetails.HasAutomatedMasterRecovery) &#123; return false, nil, nil &#125; // 实参 &amp;analysisEntry, true, true // 尝试注册一个恢复条目, 如果失败, 意味着 recovery is already in place. // 让我们检查一下这个实例是否最近刚刚被提升或集群刚刚经历failover，并且仍然处于活动期。如果是这样，我们就拒绝恢复注册，to avoid flapping. // 就是说刚Failover没多久又Failover不行, 类似MHA 8小时限制 // 时间由RecoveryPeriodBlockMinutes控制, 默认1小时. 也是在recoveryTick启动协程是会去清理in_active_period标记(update为0) // 如果一切没问题, 会向数据库topology_recovery插入一条记录, 并new一个topologyRecovery结构体返回. // 否则 topologyRecovery 是 nil topologyRecovery, err = AttemptRecoveryRegistration(&amp;analysisEntry, !forceInstanceRecovery, !forceInstanceRecovery) if topologyRecovery == nil &#123; // 如果 topologyRecovery = nil, 说明最近刚恢复完, 或者正在恢复, 直接return AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;found an active or recent recovery on %+v. Will not issue another RecoverDeadMaster.&quot;, analysisEntry.AnalyzedInstanceKey)) return false, nil, err &#125; // That&#x27;s it! We must do recovery! AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;will handle DeadMaster event on %+v&quot;, analysisEntry.ClusterDetails.ClusterName)) recoverDeadMasterCounter.Inc(1) // 正式开始恢复 // 实参为 topologyRecovery, nil, false recoveryAttempted, promotedReplica, lostReplicas, err := recoverDeadMaster(topologyRecovery, candidateInstanceKey, skipProcesses) ...省略部分代码 topologyRecovery.LostReplicas.AddInstances(lostReplicas) // lostReplicas 由于种种原因, 没有成功change到new master的从库 if !recoveryAttempted &#123; return false, topologyRecovery, err &#125; // 代码运行到这里时, 可能已经选举出了new master, 即promotedReplica // 这个函数补充了一些判断条件, 判断这个promotedReplica是不是可以做new master // 主要就是这几个参数: // - PreventCrossDataCenterMasterFailover // - PreventCrossRegionMasterFailover // - FailMasterPromotionOnLagMinutes // - FailMasterPromotionIfSQLThreadNotUpToDate // - DelayMasterPromotionIfSQLThreadNotUpToDate overrideMasterPromotion := func() (*inst.Instance, error) &#123; if promotedReplica == nil &#123; // No promotion; nothing to override. return promotedReplica, err &#125; // Scenarios where we might cancel the promotion. // 这个函数就是通过以下参数: // - PreventCrossDataCenterMasterFailover // - PreventCrossRegionMasterFailover // 判断 promotedReplica 是否可以做 new master // PreventCrossDataCenterMasterFailover: // 默认false . 当为true 时, orchestrator将只用与故障集群主库位于同一DC的从库替换故障的主库. 它将尽最大努力从同一DC中找到一个替代者, 如果找不到, 将中止（失败）故障转移. // PreventCrossRegionMasterFailover: // 默认false . 当为true 时, orchestrator将只用与故障集群主库位于同一region的从库替换故障的主库. 它将尽最大努力找到同一region的替代者, 如果找不到, 将中止（失败）故障转移. if satisfied, reason := MasterFailoverGeographicConstraintSatisfied(&amp;analysisEntry, promotedReplica); !satisfied &#123; return nil, fmt.Errorf(&quot;RecoverDeadMaster: failed %+v promotion; %s&quot;, promotedReplica.Key, reason) &#125; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: promoted replica lag seconds: %+v&quot;, promotedReplica.ReplicationLagSeconds.Int64)) if config.Config.FailMasterPromotionOnLagMinutes &gt; 0 &amp;&amp; time.Duration(promotedReplica.ReplicationLagSeconds.Int64)*time.Second &gt;= time.Duration(config.Config.FailMasterPromotionOnLagMinutes)*time.Minute &#123; // candidate replica lags too much return nil, fmt.Errorf(&quot;RecoverDeadMaster: failed promotion. FailMasterPromotionOnLagMinutes is set to %d (minutes) and promoted replica %+v &#x27;s lag is %d (seconds)&quot;, config.Config.FailMasterPromotionOnLagMinutes, promotedReplica.Key, promotedReplica.ReplicationLagSeconds.Int64) &#125; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: promoted replica sql thread up-to-date: %+v&quot;, promotedReplica.SQLThreadUpToDate())) if config.Config.FailMasterPromotionIfSQLThreadNotUpToDate &amp;&amp; !promotedReplica.SQLThreadUpToDate() &#123; return nil, fmt.Errorf(&quot;RecoverDeadMaster: failed promotion. FailMasterPromotionIfSQLThreadNotUpToDate is set and promoted replica %+v &#x27;s sql thread is not up to date (relay logs still unapplied). Aborting promotion&quot;, promotedReplica.Key) &#125; if config.Config.DelayMasterPromotionIfSQLThreadNotUpToDate &amp;&amp; !promotedReplica.SQLThreadUpToDate() &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;DelayMasterPromotionIfSQLThreadNotUpToDate: waiting for SQL thread on %+v&quot;, promotedReplica.Key)) if _, err := inst.WaitForSQLThreadUpToDate(&amp;promotedReplica.Key, 0, 0); err != nil &#123; return nil, fmt.Errorf(&quot;DelayMasterPromotionIfSQLThreadNotUpToDate error: %+v&quot;, err) &#125; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;DelayMasterPromotionIfSQLThreadNotUpToDate: SQL thread caught up on %+v&quot;, promotedReplica.Key)) &#125; // All seems well. No override done. AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: found no reason to override promotion of %+v&quot;, promotedReplica.Key)) return promotedReplica, err &#125; // 运行 overrideMasterPromotion if promotedReplica, err = overrideMasterPromotion(); err != nil &#123; AuditTopologyRecovery(topologyRecovery, err.Error()) &#125; // And this is the end; whether successful or not, we&#x27;re done. resolveRecovery(topologyRecovery, promotedReplica) // Now, see whether we are successful or not. From this point there&#x27;s no going back. if promotedReplica != nil &#123; // 成功选举除了新主库 // Success! recoverDeadMasterSuccessCounter.Inc(1) AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: successfully promoted %+v&quot;, promotedReplica.Key)) AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: promoted server coordinates: %+v&quot;, promotedReplica.SelfBinlogCoordinates)) // 当为true 时, orchestrator 将在选举出的新主库上执行reset slave all 和set read_only=0 . 默认: true . 当该参数为true 时, 将覆盖MasterFailoverDetachSlaveMasterHost . // 所以这个if列操作就是执行在新主库执行 reset slave all 和set read_only=0 if config.Config.ApplyMySQLPromotionAfterMasterFailover || analysisEntry.CommandHint == inst.GracefulMasterTakeoverCommandHint &#123; // on GracefulMasterTakeoverCommandHint it makes utter sense to RESET SLAVE ALL and read_only=0, and there is no sense in not doing so. AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: will apply MySQL changes to promoted master&quot;)) &#123; _, err := inst.ResetReplicationOperation(&amp;promotedReplica.Key) if err != nil &#123; // Ugly, but this is important. Let&#x27;s give it another try _, err = inst.ResetReplicationOperation(&amp;promotedReplica.Key) &#125; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: applying RESET SLAVE ALL on promoted master: success=%t&quot;, (err == nil))) if err != nil &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: NOTE that %+v is promoted even though SHOW SLAVE STATUS may still show it has a master&quot;, promotedReplica.Key)) &#125; &#125; &#123; _, err := inst.SetReadOnly(&amp;promotedReplica.Key, false) AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: applying read-only=0 on promoted master: success=%t&quot;, (err == nil))) &#125; // Let&#x27;s attempt, though we won&#x27;t necessarily succeed, to set old master as read-only go func() &#123; // 并且尝试给旧主库打开只读, 成功与否无所 _, err := inst.SetReadOnly(&amp;analysisEntry.AnalyzedInstanceKey, true) AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: applying read-only=1 on demoted master: success=%t&quot;, (err == nil))) &#125;() &#125; ...省略部分代码 // 当该参数为true 时, orchestrator将对被选举为新主库的节点执行detach-replica-master-host（这确保了即使旧主库&quot;复活了&quot;, 新主库也不会试图从旧主库复制数据）. 默认值: false. 如果ApplyMySQLPromotionAfterMasterFailover为真，这个参数将失去意义. if config.Config.MasterFailoverDetachReplicaMasterHost &#123; postponedFunction := func() error &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;- RecoverDeadMaster: detaching master host on promoted master&quot;)) // 所以说 如果 ApplyMySQLPromotionAfterMasterFailover=true, 就已经完成了reset slave all了. // 那么DetachReplicaMasterHost也就失去意义了 inst.DetachReplicaMasterHost(&amp;promotedReplica.Key) return nil &#125; topologyRecovery.AddPostponedFunction(postponedFunction, fmt.Sprintf(&quot;RecoverDeadMaster, detaching promoted master host %+v&quot;, promotedReplica.Key)) &#125; ...省略部分代码 if !skipProcesses &#123; // Execute post master-failover processes // 执行钩子 PostMasterFailoverProcesses executeProcesses(config.Config.PostMasterFailoverProcesses, &quot;PostMasterFailoverProcesses&quot;, topologyRecovery, false) &#125; &#125; else &#123; // 否则, 说明恢复失败了 recoverDeadMasterFailureCounter.Inc(1) &#125; return true, topologyRecovery, err&#125; checkAndRecoverDeadMaster 首先通过RecoverMasterClusterFilters再次判断这个实例是否可以进行自动故障恢复.接着, 调用AttemptRecoveryRegistration检查这个实例是否最近刚刚被提升或该实例所处集群刚刚经历failover, 并且仍然处于活动期. 就是说刚Failover没多久又Failover不行, 类似MHA 8小时限制 活动期的时间由RecoveryPeriodBlockMinutes控制, 默认1小时. 也是在recoveryTick启动协程时会去清理in_active_period标记(update为0)如果一切没问题, 会向数据库topology_recovery插入一条记录, 并new一个topologyRecovery结构体返回. 否则 topologyRecovery 是 nil 以上检查都没问题的话, 就正式开始执行恢复, 调用recoverDeadMaster. 我们先不看recoverDeadMaster代码, 继续往后看. recoverDeadMaster最重要的是会返回promotedReplica, 即选举出来的新主库代码运行到这里时, 可能已经选举出了new master, 即promotedReplica. 随后会运行overrideMasterPromotion函数这个函数补充了一些判断条件, 判断这个promotedReplica是不是可以做new master主要就是这几个参数: PreventCrossDataCenterMasterFailover PreventCrossRegionMasterFailover FailMasterPromotionOnLagMinutes FailMasterPromotionIfSQLThreadNotUpToDate DelayMasterPromotionIfSQLThreadNotUpToDate 以上任意一个判断有问题, 都会终止后续恢复动作, 直接return 最后, checkAndRecoverDeadMaster会对new master和old master做一些收尾工作, 如: 如果ApplyMySQLPromotionAfterMasterFailover为true, 则会在新主库执行 reset slave all 和set read_only=0, 否则走MasterFailoverDetachReplicaMasterHost逻辑 尝试连接旧主库打开只读 至此, 恢复成功完成.具体的恢复逻辑, 还要看recoverDeadMaster recoverDeadMaster12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// recoverDeadMaster recovers a dead master, complete logic insidefunc recoverDeadMaster(topologyRecovery *TopologyRecovery, candidateInstanceKey *inst.InstanceKey, skipProcesses bool) (recoveryAttempted bool, promotedReplica *inst.Instance, lostReplicas [](*inst.Instance), err error) &#123; // 传进来的实参, topologyRecovery, nil, false topologyRecovery.Type = MasterRecovery analysisEntry := &amp;topologyRecovery.AnalysisEntry failedInstanceKey := &amp;analysisEntry.AnalyzedInstanceKey var cannotReplicateReplicas [](*inst.Instance) postponedAll := false inst.AuditOperation(&quot;recover-dead-master&quot;, failedInstanceKey, &quot;problem found; will recover&quot;) if !skipProcesses &#123; // 执行钩子 if err := executeProcesses(config.Config.PreFailoverProcesses, &quot;PreFailoverProcesses&quot;, topologyRecovery, true); err != nil &#123; return false, nil, lostReplicas, topologyRecovery.AddError(err) &#125; &#125; ...省略部分代码 // topologyRecovery.RecoveryType = MasterRecoveryGTID topologyRecovery.RecoveryType = GetMasterRecoveryType(analysisEntry) ...省略部分代码 // 这个函数判断GetCandidateReplica返回的candidate是不是&quot;理想的&quot; // 如果是&quot;理想的&quot; 一些从库的恢复操作可以异步推迟执行 // 否则要同步执行 promotedReplicaIsIdeal := func(promoted *inst.Instance, hasBestPromotionRule bool) bool &#123; if promoted == nil &#123; return false &#125; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: promotedReplicaIsIdeal(%+v)&quot;, promoted.Key)) // candidateInstanceKey实参是nil, 所以走不到这个if if candidateInstanceKey != nil &#123; //explicit request to promote a specific server return promoted.Key.Equals(candidateInstanceKey) &#125; if promoted.DataCenter == topologyRecovery.AnalysisEntry.AnalyzedInstanceDataCenter &amp;&amp; promoted.PhysicalEnvironment == topologyRecovery.AnalysisEntry.AnalyzedInstancePhysicalEnvironment &#123; if promoted.PromotionRule == inst.MustPromoteRule || promoted.PromotionRule == inst.PreferPromoteRule || (hasBestPromotionRule &amp;&amp; promoted.PromotionRule != inst.MustNotPromoteRule) &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: found %+v to be ideal candidate; will optimize recovery&quot;, promoted.Key)) postponedAll = true return true &#125; &#125; return false &#125; switch topologyRecovery.RecoveryType &#123; case MasterRecoveryGTID: &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: regrouping replicas via GTID&quot;)) // promotedReplica 有可能==nil lostReplicas, _, cannotReplicateReplicas, promotedReplica, err = inst.RegroupReplicasGTID(failedInstanceKey, true, false, nil, &amp;topologyRecovery.PostponedFunctionsContainer, promotedReplicaIsIdeal) &#125; // case MasterRecoveryPseudoGTID: // ...省略部分代码 // case MasterRecoveryBinlogServer: // ...省略部分代码 &#125; topologyRecovery.AddError(err) lostReplicas = append(lostReplicas, cannotReplicateReplicas...) ...省略部分代码 if promotedReplica != nil &amp;&amp; len(lostReplicas) &gt; 0 &amp;&amp; config.Config.DetachLostReplicasAfterMasterFailover &#123; postponedFunction := func() error &#123; AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: lost %+v replicas during recovery process; detaching them&quot;, len(lostReplicas))) for _, replica := range lostReplicas &#123; replica := replica inst.DetachReplicaMasterHost(&amp;replica.Key) &#125; return nil &#125; // 对lostReplicas并发执行 DetachReplicaMasterHost topologyRecovery.AddPostponedFunction(postponedFunction, fmt.Sprintf(&quot;RecoverDeadMaster, detach %+v lost replicas&quot;, len(lostReplicas))) &#125; ...省略部分代码 AuditTopologyRecovery(topologyRecovery, fmt.Sprintf(&quot;RecoverDeadMaster: %d postponed functions&quot;, topologyRecovery.PostponedFunctionsContainer.Len())) if promotedReplica != nil &amp;&amp; !postponedAll &#123; // 有候选人, 并且不是理想的候选人 // 这里的意思是, 我们一开始选了一个new master, 可能因为他有最全的日志, 但是, 他不是我们设置的prefer的实例, 那么我们就重新组织一下拓扑, 把prefer的再变成new master promotedReplica, err = replacePromotedReplicaWithCandidate(topologyRecovery, &amp;analysisEntry.AnalyzedInstanceKey, promotedReplica, candidateInstanceKey) topologyRecovery.AddError(err) &#125; if promotedReplica == nil &#123; message := &quot;Failure: no replica promoted.&quot; AuditTopologyRecovery(topologyRecovery, message) inst.AuditOperation(&quot;recover-dead-master&quot;, failedInstanceKey, message) &#125; else &#123; message := fmt.Sprintf(&quot;promoted replica: %+v&quot;, promotedReplica.Key) AuditTopologyRecovery(topologyRecovery, message) inst.AuditOperation(&quot;recover-dead-master&quot;, failedInstanceKey, message) &#125; return true, promotedReplica, lostReplicas, err&#125; recoverDeadMaster 主要做了几件事 ^a0c808 GetMasterRecoveryType, 确定到底用什么方式恢复, 是基于GTID? PseudoGTID? 还是BinlogServer? 重组拓扑, 我们的案例是使用RegroupReplicasGTID. 但这里有一个问题, 可能现在我们的新主库并不是我们”期望”的实例, 就是说之所以选他做主库可能是因为他有最全的日志. 但不是我们设置的prefer的 所以通过一个闭包promotedReplicaIsIdeal去做了判断和标记(通过postponedAll) 如果postponedAll=false, 则需要重组拓扑, 选择prefer的replica作为新主库 postponedAll=true表是candidate就是”理想型”, 所有replica恢复可以并行异步执行 如果存在lostReplicas, 并且开启了DetachLostReplicasAfterMasterFailover, 那么会并行的对所有lostReplicas执行DetachReplicaMasterHost 其实就是执行change master to master_host=’// {host}’ 如果当前选举的new master不是我们prefer的实例, 重组拓扑, 用prefer做新主库 recoverDeadMaster还是干了挺多事儿的, 尤其是RegroupReplicasGTID中的逻辑还是很复杂的. 本文就先不继续展开了, 要知后事如何, 请看Orchestrator Failover过程源码分析-III 初步流程总结","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Orchestrator","slug":"Orchestrator","permalink":"http://fuxkdb.com/tags/Orchestrator/"}]},{"title":"Orchestrator Failover过程源码分析-I","slug":"2022-05-11-Orchestrator-Failover过程源码分析-I","date":"2022-04-28T07:08:00.000Z","updated":"2022-05-15T08:46:18.773Z","comments":true,"path":"2022/04/28/2022-05-11-Orchestrator-Failover过程源码分析-I/","link":"","permalink":"http://fuxkdb.com/2022/04/28/2022-05-11-Orchestrator-Failover%E8%BF%87%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-I/","excerpt":"模拟故障使用测试环境, 模拟3307集群故障 角色 IP 端口 主机名 主库 172.16.120.10 3307 centos-1 从库 172.16.120.11 3307 centos-2 从库 172.16.120.12 3307 centos-3 关闭3307主库172.16.120.10:3307123456[2022-04-25 13:10:56][root@centos-1 13:10:56 ~][2022-04-25 13:11:22]#systemctl stop mysql3307mysql日志2022-04-25T13:11:35.959667+08:00 0 [Note] /usr/local/mysql5732/bin/mysqld: Shutdown complete","text":"模拟故障使用测试环境, 模拟3307集群故障 角色 IP 端口 主机名 主库 172.16.120.10 3307 centos-1 从库 172.16.120.11 3307 centos-2 从库 172.16.120.12 3307 centos-3 关闭3307主库172.16.120.10:3307123456[2022-04-25 13:10:56][root@centos-1 13:10:56 ~][2022-04-25 13:11:22]#systemctl stop mysql3307mysql日志2022-04-25T13:11:35.959667+08:00 0 [Note] /usr/local/mysql5732/bin/mysqld: Shutdown complete 源码分析我的思路是通过日志找入口.在下文中: 对主库简称为centos-1 对两个从库分别简称为: centos-2 centos-3123456789101112131415[mysql] 2022/04/25 13:11:27 packets.go:37: unexpected EOF2022-04-25 13:11:27 ERROR invalid connection2022-04-25 13:11:27 ERROR ReadTopologyInstance(172.16.120.10:3307) show global status like &#x27;Uptime&#x27;: Error 1053: Server shutdown in progress[mysql] 2022/04/25 13:11:27 packets.go:37: unexpected EOF2022-04-25 13:11:27 ERROR invalid connection[mysql] 2022/04/25 13:11:27 packets.go:37: unexpected EOF2022-04-25 13:11:27 ERROR invalid connection2022-04-25 13:11:27 ERROR dial tcp 172.16.120.10:3307: connect: connection refused2022-04-25 13:11:28 ERROR dial tcp 172.16.120.10:3307: connect: connection refused2022-04-25 13:11:28 DEBUG writeInstance: will not update database_instance due to error: invalid connection2022-04-25 13:11:32 WARNING DiscoverInstance(172.16.120.10:3307) instance is nil in 0.104s (Backend: 0.001s, Instance: 0.103s), error=dial tcp 172.16.120.10:3307: connect: connection refused2022-04-25 13:11:33 DEBUG analysis: ClusterName: 172.16.120.10:3307, IsMaster: true, LastCheckValid: false, LastCheckPartialSuccess: false, CountReplicas: 2, CountValidReplicas: 2, CountValidReplicatingReplicas: 2, CountLaggingReplicas: 0, CountDelayedReplicas: 0, CountReplicasFailingToConnectToMaster: 02022-04-25 13:11:33 INFO executeCheckAndRecoverFunction: proceeding with UnreachableMaster detection on 172.16.120.10:3307; isActionable?: false; skipProcesses: false2022-04-25 13:11:33 INFO topology_recovery: detected UnreachableMaster failure on 172.16.120.10:33072022-04-25 13:11:33 INFO topology_recovery: Running 1 OnFailureDetectionProcesses hooks 关闭centos-1后, 从日志可以看出: orchestrator对centos-1的一些探测操作失败了 executeCheckAndRecoverFunction: proceeding with UnreachableMaster 通过第二条信息找”入口”全局搜索executeCheckAndRecoverFunction: proceeding with 搜索到函数executeCheckAndRecoverFunction 这个函数比较长, 先不看. 先看一下是谁调用 executeCheckAndRecoverFunction . 搜索executeCheckAndRecoverFunction(, 搜到CheckAndRecover 继续搜索CheckAndRecover, 查到是ContinuousDiscovery在调用它 ContinuousDiscovery 在logic包中, 被http.standardHttp调用, 而http.standardHttp又被http.Http调用, http.Http是在启动orchestrator时被调用的123456789101112131415161718go/cmd/orchestrator/main.go// 截取部分代码 switch &#123; case helpTopic != &quot;&quot;: app.HelpCommand(helpTopic) case len(flag.Args()) == 0 || flag.Arg(0) == &quot;cli&quot;: app.CliWrapper(*command, *strict, *instance, *destination, *owner, *reason, *duration, *pattern, *clusterAlias, *pool, *hostnameFlag) case flag.Arg(0) == &quot;http&quot;: app.Http(*discovery) default: fmt.Fprintln(os.Stderr, `Usage: orchestrator --options... [cli|http]See complete list of commands: orchestrator -c helpFull blown documentation: orchestrator`) os.Exit(1) &#125;&#125; 也就是说, 当我们用以下命令启动orchestrator后1orchestrator -config orchestrator.conf.json -debug http 就会 http.Http -&gt; http.standardHttp -&gt; go logic.ContinuousDiscovery() ContinuousDiscovery都干了啥持续发现1234// ContinuousDiscovery starts an asynchronuous infinite discovery process where instances are// periodically investigated and their status captured, and long since unseen instances are // purged and forgotten.ContinuousDiscovery启动一个永不停止的异步&quot;发现&quot;过程, 在这个过程中, 实例被周期性地调查并捕获它们的状态, 长期以来不可见的实例被清除和遗忘. 这段注释中 asynchronuous 还拼写错了, 应该是 asynchronous ContinuousDiscovery 先启动一个协程123func ContinuousDiscovery() &#123; ... go handleDiscoveryRequests() handleDiscoveryRequests 在Orchestrator Discover源码分析中介绍过handleDiscoveryRequests迭代discoveryQueue channel 并在每个条目上调用DiscoverInstance, 而DiscoverInstance又会调用ReadTopologyInstanceBufferable, 后者会实际连接MySQL实例, 获取各种指标/参数信息, 最终将结果写入database_instance database_instance 表 那么discoveryQueue里的”数据”又是谁放进来的呢?, 有两个地方 通过命令行或前端页面手动触发”发现”时(本质是调用orchestrator discover接口), 会将指定instance的ReplicaKey和MasterKey放入discoveryQueue ContinuousDiscovery 还会创建一个healthTick定时器, 周期性(每秒)调用onHealthTick, onHealthTick会取出所有”过期”的instance, 放到discoverQueue中123456789101112131415const HealthPollSeconds = 1func ContinuousDiscovery() &#123; ...省略部分代码 healthTick := time.Tick(config.HealthPollSeconds * time.Second) ...省略部分代码 for &#123; select &#123; case &lt;-healthTick: go func() &#123; onHealthTick() &#125;() ...省略部分代码&#125; 1234567891011121314// onHealthTick handles the actions to take to discover/poll instancesfunc onHealthTick() &#123;...省略部分代码 instanceKeys, err := inst.ReadOutdatedInstanceKeys() // 读出过期的实例. 过期的定义是: InstancePollSeconds秒未探测的connectable实例 或 2*InstancePollSeconds秒未探测的连接出现异常(hang)的实例 // avoid any logging unless there&#x27;s something to be done if len(instanceKeys) &gt; 0 &#123; for _, instanceKey := range instanceKeys &#123; if instanceKey.IsValid() &#123; discoveryQueue.Push(instanceKey) &#125; &#125;&#125; 那么就是说, 每秒钟(HealthPollSeconds=1), onHealthTick会把所有”过期”的实例放到discoveryQueue 流程图 需要与实例轮询(或大致)相同频率的常规操作123456789101112131415161718192021func ContinuousDiscovery() &#123; ...省略部分代码 instancePollTick := time.Tick(instancePollSecondsDuration()) // InstancePollSeconds 默认5秒 ...省略部分代码 for &#123; select &#123; ...省略部分代码 case &lt;-instancePollTick: // 5秒一次 go func() &#123; // This tick does NOT do instance poll (these are handled by the oversampling discoveryTick) // But rather should invoke such routinely operations that need to be as (or roughly as) frequent // as instance poll if IsLeaderOrActive() &#123; go inst.UpdateClusterAliases() go inst.ExpireDowntime() go injectSeeds(&amp;seedOnce) &#125; &#125;() ...省略部分代码&#125; 看起来都是些不太重要的操作 定时注入伪GTID1234567891011121314151617const PseudoGTIDIntervalSeconds = 5func ContinuousDiscovery() &#123; ...省略部分代码 autoPseudoGTIDTick := time.Tick(time.Duration(config.PseudoGTIDIntervalSeconds) * time.Second) ...省略部分代码 for &#123; select &#123; ...省略部分代码 case &lt;-autoPseudoGTIDTick: go func() &#123; if config.Config.AutoPseudoGTID &amp;&amp; IsLeader() &#123; go InjectPseudoGTIDOnWriters() &#125; &#125;() ...省略部分代码&#125; Pseudo GTID , 不太重要, 现在还会有人不开GTID吗? 护理工作12345678910111213141516171819202122232425262728293031323334353637383940414243444546func ContinuousDiscovery() &#123; ...省略部分代码 caretakingTick := time.Tick(time.Minute) ...省略部分代码 for &#123; select &#123; ...省略部分代码 case &lt;-caretakingTick: // Various periodic internal maintenance tasks go func() &#123; if IsLeaderOrActive() &#123; go inst.RecordInstanceCoordinatesHistory() go inst.ReviewUnseenInstances() go inst.InjectUnseenMasters() go inst.ForgetLongUnseenInstances() go inst.ForgetLongUnseenClusterAliases() go inst.ForgetUnseenInstancesDifferentlyResolved() go inst.ForgetExpiredHostnameResolves() go inst.DeleteInvalidHostnameResolves() go inst.ResolveUnknownMasterHostnameResolves() go inst.ExpireMaintenance() go inst.ExpireCandidateInstances() go inst.ExpireHostnameUnresolve() go inst.ExpireClusterDomainName() go inst.ExpireAudit() go inst.ExpireMasterPositionEquivalence() go inst.ExpirePoolInstances() go inst.FlushNontrivialResolveCacheToDatabase() go inst.ExpireInjectedPseudoGTID() go inst.ExpireStaleInstanceBinlogCoordinates() go process.ExpireNodesHistory() go process.ExpireAccessTokens() go process.ExpireAvailableNodes() go ExpireFailureDetectionHistory() go ExpireTopologyRecoveryHistory() go ExpireTopologyRecoveryStepsHistory() if runCheckAndRecoverOperationsTimeRipe() &amp;&amp; IsLeader() &#123; go SubmitMastersToKvStores(&quot;&quot;, false) &#125; &#125; else &#123; // Take this opportunity to refresh yourself go inst.LoadHostnameResolveCache() &#125; &#125;() 从方法名字可以看出来, 就是做一些”护理”工作, 如: 清理unseened instance, 详见参数: UnseenInstanceForgetHours 清理过期审计日志 raft护理工作12345678910111213func ContinuousDiscovery() &#123; raftCaretakingTick := time.Tick(10 * time.Minute) ...省略部分代码 for &#123; select &#123; ...省略部分代码 case &lt;-raftCaretakingTick: if orcraft.IsRaftEnabled() &amp;&amp; orcraft.IsLeader() &#123; // publishDiscoverMasters will publish to raft a discovery request for all known masters. // This makes for a best-effort keep-in-sync between raft nodes, where some may have // inconsistent data due to hosts being forgotten, for example. go publishDiscoverMasters() &#125; 如果orchestrator是[[raft模式部署]]的, 并且本节点是leader, 那么leader会发布一个discovery request给每个raft节点, 这些节点会对所有MySQL主库进行discover. 这么做的目的是保持所有raft nodes的数据”同步” A visual example 如上图所示, 三个orchestrator 组成一个raft cluster, 每个orchestrator 节点使用自己的专用数据库(MySQL或SQLite) orchestrator 节点之间会进行通信. 只有一个orchestrator 节点会成为leader. 所有orchestrator节点探测整个MySQL舰队. 每个MySQL server都被每个raft成员探测. 保存拓扑快照如果SnapshotTopologiesIntervalHours值大于0, 那么会每SnapshotTopologiesIntervalHours小时保存database_instance到database_instance_topology_history1234567891011121314151617func ContinuousDiscovery() &#123; ...省略部分代码 var snapshotTopologiesTick &lt;-chan time.Time if config.Config.SnapshotTopologiesIntervalHours &gt; 0 &#123; snapshotTopologiesTick = time.Tick(time.Duration(config.Config.SnapshotTopologiesIntervalHours) * time.Hour) &#125; ...省略部分代码 for &#123; select &#123; ..省略部分代码 case &lt;-snapshotTopologiesTick: go func() &#123; if IsLeaderOrActive() &#123; go inst.SnapshotTopologies() &#125; &#125;() &#125;就是执行insert ignore into database_instance_topology_history select x from database_instance12345678910111213141516171819202122// SnapshotTopologies records topology graph for all existing topologiesfunc SnapshotTopologies() error &#123; writeFunc := func() error &#123; _, err := db.ExecOrchestrator(` insert ignore into database_instance_topology_history (snapshot_unix_timestamp, hostname, port, master_host, master_port, cluster_name, version) select UNIX_TIMESTAMP(NOW()), hostname, port, master_host, master_port, cluster_name, version from database_instance `, ) if err != nil &#123; return log.Errore(err) &#125; return nil &#125; return ExecDBWriteFunc(writeFunc)&#125; recover工作123456789101112131415161718192021222324252627282930313233343536373839const RecoveryPollSeconds = 1func ContinuousDiscovery() &#123; ...省略部分代码 continuousDiscoveryStartTime := time.Now() checkAndRecoverWaitPeriod := 3 * instancePollSecondsDuration() ...省略部分代码 runCheckAndRecoverOperationsTimeRipe := func() bool &#123; return time.Since(continuousDiscoveryStartTime) &gt;= checkAndRecoverWaitPeriod &#125; ...省略部分代码 recoveryTick := time.Tick(time.Duration(config.RecoveryPollSeconds) * time.Second) for &#123; select &#123; case &lt;-recoveryTick: go func() &#123; if IsLeaderOrActive() &#123; go ClearActiveFailureDetections() go ClearActiveRecoveries() go ExpireBlockedRecoveries() go AcknowledgeCrashedRecoveries() go inst.ExpireInstanceAnalysisChangelog() go func() &#123; // This function is non re-entrant (it can only be running once at any point in time) if atomic.CompareAndSwapInt64(&amp;recoveryEntrance, 0, 1) &#123; // 如果返回true, 说明当时没有运行中的恢复任务 defer atomic.StoreInt64(&amp;recoveryEntrance, 0) &#125; else &#123; // 否则直接return return &#125; if runCheckAndRecoverOperationsTimeRipe() &#123; // 从开始运行ContinuousDiscovery至今的时间 &gt; (3 * InstancePollSeconds = 15秒) 才可以运行recover CheckAndRecover(nil, nil, false) &#125; else &#123; log.Debugf(&quot;Waiting for %+v seconds to pass before running failure detection/recovery&quot;, checkAndRecoverWaitPeriod.Seconds()) &#125; &#125;() &#125; &#125;() 从注释// This function is non re-entrant (it can only be running once at any point in time) 可以看出, 同一时间只能有一个恢复任务运行 atomic.CompareAndSwapInt64在Go语言中，原子包提供lower-level原子内存，这对实现同步算法很有帮助。 Go语言中的CompareAndSwapInt64()函数用于对int64值执行比较和交换操作。此函数在原子包下定义。在这里，您需要导入“sync/atomic”软件包才能使用这些函数。用法:1&gt;func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool)在这里，addr表示地址，old表示int64值，它是从交换操作返回的旧交换值，而new则是int64新值，它将与旧交换值进行交换。返回值：如果交换完成，则返回true，否则返回false。 恢复机制的入口在CheckAndRecover CheckAndRecoverCheckAndRecover(nil, nil, false) CheckAndRecover 首先调用GetReplicationAnalysis 获取分析结果replicationAnalysis(是一个切片). 后者实际是通过查询database_instance表, 查出所有有问题的实例信息, 封装成ReplicationAnalysis结构体, 这个结构体中包含实例基础信息(如是否是主库, 是否开启GTID等), Analysis(即orc定义故障名称, 详见Failure detection scenarios 故障检测场景)和StructureAnalysis(即拓扑结构的故障列表, 如NotEnoughValidSemiSyncReplicasStructureWarning等) 当然, GetReplicationAnalysis有可能返回一个空切片, 即代表当前无任何故障如果GetReplicationAnalysis返回err!=nil, 那么整个CheckAndRecover也会就此退出return error 接着, CheckAndRecover 按随机顺序迭代replicationAnalysis, 对每一个analysisEntry开启协程调用executeCheckAndRecoverFunction1234go func() &#123; _, _, err := executeCheckAndRecoverFunction(analysisEntry, candidateInstanceKey, false, skipProcesses) // 实际参数是 analysisEntry, nil, false, false log.Errore(err) &#125;() executeCheckAndRecoverFunction 函数注释// executeCheckAndRecoverFunction will choose the correct check &amp; recovery function based on analysis.// It executes the function synchronuously synchronuously拼写错误, 应为synchronously 直译: executeCheckAndRecoverFunction将根据分析选择正确的检查和恢复函数。它同步地执行该功能 executeCheckAndRecoverFunctionexecuteCheckAndRecoverFunction 首先调用getCheckAndRecoverFunction, 后者根据analysisEntry.Analysis(即orc定义故障名称, 详见Failure detection scenarios 故障检测场景)返回对应的checkAndRecoverFunction, 以及一个布尔值isActionableRecovery, 这个值会赋值给analysisEntry.IsActionableRecovery1checkAndRecoverFunction, isActionableRecovery := getCheckAndRecoverFunction(analysisEntry.Analysis, &amp;analysisEntry.AnalyzedInstanceKey)以本次实验模拟的主库宕机为例, 我们关闭主库后, 主库会先被认为处于UnreachableMaster状态1234567891011121314151617func getCheckAndRecoverFunction(analysisCode inst.AnalysisCode, analyzedInstanceKey *inst.InstanceKey) ( checkAndRecoverFunction func(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (recoveryAttempted bool, topologyRecovery *TopologyRecovery, err error), isActionableRecovery bool,) &#123; switch analysisCode &#123; ...省略部分代码 case inst.UnreachableMaster: return checkAndRecoverGenericProblem, false ...省略部分代码 &#125; // Right now this is mostly causing noise with no clear action. // Will revisit this in the future. // case inst.AllMasterReplicasStale: // return checkAndRecoverGenericProblem, false return nil, false&#125;于是第一次, 拿到的checkAndRecoverFunction是checkAndRecoverGenericProblem, 这个函数啥也没干, 就是返回fale, nil, nil1234// checkAndRecoverGenericProblem is a general-purpose recovery functionfunc checkAndRecoverGenericProblem(analysisEntry inst.ReplicationAnalysis, candidateInstanceKey *inst.InstanceKey, forceInstanceRecovery bool, skipProcesses bool) (bool, *TopologyRecovery, error) &#123; return false, nil, nil &#125; 随后会运行runEmergentOperations1runEmergentOperations(&amp;analysisEntry)以本次实验模拟的主库宕机为例, 我们关闭主库后, 主库会先被认为处于UnreachableMaster状态123456func runEmergentOperations(analysisEntry *inst.ReplicationAnalysis) &#123; switch analysisEntry.Analysis &#123; ...省略部分代码 case inst.UnreachableMaster: go emergentlyReadTopologyInstance(&amp;analysisEntry.AnalyzedInstanceKey, analysisEntry.Analysis) go emergentlyReadTopologyInstanceReplicas(&amp;analysisEntry.AnalyzedInstanceKey, analysisEntry.Analysis) 那么按照代码逻辑, 会运行: emergentlyReadTopologyInstance emergentlyReadTopologyInstanceReplicas这两步实际是去连接数据库实例(主库, 和其所有的从库), 获取实例的各项信息(如从库的复制延迟, IO_THREAD状态等) 下面分别展示了上述两个函数的注释:12345678// Force a re-read of a topology instance; this is done because we need to substantiate a suspicion// that we may have a failover scenario. we want to speed up reading the complete picture.强制重新读取一个拓扑实例；这样做是因为我们需要证实一个怀疑，即我们可能有一个故障转移的情况。我们希望加快读取完整的图片。// Force reading of replicas of given instance. This is because we suspect the instance is dead, and want to speed up// detection of replication failure from its replicas.强制读取给定实例的副本。这是因为我们怀疑该实例已经死亡，并希望加快从其副本中检测复制失败。从这是可以看出, UnreachableMaster时, orc会立即触发对主从的探测, 目的是加速整个Failover速度, 而不依赖与周期性持续探测 接着, executeCheckAndRecoverFunction运行checkAndRecoverFunction(即checkAndRecoverGenericProblem)12recoveryAttempted, topologyRecovery, err = checkAndRecoverFunction(analysisEntry, candidateInstanceKey, forceInstanceRecovery, skipProcesses)// 实参为 analysisEntry, nil, false, false所以这里是recoveryAttempted, topologyRecovery, err就是false, nil .然后代码判断recoveryAttempted为false时, 直接return了123if !recoveryAttempted &#123; return recoveryAttempted, topologyRecovery, err &#125;于是至此流程就是 那么问题来了, 只要GetReplicationAnalysis分析后一直认为故障处于UnreachableMaster状态, 就会一直处于这个循环. 所以要看一下DeadMaster的判断逻辑是什么.在官方文档中是这样定义的: 主库访问失败 所有主库的副本复制失败 这里列出分别列出 UnreachableMaster 和 DeadMaster 代码判断逻辑123456789101112131415&#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; a.CountValidReplicas == a.CountReplicas &amp;&amp; a.CountValidReplicatingReplicas == 0 &#123; a.Analysis = DeadMaster a.Description = &quot;Master cannot be reached by orchestrator and none of its replicas is replicating&quot; //&#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; !a.LastCheckPartialSuccess &amp;&amp; a.CountValidReplicas &gt; 0 &amp;&amp; a.CountValidReplicatingReplicas &gt; 0 &#123; // partial success is here to reduce noise a.Analysis = UnreachableMaster a.Description = &quot;Master cannot be reached by orchestrator but it has replicating replicas; possibly a network/host issue&quot; // &#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; a.LastCheckPartialSuccess &amp;&amp; a.CountReplicasFailingToConnectToMaster &gt; 0 &amp;&amp; a.CountValidReplicas &gt; 0 &amp;&amp; a.CountValidReplicatingReplicas &gt; 0 &#123; // there&#x27;s partial success, but also at least one replica is failing to connect to master a.Analysis = UnreachableMaster a.Description = &quot;Master cannot be reached by orchestrator but it has replicating replicas; possibly a network/host issue&quot; // database_instance重要列含义解读首先, 上面的a.IsMaster, a.LastCheckValid等属性是GetReplicationAnalysis通过SQL查询backend db的database_instance等表获取的. 所以要说清楚这些判断条件, 就要理解SQL含义, 要理解SQL含义, 就又要先了解database_instance表中几个列的含义: last_checked 无论被探测的实例是否可以连接, 都会更新此列值为Now() last_seen 只有实例探测正常, 才会更新此列值为Now() last_check_partial_success 如果被探测实例至少可以连接并执行select @@global.hostname.. 则此列值为1 last_attempted_check 这个列含义比较复杂. 简单来说, 如果 last_attempted_check &lt;= last_checked 那么这目标实例是正常的, 没有遇到连接hang住的问题 ReplicationAnalysis 属性含义解读IsMaster 主库(本身没有主库, 也不是MGR成员)1234567891011121314/* To be considered a master, traditional async replication must not be present/valid AND the host should either *//* not be a replication group member OR be the primary of the replication group */MIN(master_instance.last_check_partial_success) as last_check_partial_success,MIN( ( master_instance.master_host IN (&#x27;&#x27;, &#x27;_&#x27;) OR master_instance.master_port = 0 OR substr(master_instance.master_host, 1, 2) = &#x27;//&#x27; ) AND ( master_instance.replication_group_name = &#x27;&#x27; OR master_instance.replication_group_member_role = &#x27;PRIMARY&#x27; )) AS is_master, substr(master_instance.master_host, 1, 2) = ‘//‘的含义详见DetachLostReplicasAfterMasterFailover LastCheckValid 本实例最近一次探测正常1234567891011121314151617181920212223 MIN( master_instance.last_checked &lt;= master_instance.last_seen and master_instance.last_attempted_check &lt;= master_instance.last_seen + interval ? second ) = 1 AS is_last_check_validinterval ? 的实际值是// ValidSecondsFromSeenToLastAttemptedCheck returns the maximum allowed elapsed time// between last_attempted_check to last_checked before we consider the instance as invalid. func ValidSecondsFromSeenToLastAttemptedCheck() uint &#123; return config.Config.InstancePollSeconds + config.Config.ReasonableInstanceCheckSeconds &#125;5 + 1 = 6smaster_instance.last_checked &lt;= master_instance.last_seen 表示主库探测一切正常, 否则表是探测是无法联机数据库或查询出现错误等master_instance.last_attempted_check &lt;= master_instance.last_seen + 6s 假设 last_attempted_check = 10:10 last_seen = 10:00那么这种情况表示主库探测有问题, 可能连接hang住了两个都为true, true and true就是 1. 然后再和1比较. LastCheckPartialSuccess 本实例至少可以连接并执行select @@global.hostname..1MIN(master_instance.last_check_partial_success) as last_check_partial_success, CountReplicas 本实例的从库数量(无论死活)1COUNT(replica_instance.server_id) AS count_replicas, CountValidReplicas 本实例正常的从库数量(只表示实例正常, 能连接能查询, 但不一定复制正常)123456IFNULL( SUM( replica_instance.last_checked &lt;= replica_instance.last_seen ), 0) AS count_valid_replicas, CountValidReplicatingReplicas 本实例正常且复制状态正常的从库数量12345678IFNULL( SUM( replica_instance.last_checked &lt;= replica_instance.last_seen AND replica_instance.slave_io_running != 0 AND replica_instance.slave_sql_running != 0 ), 0) AS count_valid_replicating_replicas, CountReplicasFailingToConnectToMaster 本实例的, 自身正常(可连接可查询), IO线程处于连接异常, SQL线程正常的从库数量12345678910IFNULL( SUM( replica_instance.last_checked &lt;= replica_instance.last_seen AND replica_instance.slave_io_running = 0 AND replica_instance.last_io_error like &#x27;%%error %%connecting to master%%&#x27; AND replica_instance.slave_sql_running = 1 ), 0) AS count_replicas_failing_to_connect_to_master, 再看 UnreachableMaster 和 DeadMaster123456789101112131415161718192021&#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; a.CountValidReplicas == a.CountReplicas &amp;&amp; a.CountValidReplicatingReplicas == 0 &#123; a.Analysis = DeadMaster a.Description = &quot;Master cannot be reached by orchestrator and none of its replicas is replicating&quot; //本实例是主库(本身没有主库, 也不是MGR成员) &amp;&amp; 本实例最近一次探测异常 &amp;&amp; 本实例正常的从库数量(只表示实例正常, 能连接能查询, 但不一定复制正常) == 本实例的从库数量(无论死活) &amp;&amp; 本实例正常且复制状态正常的从库数量为0&#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; !a.LastCheckPartialSuccess &amp;&amp; a.CountValidReplicas &gt; 0 &amp;&amp; a.CountValidReplicatingReplicas &gt; 0 &#123; // partial success is here to reduce noise a.Analysis = UnreachableMaster a.Description = &quot;Master cannot be reached by orchestrator but it has replicating replicas; possibly a network/host issue&quot; // 本实例是主库(本身没有主库, 也不是MGR成员) &amp;&amp; 本实例最近一次探测异常 &amp;&amp; 本实例无法连接 &amp;&amp; 本实例正常的从库数量(只表示实例正常, 能连接能查询, 但不一定复制正常) &gt; 0 &amp;&amp; 本实例正常且复制状态正常的从库数量 &gt; 0&#125; else if a.IsMaster &amp;&amp; !a.LastCheckValid &amp;&amp; a.LastCheckPartialSuccess &amp;&amp; a.CountReplicasFailingToConnectToMaster &gt; 0 &amp;&amp; a.CountValidReplicas &gt; 0 &amp;&amp; a.CountValidReplicatingReplicas &gt; 0 &#123; // there&#x27;s partial success, but also at least one replica is failing to connect to master a.Analysis = UnreachableMaster a.Description = &quot;Master cannot be reached by orchestrator but it has replicating replicas; possibly a network/host issue&quot; //本实例是主库(本身没有主库, 也不是MGR成员) &amp;&amp; 本实例最近一次探测异常 &amp;&amp; 本实例无法连接 &amp;&amp; 本实例的, 自身正常(可连接可查询), IO线程处于连接异常, SQL线程正常的从库数量 &gt; 0 &amp;&amp; 本实例正常且复制状态正常的从库数量 &gt; 0 由此可以看出, Master宕机后的最初一段时间内, orchestrator已经无法连接Master, 但是并非所有从库都意识到主库已经宕机, IO线程可能还是处于RUNNING状态(这与slave_net_timeout有关, Orchestrator官方文档也有描述) . 所以在这段时间内, orchestrator认为Master处于UnreachableMaster 状态, 通过getCheckAndRecoverFunction获取的就永远是checkAndRecoverGenericProblem(也就是啥都不干, 直接return) 直到所有从库复制状态都出现异常, orchestrator才会认为Master处于DeadMaster状态. 那么此后getCheckAndRecoverFunction会返回checkAndRecoverDeadMaster, 而这才是Failover的真正开始 初步流程总结","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Orchestrator","slug":"Orchestrator","permalink":"http://fuxkdb.com/tags/Orchestrator/"}]},{"title":"Orchestrator Discover源码分析","slug":"2022-04-26-Orchestrator-Discover源码分析","date":"2022-04-26T15:04:00.000Z","updated":"2022-05-14T13:05:42.985Z","comments":true,"path":"2022/04/26/2022-04-26-Orchestrator-Discover源码分析/","link":"","permalink":"http://fuxkdb.com/2022/04/26/2022-04-26-Orchestrator-Discover%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"Orchestrator Discover源码分析在梳理HostnameResolveMethod和 MySQLHostnameResolveMethod 两个参数时我产生了一些迷惑. 所以深入看了下orchestrator源码, 再此记录下. orchestrator-client阅读orchestrator-client可以发现, 我们可以通过运行以下命令做”服务发现” 123orchestrator-client -c discover -i 172.16.120.10:3306假设 172.16.120.10 主机名为 centos-1 orchestrator-client是一个脚本, 用方便的命令行界面包装API调用. 它可以自动确定orchestrator集群的leader, 并在这种情况下将所有请求转发给leader. 它非常接近于orchestrator command line interface. orchestrator-client -help 有bug, 已提交PR. orchestrator-client help信息也没有介绍-i参数 查看orchestrator-client源码 123456while getopts &quot;c:i:d:s:a:D:U:o:r:u:R:t:l:H:P:q:b:e:n:S:h&quot; OPTIONdo case $OPTION in h) command=&quot;help&quot; ;; c) command=&quot;$OPTARG&quot; ;; i) instance=&quot;$OPTARG&quot; ;; 可以看出-i 的值给了instance变量. 在main行数中会先处理instance 123456789function main &#123; check_requirements detect_leader_api instance_hostport=$(to_hostport $instance) destination_hostport=$(to_hostport $destination) run_command&#125;","text":"Orchestrator Discover源码分析在梳理HostnameResolveMethod和 MySQLHostnameResolveMethod 两个参数时我产生了一些迷惑. 所以深入看了下orchestrator源码, 再此记录下. orchestrator-client阅读orchestrator-client可以发现, 我们可以通过运行以下命令做”服务发现” 123orchestrator-client -c discover -i 172.16.120.10:3306假设 172.16.120.10 主机名为 centos-1 orchestrator-client是一个脚本, 用方便的命令行界面包装API调用. 它可以自动确定orchestrator集群的leader, 并在这种情况下将所有请求转发给leader. 它非常接近于orchestrator command line interface. orchestrator-client -help 有bug, 已提交PR. orchestrator-client help信息也没有介绍-i参数 查看orchestrator-client源码 123456while getopts &quot;c:i:d:s:a:D:U:o:r:u:R:t:l:H:P:q:b:e:n:S:h&quot; OPTIONdo case $OPTION in h) command=&quot;help&quot; ;; c) command=&quot;$OPTARG&quot; ;; i) instance=&quot;$OPTARG&quot; ;; 可以看出-i 的值给了instance变量. 在main行数中会先处理instance 123456789function main &#123; check_requirements detect_leader_api instance_hostport=$(to_hostport $instance) destination_hostport=$(to_hostport $destination) run_command&#125; to_hostport是一个函数. 可以看出 -i 可接受hostname:port / ip:port / hostname / ip 1234567891011121314151617# to_hostport transforms:# - fqdn:port =&gt; fqdn/port# - fqdn =&gt; fqdn/default_portfunction to_hostport &#123; instance_key=&quot;$1&quot; if [ -z &quot;$instance_key&quot; ] ; then # 如果 instance_key 为空 echo &quot;&quot; return fi if [[ $instance_key == *&quot;:&quot;* ]]; then # 如果-i中包含 &#x27;:&#x27; echo $instance_key | tr &#x27;:&#x27; &#x27;/&#x27; # 将引号替换为 &#x27;/&#x27; else # 否则, 认为只指定了 hostname/ip, 那么端口用default_port. default_port在脚本开头定义了, 就是3306 echo &quot;$instance_key/$default_port&quot; fi&#125; run_command会实际根据命令行传参, 调用对应的函数 123456789function run_command &#123; if [ -z &quot;$command&quot; ] ; then fail &quot;No command given. Use $myname -c &lt;command&gt; [...] or $myname --command &lt;command&gt; [...] to do something useful&quot; fi command=$(echo $command | universal_sed -e &#x27;s/slave/replica/&#x27;) case $command in &quot;help&quot;) prompt_help ;; # Show available commands... &quot;discover&quot;) discover ;; # Lookup an instance, investigate it discover函数 12345function discover &#123; assert_nonempty &quot;instance&quot; &quot;$instance_hostport&quot; api &quot;discover/$instance_hostport&quot; print_details | filter_key | print_key&#125; 这里api其实也是个函数, 就不展开看了. 其本质最终是执行curl命令 1curl $&#123;curl_auth_params&#125; -s &quot;/api/discover/$instance_hostport&quot; | jq &#x27;.&#x27; 其实就是调用http接口 orchestrator discover接口^99dde2 在 go/http/api.go 中定义了orchestrator提供的接口. 其中, 可以找到discover对应的路由信息 1this.registerAPIRequest(m, &quot;discover/:host/:port&quot;, this.Discover) 那接下来重点, 就是看Discover方法了 Discover 以下内中↓↓↓ 表示下钻进对应函数中 这里一点一点看 12345678910111213141516171819202122232425// Discover issues a synchronous read on an instancefunc (this *HttpAPI) Discover(params martini.Params, r render.Render, req *http.Request, user auth.User) &#123; if !isAuthorizedForAction(req, user) &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: &quot;Unauthorized&quot;&#125;) return &#125; instanceKey, err := this.getInstanceKey(params[&quot;host&quot;], params[&quot;port&quot;]) if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; instance, err := inst.ReadTopologyInstance(&amp;instanceKey) if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; if orcraft.IsRaftEnabled() &#123; orcraft.PublishCommand(&quot;discover&quot;, instanceKey) &#125; else &#123; logic.DiscoverInstance(instanceKey) &#125; Respond(r, &amp;APIResponse&#123;Code: OK, Message: fmt.Sprintf(&quot;Instance discovered: %+v&quot;, instance.Key), Details: instance&#125;)&#125; 看第7行 -&gt;getInstanceKey12345678instanceKey, err := this.getInstanceKey(params[&quot;host&quot;], params[&quot;port&quot;]) ↓↓↓ getInstanceKeyfunc (this *HttpAPI) getInstanceKey(host string, port string) (inst.InstanceKey, error) &#123; // host, port就是 orchestrator-client -c discover -i 172.16.120.10:3306 中的 -i return this.getInstanceKeyInternal(host, port, true)&#125; getInstanceKeyInternal123456789101112131415161718192021func (this *HttpAPI) getInstanceKeyInternal(host string, port string, resolve bool) (inst.InstanceKey, error) &#123; var instanceKey *inst.InstanceKey var err error if resolve &#123; // getInstanceKey传的resolve是true // 所以走这里 instanceKey, err = inst.NewResolveInstanceKeyStrings(host, port) &#125; else &#123; instanceKey, err = inst.NewRawInstanceKeyStrings(host, port) &#125; if err != nil &#123; return emptyInstanceKey, err &#125; instanceKey, err = inst.FigureInstanceKey(instanceKey, nil) if err != nil &#123; return emptyInstanceKey, err &#125; if instanceKey == nil &#123; return emptyInstanceKey, fmt.Errorf(&quot;Unexpected nil instanceKey in getInstanceKeyInternal(%+v, %+v, %+v)&quot;, host, port, resolve) &#125; return *instanceKey, nil&#125; -&gt;NewResolveInstanceKeyStrings12345678910111213141516171819202122232425262728293031323334353637383940414243// NewResolveInstanceKeyStrings creates and resolves a new instance key based on string paramsfunc NewResolveInstanceKeyStrings(hostname string, port string) (*InstanceKey, error) &#123; return newInstanceKeyStrings(hostname, port, true)&#125; ↓↓↓ newInstanceKeyStrings// newInstanceKeyStringsfunc newInstanceKeyStrings(hostname string, port string, resolve bool) (*InstanceKey, error) &#123; if portInt, err := strconv.Atoi(port); err != nil &#123; // 将port字符串转int return nil, fmt.Errorf(&quot;Invalid port: %s&quot;, port) &#125; else &#123; // 转换成功, 走这里 return newInstanceKey(hostname, portInt, resolve) &#125;&#125; ↓↓↓ newInstanceKeyfunc newInstanceKey(hostname string, port int, resolve bool) (instanceKey *InstanceKey, err error) &#123; if hostname == &quot;&quot; &#123; return instanceKey, fmt.Errorf(&quot;NewResolveInstanceKey: Empty hostname&quot;) &#125; instanceKey = &amp;InstanceKey&#123;Hostname: hostname, Port: port&#125; // 构造一个instanceKey if resolve &#123; // 传的是true, 所以走这里 instanceKey, err = instanceKey.ResolveHostname() &#125; return instanceKey, err&#125; ↓↓↓ instanceKey.ResolveHostnamefunc (this *InstanceKey) ResolveHostname() (*InstanceKey, error) &#123; if !this.IsValid() &#123; // 进行一些校验, 需满足: Hostname!=&quot;_&quot; Hostname不以&quot;//&quot;开头, Hostname长度&gt;0, port&gt;0 return this, nil &#125; hostname, err := ResolveHostname(this.Hostname) // 解析主机名, 解析后重新赋值给instanceKey.Hostname if err == nil &#123; this.Hostname = hostname &#125; return this, err&#125; ####### ResolveHostname这个函数代码较多, 主要是这段 1234567891011121314151617resolvedHostname, err := resolveHostname(hostname) ↓↓↓ resolveHostnamefunc resolveHostname(hostname string) (string, error) &#123; switch strings.ToLower(config.Config.HostnameResolveMethod) &#123; case &quot;none&quot;: return hostname, nil case &quot;default&quot;: return hostname, nil case &quot;cname&quot;: return GetCNAME(hostname) case &quot;ip&quot;: return getHostnameIP(hostname) &#125; return hostname, nil&#125; 这里就是根据 HostnameResolveMethod参数进行解析了 none, default: 什么也不处理, 直接原样返回 cname: 取CNAME ip: 根据hostname取ip getHostnameIP 实际会调用 net.LookupIP LookupIP looks up host using the local resolver. It returns a slice of that host’s IPv4 and IPv6 addresses. 简单看一下net.LookupIP 1234567891011121314151617 // /etc/host // 172.16.201.206 namenode-1 // 172.16.201.220 namenode-2 fmt.Println(net.LookupIP(&quot;namenode-1&quot;)) fmt.Println(net.LookupIP(&quot;namenode-8&quot;)) // ping 172.16.120.18 // PING 172.16.120.18 (172.16.120.18): 56 data bytes // Request timeout for icmp_seq 0 fmt.Println(net.LookupIP(&quot;172.16.120.18&quot;)) // 不存在的一个ip, 也ping不通 fmt.Println(net.LookupIP(&quot;192.168.124.130&quot;))output=&gt;[172.16.201.206] &lt;nil&gt;[] lookup namenode-8: no such host[172.16.120.18] &lt;nil&gt;[192.168.124.130] &lt;nil&gt; 所以说, getHostnameIP, 你给他传ip, 返回的还是ip. 给他传主机名, 就要看你有没有配置解析了 &lt;-NewResolveInstanceKeyStrings所以NewResolveInstanceKeyStrings就是根据HostnameResolveMethod将传进来的host 解析了一下. 以HostnameResolveMethod默认值default为例, 就是原样返回. instanceKey.Hostname就是-i host:port 中的host. 这个host可以是ip也可以是主机名 123456789101112131415161718192021222324252627func (this *HttpAPI) getInstanceKeyInternal(host string, port string, resolve bool) (inst.InstanceKey, error) &#123; var instanceKey *inst.InstanceKey var err error if resolve &#123; // getInstanceKey传的resolve是true // 所以走这里 instanceKey, err = inst.NewResolveInstanceKeyStrings(host, port) &#125; else &#123; instanceKey, err = inst.NewRawInstanceKeyStrings(host, port) &#125; if err != nil &#123; return emptyInstanceKey, err &#125; // 从这里继续 // FigureInstanceKey是尝试去backend db模糊匹配 &#x27;%host%&#x27;, port, 如果取到了, 就从backend db把这个instanceKey取出来(还包含ServerID, ServerUUID, Binlog等信息) // 如果instanceKey.Hostname是 ip, 就不模糊匹配了, 直接原样返回 // 如果模糊匹配到多行, 也是原因返回, 只有模糊匹配到1行才从数据库取instanceKey信息返回 instanceKey instanceKey, err = inst.FigureInstanceKey(instanceKey, nil) if err != nil &#123; return emptyInstanceKey, err &#125; if instanceKey == nil &#123; return emptyInstanceKey, fmt.Errorf(&quot;Unexpected nil instanceKey in getInstanceKeyInternal(%+v, %+v, %+v)&quot;, host, port, resolve) &#125; return *instanceKey, nil&#125; &lt;-getInstanceKey继续往下读Discover 123456789101112131415161718192021222324252627// Discover issues a synchronous read on an instancefunc (this *HttpAPI) Discover(params martini.Params, r render.Render, req *http.Request, user auth.User) &#123; if !isAuthorizedForAction(req, user) &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: &quot;Unauthorized&quot;&#125;) return &#125; instanceKey, err := this.getInstanceKey(params[&quot;host&quot;], params[&quot;port&quot;]) // 从这继续, 上面简单来说就是解析了下 host if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; instance, err := inst.ReadTopologyInstance(&amp;instanceKey) if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; if orcraft.IsRaftEnabled() &#123; orcraft.PublishCommand(&quot;discover&quot;, instanceKey) &#125; else &#123; logic.DiscoverInstance(instanceKey) &#125; Respond(r, &amp;APIResponse&#123;Code: OK, Message: fmt.Sprintf(&quot;Instance discovered: %+v&quot;, instance.Key), Details: instance&#125;)&#125; -&gt;ReadTopologyInstance这是重点, 开始”发现” 拓扑结构 123456789101112instance, err := inst.ReadTopologyInstance(&amp;instanceKey) ↓↓↓ inst.ReadTopologyInstance// ReadTopologyInstance collects information on the state of a MySQL// server and writes the result synchronously to the orchestrator// backend.func ReadTopologyInstance(instanceKey *InstanceKey) (*Instance, error) &#123; return ReadTopologyInstanceBufferable(instanceKey, false, nil) // 这里bufferWrites=false&#125;↓↓↓ ReadTopologyInstanceBufferable -&gt; ReadTopologyInstanceBufferable就是读拓扑节点, 读出这个节点的信息, 和他有哪些从库以及他的主库. 然后将一些信息写入backend db, 还会缓存一些信息到”buffer”, 下次读可以直接从”buffer”读 这个函数很长, 这里只摘部分代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211// ReadTopologyInstanceBufferable connects to a topology MySQL instance// and collects information on the server and its replication state.// It writes the information retrieved into orchestrator&#x27;s backend.// - writes are optionally buffered.// - timing information can be collected for the stages performed.func ReadTopologyInstanceBufferable(instanceKey *InstanceKey, bufferWrites bool, latency *stopwatch.NamedStopwatch) (inst *Instance, err error) &#123; // bufferWrites传进来的是false // time.AfterFunc函数作用 // AfterFunc waits for the duration to elapse and then calls f // in its own goroutine. It returns a Timer that can // be used to cancel the call using its Stop method. // 所以这里就是1秒后调用UpdateInstanceLastAttemptedCheck, 这个函数更新指定instanceKey database_instance表中的last_attempted_check字段为当前时间 lastAttemptedCheckTimer := time.AfterFunc(time.Second, func() &#123; go UpdateInstanceLastAttemptedCheck(instanceKey) &#125;)... 省略部分不重要代码 db, err := db.OpenDiscovery(instanceKey.Hostname, instanceKey.Port) // 连到指定的实例 if err != nil &#123; goto Cleanup // 注意这里很重要, 如果连接数据库报错, 会走goto &#125; instance.Key = *instanceKey... 省略部分不重要代码 var mysqlHostname, mysqlReportHost string err = db.QueryRow(&quot;select @@global.hostname, ifnull(@@global.report_host, &#x27;&#x27;), @@global.server_id, @@global.version, @@global.version_comment, @@global.read_only, @@global.binlog_format, @@global.log_bin, @@global.log_slave_updates&quot;).Scan( &amp;mysqlHostname, &amp;mysqlReportHost, &amp;instance.ServerID, &amp;instance.Version, &amp;instance.VersionComment, &amp;instance.ReadOnly, &amp;instance.Binlog_format, &amp;instance.LogBinEnabled, &amp;instance.LogReplicationUpdatesEnabled) if err != nil &#123; goto Cleanup &#125; partialSuccess = true // We at least managed to read something from the server. // 上面通过 select @@global.hostname 取到了主机名 // 根据MySQLHostnameResolveMethod 来取resolvedHostname switch strings.ToLower(config.Config.MySQLHostnameResolveMethod) &#123; case &quot;none&quot;: resolvedHostname = instance.Key.Hostname case &quot;default&quot;, &quot;hostname&quot;, &quot;@@hostname&quot;: // MySQLHostnameResolveMethod 默认值 就是 @@hostname. 以此为例 resolvedHostname就是select @@global.hostname取到的主机名 resolvedHostname = mysqlHostname case &quot;report_host&quot;, &quot;@@report_host&quot;: if mysqlReportHost == &quot;&quot; &#123; err = fmt.Errorf(&quot;MySQLHostnameResolveMethod configured to use @@report_host but %+v has NULL/empty @@report_host&quot;, instanceKey) goto Cleanup &#125; resolvedHostname = mysqlReportHost default: resolvedHostname = instance.Key.Hostname &#125;... 省略部分不重要代码 // 以我们的例子 resolvedHostname是主机名: centos-1 // instance.Key.Hostname 是 172.16.120.10 if resolvedHostname != instance.Key.Hostname &#123; latency.Start(&quot;backend&quot;) UpdateResolvedHostname(instance.Key.Hostname, resolvedHostname) // UpdateResolvedHostname 将 hostname, resolved_hostname 记录到backend db // _, err := db.ExecOrchestrator(` // insert into // hostname_resolve (hostname, resolved_hostname, resolved_timestamp) // values // (?, ?, NOW()) // on duplicate key update // resolved_hostname = VALUES(resolved_hostname), // resolved_timestamp = VALUES(resolved_timestamp) // `, // hostname, // resolvedHostname) latency.Stop(&quot;backend&quot;) instance.Key.Hostname = resolvedHostname // 并将instance.Key.Hostname 从172.16.120.10 改成了 centos-1 &#125; if instance.Key.Hostname == &quot;&quot; &#123; err = fmt.Errorf(&quot;ReadTopologyInstance: empty hostname (%+v). Bailing out&quot;, *instanceKey) goto Cleanup &#125; go ResolveHostnameIPs(instance.Key.Hostname) // 这里有尝试取ip, 这里以我们的例子instance.Key.Hostname已经是centos-1了. 如果orchestrator所在服务器没配置主机名解析(如/etc/hosts), 那么是取不到的 // 如果能取到, 会往 insert into hostname_ips (hostname, ipv4, ipv6, last_updated) ... ODKU ...... 省略部分不重要代码// 下面这段就是取了 172.16.120.10:3306 的master主库// 注意这里通过show slave status 查主库. show slave status多源复制是可能返回多行的. orchestrator官方文档说了, 不支持多源复制, 所以这里instance.MasterKey只是一个结构体, 不是切片 err = sqlutils.QueryRowsMap(db, &quot;show slave status&quot;, func(m sqlutils.RowMap) error &#123; ... masterHostname := m.GetString(&quot;Master_Host&quot;) if isMaxScale110 &#123; // Buggy buggy maxscale 1.1.0. Reported Master_Host can be corrupted. // Therefore we (currently) take @@hostname (which is masquarading as master host anyhow) masterHostname = maxScaleMasterHostname &#125; masterKey, err := NewResolveInstanceKey(masterHostname, m.GetInt(&quot;Master_Port&quot;)) if err != nil &#123; logReadTopologyInstanceError(instanceKey, &quot;NewResolveInstanceKey&quot;, err) &#125; masterKey.Hostname, resolveErr = ResolveHostname(masterKey.Hostname) if resolveErr != nil &#123; logReadTopologyInstanceError(instanceKey, fmt.Sprintf(&quot;ResolveHostname(%q)&quot;, masterKey.Hostname), resolveErr) &#125; instance.MasterKey = *masterKey... 省略部分不重要代码 instanceFound = true // ------------------------------------------------------------------------- // Anything after this point does not affect the fact the instance is found. // No `goto Cleanup` after this point. // ------------------------------------------------------------------------- // Get replicas, either by SHOW SLAVE HOSTS or via PROCESSLIST // MaxScale does not support PROCESSLIST, so SHOW SLAVE HOSTS is the only option // 下面这一大段代码, 就是取 172.16.120.10:3306的从库, 如果有从库, 就放到 instance.Replicas里 // Get replicas, either by SHOW SLAVE HOSTS or via PROCESSLIST // MaxScale does not support PROCESSLIST, so SHOW SLAVE HOSTS is the only option if config.Config.DiscoverByShowSlaveHosts || isMaxScale &#123; err := sqlutils.QueryRowsMap(db, `show slave hosts`, func(m sqlutils.RowMap) error &#123; // MaxScale 1.1 may trigger an error with this command, but // also we may see issues if anything on the MySQL server locks up. // Consequently it&#x27;s important to validate the values received look // good prior to calling ResolveHostname() host := m.GetString(&quot;Host&quot;) port := m.GetIntD(&quot;Port&quot;, 0) if host == &quot;&quot; || port == 0 &#123; if isMaxScale &amp;&amp; host == &quot;&quot; &amp;&amp; port == 0 &#123; // MaxScale reports a bad response sometimes so ignore it. // - seen in 1.1.0 and 1.4.3.4 return nil &#125; // otherwise report the error to the caller return fmt.Errorf(&quot;ReadTopologyInstance(%+v) &#x27;show slave hosts&#x27; returned row with &lt;host,port&gt;: &lt;%v,%v&gt;&quot;, instanceKey, host, port) &#125; replicaKey, err := NewResolveInstanceKey(host, port) if err == nil &amp;&amp; replicaKey.IsValid() &#123; if !FiltersMatchInstanceKey(replicaKey, config.Config.DiscoveryIgnoreReplicaHostnameFilters) &#123; instance.AddReplicaKey(replicaKey) &#125; foundByShowSlaveHosts = true &#125; return err &#125;) logReadTopologyInstanceError(instanceKey, &quot;show slave hosts&quot;, err) &#125; if !foundByShowSlaveHosts &amp;&amp; !isMaxScale &#123; // Either not configured to read SHOW SLAVE HOSTS or nothing was there. // Discover by information_schema.processlist waitGroup.Add(1) go func() &#123; defer waitGroup.Done() err := sqlutils.QueryRowsMap(db, ` select substring_index(host, &#x27;:&#x27;, 1) as slave_hostname from information_schema.processlist where command IN (&#x27;Binlog Dump&#x27;, &#x27;Binlog Dump GTID&#x27;) `, func(m sqlutils.RowMap) error &#123; cname, resolveErr := ResolveHostname(m.GetString(&quot;slave_hostname&quot;)) if resolveErr != nil &#123; logReadTopologyInstanceError(instanceKey, &quot;ResolveHostname: processlist&quot;, resolveErr) &#125; replicaKey := InstanceKey&#123;Hostname: cname, Port: instance.Key.Port&#125; if !FiltersMatchInstanceKey(&amp;replicaKey, config.Config.DiscoveryIgnoreReplicaHostnameFilters) &#123; instance.AddReplicaKey(&amp;replicaKey) &#125; return err &#125;) logReadTopologyInstanceError(instanceKey, &quot;processlist&quot;, err) &#125;() &#125;... 省略部分不重要代码 Cleanup: // 注意Cleanup的位置 是在 设置 instanceFound = true 之后, 所以一但 db, err := db.OpenDiscovery(instanceKey.Hostname, instanceKey.Port)报错, 就跳转到这里, 于是instanceFound还是false xxxx ... 省略部分不重要代码 if instanceFound &#123; instance.LastDiscoveryLatency = time.Since(readingStartTime) instance.IsLastCheckValid = true instance.IsRecentlyChecked = true instance.IsUpToDate = true latency.Start(&quot;backend&quot;) if bufferWrites &#123; // bufferWrites传进来的是false enqueueInstanceWrite(instance, instanceFound, err) &#125; else &#123; WriteInstance(instance, instanceFound, err) // 这里最终是把获取的instance各种信息写入 database_instance表, 以及很重要的, 更新 last_check=Now() last_seen=Now() last_attempted_check=Now() last_check_parital_success=1 &#125; lastAttemptedCheckTimer.Stop() latency.Stop(&quot;backend&quot;) return instance, nil &#125; // Something is wrong, could be network-wise. Record that we // tried to check the instance. last_attempted_check is also // updated on success by writeInstance. latency.Start(&quot;backend&quot;) _ = UpdateInstanceLastChecked(&amp;instance.Key, partialSuccess) // 连接数据库失败, 或执行一些查询如show slave status失败, 会Goto到Cleanup, 此时instanceFound仍然是false // 于是就会执行UpdateInstanceLastChecked. 这函数是更新指定instance.Key的 last_checked = NOW(), last_check_partial_success = 0/1(如过在err = db.QueryRow(&quot;select @@global.hostname...执行前Goto Cleanup, 则partialSuccess=False , last_check_partial_success=0, 否则partialSuccess是True, last_check_partial_success=1) // last_check_partial_success 表示, 至少我们连接到数据库, 而且能执行select @@global.hostname... 查询 // 由此可以看出 latency.Stop(&quot;backend&quot;) return nil, err &lt;- ReadTopologyInstance继续往下读Discover 1234567891011121314151617181920212223242526272829// Discover issues a synchronous read on an instancefunc (this *HttpAPI) Discover(params martini.Params, r render.Render, req *http.Request, user auth.User) &#123; if !isAuthorizedForAction(req, user) &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: &quot;Unauthorized&quot;&#125;) return &#125; instanceKey, err := this.getInstanceKey(params[&quot;host&quot;], params[&quot;port&quot;]) if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; instance, err := inst.ReadTopologyInstance(&amp;instanceKey) // 从这继续, 上面简单来说就是解析了下 host if err != nil &#123; Respond(r, &amp;APIResponse&#123;Code: ERROR, Message: err.Error()&#125;) return &#125; if orcraft.IsRaftEnabled() &#123; // 如果orch是raft模式部署 // 这段代码我没看, 我猜测是让其他节点也去discover一遍 orcraft.PublishCommand(&quot;discover&quot;, instanceKey) &#125; else &#123; // 继续顺藤摸瓜, &quot;发现&quot; logic.DiscoverInstance(instanceKey) &#125; Respond(r, &amp;APIResponse&#123;Code: OK, Message: fmt.Sprintf(&quot;Instance discovered: %+v&quot;, instance.Key), Details: instance&#125;)&#125; 树藤摸瓜DiscoverInstance从上面可以看到, 只发现了 172.16.120.10:3306 自身, 也发现了他的从库和主库, 但只是把他的从库ip,port存入了instance.Replicas, 主库存入了instance.MasterKey, 没有再进一步探测这些从库和主库. 其实继续的”发现”工作是在DiscoverInstance 中做的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// DiscoverInstance will attempt to discover (poll) an instance (unless// it is already up to date) and will also ensure that its master and// replicas (if any) are also checked.func DiscoverInstance(instanceKey inst.InstanceKey) &#123;... 省略部分不重要代码 // Calculate the expiry period each time as InstancePollSeconds // _may_ change during the run of the process (via SIGHUP) and // it is not possible to change the cache&#x27;s default expiry.. // 这个if很重要, 这里尝试向recentDiscoveryOperationKeys这个cache里add key, key就是instanceKey.DisplayString() // add前会先查这个cache里有没有这个key, 如果有, 会返回err // 如果没查到这个key, 就会添加到cache, 并且设置过期时间为Config.InstancePollSeconds默认5秒, 所以从这里控制每个instance的发现间隔为5秒 if existsInCacheError := recentDiscoveryOperationKeys.Add(instanceKey.DisplayString(), true, instancePollSecondsDuration()); existsInCacheError != nil &#123; // Just recently attempted return &#125;... 省略部分不重要代码 // 从库里再把`172.16.120.10:3306` 信息取出来 instance, found, err := inst.ReadInstance(&amp;instanceKey)... 省略部分不重要代码 // First we&#x27;ve ever heard of this instance. Continue investigation: instance, err = inst.ReadTopologyInstanceBufferable(&amp;instanceKey, config.Config.BufferInstanceWrites, latency)... 省略部分不重要代码 // Investigate replicas and members of the same replication group: // 把从库取出来了 for _, replicaKey := range append(instance.ReplicationGroupMembers.GetInstanceKeys(), instance.Replicas.GetInstanceKeys()...) &#123; replicaKey := replicaKey // not needed? no concurrency here? // Avoid noticing some hosts we would otherwise discover if inst.FiltersMatchInstanceKey(&amp;replicaKey, config.Config.DiscoveryIgnoreReplicaHostnameFilters) &#123; continue &#125; if replicaKey.IsValid() &#123; // 放到一个discoveryQueue发现队列里了 discoveryQueue.Push(replicaKey) // Push时, 会记录这个key是什么时间push进去的 &#125; &#125; // Investigate master: // 把主库也取出来了 if instance.MasterKey.IsValid() &#123; if !inst.FiltersMatchInstanceKey(&amp;instance.MasterKey, config.Config.DiscoveryIgnoreMasterHostnameFilters) &#123; // 主库也放到一个discoveryQueue发现队列里了 discoveryQueue.Push(instance.MasterKey) &#125; &#125; discoveryQueue全局搜discoveryQueue, 肯定有人消费这个队列 果然搜到 instanceKey := discoveryQueue.Consume() 12345678910111213141516171819202122232425// handleDiscoveryRequests iterates the discoveryQueue channel and calls upon// instance discovery per entry.func handleDiscoveryRequests() &#123; discoveryQueue = discovery.CreateOrReturnQueue(&quot;DEFAULT&quot;) // create a pool of discovery workers for i := uint(0); i &lt; config.Config.DiscoveryMaxConcurrency; i++ &#123; go func() &#123; for &#123; instanceKey := discoveryQueue.Consume() // Possibly this used to be the elected node, but has // been demoted, while still the queue is full. if !IsLeaderOrActive() &#123; log.Debugf(&quot;Node apparently demoted. Skipping discovery of %+v. &quot;+ &quot;Remaining queue size: %+v&quot;, instanceKey, discoveryQueue.QueueLen()) discoveryQueue.Release(instanceKey) continue &#125; // 看这里, 继续通过DiscoverInstance发现 DiscoverInstance(instanceKey) discoveryQueue.Release(instanceKey) &#125; &#125;() &#125;&#125;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Orchestrator","slug":"Orchestrator","permalink":"http://fuxkdb.com/tags/Orchestrator/"}]},{"title":"MySQL page cleaner占用CPU较高问题","slug":"2021-10-15-MySQL page cleaner占用CPU较高问题","date":"2021-10-15T04:23:00.000Z","updated":"2021-10-17T13:01:36.109Z","comments":true,"path":"2021/10/15/2021-10-15-MySQL page cleaner占用CPU较高问题/","link":"","permalink":"http://fuxkdb.com/2021/10/15/2021-10-15-MySQL%20page%20cleaner%E5%8D%A0%E7%94%A8CPU%E8%BE%83%E9%AB%98%E9%97%AE%E9%A2%98/","excerpt":"背景说明众所周知, Seconds_Behind_Master 无法准确反应复制延迟. 为了准确的反应复制延迟, 业界的办法是, 创建一个延迟监控表, 周期性(往往是每秒)更新这个表的时间戳字段, 计算当前时间与该字段差值, 以此判断复制延迟. 典型的例子是Percona的pt-heartbeat. 另外TIDB DM也使用了相同的方法监控同步延迟. 在我们这里, 使用了主从和MGR两种架构集群, 为了更好地监控延迟, DBA开发了一个python脚本, 脚本从CMDB获取所有集群ProxySQL节点, 连接ProxySQL, 每秒更新dbms_monitor.monitor_delay表. 表结构和执行的语句为: 1234567891011121314root@localhost 13:23:42 [dbms_monitor]&gt; show create table monitor_delay\\G*************************** 1. row *************************** Table: monitor_delayCreate Table: CREATE TABLE `monitor_delay` ( `id` tinyint(3) unsigned NOT NULL, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci1 row in set (0.00 sec) 更新时间戳语句REPLACE INTO dbms_monitor.monitor_delay(id) VALUES(1) 为了保证”每秒”更新, 脚本做了超时处理, 如果某个集群执行语句(连接超时/执行语句超时等)超时, 则抛出异常, sleep 一段时间(这个一段时间+ 本次循环已用时间=1s), 进入下次循环","text":"背景说明众所周知, Seconds_Behind_Master 无法准确反应复制延迟. 为了准确的反应复制延迟, 业界的办法是, 创建一个延迟监控表, 周期性(往往是每秒)更新这个表的时间戳字段, 计算当前时间与该字段差值, 以此判断复制延迟. 典型的例子是Percona的pt-heartbeat. 另外TIDB DM也使用了相同的方法监控同步延迟. 在我们这里, 使用了主从和MGR两种架构集群, 为了更好地监控延迟, DBA开发了一个python脚本, 脚本从CMDB获取所有集群ProxySQL节点, 连接ProxySQL, 每秒更新dbms_monitor.monitor_delay表. 表结构和执行的语句为: 1234567891011121314root@localhost 13:23:42 [dbms_monitor]&gt; show create table monitor_delay\\G*************************** 1. row *************************** Table: monitor_delayCreate Table: CREATE TABLE `monitor_delay` ( `id` tinyint(3) unsigned NOT NULL, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci1 row in set (0.00 sec) 更新时间戳语句REPLACE INTO dbms_monitor.monitor_delay(id) VALUES(1) 为了保证”每秒”更新, 脚本做了超时处理, 如果某个集群执行语句(连接超时/执行语句超时等)超时, 则抛出异常, sleep 一段时间(这个一段时间+ 本次循环已用时间=1s), 进入下次循环 问题产生2021.10.14 15:45左右, DBA在rc环境进行演练, 部分集群宕机 从脚本日志也可以看出, 部分rc集群异常 随后数据库cpu趋势监控报警 查看监控, 发现很多8核心服务器cpu使用率有明显上升 这部分服务器会采用多实例部署方式, 部署非核心业务集群, 往往比较空闲 DBA在经过一系列分析定位后排查后, 发现一个奇怪的现象: 关闭延迟监控脚本CPU使用率上升 开启延迟监控脚本CPU使用率下降 问题原因定位占用CPU真凶上面的现象奇怪就奇怪在与我们的通常认知相反; 如果是开启脚本CPU使用率上升, 关闭脚本CPU使用率下降还可以理解, 这种情况8成是脚本问题, 但实时情况是完全相反的 我们登录一个问题服务器, 选择一个mysql实例, 使用如下命令找到占用CPU高的mysql线程id 123456789101112pidstat -t -p 16904 1 01:39:22 PM UID TGID TID %usr %system %guest %CPU CPU Command01:39:23 PM 3001 16904 - 100.00 0.00 0.00 100.00 6 mysqld01:39:23 PM 3001 - 16904 0.00 0.00 0.00 0.00 6 |__mysqld...01:39:23 PM 3001 - 17986 0.00 0.00 0.00 0.00 6 |__mysqld01:39:23 PM 3001 - 17987 100.00 0.00 0.00 100.00 5 |__mysqld01:39:23 PM 3001 - 17988 0.00 0.00 0.00 0.00 6 |__mysqld01:39:23 PM 3001 - 18016 0.00 0.00 0.00 0.00 0 |__mysqld... 如上所示, TID:17987 占用CPU高, 我们登录数据库查看这个线程在干什么 1234567891011121314151617181920fan@127.0.0.1 13:41:03 [(none)]&gt; select * from performance_schema.threads where THREAD_OS_ID = 17987 \\G*************************** 1. row *************************** THREAD_ID: 19 NAME: thread/innodb/page_cleaner_thread TYPE: BACKGROUND PROCESSLIST_ID: NULL PROCESSLIST_USER: NULL PROCESSLIST_HOST: NULL PROCESSLIST_DB: NULLPROCESSLIST_COMMAND: NULL PROCESSLIST_TIME: NULL PROCESSLIST_STATE: NULL PROCESSLIST_INFO: NULL PARENT_THREAD_ID: NULL ROLE: NULL INSTRUMENTED: YES HISTORY: YES CONNECTION_TYPE: NULL THREAD_OS_ID: 179871 row in set (0.00 sec) 如上所示, 这个线程是innodb后台线程page_cleaner_thread; page_cleaner_thread是用来刷脏页的 看到这里, 很自然的会认为是写操作过多, 导致频繁刷脏, 但事实是, 这些实例基本没有写入操作(通过分析binlog). 通过google 搜索 “page_cleaner High cpu usage”, 找到一篇阿里内核月报 MySQL · 引擎特性 · page cleaner 算法 这篇文章大致说的是, page cleaner的刷脏逻辑, 逻辑如下图所示 pc_sleep_if_neededpage cleaner 的循环刷脏周期是 1s，如果不足 1s 就需要 sleep，超过 1s 可能是刷脏太慢，不足 1s 可能是被其它线程唤醒的。 是否持续缓慢刷脏错误日志里有时候会看到这样的日志： Page cleaner took xx ms to flush xx and evict xx pages 这个表示上一轮刷脏进行的比较缓慢，首先 ret_sleep == OS_SYNC_TIME_EXCEEDED, 并且本轮刷脏和上一轮刷脏超过 3s，warn_interval 控制输出日志的频率，如果持续打日志，就要看看 IO 延迟了。 sync flushSync flush 不受 io_capacity/io_capacity_max 的限制，所以会对性能产生比较大的影响。 normal flush当系统有负载的时候，为了避免频繁刷脏影响用户，会计算出每次刷脏的 page 数量 idle flush系统空闲的时候不用担心刷脏影响用户线程，可以使用最大的 io_capacity 刷脏。RDS 有参数 srv_idle_flush_pct 控制刷脏比例，默认是 100%。 这里就是问题的关键: 系统空闲的时候不用担心刷脏影响用户线程，可以使用最大的 io_capacity 刷脏; 当系统有负载的时候，为了避免频繁刷脏影响用户，会计算出每次刷脏的 page 数量进行刷脏 看到这里我猜测, 当监控脚本启动时, innodb认为”系统有负载”, 采用normal flush方式刷脏页, 避免频繁刷脏影响用户; 当脚本关闭时, innodb认为系统空闲, 采用idle flush方式尽力刷脏 我在测试服务器, 使用 while true; do mysql -udbms_monitor_rw -psuperpass -h172.16.23.x -P3307 dbms_monitor -e”REPLACE INTO dbms_monitor.monitor_delay(id) VALUES(1)” ; sleep 1 ; done 模拟监控脚本每秒更新 测试服务器又N个实例, 就启动N个脚本, 保证每个实例每秒都有写入, 我这里有5个实例 3306-3310, 所以我开了5个窗口对每个实例运行上述命令 这样做也可以排除python脚本和proxysql 结果CPU使用率有明显下降 关闭脚本CPU使用率上升 我们选取非核心业务节点测试, 也产生了相同的效果, 至此基本定位了问题发生原因 如何处理?不需要处理. 在8.0版本增加了参数innodb_idle_flush_pct 来控制idle flush模式下的刷脏量","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"8.0MGR Subquery returns more than 1 row bug","slug":"2021-04-28-8.0MGR-Subquery-returns-more-than-1-row-bug","date":"2021-04-28T13:23:00.000Z","updated":"2021-04-28T13:06:05.588Z","comments":true,"path":"2021/04/28/2021-04-28-8.0MGR-Subquery-returns-more-than-1-row-bug/","link":"","permalink":"http://fuxkdb.com/2021/04/28/2021-04-28-8.0MGR-Subquery-returns-more-than-1-row-bug/","excerpt":"这是一个Percona Server8.0.22下使用MGR+ProxySQL时遇到的bug 使用mydumper备份mgr时发现ProxySQL报错了 1234567# cat proxysql.log | grep -i subquery2021-04-28 15:14:22 MySQL_HostGroups_Manager.cpp:3875:update_group_replication_set_offline(): [WARNING] Group Replication: setting host 172.16.23.224:3309 offline because: Subquery returns more than 1 row2021-04-28 15:14:22 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee3540 , MYSQL 0x7fd62b804600 , FD 39 : Subquery returns more than 1 row2021-04-28 15:14:27 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee1440 , MYSQL 0x7fd62b800000 , FD 41 : Subquery returns more than 1 row2021-04-28 15:17:37 MySQL_HostGroups_Manager.cpp:3875:update_group_replication_set_offline(): [WARNING] Group Replication: setting host 172.16.23.151:3309 offline because: Subquery returns more than 1 row2021-04-28 15:17:37 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee78c0 , MYSQL 0x7fd62bc06400 , FD 51 : Subquery returns more than 1 row2021-04-28 15:17:42 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee0300 , MYSQL 0x7fd62bc00000 , FD 40 : Subquery returns more than 1 row","text":"这是一个Percona Server8.0.22下使用MGR+ProxySQL时遇到的bug 使用mydumper备份mgr时发现ProxySQL报错了 1234567# cat proxysql.log | grep -i subquery2021-04-28 15:14:22 MySQL_HostGroups_Manager.cpp:3875:update_group_replication_set_offline(): [WARNING] Group Replication: setting host 172.16.23.224:3309 offline because: Subquery returns more than 1 row2021-04-28 15:14:22 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee3540 , MYSQL 0x7fd62b804600 , FD 39 : Subquery returns more than 1 row2021-04-28 15:14:27 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee1440 , MYSQL 0x7fd62b800000 , FD 41 : Subquery returns more than 1 row2021-04-28 15:17:37 MySQL_HostGroups_Manager.cpp:3875:update_group_replication_set_offline(): [WARNING] Group Replication: setting host 172.16.23.151:3309 offline because: Subquery returns more than 1 row2021-04-28 15:17:37 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee78c0 , MYSQL 0x7fd62bc06400 , FD 51 : Subquery returns more than 1 row2021-04-28 15:17:42 MySQL_Monitor.cpp:1472:monitor_group_replication_thread(): [ERROR] Got error. mmsd 0x7fd62cee0300 , MYSQL 0x7fd62bc00000 , FD 40 : Subquery returns more than 1 row 看到这个错误我怀疑是gr_member_routing_candidate_status视图有问题(因为之前被这个视图坑过, 详见https://github.com/sysown/proxysql/issues/3406), 并且猜测是由于备份时执行FTWRL导致的. 进行了几次模拟, 发现了复现方法 环境123456789101112131415161718192021222324[root@bj2-mysql-huoyun-prod-01 data]# mysql -uroot -p -S /data/mysql_3310/run/mysql.sockEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 126072Server version: 8.0.22-13 Percona Server (GPL), Release 13, Revision 6f7822fCopyright (c) 2009-2020 Percona LLC and/or its affiliatesCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.root@localhost 17:54:49 [(none)]&gt; select * from performance_schema.replication_group_members;+---------------------------+--------------------------------------+---------------+-------------+--------------+-------------+----------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE | MEMBER_ROLE | MEMBER_VERSION |+---------------------------+--------------------------------------+---------------+-------------+--------------+-------------+----------------+| group_replication_applier | 06c6f8b7-a195-11eb-994b-fa163e39df9a | 172.16.xx.151 | 3310 | ONLINE | PRIMARY | 8.0.22 || group_replication_applier | 0710790b-a195-11eb-a621-fa163eb36fc8 | 172.16.xx.219 | 3310 | ONLINE | SECONDARY | 8.0.22 || group_replication_applier | 073ebcfe-a195-11eb-8b08-fa163efdcb19 | 172.16.xx.224 | 3310 | ONLINE | SECONDARY | 8.0.22 |+---------------------------+--------------------------------------+---------------+-------------+--------------+-------------+----------------+3 rows in set (0.00 sec) ProxySQL监控检查需要查询gr_member_routing_candidate_status表, 定义如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849DELIMITER $$CREATE FUNCTION gr_applier_queue_length()RETURNS INTDETERMINISTICBEGIN RETURN (SELECT sys.gtid_count( GTID_SUBTRACT( (SELECTReceived_transaction_set FROM performance_schema.replication_connection_statusWHERE Channel_name = &#x27;group_replication_applier&#x27; ), (SELECT@@global.GTID_EXECUTED) )));END$$CREATE FUNCTION gr_member_in_primary_partition()RETURNS VARCHAR(3)DETERMINISTICBEGIN RETURN (SELECT IF( MEMBER_STATE=&#x27;ONLINE&#x27; AND ((SELECT COUNT(*) FROMperformance_schema.replication_group_members WHERE MEMBER_STATE != &#x27;ONLINE&#x27;) &gt;=((SELECT COUNT(*) FROM performance_schema.replication_group_members)/2) = 0),&#x27;YES&#x27;, &#x27;NO&#x27; ) FROM performance_schema.replication_group_members JOINperformance_schema.replication_group_member_stats rgms USING(member_id) WHERE rgms.MEMBER_ID=@@SERVER_UUID ) ;END$$CREATE FUNCTION gr_transactions_to_cert() RETURNS int(11) DETERMINISTICBEGIN RETURN (select performance_schema.replication_group_member_stats.COUNT_TRANSACTIONS_IN_QUEUE AS transactions_to_cert FROM performance_schema.replication_group_member_stats where MEMBER_ID=@@SERVER_UUID );END$$CREATE VIEW gr_member_routing_candidate_status AS SELECT IFNULL(sys.gr_member_in_primary_partition(),&#x27;NO&#x27;) AS viable_candidate, IF((SELECT ((SELECT GROUP_CONCAT(performance_schema.global_variables.VARIABLE_VALUE SEPARATOR &#x27;,&#x27;) FROM performance_schema.global_variables WHERE (performance_schema.global_variables.VARIABLE_NAME IN (&#x27;read_only&#x27; , &#x27;super_read_only&#x27;))) &lt;&gt; &#x27;OFF,OFF&#x27;) ), &#x27;YES&#x27;, &#x27;NO&#x27;) AS read_only, IFNULL(sys.gr_applier_queue_length(),0) AS transactions_behind, IFNULL(sys.gr_transactions_to_cert(),0) AS transactions_to_cert;$$DELIMITER ; 启动两个session, session1, session2, 连接MGR任意一个节点(PRIMARY还是SECONDARY无所谓) session2执行 1234567select * from gr_member_in_primary_partition();+--------------------------------------+| sys.gr_member_in_primary_partition() |+--------------------------------------+| YES |+--------------------------------------+1 row in set (0.00 sec) session1执行 1flush tables with read lock; 此时session2再次执行 1234567&gt; select sys.gr_member_in_primary_partition();ERROR 1242 (21000): Subquery returns more than 1 row导致查询gr_member_routing_candidate_status也报错&gt; select * from gr_member_routing_candidate_status;ERROR 1242 (21000): Subquery returns more than 1 row 然而在session2直接运行gr_member_in_primary_partition定义中的sql语句却一切正常 1234567891011121314&gt; SELECT IF( MEMBER_STATE=&#x27;ONLINE&#x27; AND ((SELECT COUNT(*) FROM -&gt; performance_schema.replication_group_members WHERE MEMBER_STATE != &#x27;ONLINE&#x27;) &gt;= -&gt; ((SELECT COUNT(*) FROM performance_schema.replication_group_members)/2) = 0), -&gt; &#x27;YES&#x27;, &#x27;NO&#x27; ) FROM performance_schema.replication_group_members JOIN -&gt; performance_schema.replication_group_member_stats rgms USING(member_id) WHERE rgms.MEMBER_ID=@@SERVER_UUID;+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| IF( MEMBER_STATE=&#x27;ONLINE&#x27; AND ((SELECT COUNT(*) FROMperformance_schema.replication_group_members WHERE MEMBER_STATE != &#x27;ONLINE&#x27;) &gt;=((SELECT COUNT(*) FROM performance_schema.replication_group_members)/2) = 0),&#x27;YES&#x27;, &#x27;NO&#x27; ) |+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| YES |+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 测试启动session3, 运行查询select sys.gr_member_in_primary_partition(); 无异常 session1 unlock tables 后 session2 执行查询select sys.gr_member_in_primary_partition();仍然报错 怀疑是MySQL bug 解决办法如何解决呢, 不使用函数, 直接用gr_member_in_primary_partition中的sql替换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124SET @TEMP_LOG_BIN = @@SESSION.SQL_LOG_BIN;SET @@SESSION.SQL_LOG_BIN= 0;SET @TEMP_READ_ONLY = @@GLOBAL.READ_ONLY;SET @TEMP_SUPER_READ_ONLY = @@GLOBAL.SUPER_READ_ONLY;SET @@GLOBAL.READ_ONLY = 0;USE sys;DROP VIEW IF EXISTS gr_member_routing_candidate_status;DROP FUNCTION IF EXISTS IFZERO;DROP FUNCTION IF EXISTS LOCATE2;DROP FUNCTION IF EXISTS GTID_NORMALIZE;DROP FUNCTION IF EXISTS GTID_COUNT;DROP FUNCTION IF EXISTS gr_applier_queue_length;DROP FUNCTION IF EXISTS gr_member_in_primary_partition;DROP FUNCTION IF EXISTS gr_transactions_to_cert;DELIMITER $$CREATE FUNCTION IFZERO(a INT, b INT)RETURNS INTDETERMINISTICRETURN IF(a = 0, b, a)$$CREATE FUNCTION LOCATE2(needle TEXT(10000), haystack TEXT(10000), offset INT)RETURNS INTDETERMINISTICRETURN IFZERO(LOCATE(needle, haystack, offset), LENGTH(haystack) + 1)$$CREATE FUNCTION GTID_NORMALIZE(g TEXT(10000))RETURNS TEXT(10000)DETERMINISTICRETURN GTID_SUBTRACT(g, &#x27;&#x27;)$$CREATE FUNCTION GTID_COUNT(gtid_set TEXT(10000))RETURNS INTDETERMINISTICBEGIN DECLARE result BIGINT DEFAULT 0; DECLARE colon_pos INT; DECLARE next_dash_pos INT; DECLARE next_colon_pos INT; DECLARE next_comma_pos INT; SET gtid_set = GTID_NORMALIZE(gtid_set); SET colon_pos = LOCATE2(&#x27;:&#x27;, gtid_set, 1); WHILE colon_pos != LENGTH(gtid_set) + 1 DO SET next_dash_pos = LOCATE2(&#x27;-&#x27;, gtid_set, colon_pos + 1); SET next_colon_pos = LOCATE2(&#x27;:&#x27;, gtid_set, colon_pos + 1); SET next_comma_pos = LOCATE2(&#x27;,&#x27;, gtid_set, colon_pos + 1); IF next_dash_pos &lt; next_colon_pos AND next_dash_pos &lt; next_comma_pos THEN SET result = result + SUBSTR(gtid_set, next_dash_pos + 1, LEAST(next_colon_pos, next_comma_pos) - (next_dash_pos + 1)) - SUBSTR(gtid_set, colon_pos + 1, next_dash_pos - (colon_pos + 1)) + 1; ELSE SET result = result + 1; END IF; SET colon_pos = next_colon_pos; END WHILE; RETURN result;END$$CREATE FUNCTION gr_applier_queue_length()RETURNS INTDETERMINISTICBEGIN RETURN (SELECT sys.gtid_count( GTID_SUBTRACT( (SELECTReceived_transaction_set FROM performance_schema.replication_connection_statusWHERE Channel_name = &#x27;group_replication_applier&#x27; ), (SELECT@@global.GTID_EXECUTED) )));END$$CREATE FUNCTION gr_transactions_to_cert() RETURNS int(11) DETERMINISTICBEGIN RETURN (select performance_schema.replication_group_member_stats.COUNT_TRANSACTIONS_IN_QUEUE AS transactions_to_cert FROM performance_schema.replication_group_member_stats where MEMBER_ID=@@SERVER_UUID );END$$CREATE FUNCTION my_server_uuid() RETURNS TEXT(36) DETERMINISTIC NO SQL RETURN (SELECT @@global.server_uuid as my_id);$$CREATE VIEW gr_member_routing_candidate_status AS SELECT IFNULL((SELECT IF(MEMBER_STATE = &#x27;ONLINE&#x27; AND ((SELECT COUNT(*) FROM performance_schema.replication_group_members WHERE MEMBER_STATE != &#x27;ONLINE&#x27;) &gt;= ((SELECT COUNT(*) FROM performance_schema.replication_group_members) / 2) = 0), &#x27;YES&#x27;, &#x27;NO&#x27;) FROM performance_schema.replication_group_members JOIN performance_schema.replication_group_member_stats rgms USING (member_id) WHERE rgms.MEMBER_ID = my_server_uuid()), &#x27;NO&#x27;) AS viable_candidate, IF((SELECT ((SELECT GROUP_CONCAT(performance_schema.global_variables.VARIABLE_VALUE SEPARATOR &#x27;,&#x27;) FROM performance_schema.global_variables WHERE (performance_schema.global_variables.VARIABLE_NAME IN (&#x27;read_only&#x27; , &#x27;super_read_only&#x27;))) &lt;&gt; &#x27;OFF,OFF&#x27;) ), &#x27;YES&#x27;, &#x27;NO&#x27;) AS read_only, IFNULL(sys.gr_applier_queue_length(), 0) AS transactions_behind, IFNULL(sys.gr_transactions_to_cert(), 0) AS transactions_to_cert;$$DELIMITER ;SET @@SESSION.SQL_LOG_BIN = @TEMP_LOG_BIN;SET @@GLOBAL.READ_ONLY = @TEMP_READ_ONLY;SET @@GLOBAL.SUPER_READ_ONLY = @TEMP_SUPER_READ_ONLY; 完整版addtion_to_sys123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123-- SET @TEMP_LOG_BIN = @@SESSION.SQL_LOG_BIN;-- SET @@SESSION.SQL_LOG_BIN= 0;-- SET @TEMP_READ_ONLY = @@GLOBAL.READ_ONLY;-- SET @TEMP_SUPER_READ_ONLY = @@GLOBAL.SUPER_READ_ONLY;-- SET @@GLOBAL.READ_ONLY = 0;USE sys;DROP VIEW IF EXISTS gr_member_routing_candidate_status;DROP FUNCTION IF EXISTS IFZERO;DROP FUNCTION IF EXISTS LOCATE2;DROP FUNCTION IF EXISTS GTID_NORMALIZE;DROP FUNCTION IF EXISTS GTID_COUNT;DROP FUNCTION IF EXISTS gr_applier_queue_length;DROP FUNCTION IF EXISTS gr_member_in_primary_partition;DROP FUNCTION IF EXISTS gr_transactions_to_cert;DELIMITER $$CREATE FUNCTION IFZERO(a INT, b INT)RETURNS INTDETERMINISTICRETURN IF(a = 0, b, a)$$CREATE FUNCTION LOCATE2(needle TEXT(10000), haystack TEXT(10000), offset INT)RETURNS INTDETERMINISTICRETURN IFZERO(LOCATE(needle, haystack, offset), LENGTH(haystack) + 1)$$CREATE FUNCTION GTID_NORMALIZE(g TEXT(10000))RETURNS TEXT(10000)DETERMINISTICRETURN GTID_SUBTRACT(g, &#x27;&#x27;)$$CREATE FUNCTION GTID_COUNT(gtid_set TEXT(10000))RETURNS INTDETERMINISTICBEGIN DECLARE result BIGINT DEFAULT 0; DECLARE colon_pos INT; DECLARE next_dash_pos INT; DECLARE next_colon_pos INT; DECLARE next_comma_pos INT; SET gtid_set = GTID_NORMALIZE(gtid_set); SET colon_pos = LOCATE2(&#x27;:&#x27;, gtid_set, 1); WHILE colon_pos != LENGTH(gtid_set) + 1 DO SET next_dash_pos = LOCATE2(&#x27;-&#x27;, gtid_set, colon_pos + 1); SET next_colon_pos = LOCATE2(&#x27;:&#x27;, gtid_set, colon_pos + 1); SET next_comma_pos = LOCATE2(&#x27;,&#x27;, gtid_set, colon_pos + 1); IF next_dash_pos &lt; next_colon_pos AND next_dash_pos &lt; next_comma_pos THEN SET result = result + SUBSTR(gtid_set, next_dash_pos + 1, LEAST(next_colon_pos, next_comma_pos) - (next_dash_pos + 1)) - SUBSTR(gtid_set, colon_pos + 1, next_dash_pos - (colon_pos + 1)) + 1; ELSE SET result = result + 1; END IF; SET colon_pos = next_colon_pos; END WHILE; RETURN result;END$$CREATE FUNCTION gr_applier_queue_length()RETURNS INTDETERMINISTICBEGIN RETURN (SELECT sys.gtid_count( GTID_SUBTRACT( (SELECTReceived_transaction_set FROM performance_schema.replication_connection_statusWHERE Channel_name = &#x27;group_replication_applier&#x27; ), (SELECT@@global.GTID_EXECUTED) )));END$$CREATE FUNCTION gr_transactions_to_cert() RETURNS int(11) DETERMINISTICBEGIN RETURN (select performance_schema.replication_group_member_stats.COUNT_TRANSACTIONS_IN_QUEUE AS transactions_to_cert FROM performance_schema.replication_group_member_stats where MEMBER_ID=@@SERVER_UUID );END$$CREATE FUNCTION my_server_uuid() RETURNS TEXT(36) DETERMINISTIC NO SQL RETURN (SELECT @@global.server_uuid as my_id);$$CREATE VIEW gr_member_routing_candidate_status AS SELECT IFNULL((SELECT IF(MEMBER_STATE = &#x27;ONLINE&#x27; AND ((SELECT COUNT(*) FROM performance_schema.replication_group_members WHERE MEMBER_STATE != &#x27;ONLINE&#x27;) &gt;= ((SELECT COUNT(*) FROM performance_schema.replication_group_members) / 2) = 0), &#x27;YES&#x27;, &#x27;NO&#x27;) FROM performance_schema.replication_group_members JOIN performance_schema.replication_group_member_stats rgms USING (member_id) WHERE rgms.MEMBER_ID = my_server_uuid()), &#x27;NO&#x27;) AS viable_candidate, IF((SELECT ((SELECT GROUP_CONCAT(performance_schema.global_variables.VARIABLE_VALUE SEPARATOR &#x27;,&#x27;) FROM performance_schema.global_variables WHERE (performance_schema.global_variables.VARIABLE_NAME IN (&#x27;read_only&#x27; , &#x27;super_read_only&#x27;))) &lt;&gt; &#x27;OFF,OFF&#x27;) ), &#x27;YES&#x27;, &#x27;NO&#x27;) AS read_only, IFNULL(sys.gr_applier_queue_length(), 0) AS transactions_behind, IFNULL(sys.gr_transactions_to_cert(), 0) AS transactions_to_cert;$$DELIMITER ;-- SET @@SESSION.SQL_LOG_BIN = @TEMP_LOG_BIN;-- SET @@GLOBAL.READ_ONLY = @TEMP_READ_ONLY;-- SET @@GLOBAL.SUPER_READ_ONLY = @TEMP_SUPER_READ_ONLY;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"},{"name":"ProxySQL","slug":"ProxySQL","permalink":"http://fuxkdb.com/tags/ProxySQL/"}]},{"title":"skeema简单使用","slug":"2021-02-18-skeema简单使用","date":"2021-02-18T05:08:00.000Z","updated":"2021-02-18T05:15:11.043Z","comments":true,"path":"2021/02/18/2021-02-18-skeema简单使用/","link":"","permalink":"http://fuxkdb.com/2021/02/18/2021-02-18-skeema%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","excerpt":"简介Skeema is a tool for managing MySQL tables and schema changes in a declarative fashion using pure SQL. It provides a CLI tool allowing you to: Export CREATE TABLE statements to the filesystem, for tracking in a repo (git, hg, svn, etc) Diff changes in the schema repo against live DBs to automatically generate DDL Manage multiple environments (e.g. dev, staging, prod) and keep them in sync with ease Configure use of online schema change tools, such as pt-online-schema-change, for performing ALTERs Convert non-online migrations from frameworks like Rails or Django into online schema changes in production Skeema supports a pull-request-based workflow for schema change submission, review, and execution. This permits your team to manage schema changes in exactly the same way as you manage code changes. Our new companion Cloud Linter for GitHub repos provides automatic linting of schema change commits and pull requests. 我这的需求是同步生产环境表结构到演练环境, 以下是针对我这个场景的具体使用方法 权限skeema需要的用户权限具体见https://www.skeema.io/docs/requirements/ 这里说一下重点, skeema diff时会在目标环境创建一个_skeema_tmp临时数据库, 然后会删除这个库","text":"简介Skeema is a tool for managing MySQL tables and schema changes in a declarative fashion using pure SQL. It provides a CLI tool allowing you to: Export CREATE TABLE statements to the filesystem, for tracking in a repo (git, hg, svn, etc) Diff changes in the schema repo against live DBs to automatically generate DDL Manage multiple environments (e.g. dev, staging, prod) and keep them in sync with ease Configure use of online schema change tools, such as pt-online-schema-change, for performing ALTERs Convert non-online migrations from frameworks like Rails or Django into online schema changes in production Skeema supports a pull-request-based workflow for schema change submission, review, and execution. This permits your team to manage schema changes in exactly the same way as you manage code changes. Our new companion Cloud Linter for GitHub repos provides automatic linting of schema change commits and pull requests. 我这的需求是同步生产环境表结构到演练环境, 以下是针对我这个场景的具体使用方法 权限skeema需要的用户权限具体见https://www.skeema.io/docs/requirements/ 这里说一下重点, skeema diff时会在目标环境创建一个_skeema_tmp临时数据库, 然后会删除这个库 安装安装go环境 123wget https://dl.google.com/go/go1.16.linux-amd64.tar.gztar -C /usr/local -xzf go1.16.linux-amd64.tar.gz export PATH=$PATH:/usr/local/go/bin 安装skeema 12curl -LO https://github.com/skeema/skeema/releases/latest/download/skeema_amd64.rpmrpm -ivh skeema 使用init在演练环境 1skeema init -h 127.0.0.1 -P 3358 -u fanboshi -p -d 3358 --schema sss -d dump的sql文件存储路径 –schema 只导出sss这个数据库的结构信息 随后3358目录下回包含一些.sql文件和一个.skeema文件 123456789101112131415-rw-r--r-- 1 root root 213 Feb 18 12:37 .skeema-rw-r--r-- 1 root root 1521 Feb 18 12:33 sss_cmdb_instances.sql-rw-r--r-- 1 root root 305 Feb 18 12:33 sss_cmdb_vip.sql-rw-r--r-- 1 root root 1122 Feb 18 12:33 sss_host_host.sql-rw-r--r-- 1 root root 388 Feb 18 12:33 sss_cmdb_cluster_domain_name_config.sql-rw-r--r-- 1 root root 369 Feb 18 12:33 sss_cmdb_cluster_portrayal_mapping.sql-rw-r--r-- 1 root root 602 Feb 18 12:33 sss_cmdb_cluster.sql-rw-r--r-- 1 root root 1148 Feb 18 12:33 sss_cmdb_cluster_users.sql-rw-r--r-- 1 root root 304 Feb 18 12:33 sss_cmdb_instance_ports.sql-rw-r--r-- 1 root root 756 Feb 18 12:33 sss_cmdb_instances_port.sql-rw-r--r-- 1 root root 244 Feb 18 12:33 sss_cmdb_productlines.sql-rw-r--r-- 1 root root 595 Feb 18 12:33 sss_cmdb_services.sql-rw-r--r-- 1 root root 274 Feb 18 12:33 sss_dbms_environment.sql-rw-r--r-- 1 root root 815 Feb 18 12:33 sss_dbms_sql_config.sql-rw-r--r-- 1 root root 625 Feb 18 12:33 sss_cmdb_environments.sql 编辑.skeema文件 12345678910#cat .skeema default-character-set=utf8default-collation=utf8_general_cischema=sss[production]flavor=percona:5.7host=127.0.0.1port=3358user=fanboshi 我们这里修改[production]为[drill], 并增加一些参数 1234567891011alter-wrapper=&quot;gh-ost --allow-on-master --assume-rbr --initially-drop-ghost-table --initially-drop-old-table -exact-rowcount --approve-renamed-columns --concurrent-rowcount=false --chunk-size=800 --hooks-path=/tmp/hook --user=&#123;USER&#125; --ask-pass --host=&#123;HOST&#125; --port=&#123;PORT&#125; --database=&#123;SCHEMA&#125; --table=&#123;TABLE&#125; --alter=&#123;CLAUSES&#125; --execute&quot;default-character-set=utf8default-collation=utf8_general_cischema=sss[drill]flavor=percona:5.7host=127.0.0.1port=3358user=fanboshipassword=superpass #可加可不加, 如果没有定义password, 则需要在命令行增加-p参数 添加生产环境 1skeema add-environment product -h 172.16.120.11 -P 3358 -u fanboshi 或者直接在.skeema添加如下信息 12345[product]flavor=percona:5.7host=172.16.120.11port=3358user=fanboshi 最终的.skeema文件如下 1234567891011121314151617alter-wrapper=&quot;gh-ost --allow-on-master --assume-rbr --initially-drop-ghost-table --initially-drop-old-table -exact-rowcount --approve-renamed-columns --concurrent-rowcount=false --chunk-size=800 --hooks-path=/tmp/hook --user=&#123;USER&#125; --ask-pass --host=&#123;HOST&#125; --port=&#123;PORT&#125; --database=&#123;SCHEMA&#125; --table=&#123;TABLE&#125; --alter=&#123;CLAUSES&#125; --execute&quot;default-character-set=utf8default-collation=utf8_general_cischema=sss[drill]flavor=percona:5.7host=127.0.0.1port=3358user=fanboshi#password=superpass #可加可不加, 如果没有定义password, 则需要在命令行增加-p参数[product]flavor=percona:5.7host=172.16.120.11port=3358user=fanboshi 拉取生产环境表结构12cd 3358skeema pull product -p 生成drill环境与生产环境的差异sql文件1skeema diff drill --allow-unsafe -psuperpass &gt; /tmp/diff.sql 如果没有在.skeema文件中指定password参数, 则要在命令行指定-p参数并填写密码 查看生产的文件 1234567891011121314151617-- instance: 127.0.0.1:3358USE `sss`;\\! gh-ost --allow-on-master --assume-rbr --initially-drop-ghost-table --initially-drop-old-table -exact-rowcount --approve-renamed-columns --concurrent-rowcount=false --chunk-size=800 --hooks-path=/tmp/hook --user=fanboshi --ask-pass --host=127.0.0.1 --port=3358 --database=sss --table=sss_cmdb_service_name --alter=&#x27;DROP COLUMN `environment_name`, DROP COLUMN `productline_name`, DROP COLUMN `cmdb_cluster_name`&#x27; --executeCREATE TABLE `sss_domain_manage` ( `id` int(11) NOT NULL AUTO_INCREMENT,... PRIMARY KEY (`id`), UNIQUE KEY `domain_name` (`domain_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `sss_iam_role_members` ( `id` int(11) NOT NULL AUTO_INCREMENT,... UNIQUE KEY `sss_iam_role_members_role_id_profile_id_93c492ff_uniq` (`role_id`,`profile_id`), KEY `ytree_iam_role_members_profile_id_ce949eff_fk_users_profile_uid` (`profile_id`), CONSTRAINT `sss_iam_role_members_profile_id_ce949eff_fk_users_profile_uid` FOREIGN KEY (`profile_id`) REFERENCES `sss_users_profile` (`uid`), CONSTRAINT `sss_iam_role_members_role_id_8a1e407b_fk_ytree_iam_role_id` FOREIGN KEY (`role_id`) REFERENCES `sss_iam_role` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 可以看到这就是一个.sql文件, 注意生成的gh-ost语句前有一个\\!, 大家应该明白是啥意思吧 如果是想手动执行gh-ost命令, 记得把”`”转义掉","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"DorisDB vs ClickHouse SSB对比测试","slug":"2021-02-12-DorisDB-vs-ClickHouse-SSB对比测试","date":"2021-02-12T09:24:00.000Z","updated":"2021-02-12T09:25:10.704Z","comments":true,"path":"2021/02/12/2021-02-12-DorisDB-vs-ClickHouse-SSB对比测试/","link":"","permalink":"http://fuxkdb.com/2021/02/12/2021-02-12-DorisDB-vs-ClickHouse-SSB%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%95/","excerpt":"DorisDB vs ClickHouse SSB对比测试TL;DR 进行本次测试时对DorisDB了解甚微 本次测试由于服务器资源有限, 没有严格遵循单一变量原则进行测试 本次测试有一定参考意义 数据导入速度 ClickHouse: 3500s DorisDB: 5160s 数据压缩情况(通过磁盘占用空间比较) ClickHouse: 85.2G DorisDB: 132G 查询速度单表查询 DorisDB1 DorisDB2 ClickHouse1 ClickHouse2 Q1.1 350 290 226 195 Q1.2 270 190 34 63 Q1.3 310 240 43 22 Q2.1 410 370 1,723 1,791 Q2.2 780 720 1,463 1,470 Q2.3 340 280 659 1,337 Q3.1 1,560 860 3,488 1,254 Q3.2 1,080 790 1,272 966 Q3.3 250 290 979 889 Q3.4 230 260 36 20 Q4.1 870 720 5,067 2,791 Q4.2 720 490 804 752 Q4.3 510 380 561 482 多表查询 DorisDB1 DorisDB2 ClickHouse1 ClickHouse2 Q1.1 450 490 1,496 1,424 Q1.2 410 450 1,366 659 Q1.3 510 340 678 1,377 Q2.1 1,560 1,600 4,360 2,667 Q2.2 1,690 1,060 4,498 1,554 Q2.3 780 1,150 2,569 2,577 Q3.1 3,480 3,700 10,190 12,960 Q3.2 1,320 1,850 5,926 5,743 Q3.3 1,030 1,040 3,445 3,300 Q3.4 1,330 1,170 3,455 3,330 Q4.1 3,480 3,750 15,560 9,494 Q4.2 2,830 3,170 16,109 18,048 Q4.3 1,560 2,140 15,685 14,838","text":"DorisDB vs ClickHouse SSB对比测试TL;DR 进行本次测试时对DorisDB了解甚微 本次测试由于服务器资源有限, 没有严格遵循单一变量原则进行测试 本次测试有一定参考意义 数据导入速度 ClickHouse: 3500s DorisDB: 5160s 数据压缩情况(通过磁盘占用空间比较) ClickHouse: 85.2G DorisDB: 132G 查询速度单表查询 DorisDB1 DorisDB2 ClickHouse1 ClickHouse2 Q1.1 350 290 226 195 Q1.2 270 190 34 63 Q1.3 310 240 43 22 Q2.1 410 370 1,723 1,791 Q2.2 780 720 1,463 1,470 Q2.3 340 280 659 1,337 Q3.1 1,560 860 3,488 1,254 Q3.2 1,080 790 1,272 966 Q3.3 250 290 979 889 Q3.4 230 260 36 20 Q4.1 870 720 5,067 2,791 Q4.2 720 490 804 752 Q4.3 510 380 561 482 多表查询 DorisDB1 DorisDB2 ClickHouse1 ClickHouse2 Q1.1 450 490 1,496 1,424 Q1.2 410 450 1,366 659 Q1.3 510 340 678 1,377 Q2.1 1,560 1,600 4,360 2,667 Q2.2 1,690 1,060 4,498 1,554 Q2.3 780 1,150 2,569 2,577 Q3.1 3,480 3,700 10,190 12,960 Q3.2 1,320 1,850 5,926 5,743 Q3.3 1,030 1,040 3,445 3,300 Q3.4 1,330 1,170 3,455 3,330 Q4.1 3,480 3,750 15,560 9,494 Q4.2 2,830 3,170 16,109 18,048 Q4.3 1,560 2,140 15,685 14,838 环境信息ClickHouse: 3台 华为云ECS 高性能计算型 | h3.xlarge.2 | 4vCPUs | 8GB | 超高IO SSD DorisDB: 3台 华为云ECS 高性能计算型 | h3.2xlarge.4 | 8vCPUs | 32GB | 超高IO SSD 由于资源紧张, DorisDB所在服务器上还部署了rc mysql, 但过年期间无人使用, 实际可用内存16G. DorisDB: DorisDB-SE-1.12.1 3Fe 3Be, fe和be部署在一起 ClickHouse: 20.10.3.30, 三分片两副本混合部署, 部署方法详见ClickHouse集群多实例部署 注意 实际数据导入后DorsiDB和ClickHouse除lineorder_flat外数据无任何差异 lineorder_flat: DorsiDB: 546669614 ClickHouse: 622259902 DorisDB部署略 构建数据首先下载ssb-poc工具包并编译 1234wget http://dorisdb-public.oss-cn-zhangjiakou.aliyuncs.com/ssb-poc-0.9.zipunzip ssb-poc-0.9.zipcd ssb-pocmake &amp;&amp; make install 所有相关工具安装到output目录。 进入output目录，生成数据 12cd outputbin/gen-ssb.sh 100 data_dir 建表修改配置文件conf/doris.conf，指定脚本操作的Doris集群地址 12345678910111213 # for mysql cmd mysql_host: 192.168.1.1 mysql_port: 9030 mysql_user: root mysql_password: doris_db: ssb # cluster ports http_port: 8030 be_heartbeat_port: 9050 broker_port: 8000 ... 执行脚本建表 1bin/create_db_table.sh ddl_100 我这里建表跑建表脚本报错了, 改为手动建表, 参考http://doc.dorisdb.com/2146807 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169CREATE TABLE IF NOT EXISTS `lineorder` ( `lo_orderkey` int(11) NOT NULL COMMENT &quot;&quot;, `lo_linenumber` int(11) NOT NULL COMMENT &quot;&quot;, `lo_custkey` int(11) NOT NULL COMMENT &quot;&quot;, `lo_partkey` int(11) NOT NULL COMMENT &quot;&quot;, `lo_suppkey` int(11) NOT NULL COMMENT &quot;&quot;, `lo_orderdate` int(11) NOT NULL COMMENT &quot;&quot;, `lo_orderpriority` varchar(16) NOT NULL COMMENT &quot;&quot;, `lo_shippriority` int(11) NOT NULL COMMENT &quot;&quot;, `lo_quantity` int(11) NOT NULL COMMENT &quot;&quot;, `lo_extendedprice` int(11) NOT NULL COMMENT &quot;&quot;, `lo_ordtotalprice` int(11) NOT NULL COMMENT &quot;&quot;, `lo_discount` int(11) NOT NULL COMMENT &quot;&quot;, `lo_revenue` int(11) NOT NULL COMMENT &quot;&quot;, `lo_supplycost` int(11) NOT NULL COMMENT &quot;&quot;, `lo_tax` int(11) NOT NULL COMMENT &quot;&quot;, `lo_commitdate` int(11) NOT NULL COMMENT &quot;&quot;, `lo_shipmode` varchar(11) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`lo_orderkey`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`lo_orderkey`) BUCKETS 96PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;colocate_with&quot; = &quot;group1&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;);CREATE TABLE IF NOT EXISTS `customer` ( `c_custkey` int(11) NOT NULL COMMENT &quot;&quot;, `c_name` varchar(26) NOT NULL COMMENT &quot;&quot;, `c_address` varchar(41) NOT NULL COMMENT &quot;&quot;, `c_city` varchar(11) NOT NULL COMMENT &quot;&quot;, `c_nation` varchar(16) NOT NULL COMMENT &quot;&quot;, `c_region` varchar(13) NOT NULL COMMENT &quot;&quot;, `c_phone` varchar(16) NOT NULL COMMENT &quot;&quot;, `c_mktsegment` varchar(11) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`c_custkey`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`c_custkey`) BUCKETS 12PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;colocate_with&quot; = &quot;groupa2&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;);CREATE TABLE IF NOT EXISTS `dates` ( `d_datekey` int(11) NOT NULL COMMENT &quot;&quot;, `d_date` varchar(20) NOT NULL COMMENT &quot;&quot;, `d_dayofweek` varchar(10) NOT NULL COMMENT &quot;&quot;, `d_month` varchar(11) NOT NULL COMMENT &quot;&quot;, `d_year` int(11) NOT NULL COMMENT &quot;&quot;, `d_yearmonthnum` int(11) NOT NULL COMMENT &quot;&quot;, `d_yearmonth` varchar(9) NOT NULL COMMENT &quot;&quot;, `d_daynuminweek` int(11) NOT NULL COMMENT &quot;&quot;, `d_daynuminmonth` int(11) NOT NULL COMMENT &quot;&quot;, `d_daynuminyear` int(11) NOT NULL COMMENT &quot;&quot;, `d_monthnuminyear` int(11) NOT NULL COMMENT &quot;&quot;, `d_weeknuminyear` int(11) NOT NULL COMMENT &quot;&quot;, `d_sellingseason` varchar(14) NOT NULL COMMENT &quot;&quot;, `d_lastdayinweekfl` int(11) NOT NULL COMMENT &quot;&quot;, `d_lastdayinmonthfl` int(11) NOT NULL COMMENT &quot;&quot;, `d_holidayfl` int(11) NOT NULL COMMENT &quot;&quot;, `d_weekdayfl` int(11) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`d_datekey`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`d_datekey`) BUCKETS 1PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;colocate_with&quot; = &quot;groupa3&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;); CREATE TABLE IF NOT EXISTS `supplier` ( `s_suppkey` int(11) NOT NULL COMMENT &quot;&quot;, `s_name` varchar(26) NOT NULL COMMENT &quot;&quot;, `s_address` varchar(26) NOT NULL COMMENT &quot;&quot;, `s_city` varchar(11) NOT NULL COMMENT &quot;&quot;, `s_nation` varchar(16) NOT NULL COMMENT &quot;&quot;, `s_region` varchar(13) NOT NULL COMMENT &quot;&quot;, `s_phone` varchar(16) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`s_suppkey`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`s_suppkey`) BUCKETS 12PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;colocate_with&quot; = &quot;groupa4&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;);CREATE TABLE IF NOT EXISTS `part` ( `p_partkey` int(11) NOT NULL COMMENT &quot;&quot;, `p_name` varchar(23) NOT NULL COMMENT &quot;&quot;, `p_mfgr` varchar(7) NOT NULL COMMENT &quot;&quot;, `p_category` varchar(8) NOT NULL COMMENT &quot;&quot;, `p_brand` varchar(10) NOT NULL COMMENT &quot;&quot;, `p_color` varchar(12) NOT NULL COMMENT &quot;&quot;, `p_type` varchar(26) NOT NULL COMMENT &quot;&quot;, `p_size` int(11) NOT NULL COMMENT &quot;&quot;, `p_container` varchar(11) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`p_partkey`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`p_partkey`) BUCKETS 12PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;colocate_with&quot; = &quot;groupa5&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;);CREATE TABLE IF NOT EXISTS `lineorder_flat` ( `LO_ORDERKEY` int(11) NOT NULL COMMENT &quot;&quot;, `LO_ORDERDATE` date NOT NULL COMMENT &quot;&quot;, `LO_LINENUMBER` tinyint(4) NOT NULL COMMENT &quot;&quot;, `LO_CUSTKEY` int(11) NOT NULL COMMENT &quot;&quot;, `LO_PARTKEY` int(11) NOT NULL COMMENT &quot;&quot;, `LO_SUPPKEY` int(11) NOT NULL COMMENT &quot;&quot;, `LO_ORDERPRIORITY` varchar(100) NOT NULL COMMENT &quot;&quot;, `LO_SHIPPRIORITY` tinyint(4) NOT NULL COMMENT &quot;&quot;, `LO_QUANTITY` tinyint(4) NOT NULL COMMENT &quot;&quot;, `LO_EXTENDEDPRICE` int(11) NOT NULL COMMENT &quot;&quot;, `LO_ORDTOTALPRICE` int(11) NOT NULL COMMENT &quot;&quot;, `LO_DISCOUNT` tinyint(4) NOT NULL COMMENT &quot;&quot;, `LO_REVENUE` int(11) NOT NULL COMMENT &quot;&quot;, `LO_SUPPLYCOST` int(11) NOT NULL COMMENT &quot;&quot;, `LO_TAX` tinyint(4) NOT NULL COMMENT &quot;&quot;, `LO_COMMITDATE` date NOT NULL COMMENT &quot;&quot;, `LO_SHIPMODE` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_NAME` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_ADDRESS` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_CITY` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_NATION` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_REGION` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_PHONE` varchar(100) NOT NULL COMMENT &quot;&quot;, `C_MKTSEGMENT` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_NAME` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_ADDRESS` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_CITY` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_NATION` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_REGION` varchar(100) NOT NULL COMMENT &quot;&quot;, `S_PHONE` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_NAME` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_MFGR` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_CATEGORY` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_BRAND` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_COLOR` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_TYPE` varchar(100) NOT NULL COMMENT &quot;&quot;, `P_SIZE` tinyint(4) NOT NULL COMMENT &quot;&quot;, `P_CONTAINER` varchar(100) NOT NULL COMMENT &quot;&quot;) ENGINE=OLAPDUPLICATE KEY(`LO_ORDERKEY`)COMMENT &quot;OLAP&quot;DISTRIBUTED BY HASH(`LO_ORDERKEY`) BUCKETS 192PROPERTIES (&quot;replication_num&quot; = &quot;1&quot;,&quot;colocate_with&quot; = &quot;groupxx1&quot;,&quot;in_memory&quot; = &quot;false&quot;,&quot;storage_format&quot; = &quot;DEFAULT&quot;); 连接一个fe执行以上sql即可 导入数据 DorisDB测试中使用python脚本导入数据, 需要安装pymysql库 使用Stream load导入单表数据 12345678910111213141516171819202122232425262728293031323334353637383940414243(myrecover) [root@bj2-mysql-rc-drill-02 output]# time bin/stream_load.sh data_dirstream load start. table: lineorder, path: data_dir/lineorder.tbl.1stream load start. table: lineorder, path: data_dir/lineorder.tbl.2stream load start. table: lineorder, path: data_dir/lineorder.tbl.3stream load start. table: lineorder, path: data_dir/lineorder.tbl.4stream load start. table: lineorder, path: data_dir/lineorder.tbl.5stream load start. table: lineorder, path: data_dir/lineorder.tbl.6stream load start. table: lineorder, path: data_dir/lineorder.tbl.7stream load start. table: lineorder, path: data_dir/lineorder.tbl.8stream load start. table: lineorder, path: data_dir/lineorder.tbl.9stream load start. table: lineorder, path: data_dir/lineorder.tbl.10...stream load success. table: lineorder, path: data_dir/lineorder.tbl.100stream load success. table: lineorder, path: data_dir/lineorder.tbl.97stream load success. table: lineorder, path: data_dir/lineorder.tbl.99stream load success. table: lineorder, path: data_dir/lineorder.tbl.93stream load success. table: lineorder, path: data_dir/lineorder.tbl.98stream load success. table: lineorder, path: data_dir/lineorder.tbl.94stream load success. table: lineorder, path: data_dir/lineorder.tbl.92stream load success. table: lineorder, path: data_dir/lineorder.tbl.95stream load success. table: lineorder, path: data_dir/lineorder.tbl.91stream load success. table: lineorder, path: data_dir/lineorder.tbl.96stream load start. table: customer, path: data_dir/customer.tblstream load success. table: customer, path: data_dir/customer.tblstream load start. table: dates, path: data_dir/dates.tblstream load success. table: dates, path: data_dir/dates.tblstream load start. table: part, path: data_dir/part.tblstream load success. table: part, path: data_dir/part.tblstream load start. table: supplier, path: data_dir/supplier.tblstream load success. table: supplier, path: data_dir/supplier.tblreal 82m31.323suser 0m6.385ssys 0m36.761s(myrecover) [root@bj2-mysql-rc-drill-02 output]# time bin/flat_insert.sh sql: ssb_flat_insert startsql: ssb_flat_insert successreal 1m36.697suser 0m0.078ssys 0m0.022s 插入数据到宽表lineorder_flat 1234567(myrecover) [root@bj2-mysql-rc-drill-02 output]# time bin/flat_insert.shsql: ssb_flat_insert startsql: ssb_flat_insert successreal 2m37.530suser 0m0.063ssys 0m0.023s 有一个不理解的现象是, flat_insert.sh已经执行完毕, 但是查看lineorder_flat表行数时, 发现其值是在不断增大 123456789101112131415161718192021222324mysql&gt; select count(*) from lineorder_flat;+-----------+| count(*) |+-----------+| 182256332 |+-----------+1 row in set (0.10 sec)mysql&gt; select count(*) from lineorder_flat;+-----------+| count(*) |+-----------+| 364316982 |+-----------+1 row in set (0.16 sec)mysql&gt; select count(*) from lineorder_flat;+-----------+| count(*) |+-----------+| 546669614 |+-----------+1 row in set (0.41 sec) 可以看到DorisDB数据导入耗时约86分钟 最终数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mysql&gt; select count(*) from customer ;+----------+| count(*) |+----------+| 3000000 |+----------+1 row in set (0.02 sec)mysql&gt; select count(*) from dates ;+----------+| count(*) |+----------+| 2556 |+----------+1 row in set (0.01 sec)mysql&gt; select count(*) from lineorder ;+-----------+| count(*) |+-----------+| 600037902 |+-----------+1 row in set (0.53 sec)mysql&gt; select count(*) from lineorder_flat ;+-----------+| count(*) |+-----------+| 546669614 |+-----------+1 row in set (0.40 sec)mysql&gt; select count(*) from part ;+----------+| count(*) |+----------+| 1400000 |+----------+1 row in set (0.01 sec)mysql&gt; select count(*) from supplier ;+----------+| count(*) |+----------+| 200000 |+----------+1 row in set (0.01 sec) 数据占用空间1234du -sh /data/DorisDB-SE-1.12.1/be/storage/data/44G /data/DorisDB-SE-1.12.1/be/storage/data/44*3 = 132G 单表查询测试1set global parallel_fragment_exec_instance_num = 4; Q1.112345678910SELECT sum(lo_extendedprice * lo_discount) AS `revenue` FROM lineorder_flat WHERE lo_orderdate &gt;= &#x27;1993-01-01&#x27; and lo_orderdate &lt;= &#x27;1993-12-31&#x27; AND lo_discount BETWEEN 1 AND 3 AND lo_quantity &lt; 25; +----------------+| revenue |+----------------+| 44652567249651 |+----------------+1 row in set (0.35 sec)1 row in set (0.29 sec) Q1.2123456789SELECT sum(lo_extendedprice * lo_discount) AS revenue FROM lineorder_flat WHERE lo_orderdate &gt;= &#x27;1994-01-01&#x27; and lo_orderdate &lt;= &#x27;1994-01-31&#x27; AND lo_discount BETWEEN 4 AND 6 AND lo_quantity BETWEEN 26 AND 35; +---------------+| revenue |+---------------+| 9624332170119 |+---------------+1 row in set (0.27 sec)1 row in set (0.19 sec) Q1.31234567891011SELECT sum(lo_extendedprice * lo_discount) AS revenue FROM lineorder_flat WHERE weekofyear(lo_orderdate) = 6 AND lo_orderdate &gt;= &#x27;1994-01-01&#x27; and lo_orderdate &lt;= &#x27;1994-12-31&#x27; AND lo_discount BETWEEN 5 AND 7 AND lo_quantity BETWEEN 26 AND 35;+---------------+| revenue |+---------------+| 2611093671163 |+---------------+1 row in set (0.31 sec)1 row in set (0.24 sec) Q2.112345678910SELECT sum(lo_revenue), year(lo_orderdate) AS year, p_brand FROM lineorder_flat WHERE p_category = &#x27;MFGR#12&#x27; AND s_region = &#x27;AMERICA&#x27; GROUP BY year, p_brand ORDER BY year, p_brand; ...240 rows in set (0.41 sec)240 rows in set (0.37 sec) Q2.2123456789SELECT sum(lo_revenue), year(lo_orderdate) AS year, p_brand FROM lineorder_flat WHERE p_brand &gt;= &#x27;MFGR#2221&#x27; AND p_brand &lt;= &#x27;MFGR#2228&#x27; AND s_region = &#x27;ASIA&#x27; GROUP BY year, p_brand ORDER BY year, p_brand; 48 rows in set (0.78 sec)48 rows in set (0.72 sec) Q2.312345678SELECT sum(lo_revenue), year(lo_orderdate) AS year, p_brand FROM lineorder_flat WHERE p_brand = &#x27;MFGR#2239&#x27; AND s_region = &#x27;EUROPE&#x27; GROUP BY year, p_brand ORDER BY year, p_brand; 6 rows in set (0.34 sec)6 rows in set (0.28 sec) Q3.11234567SELECT c_nation, s_nation, year(lo_orderdate) AS year, sum(lo_revenue) AS revenue FROM lineorder_flat WHERE c_region = &#x27;ASIA&#x27; AND s_region = &#x27;ASIA&#x27; AND lo_orderdate &gt;= &#x27;1992-01-01&#x27; AND lo_orderdate &lt;= &#x27;1997-12-31&#x27; GROUP BY c_nation, s_nation, year ORDER BY year ASC, revenue DESC; 150 rows in set (1.56 sec)150 rows in set (0.86 sec) Q3.212345678SELECT c_city, s_city, year(lo_orderdate) AS year, sum(lo_revenue) AS revenueFROM lineorder_flat WHERE c_nation = &#x27;UNITED STATES&#x27; AND s_nation = &#x27;UNITED STATES&#x27; AND lo_orderdate &gt;= &#x27;1992-01-01&#x27; AND lo_orderdate &lt;= &#x27;1997-12-31&#x27; GROUP BY c_city, s_city, year ORDER BY year ASC, revenue DESC; 600 rows in set (1.08 sec)600 rows in set (0.79 sec) Q3.312345678SELECT c_city, s_city, year(lo_orderdate) AS year, sum(lo_revenue) AS revenue FROM lineorder_flat WHERE c_city in ( &#x27;UNITED KI1&#x27; ,&#x27;UNITED KI5&#x27;) AND s_city in ( &#x27;UNITED KI1&#x27; ,&#x27;UNITED KI5&#x27;) AND lo_orderdate &gt;= &#x27;1992-01-01&#x27; AND lo_orderdate &lt;= &#x27;1997-12-31&#x27; GROUP BY c_city, s_city, year ORDER BY year ASC, revenue DESC; 24 rows in set (0.25 sec)24 rows in set (0.29 sec) Q3.412345678SELECT c_city, s_city, year(lo_orderdate) AS year, sum(lo_revenue) AS revenue FROM lineorder_flat WHERE c_city in (&#x27;UNITED KI1&#x27;, &#x27;UNITED KI5&#x27;) AND s_city in ( &#x27;UNITED KI1&#x27;, &#x27;UNITED KI5&#x27;) AND lo_orderdate &gt;= &#x27;1997-12-01&#x27; AND lo_orderdate &lt;= &#x27;1997-12-31&#x27; GROUP BY c_city, s_city, year ORDER BY year ASC, revenue DESC; 4 rows in set (0.23 sec)4 rows in set (0.26 sec) Q4.1在DorisDB测试文档中对该SQL执行前执行了 1set vectorized_engine_enable = FALSE; 但实际发现执行上面语句后反而会慢很多 1234567SELECT year(lo_orderdate) AS year, c_nation, sum(lo_revenue - lo_supplycost) AS profit FROM lineorder_flat WHERE c_region = &#x27;AMERICA&#x27; AND s_region = &#x27;AMERICA&#x27; AND p_mfgr in ( &#x27;MFGR#1&#x27; , &#x27;MFGR#2&#x27;) GROUP BY year, c_nation ORDER BY year ASC, c_nation ASC;30 rows in set (1 min 24.46 sec)30 rows in set (1 min 25.10 sec) 12345678set vectorized_engine_enable = TRUE; SELECT year(lo_orderdate) AS year, c_nation, sum(lo_revenue - lo_supplycost) AS profit FROM lineorder_flat WHERE c_region = &#x27;AMERICA&#x27; AND s_region = &#x27;AMERICA&#x27; AND p_mfgr in ( &#x27;MFGR#1&#x27; , &#x27;MFGR#2&#x27;) GROUP BY year, c_nation ORDER BY year ASC, c_nation ASC;30 rows in set (0.87 sec)30 rows in set (0.72 sec) 我比对了开启和关闭vectorized_engine_enable后的查询结果, 发现是一样的, 这里我就不明白为啥要设置vectorized_engine_enable为False了 Q4.2123456789SELECT year(lo_orderdate) AS year, s_nation, p_category, sum(lo_revenue - lo_supplycost) AS profit FROM lineorder_flat WHERE c_region = &#x27;AMERICA&#x27; AND s_region = &#x27;AMERICA&#x27; AND lo_orderdate &gt;= &#x27;1997-01-01&#x27; and lo_orderdate &lt;= &#x27;1998-12-31&#x27; AND p_mfgr in ( &#x27;MFGR#1&#x27; , &#x27;MFGR#2&#x27;) GROUP BY year, s_nation, p_category ORDER BY year ASC, s_nation ASC, p_category ASC; 30 rows in set (0.72 sec)50 rows in set (0.49 sec) Q4.3123456789SELECT year(lo_orderdate) AS year, s_city, p_brand, sum(lo_revenue - lo_supplycost) AS profit FROM lineorder_flat WHERE s_nation = &#x27;UNITED STATES&#x27; AND lo_orderdate &gt;= &#x27;1997-01-01&#x27; and lo_orderdate &lt;= &#x27;1998-12-31&#x27; AND p_category = &#x27;MFGR#14&#x27; GROUP BY year, s_city, p_brand ORDER BY year ASC, s_city ASC, p_brand ASC; 400 rows in set (0.51 sec)400 rows in set (0.38 sec) 多表关联测试执行 1set global parallel_fragment_exec_instance_num = 8; Q1.1123456select sum(lo_revenue) as revenuefrom lineorder join dates on lo_orderdate = d_datekeywhere d_year = 1993 and lo_discount between 1 and 3 and lo_quantity &lt; 25;1 row in set (0.45 sec)1 row in set (0.49 sec) Q1.2123456789select sum(lo_revenue) as revenuefrom lineorderjoin dates on lo_orderdate = d_datekeywhere d_yearmonthnum = 199401and lo_discount between 4 and 6and lo_quantity between 26 and 35;1 row in set (0.41 sec)1 row in set (0.45 sec) Q1.3123456789select sum(lo_revenue) as revenuefrom lineorderjoin dates on lo_orderdate = d_datekeywhere d_weeknuminyear = 6 and d_year = 1994and lo_discount between 5 and 7and lo_quantity between 26 and 35;1 row in set (0.51 sec)1 row in set (0.34 sec) Q2.11234567891011select sum(lo_revenue) as lo_revenue, d_year, p_brandfrom lineorderinner join dates on lo_orderdate = d_datekeyjoin part on lo_partkey = p_partkeyjoin supplier on lo_suppkey = s_suppkeywhere p_category = &#x27;MFGR#12&#x27; and s_region = &#x27;AMERICA&#x27;group by d_year, p_brandorder by d_year, p_brand;280 rows in set (1.56 sec)280 rows in set (1.60 sec) Q2.21234567891011select sum(lo_revenue) as lo_revenue, d_year, p_brandfrom lineorderjoin dates on lo_orderdate = d_datekeyjoin part on lo_partkey = p_partkeyjoin supplier on lo_suppkey = s_suppkeywhere p_brand between &#x27;MFGR#2221&#x27; and &#x27;MFGR#2228&#x27; and s_region = &#x27;ASIA&#x27;group by d_year, p_brandorder by d_year, p_brand;56 rows in set (1.69 sec)56 rows in set (1.06 sec) Q2.31234567891011select sum(lo_revenue) as lo_revenue, d_year, p_brandfrom lineorderjoin dates on lo_orderdate = d_datekeyjoin part on lo_partkey = p_partkeyjoin supplier on lo_suppkey = s_suppkeywhere p_brand = &#x27;MFGR#2239&#x27; and s_region = &#x27;EUROPE&#x27;group by d_year, p_brandorder by d_year, p_brand;7 rows in set (0.78 sec)7 rows in set (1.15 sec) Q3.11234567891011select c_nation, s_nation, d_year, sum(lo_revenue) as lo_revenuefrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeywhere c_region = &#x27;ASIA&#x27; and s_region = &#x27;ASIA&#x27;and d_year &gt;= 1992 and d_year &lt;= 1997group by c_nation, s_nation, d_yearorder by d_year asc, lo_revenue desc;150 rows in set (3.48 sec)150 rows in set (3.70 sec) Q3.2123456789101112select c_city, s_city, d_year, sum(lo_revenue) as lo_revenuefrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeywhere c_nation = &#x27;UNITED STATES&#x27; and s_nation = &#x27;UNITED STATES&#x27;and d_year &gt;= 1992 and d_year &lt;= 1997group by c_city, s_city, d_yearorder by d_year asc, lo_revenue desc;600 rows in set (1.32 sec)600 rows in set (1.85 sec) Q3.312345678910111213select c_city, s_city, d_year, sum(lo_revenue) as lo_revenuefrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeywhere (c_city=&#x27;UNITED KI1&#x27; or c_city=&#x27;UNITED KI5&#x27;)and (s_city=&#x27;UNITED KI1&#x27; or s_city=&#x27;UNITED KI5&#x27;)and d_year &gt;= 1992 and d_year &lt;= 1997group by c_city, s_city, d_yearorder by d_year asc, lo_revenue desc;24 rows in set (1.03 sec)24 rows in set (1.04 sec) Q3.4123456789101112select c_city, s_city, d_year, sum(lo_revenue) as lo_revenuefrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeywhere (c_city=&#x27;UNITED KI1&#x27; or c_city=&#x27;UNITED KI5&#x27;) and (s_city=&#x27;UNITED KI1&#x27; or s_city=&#x27;UNITED KI5&#x27;) and d_yearmonth = &#x27;Dec1997&#x27;group by c_city, s_city, d_yearorder by d_year asc, lo_revenue desc;4 rows in set (1.33 sec)4 rows in set (1.17 sec) Q4.1123456789101112select d_year, c_nation, sum(lo_revenue) - sum(lo_supplycost) as profitfrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeyjoin part on lo_partkey = p_partkeywhere c_region = &#x27;AMERICA&#x27; and s_region = &#x27;AMERICA&#x27; and (p_mfgr = &#x27;MFGR#1&#x27; or p_mfgr = &#x27;MFGR#2&#x27;)group by d_year, c_nationorder by d_year, c_nation;35 rows in set (3.48 sec)35 rows in set (3.75 sec) Q4.21234567891011121314select d_year, s_nation, p_category, sum(lo_revenue) - sum(lo_supplycost) as profitfrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeyjoin part on lo_partkey = p_partkeywhere c_region = &#x27;AMERICA&#x27;and s_region = &#x27;AMERICA&#x27;and (d_year = 1997 or d_year = 1998)and (p_mfgr = &#x27;MFGR#1&#x27; or p_mfgr = &#x27;MFGR#2&#x27;)group by d_year, s_nation, p_categoryorder by d_year, s_nation, p_category;100 rows in set (2.83 sec)100 rows in set (3.17 sec) Q4.31234567891011121314select d_year, s_city, p_brand, sum(lo_revenue) - sum(lo_supplycost) as profitfrom lineorderjoin dates on lo_orderdate = d_datekeyjoin customer on lo_custkey = c_custkeyjoin supplier on lo_suppkey = s_suppkeyjoin part on lo_partkey = p_partkeywhere c_region = &#x27;AMERICA&#x27;and s_nation = &#x27;UNITED STATES&#x27;and (d_year = 1997 or d_year = 1998)and p_category = &#x27;MFGR#14&#x27;group by d_year, s_city, p_brandorder by d_year, s_city, p_brand;800 rows in set (1.56 sec)800 rows in set (2.14 sec) ClickHouse构建数据123456789$ git clone https://github.com/vadimtk/ssb-dbgen.git$ cd ssb-dbgen$ make$ ./dbgen -s 100 -T c$ ./dbgen -s 100 -T l$ ./dbgen -s 100 -T p$ ./dbgen -s 100 -T s$ ./dbgen -s 100 -T d 建表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138CREATE DATABASE ssb on cluster ck_cluster;CREATE TABLE ssb.customer_local on cluster ck_cluster( C_CUSTKEY UInt32, C_NAME String, C_ADDRESS String, C_CITY LowCardinality(String), C_NATION LowCardinality(String), C_REGION LowCardinality(String), C_PHONE String, C_MKTSEGMENT LowCardinality(String))ENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/customer&#x27;, &#x27;&#123;replica&#125;&#x27;) ORDER BY (C_CUSTKEY) SETTINGS index_granularity = 8192;CREATE TABLE ssb.customer on cluster ck_cluster AS ssb.customer_local ENGINE = Distributed( ck_cluster, ssb, customer_local, rand() );CREATE TABLE ssb.lineorder_local on cluster ck_cluster( LO_ORDERKEY UInt32, LO_LINENUMBER UInt8, LO_CUSTKEY UInt32, LO_PARTKEY UInt32, LO_SUPPKEY UInt32, LO_ORDERDATE Date, LO_ORDERPRIORITY LowCardinality(String), LO_SHIPPRIORITY UInt8, LO_QUANTITY UInt8, LO_EXTENDEDPRICE UInt32, LO_ORDTOTALPRICE UInt32, LO_DISCOUNT UInt8, LO_REVENUE UInt32, LO_SUPPLYCOST UInt32, LO_TAX UInt8, LO_COMMITDATE Date, LO_SHIPMODE LowCardinality(String))ENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/lineorder&#x27;, &#x27;&#123;replica&#125;&#x27;) PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY)SETTINGS index_granularity = 8192;CREATE TABLE ssb.lineorder on cluster ck_cluster AS ssb.lineorder_local ENGINE = Distributed( ck_cluster, ssb, lineorder_local, rand() );CREATE TABLE ssb.part_local on cluster ck_cluster( P_PARTKEY UInt32, P_NAME String, P_MFGR LowCardinality(String), P_CATEGORY LowCardinality(String), P_BRAND LowCardinality(String), P_COLOR LowCardinality(String), P_TYPE LowCardinality(String), P_SIZE UInt8, P_CONTAINER LowCardinality(String))ENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/part&#x27;, &#x27;&#123;replica&#125;&#x27;) ORDER BY P_PARTKEY SETTINGS index_granularity = 8192;CREATE TABLE ssb.part on cluster ck_cluster AS ssb.part_local ENGINE = Distributed( ck_cluster, ssb, part_local, rand() );CREATE TABLE ssb.supplier_local on cluster ck_cluster( S_SUPPKEY UInt32, S_NAME String, S_ADDRESS String, S_CITY LowCardinality(String), S_NATION LowCardinality(String), S_REGION LowCardinality(String), S_PHONE String)ENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/supplier&#x27;, &#x27;&#123;replica&#125;&#x27;) ORDER BY S_SUPPKEY SETTINGS index_granularity = 8192;CREATE TABLE ssb.supplier on cluster ck_cluster AS ssb.supplier_local ENGINE = Distributed( ck_cluster, ssb, supplier_local, rand() );CREATE TABLE ssb.dates_local on cluster ck_cluster( D_DATEKEY UInt32, D_DATE String, D_DAYOFWEEK String, D_MONTH String, D_YEAR UInt32, D_YEARMONTHNUM UInt32, D_YEARMONTH String, D_DAYNUMINWEEK UInt32, D_DAYNUMINMONTH UInt32, D_DAYNUMINYEAR UInt32, D_MONTHNUMINYEAR UInt32, D_WEEKNUMINYEAR UInt32, D_SELLINGSEASON String, D_LASTDAYINWEEKFL UInt32, D_LASTDAYINMONTHFL UInt32, D_HOLIDAYFL UInt32, D_WEEKDAYFL UInt32)ENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/dates&#x27;, &#x27;&#123;replica&#125;&#x27;) ORDER BY D_DATEKEY SETTINGS index_granularity = 8192;CREATE TABLE ssb.dates on cluster ck_cluster AS ssb.dates_local ENGINE = Distributed( ck_cluster, ssb, dates_local, rand() ); 导入数据12345678cd ssb-dbgenclickhouse-client --database=ssb --query &quot;INSERT INTO customer FORMAT CSV&quot; &lt; customer.tblclickhouse-client --database=ssb --query &quot;INSERT INTO part FORMAT CSV&quot; &lt; part.tblclickhouse-client --database=ssb --query &quot;INSERT INTO supplier FORMAT CSV&quot; &lt; supplier.tblclickhouse-client --database=ssb --query &quot;INSERT INTO lineorder FORMAT CSV&quot; &lt; lineorder.tblclickhouse-client --database=ssb --query &quot;INSERT INTO dates FORMAT CSV&quot; &lt; date.tbl以上用时不到600s Converting “star schema” to denormalized “flat schema”:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990在一个节点执行set max_bytes_before_external_group_by=2000000000;set max_memory_usage=4000000000;CREATE TABLE lineorder_flat_tmpENGINE = MergeTreePARTITION BY toYear(LO_ORDERDATE)ORDER BY (LO_ORDERDATE, LO_ORDERKEY) ASSELECT l.LO_ORDERKEY AS LO_ORDERKEY, l.LO_LINENUMBER AS LO_LINENUMBER, l.LO_CUSTKEY AS LO_CUSTKEY, l.LO_PARTKEY AS LO_PARTKEY, l.LO_SUPPKEY AS LO_SUPPKEY, l.LO_ORDERDATE AS LO_ORDERDATE, l.LO_ORDERPRIORITY AS LO_ORDERPRIORITY, l.LO_SHIPPRIORITY AS LO_SHIPPRIORITY, l.LO_QUANTITY AS LO_QUANTITY, l.LO_EXTENDEDPRICE AS LO_EXTENDEDPRICE, l.LO_ORDTOTALPRICE AS LO_ORDTOTALPRICE, l.LO_DISCOUNT AS LO_DISCOUNT, l.LO_REVENUE AS LO_REVENUE, l.LO_SUPPLYCOST AS LO_SUPPLYCOST, l.LO_TAX AS LO_TAX, l.LO_COMMITDATE AS LO_COMMITDATE, l.LO_SHIPMODE AS LO_SHIPMODE, c.C_NAME AS C_NAME, c.C_ADDRESS AS C_ADDRESS, c.C_CITY AS C_CITY, c.C_NATION AS C_NATION, c.C_REGION AS C_REGION, c.C_PHONE AS C_PHONE, c.C_MKTSEGMENT AS C_MKTSEGMENT, s.S_NAME AS S_NAME, s.S_ADDRESS AS S_ADDRESS, s.S_CITY AS S_CITY, s.S_NATION AS S_NATION, s.S_REGION AS S_REGION, s.S_PHONE AS S_PHONE, p.P_NAME AS P_NAME, p.P_MFGR AS P_MFGR, p.P_CATEGORY AS P_CATEGORY, p.P_BRAND AS P_BRAND, p.P_COLOR AS P_COLOR, p.P_TYPE AS P_TYPE, p.P_SIZE AS P_SIZE, p.P_CONTAINER AS P_CONTAINERFROM lineorder AS lINNER JOIN customer AS c ON c.C_CUSTKEY = l.LO_CUSTKEYINNER JOIN supplier AS s ON s.S_SUPPKEY = l.LO_SUPPKEYINNER JOIN part AS p ON p.P_PARTKEY = l.LO_PARTKEY;0 rows in set. Elapsed: 2086.025 sec. Processed 604.64 million rows, 26.12 GB (289.85 thousand rows/s., 12.52 MB/s.) bj2-all-clickhouse-test-01 :) select count(*) from lineorder_flat;SELECT count(*)FROM lineorder_flat┌───count()─┐│ 600037902 │└───────────┘1 rows in set. Elapsed: 0.006 sec. CREATE TABLE ssb.lineorder_flat_local on cluster ck_clusterAS ssb.lineorder_flat_tmpENGINE = ReplicatedMergeTree( &#x27;/clickhouse/ssb/tables/&#123;layer&#125;-&#123;shard&#125;/lineorder_flat&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toYear(LO_ORDERDATE)ORDER BY (LO_ORDERDATE, LO_ORDERKEY)CREATE TABLE ssb.lineorder_flat on cluster ck_cluster AS ssb.lineorder_flat_local ENGINE = Distributed( ck_cluster, ssb, lineorder_flat_local, rand() ); 在三个分片执行:INSERT INTO ssb.lineorder_flat SELECT * from ssb_local.lineorder_flat_tmp; Ok.0 rows in set. Elapsed: 747.548 sec. Processed 600.04 million rows, 140.40 GB (802.67 thousand rows/s., 187.82 MB/s.) clickhouse数据导入用时总计约3500秒 最终数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465bj2-all-clickhouse-test-01 :) select count(*) from customer ;SELECT count(*)FROM customer┌─count()─┐│ 3000000 │└─────────┘1 rows in set. Elapsed: 0.004 sec. bj2-all-clickhouse-test-01 :) select count(*) from dates ;SELECT count(*)FROM dates┌─count()─┐│ 2556 │└─────────┘1 rows in set. Elapsed: 0.004 sec. bj2-all-clickhouse-test-01 :) select count(*) from lineorder ;SELECT count(*)FROM lineorder┌───count()─┐│ 600037902 │└───────────┘1 rows in set. Elapsed: 0.003 sec. bj2-all-clickhouse-test-01 :) select count(*) from lineorder_flat ;SELECT count(*)FROM lineorder_flat┌───count()─┐│ 622259902 │└───────────┘1 rows in set. Elapsed: 0.003 sec. bj2-all-clickhouse-test-01 :) select count(*) from part ;SELECT count(*)FROM part┌─count()─┐│ 1400000 │└─────────┘1 rows in set. Elapsed: 0.003 sec. bj2-all-clickhouse-test-01 :) select count(*) from supplier ;SELECT count(*)FROM supplier┌─count()─┐│ 200000 │└─────────┘1 rows in set. Elapsed: 0.004 sec. 数据占用空间12345678910111213141516171819202122232425262728293031[root@bj2-all-clickhouse-test-01 ssb]# lltotal 48lrwxrwxrwx 1 root root 69 Feb 9 10:51 customer -&gt; /data/clickhouse/node1/store/a61/a61a9f88-8bbb-4864-bd0a-1001f5ffcc1clrwxrwxrwx 1 root root 69 Feb 9 10:51 customer_local -&gt; /data/clickhouse/node1/store/c86/c86c8026-f804-4f93-9a2b-051d0fbc1cc9lrwxrwxrwx 1 root root 69 Feb 9 12:55 dates -&gt; /data/clickhouse/node1/store/95d/95db2800-91b5-44f3-b378-727bc80d25bclrwxrwxrwx 1 root root 69 Feb 9 12:55 dates_local -&gt; /data/clickhouse/node1/store/137/1371f45a-88bc-4d14-9f04-07895fd561balrwxrwxrwx 1 root root 69 Feb 9 11:33 lineorder -&gt; /data/clickhouse/node1/store/8f4/8f40504f-4e94-4d70-a1de-dbcb6d25d616lrwxrwxrwx 1 root root 69 Feb 9 11:54 lineorder_flat -&gt; /data/clickhouse/node1/store/9fd/9fdb7d2c-d2c2-49a0-ad7e-380fc76dff73lrwxrwxrwx 1 root root 69 Feb 9 11:49 lineorder_flat_local -&gt; /data/clickhouse/node1/store/883/883a4a97-f34b-491b-a305-398bd717cfb9lrwxrwxrwx 1 root root 69 Feb 9 10:52 lineorder_local -&gt; /data/clickhouse/node1/store/2d6/2d67810e-dfbc-438d-b7aa-6cbfee4c391flrwxrwxrwx 1 root root 69 Feb 9 11:36 part -&gt; /data/clickhouse/node1/store/45b/45b4a779-a2df-4048-a266-efc1481fce68lrwxrwxrwx 1 root root 69 Feb 9 10:53 part_local -&gt; /data/clickhouse/node1/store/83f/83f3ac5d-296b-424f-900e-f13d67d044aelrwxrwxrwx 1 root root 69 Feb 9 11:36 supplier -&gt; /data/clickhouse/node1/store/6a0/6a0b538a-8f79-4d24-bae0-82792fe8af19lrwxrwxrwx 1 root root 69 Feb 9 10:54 supplier_local -&gt; /data/clickhouse/node1/store/5e2/5e29e02d-e35b-4660-a5b9-1bfa1d9fb97d[root@bj2-all-clickhouse-test-01 ssb]# ll |awk -F&#x27;-&gt;&#x27; &#x27;&#123;print $2&#125;&#x27;|xargs du -sh20K /data/clickhouse/node1/store/a61/a61a9f88-8bbb-4864-bd0a-1001f5ffcc1c39M /data/clickhouse/node1/store/c86/c86c8026-f804-4f93-9a2b-051d0fbc1cc920K /data/clickhouse/node1/store/95d/95db2800-91b5-44f3-b378-727bc80d25bc64K /data/clickhouse/node1/store/137/1371f45a-88bc-4d14-9f04-07895fd561ba20K /data/clickhouse/node1/store/8f4/8f40504f-4e94-4d70-a1de-dbcb6d25d61620K /data/clickhouse/node1/store/9fd/9fdb7d2c-d2c2-49a0-ad7e-380fc76dff7322G /data/clickhouse/node1/store/883/883a4a97-f34b-491b-a305-398bd717cfb96.4G /data/clickhouse/node1/store/2d6/2d67810e-dfbc-438d-b7aa-6cbfee4c391f20K /data/clickhouse/node1/store/45b/45b4a779-a2df-4048-a266-efc1481fce688.3M /data/clickhouse/node1/store/83f/83f3ac5d-296b-424f-900e-f13d67d044ae20K /data/clickhouse/node1/store/6a0/6a0b538a-8f79-4d24-bae0-82792fe8af192.6M /data/clickhouse/node1/store/5e2/5e29e02d-e35b-4660-a5b9-1bfa1d9fb97d约 28.4G*3=85.2G 单表查询测试执行 123456789set max_threads=4;SELECT *FROM system.settingsWHERE name = &#x27;max_threads&#x27;┌─name────────┬─value─┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┬─type───────┐│ max_threads │ 4 │ 1 │ The maximum number of threads to execute the request. By default, it is determined automatically. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ MaxThreads │└─────────────┴───────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┴────────────┘ Q1.112345678910SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenueFROM lineorder_flatWHERE (toYear(LO_ORDERDATE) = 1993) AND ((LO_DISCOUNT &gt;= 1) AND (LO_DISCOUNT &lt;= 3)) AND (LO_QUANTITY &lt; 25)┌────────revenue─┐│ 46310268722641 │└────────────────┘1 rows in set. Elapsed: 0.226 sec. Processed 94.38 million rows, 755.01 MB (417.85 million rows/s., 3.34 GB/s.) 1 rows in set. Elapsed: 0.195 sec. Processed 94.38 million rows, 755.01 MB (484.98 million rows/s., 3.88 GB/s.) Q1.21234567891011SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenueFROM lineorder_flatWHERE (toYYYYMM(LO_ORDERDATE) = 199401) AND ((LO_DISCOUNT &gt;= 4) AND (LO_DISCOUNT &lt;= 6)) AND ((LO_QUANTITY &gt;= 26) AND (LO_QUANTITY &lt;= 35))┌───────revenue─┐│ 9979491062883 │└───────────────┘1 rows in set. Elapsed: 0.034 sec. Processed 8.07 million rows, 64.55 MB (239.67 million rows/s., 1.92 GB/s.) 1 rows in set. Elapsed: 0.063 sec. Processed 8.07 million rows, 64.55 MB (127.89 million rows/s., 1.02 GB/s.) Q1.312345678910SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenueFROM lineorder_flatWHERE (toISOWeek(LO_ORDERDATE) = 6) AND (toYear(LO_ORDERDATE) = 1994) AND ((LO_DISCOUNT &gt;= 5) AND (LO_DISCOUNT &lt;= 7)) AND ((LO_QUANTITY &gt;= 26) AND (LO_QUANTITY &lt;= 35))┌───────revenue─┐│ 2709633167278 │└───────────────┘1 rows in set. Elapsed: 0.043 sec. Processed 2.03 million rows, 16.23 MB (46.78 million rows/s., 374.26 MB/s.) 1 rows in set. Elapsed: 0.022 sec. Processed 2.03 million rows, 16.23 MB (91.09 million rows/s., 728.68 MB/s.) Q2.11234567891011121314151617SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRANDFROM lineorder_flatWHERE P_CATEGORY = &#x27;MFGR#12&#x27; AND S_REGION = &#x27;AMERICA&#x27;GROUP BY year, P_BRANDORDER BY year, P_BRAND;...280 rows in set. Elapsed: 1.723 sec. Processed 622.26 million rows, 6.42 GB (361.17 million rows/s., 3.73 GB/s.) 280 rows in set. Elapsed: 1.791 sec. Processed 622.26 million rows, 6.42 GB (347.43 million rows/s., 3.59 GB/s.) Q2.21234567891011121314151617SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRANDFROM lineorder_flatWHERE P_BRAND &gt;= &#x27;MFGR#2221&#x27; AND P_BRAND &lt;= &#x27;MFGR#2228&#x27; AND S_REGION = &#x27;ASIA&#x27;GROUP BY year, P_BRANDORDER BY year, P_BRAND;...56 rows in set. Elapsed: 1.463 sec. Processed 622.26 million rows, 5.80 GB (425.29 million rows/s., 3.96 GB/s.) 56 rows in set. Elapsed: 1.470 sec. Processed 622.26 million rows, 5.80 GB (423.20 million rows/s., 3.94 GB/s.) Q2.312345678910111213141516171819202122232425SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRANDFROM lineorder_flatWHERE (P_BRAND = &#x27;MFGR#2239&#x27;) AND (S_REGION = &#x27;EUROPE&#x27;)GROUP BY year, P_BRANDORDER BY year ASC, P_BRAND ASC┌─sum(LO_REVENUE)─┬─year─┬─P_BRAND───┐│ 68153063027 │ 1992 │ MFGR#2239 ││ 67009399598 │ 1993 │ MFGR#2239 ││ 67185675813 │ 1994 │ MFGR#2239 ││ 67988181065 │ 1995 │ MFGR#2239 ││ 67265572303 │ 1996 │ MFGR#2239 ││ 66922128523 │ 1997 │ MFGR#2239 ││ 38630664245 │ 1998 │ MFGR#2239 │└─────────────────┴──────┴───────────┘7 rows in set. Elapsed: 0.659 sec. Processed 622.26 million rows, 5.80 GB (943.74 million rows/s., 8.79 GB/s.) 7 rows in set. Elapsed: 1.337 sec. Processed 622.26 million rows, 5.80 GB (465.51 million rows/s., 4.34 GB/s.) Q3.112345678910111213141516171819SELECT C_NATION, S_NATION, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenueFROM lineorder_flatWHERE C_REGION = &#x27;ASIA&#x27; AND S_REGION = &#x27;ASIA&#x27; AND year &gt;= 1992 AND year &lt;= 1997GROUP BY C_NATION, S_NATION, yearORDER BY year ASC, revenue DESC;...150 rows in set. Elapsed: 2.488 sec. Processed 566.91 million rows, 5.68 GB (227.84 million rows/s., 2.28 GB/s.) 150 rows in set. Elapsed: 1.254 sec. Processed 566.91 million rows, 5.68 GB (452.10 million rows/s., 4.53 GB/s.) Q3.212345678910111213141516171819SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenueFROM lineorder_flatWHERE C_NATION = &#x27;UNITED STATES&#x27; AND S_NATION = &#x27;UNITED STATES&#x27; AND year &gt;= 1992 AND year &lt;= 1997GROUP BY C_CITY, S_CITY, yearORDER BY year ASC, revenue DESC;...600 rows in set. Elapsed: 1.272 sec. Processed 566.91 million rows, 5.77 GB (445.80 million rows/s., 4.54 GB/s.) 600 rows in set. Elapsed: 0.966 sec. Processed 566.91 million rows, 5.77 GB (587.07 million rows/s., 5.98 GB/s.) Q3.312345678910111213141516171819SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenueFROM lineorder_flatWHERE ((C_CITY = &#x27;UNITED KI1&#x27;) OR (C_CITY = &#x27;UNITED KI5&#x27;)) AND ((S_CITY = &#x27;UNITED KI1&#x27;) OR (S_CITY = &#x27;UNITED KI5&#x27;)) AND (year &gt;= 1992) AND (year &lt;= 1997)GROUP BY C_CITY, S_CITY, yearORDER BY year ASC, revenue DESC...24 rows in set. Elapsed: 0.979 sec. Processed 566.91 million rows, 4.63 GB (579.03 million rows/s., 4.73 GB/s.)24 rows in set. Elapsed: 0.889 sec. Processed 566.91 million rows, 4.63 GB (637.83 million rows/s., 5.21 GB/s.) Q3.4123456789101112131415161718192021222324SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenueFROM lineorder_flatWHERE ((C_CITY = &#x27;UNITED KI1&#x27;) OR (C_CITY = &#x27;UNITED KI5&#x27;)) AND ((S_CITY = &#x27;UNITED KI1&#x27;) OR (S_CITY = &#x27;UNITED KI5&#x27;)) AND (toYYYYMM(LO_ORDERDATE) = 199712)GROUP BY C_CITY, S_CITY, yearORDER BY year ASC, revenue DESC┌─C_CITY─────┬─S_CITY─────┬─year─┬───revenue─┐│ UNITED KI1 │ UNITED KI1 │ 1997 │ 495955110 ││ UNITED KI5 │ UNITED KI5 │ 1997 │ 421714882 ││ UNITED KI5 │ UNITED KI1 │ 1997 │ 387387637 ││ UNITED KI1 │ UNITED KI5 │ 1997 │ 380708672 │└────────────┴────────────┴──────┴───────────┘4 rows in set. Elapsed: 0.036 sec. Processed 8.09 million rows, 66.07 MB (221.65 million rows/s., 1.81 GB/s.) 4 rows in set. Elapsed: 0.020 sec. Processed 8.09 million rows, 66.07 MB (402.92 million rows/s., 3.29 GB/s.) Q4.11234567891011121314151617SELECT toYear(LO_ORDERDATE) AS year, C_NATION, sum(LO_REVENUE - LO_SUPPLYCOST) AS profitFROM lineorder_flatWHERE (C_REGION = &#x27;AMERICA&#x27;) AND (S_REGION = &#x27;AMERICA&#x27;) AND ((P_MFGR = &#x27;MFGR#1&#x27;) OR (P_MFGR = &#x27;MFGR#2&#x27;))GROUP BY year, C_NATIONORDER BY year ASC, C_NATION ASC...35 rows in set. Elapsed: 5.067 sec. Processed 622.26 million rows, 8.72 GB (122.80 million rows/s., 1.72 GB/s.) 35 rows in set. Elapsed: 2.791 sec. Processed 622.26 million rows, 8.72 GB (222.97 million rows/s., 3.12 GB/s.) Q4.2123456789101112131415161718192021SELECT toYear(LO_ORDERDATE) AS year, S_NATION, P_CATEGORY, sum(LO_REVENUE - LO_SUPPLYCOST) AS profitFROM lineorder_flatWHERE C_REGION = &#x27;AMERICA&#x27; AND S_REGION = &#x27;AMERICA&#x27; AND (year = 1997 OR year = 1998) AND (P_MFGR = &#x27;MFGR#1&#x27; OR P_MFGR = &#x27;MFGR#2&#x27;)GROUP BY year, S_NATION, P_CATEGORYORDER BY year ASC, S_NATION ASC, P_CATEGORY ASC;...100 rows in set. Elapsed: 0.804 sec. Processed 149.77 million rows, 2.25 GB (186.21 million rows/s., 2.80 GB/s.) 100 rows in set. Elapsed: 0.752 sec. Processed 149.77 million rows, 2.25 GB (199.13 million rows/s., 2.99 GB/s.) Q4.31234567891011121314151617181920SELECT toYear(LO_ORDERDATE) AS year, S_CITY, P_BRAND, sum(LO_REVENUE - LO_SUPPLYCOST) AS profitFROM lineorder_flatWHERE S_NATION = &#x27;UNITED STATES&#x27; AND (year = 1997 OR year = 1998) AND P_CATEGORY = &#x27;MFGR#14&#x27;GROUP BY year, S_CITY, P_BRANDORDER BY year ASC, S_CITY ASC, P_BRAND ASC;...800 rows in set. Elapsed: 0.561 sec. Processed 149.77 million rows, 2.32 GB (266.84 million rows/s., 4.13 GB/s.) 800 rows in set. Elapsed: 0.483 sec. Processed 149.77 million rows, 2.32 GB (309.80 million rows/s., 4.80 GB/s.) 多表关联测试在DorisDB vs Clickhouse SSB性能测试对比报告, 不知道是不是疏漏了, Doris在测试时设置了1set global parallel_fragment_exec_instance_num = 8;但没有写有没有在clickhouse测试时设置SETTINGS max_threads=8;由于我的环境只有四个4 cpu, 所以我这里设置为4.通过以下命令可知, lineorder每个分片数据文件大概6.4G(压缩后), 我的环境内存只有8G, 不确定能否都缓存了 这里还需要注意两个问题: clickhouse分布式表相比单表关联更加不友好, JOIN要改为GLOBAL JOIN JOIN操作时一定要把数据量小的表放在右边(这会导致SQL改写更加麻烦) 详见ClickHouse查询分布式表LEFT JOIN改RIGHT JOIN的大坑 所以我对DorisDB vs Clickhouse SSB性能测试对比报告中的多表关联测试SQL进行了微调 Q1.1123456789101112SELECT SUM(LO_REVENUE) AS REVENUEFROM lineorderGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (D_YEAR = 1993) AND ((LO_DISCOUNT &gt;= 1) AND (LO_DISCOUNT &lt;= 3)) AND (LO_QUANTITY &lt; 25)SETTINGS max_threads = 4┌────────REVENUE─┐│ 21881848590256 │└────────────────┘1 rows in set. Elapsed: 1.496 sec. Processed 600.04 million rows, 4.80 GB (401.10 million rows/s., 3.21 GB/s.) 1 rows in set. Elapsed: 1.424 sec. Processed 600.04 million rows, 4.80 GB (421.33 million rows/s., 3.37 GB/s.) Q1.2123456789101112SELECT SUM(LO_REVENUE) AS REVENUEFROM lineorderGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (D_YEARMONTHNUM = 199401) AND ((LO_DISCOUNT &gt;= 4) AND (LO_DISCOUNT &lt;= 6)) AND ((LO_QUANTITY &gt;= 26) AND (LO_QUANTITY &lt;= 35))SETTINGS max_threads = 4┌───────REVENUE─┐│ 1829705342413 │└───────────────┘1 rows in set. Elapsed: 1.366 sec. Processed 600.04 million rows, 4.80 GB (439.39 million rows/s., 3.52 GB/s.) 1 rows in set. Elapsed: 0.659 sec. Processed 600.04 million rows, 4.80 GB (910.87 million rows/s., 7.29 GB/s.) Q1.3123456789101112SELECT SUM(LO_REVENUE) AS REVENUEFROM lineorderGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (D_WEEKNUMINYEAR = 6) AND (D_YEAR = 1994) AND ((LO_DISCOUNT &gt;= 5) AND (LO_DISCOUNT &lt;= 7)) AND ((LO_QUANTITY &gt;= 26) AND (LO_QUANTITY &lt;= 35))SETTINGS max_threads = 4┌──────REVENUE─┐│ 407995993835 │└──────────────┘1 rows in set. Elapsed: 0.678 sec. Processed 600.04 million rows, 4.80 GB (885.40 million rows/s., 7.08 GB/s.) 1 rows in set. Elapsed: 1.377 sec. Processed 600.04 million rows, 4.80 GB (435.84 million rows/s., 3.49 GB/s.) Q2.11234567891011121314SELECT SUM(LO_REVENUE) AS LO_REVENUE, D_YEAR, P_BRANDFROM lineorderGLOBAL JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY),&#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE P_CATEGORY = &#x27;MFGR#12&#x27; AND S_REGION = &#x27;AMERICA&#x27;GROUP BY D_YEAR, P_BRANDORDER BY D_YEAR, P_BRANDSETTINGS max_threads = 4;...280 rows in set. Elapsed: 4.367 sec. Processed 601.81 million rows, 8.45 GB (137.81 million rows/s., 1.94 GB/s.) 280 rows in set. Elapsed: 2.667 sec. Processed 601.81 million rows, 8.45 GB (225.69 million rows/s., 3.17 GB/s.) Q2.21234567891011121314SELECT SUM(LO_REVENUE) AS LO_REVENUE, D_YEAR, P_BRANDFROM lineorderGLOBAL JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY),&#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE P_BRAND BETWEEN &#x27;MFGR#2221&#x27; AND &#x27;MFGR#2228&#x27; AND S_REGION = &#x27;ASIA&#x27;GROUP BY D_YEAR, P_BRANDORDER BY D_YEAR, P_BRANDSETTINGS max_threads = 4;...56 rows in set. Elapsed: 4.498 sec. Processed 601.67 million rows, 8.45 GB (133.78 million rows/s., 1.88 GB/s.) 56 rows in set. Elapsed: 1.554 sec. Processed 601.67 million rows, 8.45 GB (387.23 million rows/s., 5.44 GB/s.) Q2.3123456789101112131415161718192021222324252627282930SELECT SUM(LO_REVENUE) AS LO_REVENUE, D_YEAR, P_BRANDFROM lineorderGLOBAL INNER JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (P_BRAND = &#x27;MFGR#2239&#x27;) AND (S_REGION = &#x27;EUROPE&#x27;)GROUP BY D_YEAR, P_BRANDORDER BY D_YEAR ASC, P_BRAND ASCSETTINGS max_threads = 4┌──LO_REVENUE─┬─D_YEAR─┬─P_BRAND───┐│ 65751589723 │ 1992 │ MFGR#2239 ││ 64532844801 │ 1993 │ MFGR#2239 ││ 64722599002 │ 1994 │ MFGR#2239 ││ 65616432683 │ 1995 │ MFGR#2239 ││ 64802884686 │ 1996 │ MFGR#2239 ││ 64485541165 │ 1997 │ MFGR#2239 ││ 37276536361 │ 1998 │ MFGR#2239 │└─────────────┴────────┴───────────┘7 rows in set. Elapsed: 2.569 sec. Processed 601.64 million rows, 8.45 GB (234.18 million rows/s., 3.29 GB/s.) 7 rows in set. Elapsed: 2.577 sec. Processed 601.64 million rows, 8.45 GB (233.47 million rows/s., 3.28 GB/s.) Q3.1123456789101112131415SELECT C_NATION, S_NATION, D_YEAR, SUM(LO_REVENUE) AS LO_REVENUEFROM lineorderGLOBAL JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY),&#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE C_REGION = &#x27;ASIA&#x27; AND S_REGION = &#x27;ASIA&#x27;AND D_YEAR &gt;= 1992 AND D_YEAR &lt;= 1997GROUP BY C_NATION, S_NATION, D_YEARORDER BY D_YEAR ASC, LO_REVENUE DESCSETTINGS max_threads = 4;...150 rows in set. Elapsed: 10.190 sec. Processed 605.04 million rows, 8.66 GB (59.38 million rows/s., 850.20 MB/s.) 150 rows in set. Elapsed: 12.960 sec. Processed 605.04 million rows, 8.66 GB (46.69 million rows/s., 668.51 MB/s.) Q3.2123456789101112131415SELECT C_CITY, S_CITY, D_YEAR, SUM(LO_REVENUE) AS LO_REVENUEFROM lineorderGLOBAL JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY),&#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE C_NATION = &#x27;UNITED STATES&#x27; AND S_NATION = &#x27;UNITED STATES&#x27;AND D_YEAR &gt;= 1992 AND D_YEAR &lt;= 1997GROUP BY C_CITY, S_CITY, D_YEARORDER BY D_YEAR ASC, LO_REVENUE DESCSETTINGS max_threads = 4;...600 rows in set. Elapsed: 5.926 sec. Processed 603.60 million rows, 8.66 GB (101.85 million rows/s., 1.46 GB/s.) 600 rows in set. Elapsed: 5.743 sec. Processed 603.60 million rows, 8.66 GB (105.10 million rows/s., 1.51 GB/s.) Q3.31234567891011121314151617181920212223SELECT C_CITY, S_CITY, D_YEAR, SUM(LO_REVENUE) AS LO_REVENUEFROM lineorderGLOBAL INNER JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE ((C_CITY = &#x27;UNITED KI1&#x27;) OR (C_CITY = &#x27;UNITED KI5&#x27;)) AND ((S_CITY = &#x27;UNITED KI1&#x27;) OR (S_CITY = &#x27;UNITED KI5&#x27;)) AND (D_YEAR &gt;= 1992) AND (D_YEAR &lt;= 1997)GROUP BY C_CITY, S_CITY, D_YEARORDER BY D_YEAR ASC, LO_REVENUE DESCSETTINGS max_threads = 4...24 rows in set. Elapsed: 3.445 sec. Processed 603.31 million rows, 8.65 GB (175.11 million rows/s., 2.51 GB/s.) 24 rows in set. Elapsed: 3.300 sec. Processed 603.31 million rows, 8.65 GB (182.79 million rows/s., 2.62 GB/s.) Q3.41234567891011121314151617181920212223242526272829SELECT C_CITY, S_CITY, D_YEAR, SUM(LO_REVENUE) AS LO_REVENUEFROM lineorderGLOBAL INNER JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE ((C_CITY = &#x27;UNITED KI1&#x27;) OR (C_CITY = &#x27;UNITED KI5&#x27;)) AND ((S_CITY = &#x27;UNITED KI1&#x27;) OR (S_CITY = &#x27;UNITED KI5&#x27;)) AND (D_YEARMONTH = &#x27;Dec1997&#x27;)GROUP BY C_CITY, S_CITY, D_YEARORDER BY D_YEAR ASC, LO_REVENUE DESCSETTINGS max_threads = 4┌─C_CITY─────┬─S_CITY─────┬─D_YEAR─┬─LO_REVENUE─┐│ UNITED KI1 │ UNITED KI1 │ 1997 │ 481119563 ││ UNITED KI5 │ UNITED KI5 │ 1997 │ 386477033 ││ UNITED KI5 │ UNITED KI1 │ 1997 │ 378048353 ││ UNITED KI1 │ UNITED KI5 │ 1997 │ 366630529 │└────────────┴────────────┴────────┴────────────┘4 rows in set. Elapsed: 3.455 sec. Processed 603.31 million rows, 8.65 GB (174.63 million rows/s., 2.50 GB/s.) 4 rows in set. Elapsed: 3.330 sec. Processed 603.31 million rows, 8.65 GB (181.19 million rows/s., 2.60 GB/s. Q4.112345678910111213141516171819202122SELECT D_YEAR, C_NATION, SUM(LO_REVENUE) - SUM(LO_SUPPLYCOST) AS PROFITFROM lineorderGLOBAL INNER JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (C_REGION = &#x27;AMERICA&#x27;) AND (S_REGION = &#x27;AMERICA&#x27;) AND ((P_MFGR = &#x27;MFGR#1&#x27;) OR (P_MFGR = &#x27;MFGR#2&#x27;))GROUP BY D_YEAR, C_NATIONORDER BY D_YEAR ASC, C_NATION ASCSETTINGS max_threads = 4...35 rows in set. Elapsed: 15.560 sec. Processed 606.44 million rows, 13.47 GB (38.97 million rows/s., 865.71 MB/s.) 35 rows in set. Elapsed: 9.494 sec. Processed 606.44 million rows, 13.47 GB (63.88 million rows/s., 1.42 GB/s.) Q4.212345678910111213141516171819202122232425SELECT D_YEAR, S_NATION, P_CATEGORY, SUM(LO_REVENUE) - SUM(LO_SUPPLYCOST) AS PROFITFROM lineorderGLOBAL INNER JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (C_REGION = &#x27;AMERICA&#x27;) AND (S_REGION = &#x27;AMERICA&#x27;) AND ((D_YEAR = 1997) OR (D_YEAR = 1998)) AND ((P_MFGR = &#x27;MFGR#1&#x27;) OR (P_MFGR = &#x27;MFGR#2&#x27;))GROUP BY D_YEAR, S_NATION, P_CATEGORYORDER BY D_YEAR ASC, S_NATION ASC, P_CATEGORY ASCSETTINGS max_threads = 4...100 rows in set. Elapsed: 16.109 sec. Processed 606.44 million rows, 13.47 GB (37.64 million rows/s., 836.01 MB/s.) 100 rows in set. Elapsed: 18.048 sec. Processed 606.44 million rows, 13.47 GB (33.60 million rows/s., 746.35 MB/s.) Q4.312345678910111213141516171819202122232425SELECT D_YEAR, S_CITY, P_BRAND, SUM(LO_REVENUE) - SUM(LO_SUPPLYCOST) AS PROFITFROM lineorderGLOBAL INNER JOIN customer ON LO_CUSTKEY = C_CUSTKEYGLOBAL INNER JOIN supplier ON LO_SUPPKEY = S_SUPPKEYGLOBAL INNER JOIN part ON LO_PARTKEY = P_PARTKEYGLOBAL INNER JOIN dates ON LO_ORDERDATE = toDate(replaceRegexpAll(toString(D_DATEKEY), &#x27;(\\\\d&#123;4&#125;)(\\\\d&#123;2&#125;)(\\\\d&#123;2&#125;)&#x27;, &#x27;\\\\1-\\\\2-\\\\3&#x27;))WHERE (C_REGION = &#x27;AMERICA&#x27;) AND (S_NATION = &#x27;UNITED STATES&#x27;) AND ((D_YEAR = 1997) OR (D_YEAR = 1998)) AND (P_CATEGORY = &#x27;MFGR#14&#x27;)GROUP BY D_YEAR, S_CITY, P_BRANDORDER BY D_YEAR ASC, S_CITY ASC, P_BRAND ASCSETTINGS max_threads = 4...800 rows in set. Elapsed: 15.685 sec. Processed 606.44 million rows, 13.47 GB (38.66 million rows/s., 858.94 MB/s.) 800 rows in set. Elapsed: 14.838 sec. Processed 606.44 million rows, 13.47 GB (40.87 million rows/s., 907.94 MB/s.)","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"},{"name":"DorisDB","slug":"DorisDB","permalink":"http://fuxkdb.com/tags/DorisDB/"}]},{"title":"MHA Failover测试","slug":"2021-02-07-MHA-Failover测试","date":"2021-02-07T14:16:00.000Z","updated":"2021-02-07T14:18:21.289Z","comments":true,"path":"2021/02/07/2021-02-07-MHA-Failover测试/","link":"","permalink":"http://fuxkdb.com/2021/02/07/2021-02-07-MHA-Failover%E6%B5%8B%E8%AF%95/","excerpt":"TL;DR 用例 ping_type=CONNECT ping_type=INSERT master too many connection 不会触发failover 不会触发failover master hang 不会触发failover 会触发failover且成功 仅manager无法连通master 不会触发failover 不会触发failover manager无法连通master, 且无法ssh slave1 不会触发failover 不会触发failover manager无法连通master, 且无法ssh slave1和slave2 不会触发failover 不会触发failover manager无法连通master, ssh到slave1后无法连通master 不会触发failover 不会触发failover manager无法连通master, ssh到slave1和slave2后均无法连通master 会触发failover且成功 会触发failover且成功(长连接断开后才会) master宕机前slave1也宕机了 会触发failover, 但failover失败 会触发failover, 但failover失败 master挂了, 在此之前slave-1 io_thread stop了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 io_thread error了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 sql_thread stop了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 sql_thread error了 会触发failover, 但failover失败 会触发failover, 但failover失败","text":"TL;DR 用例 ping_type=CONNECT ping_type=INSERT master too many connection 不会触发failover 不会触发failover master hang 不会触发failover 会触发failover且成功 仅manager无法连通master 不会触发failover 不会触发failover manager无法连通master, 且无法ssh slave1 不会触发failover 不会触发failover manager无法连通master, 且无法ssh slave1和slave2 不会触发failover 不会触发failover manager无法连通master, ssh到slave1后无法连通master 不会触发failover 不会触发failover manager无法连通master, ssh到slave1和slave2后均无法连通master 会触发failover且成功 会触发failover且成功(长连接断开后才会) master宕机前slave1也宕机了 会触发failover, 但failover失败 会触发failover, 但failover失败 master挂了, 在此之前slave-1 io_thread stop了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 io_thread error了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 sql_thread stop了 会failover且成功 会failover且成功 master挂了, 在此之前slave-1 sql_thread error了 会触发failover, 但failover失败 会触发failover, 但failover失败 环境信息1234master: 172.16.120.10 centos-1 主 + proxysqlslave1: 172.16.120.11 centos-2 从 + proxysqlslave2: 172.16.120.12 centos-3 从 + proxysql172.16.120.13 centos-4 mha manager MHA配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#cat /etc/masterha/conf/masterha_default.cnf [server default]# mysql user and password，此处的密码不能加引号user=mhapassword=xxxx#replication_userrepl_user=replerrepl_password=xxxx#checking master every 3 second ping_interval=3# 使用短连接检测，默认是长连接ping_type=INSERT#ping_type=CONNECT#下面会测试两种type#ssh userssh_user=root#发送邮件脚本report_script=/etc/masterha/scripts/send_report# 节点工作目录remote_workdir=/masterha/#cat /etc/masterha/conf/cls_new.cnf[server default]#workdir on the management servermanager_workdir=/masterha/cls_new/manager_log=/masterha/cls_new/manager.log#workdir on the node for mysql servermaster_binlog_dir=/data/mysql_3358/data/#自动故障VIP切换调用脚本master_ip_failover_script=/etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128#手动故障切换调用脚本master_ip_online_change_script=/etc/masterha/scripts/master_ip_online_change_vip --vip=172.16.120.128#检测master的可用性secondary_check_script=masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12[server1]hostname=172.16.120.10port=3358candidate_master=1[server2]hostname=172.16.120.11port=3358candidate_master=1[server3]hostname=172.16.120.12port=3358candidate_master=1 [用例测试] master too many connectionping_type=CONNECT123456789101112131415161718192021222324252627282930root@localhost 11:43:29 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 7 | | NULL | 0 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 4 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 2 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 14 | | NULL | 0 | 0 || 1256 | repler | 172.16.120.11:59594 | NULL | Binlog Dump GTID | 952922 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1257 | repler | 172.16.120.12:56540 | NULL | Binlog Dump GTID | 952902 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 2 | | NULL | 1 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 120 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 58 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 17 | | NULL | 1 | 0 || 1943 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+11 rows in set (0.00 sec)root@localhost 11:43:30 [dbms_monitor]&gt; show global variables like &#x27;%max_connec%&#x27;;+-----------------------+---------+| Variable_name | Value |+-----------------------+---------+| extra_max_connections | 1 || max_connect_errors | 1000000 || max_connections | 1024 |+-----------------------+---------+3 rows in set (0.01 sec)root@localhost 11:49:34 [dbms_monitor]&gt; set global max_connections=5;Query OK, 0 rows affected (0.01 sec); 结论: 不会failover123456789101112131415161718192021Fri Oct 9 11:42:57 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 11:42:57 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 11:42:57 2020 - [info] OK.Fri Oct 9 11:42:57 2020 - [warning] shutdown_script is not defined.Fri Oct 9 11:42:57 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 11:42:57 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 11:42:57 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 11:42:57 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 11:49:51 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Too many connections at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.1040 (Too many connections)Fri Oct 9 11:49:51 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 11:49:51 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 11:49:51 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Master is reachable from 172.16.120.11!Fri Oct 9 11:49:52 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 11:49:54 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:49:54 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:49:57 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:49:57 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:50:00 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:50:00 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check.. ping_type=INSERT1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950root@localhost 11:55:13 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 1 | | NULL | 0 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 18 | | NULL | 1 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 16 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 8 | | NULL | 0 | 0 || 1256 | repler | 172.16.120.11:59594 | NULL | Binlog Dump GTID | 953626 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1257 | repler | 172.16.120.12:56540 | NULL | Binlog Dump GTID | 953606 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 6 | | NULL | 1 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 103 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 41 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 1 | | NULL | 1 | 0 || 1943 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2160 | mha | 172.16.120.13:34660 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+12 rows in set (0.00 sec)root@localhost 11:55:14 [dbms_monitor]&gt; show global variables like &#x27;%max_connec%&#x27;;+-----------------------+---------+| Variable_name | Value |+-----------------------+---------+| extra_max_connections | 1 || max_connect_errors | 1000000 || max_connections | 1024 |+-----------------------+---------+3 rows in set (0.04 sec)root@localhost 11:55:19 [dbms_monitor]&gt; set global max_connections=5;Query OK, 0 rows affected (0.00 sec)root@localhost 11:55:25 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 6 | | NULL | 0 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 3 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 31 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 3 | | NULL | 1 | 0 || 1256 | repler | 172.16.120.11:59594 | NULL | Binlog Dump GTID | 953641 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1257 | repler | 172.16.120.12:56540 | NULL | Binlog Dump GTID | 953621 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 0 | | NULL | 0 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 118 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 56 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 6 | | NULL | 1 | 0 || 1943 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2160 | mha | 172.16.120.13:34660 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------+-----------+---------------+12 rows in set (0.00 sec) ping_type=INSERT是长连接, 不会感知too many connection. 手动kill掉mha连接 12root@localhost 11:55:29 [dbms_monitor]&gt; kill 2160;Query OK, 0 rows affected (0.01 sec) 结论: 不会failover12345678910111213141516171819202122232425262728Fri Oct 9 11:54:48 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 11:54:48 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 11:54:48 2020 - [info] OK.Fri Oct 9 11:54:48 2020 - [warning] shutdown_script is not defined.Fri Oct 9 11:54:48 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 11:54:48 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 11:54:48 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 11:54:48 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 11:56:42 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 11:56:42 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 11:56:42 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 11:56:43 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.ERROR 1040 (HY000): Too many connectionsMonitoring server 172.16.120.11 is reachable, Master is not writable from 172.16.120.11. OK.ERROR 1040 (HY000): Too many connectionsMonitoring server 172.16.120.12 is reachable, Master is not writable from 172.16.120.12. OK.Fri Oct 9 11:56:43 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Fri Oct 9 11:56:45 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:56:45 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:56:48 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:56:48 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:56:51 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:56:51 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:56:54 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:56:54 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:56:57 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections)Fri Oct 9 11:56:57 2020 - [info] Got MySQL error 1040, but this is not a MySQL crash. Continue health check..Fri Oct 9 11:57:00 2020 - [warning] Got error on MySQL connect: 1040 (Too many connections) [用例测试] master hangping_type=CONNECTmaster hang不好模拟, 这里间接模拟. 需要将ping_select的执行的select 1改为select innodb_table查询一个innodb表 123456789sub ping_select($) &#123; my $self = shift; my $log = $self-&gt;&#123;logger&#125;; my $dbh = $self-&gt;&#123;dbh&#125;; my ( $query, $sth, $href ); eval &#123; $dbh-&gt;&#123;RaiseError&#125; = 1; #$sth = $dbh-&gt;prepare(&quot;SELECT 1 As Value&quot;); $sth = $dbh-&gt;prepare(&quot;SELECT 1 As Value from infra.chk_masterha limit 1&quot;); 然后修改innodb_thread_concurrency值 12root@localhost 12:25:34 [dbms_monitor]&gt; set global innodb_thread_concurrency=1;Query OK, 0 rows affected (0.00 sec) 手动执行一个查询, 查询innodb表, 这样mha的select会被阻塞 1234567891011121314151617181920212223242526272829root@localhost 12:25:45 [dbms_monitor]&gt; select sleep(600) from infra.chk_masterha limit 1;root@localhost 12:29:09 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+---------------------------------------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+---------------------------------------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 16 | | NULL | 1 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 4 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 1 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 4 | | NULL | 1 | 0 || 1256 | repler | 172.16.120.11:59594 | NULL | Binlog Dump GTID | 955662 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1257 | repler | 172.16.120.12:56540 | NULL | Binlog Dump GTID | 955642 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 11 | | NULL | 1 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 96 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 34 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 6 | | NULL | 0 | 0 || 1943 | root | localhost | dbms_monitor | Query | 21 | User sleep | select sleep(600) from infra.chk_masterha limit 1 | 0 | 0 || 2260 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2303 | mha | 172.16.120.13:34982 | NULL | Query | 20 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2305 | mha | 172.16.120.13:34988 | NULL | Query | 17 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2308 | mha | 172.16.120.13:34994 | NULL | Query | 14 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2310 | mha | 172.16.120.13:34998 | NULL | Query | 11 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2312 | mha | 172.16.120.13:35002 | NULL | Query | 8 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2314 | mha | 172.16.120.13:35006 | NULL | Query | 5 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 || 2317 | mha | 172.16.120.13:35010 | NULL | Query | 2 | Sending data | SELECT 1 As Value from infra.chk_masterha limit 1 | 0 | 0 |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+---------------------------------------------------+-----------+---------------+19 rows in set (0.00 sec) 结论: 不会failover, mha manager可能报错退出1234567891011121314151617181920212223242526272829303132Fri Oct 9 12:28:44 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 12:28:44 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 12:28:44 2020 - [info] OK.Fri Oct 9 12:28:44 2020 - [warning] shutdown_script is not defined.Fri Oct 9 12:28:44 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 12:28:44 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 12:28:44 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 12:28:44 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 12:28:53 2020 - [warning] Got timeout on MySQL Ping(CONNECT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:28:53 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 12:28:53 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 12:28:53 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 12:28:53 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Master is reachable from 172.16.120.11!Fri Oct 9 12:28:53 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 12:28:56 2020 - [warning] Got timeout on MySQL Ping(CONNECT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:28:56 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 12:28:59 2020 - [warning] Got timeout on MySQL Ping(CONNECT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:28:59 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond.....Fri Oct 9 12:30:47 2020 - [warning] Got timeout on MySQL Ping(CONNECT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:30:47 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..手动ctrl+c终止select sleep(600) from infra.chk_masterha limit 1后, mha manager报错退出了Fri Oct 9 12:30:49 2020 - [warning] Got error when monitoring master: at /usr/local/share/perl5/MHA/MasterMonitor.pm line 489.Fri Oct 9 12:30:49 2020 - [error][/usr/local/share/perl5/MHA/MasterMonitor.pm, ln491] Target master&#x27;s advisory lock is already held by someone. Please check whether you monitor the same master from multiple monitoring processes.Fri Oct 9 12:30:49 2020 - [error][/usr/local/share/perl5/MHA/MasterMonitor.pm, ln511] Error happened on health checking. at /usr/local/bin/masterha_manager line 50.Fri Oct 9 12:30:49 2020 - [error][/usr/local/share/perl5/MHA/MasterMonitor.pm, ln525] Error happened on monitoring servers.Fri Oct 9 12:30:49 2020 - [info] Got exit code 1 (Not master dead). ping_type=INSERTmaster hang不好模拟, 这里间接模拟. 修改innodb_thread_concurrency值 12root@localhost 12:25:34 [dbms_monitor]&gt; set global innodb_thread_concurrency=1;Query OK, 0 rows affected (0.00 sec) 手动执行一个查询, 查询innodb表, 这样mha的insert会被阻塞 123456789101112131415161718192021222324root@localhost 12:35:21 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 3 | | NULL | 1 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 1 | | NULL | 1 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 8 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 1 | | NULL | 0 | 0 || 1256 | repler | 172.16.120.11:59594 | NULL | Binlog Dump GTID | 956039 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1257 | repler | 172.16.120.12:56540 | NULL | Binlog Dump GTID | 956019 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 28 | | NULL | 1 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 113 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 51 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 13 | | NULL | 0 | 0 || 1943 | root | localhost | dbms_monitor | Query | 15 | User sleep | select sleep(600) from infra.chk_masterha limit 1 | 0 | 0 || 2260 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2395 | mha | 172.16.120.13:35206 | NULL | Query | 13 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 || 2398 | mha | 172.16.120.13:35208 | NULL | Query | 11 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 || 2400 | mha | 172.16.120.11:32908 | NULL | Query | 10 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 || 2401 | mha | 172.16.120.13:35216 | NULL | Query | 8 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 || 2403 | mha | 172.16.120.12:58066 | NULL | Query | 7 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 || 2404 | mha | 172.16.120.13:35222 | NULL | Query | 5 | update | INSERT INTO infra.chk_masterha values (1,unix_timestamp()) ON DUPLICATE KEY UPDATE val=unix_timestam | 0 | 0 |+------+----------+---------------------+--------------------+------------------+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------+---------------+18 rows in set (0.00 sec) 结论: 会failover123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229Fri Oct 9 12:35:00 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 12:35:01 2020 - [info] GTID failover mode = 1Fri Oct 9 12:35:01 2020 - [info] Dead Servers:Fri Oct 9 12:35:01 2020 - [info] Alive Servers:Fri Oct 9 12:35:01 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:01 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 12:35:01 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 12:35:01 2020 - [info] Alive Slaves:Fri Oct 9 12:35:01 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:01 2020 - [info] GTID ONFri Oct 9 12:35:01 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:01 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:01 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:01 2020 - [info] GTID ONFri Oct 9 12:35:01 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:01 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:01 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:01 2020 - [info] Checking slave configurations..Fri Oct 9 12:35:01 2020 - [info] Checking replication filtering settings..Fri Oct 9 12:35:01 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 12:35:01 2020 - [info] Replication filtering check ok.Fri Oct 9 12:35:01 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 12:35:01 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 12:35:01 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 12:35:01 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 12:35:01 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 12:35:01 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 12:35:01 2020 - [info] OK.Fri Oct 9 12:35:01 2020 - [warning] shutdown_script is not defined.Fri Oct 9 12:35:01 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 12:35:01 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 12:35:01 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 12:35:01 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 12:35:16 2020 - [warning] Got timeout on MySQL Ping(INSERT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:35:16 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 12:35:16 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 12:35:17 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 12:35:19 2020 - [warning] Got timeout on MySQL Ping(INSERT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:35:19 2020 - [warning] Connection failed 2 time(s)..Monitoring server 172.16.120.11 is reachable, Master is not writable from 172.16.120.11. OK.Fri Oct 9 12:35:22 2020 - [warning] Got timeout on MySQL Ping(INSERT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:35:22 2020 - [warning] Connection failed 3 time(s)..Monitoring server 172.16.120.12 is reachable, Master is not writable from 172.16.120.12. OK.Fri Oct 9 12:35:23 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Fri Oct 9 12:35:25 2020 - [warning] Got timeout on MySQL Ping(INSERT) child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.Fri Oct 9 12:35:25 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 12:35:25 2020 - [warning] Master is not reachable from health checker!Fri Oct 9 12:35:25 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Fri Oct 9 12:35:25 2020 - [warning] SSH is reachable.Fri Oct 9 12:35:25 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Fri Oct 9 12:35:25 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Fri Oct 9 12:35:25 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 12:35:25 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 12:35:27 2020 - [info] GTID failover mode = 1Fri Oct 9 12:35:27 2020 - [info] Dead Servers:Fri Oct 9 12:35:27 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:27 2020 - [info] Alive Servers:Fri Oct 9 12:35:27 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 12:35:27 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 12:35:27 2020 - [info] Alive Slaves:Fri Oct 9 12:35:27 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:27 2020 - [info] GTID ONFri Oct 9 12:35:27 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:27 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:27 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:27 2020 - [info] GTID ONFri Oct 9 12:35:27 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:27 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:27 2020 - [info] Checking slave configurations..Fri Oct 9 12:35:27 2020 - [info] Checking replication filtering settings..Fri Oct 9 12:35:27 2020 - [info] Replication filtering check ok.Fri Oct 9 12:35:27 2020 - [info] Master is down!Fri Oct 9 12:35:27 2020 - [info] Terminating monitoring script.Fri Oct 9 12:35:27 2020 - [info] Got exit code 20 (Master dead).Fri Oct 9 12:35:27 2020 - [info] MHA::MasterFailover version 0.58.Fri Oct 9 12:35:27 2020 - [info] Starting master failover.Fri Oct 9 12:35:27 2020 - [info] Fri Oct 9 12:35:27 2020 - [info] * Phase 1: Configuration Check Phase..Fri Oct 9 12:35:27 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] GTID failover mode = 1Fri Oct 9 12:35:28 2020 - [info] Dead Servers:Fri Oct 9 12:35:28 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Alive Servers:Fri Oct 9 12:35:28 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 12:35:28 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 12:35:28 2020 - [info] Alive Slaves:Fri Oct 9 12:35:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] Starting GTID based failover.Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] ** Phase 1: Configuration Check Phase completed.Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] Forcing shutdown so that applications never connect to the current master..Fri Oct 9 12:35:28 2020 - [info] Executing master IP deactivation script:Fri Oct 9 12:35:28 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 start down vipRTNETLINK answers: Cannot assign requested addressFake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Fri Oct 9 12:35:28 2020 - [info] done.Fri Oct 9 12:35:28 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Fri Oct 9 12:35:28 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 3: Master Recovery Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000009:827239Fri Oct 9 12:35:28 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:8822-10390Fri Oct 9 12:35:28 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Fri Oct 9 12:35:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000009:827239Fri Oct 9 12:35:28 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:8822-10390Fri Oct 9 12:35:28 2020 - [info] Oldest slaves:Fri Oct 9 12:35:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 3.3: Determining New Master Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] Searching new master from slaves..Fri Oct 9 12:35:28 2020 - [info] Candidate masters from the configuration file:Fri Oct 9 12:35:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 12:35:28 2020 - [info] GTID ONFri Oct 9 12:35:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 12:35:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 12:35:28 2020 - [info] Non-candidate masters:Fri Oct 9 12:35:28 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Fri Oct 9 12:35:28 2020 - [info] New master is 172.16.120.11(172.16.120.11:3358)Fri Oct 9 12:35:28 2020 - [info] Starting master failover..Fri Oct 9 12:35:28 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.11(172.16.120.11:3358) (new master) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 3.3: New Master Recovery Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] Waiting all logs to be applied.. Fri Oct 9 12:35:28 2020 - [info] done.Fri Oct 9 12:35:28 2020 - [info] Getting new master&#x27;s binlog name and position..Fri Oct 9 12:35:28 2020 - [info] mysql-bin.000008:811243Fri Oct 9 12:35:28 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.11&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Fri Oct 9 12:35:28 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000008, 811243, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-10390,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Fri Oct 9 12:35:28 2020 - [info] Executing master IP activate script:Fri Oct 9 12:35:28 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.11 --new_master_ip=172.16.120.11 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.11 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Fri Oct 9 12:35:28 2020 - [info] OK.Fri Oct 9 12:35:28 2020 - [info] ** Finished master recovery successfully.Fri Oct 9 12:35:28 2020 - [info] * Phase 3: Master Recovery Phase completed.Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 4: Slaves Recovery Phase..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Fri Oct 9 12:35:28 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] -- Slave recovery on host 172.16.120.12(172.16.120.12:3358) started, pid: 44798. Check tmp log /masterha/cls_new//172.16.120.12_3358_20201009123527.log if it takes time..Fri Oct 9 12:35:29 2020 - [info] Fri Oct 9 12:35:29 2020 - [info] Log messages from 172.16.120.12 ...Fri Oct 9 12:35:29 2020 - [info] Fri Oct 9 12:35:28 2020 - [info] Resetting slave 172.16.120.12(172.16.120.12:3358) and starting replication from the new master 172.16.120.11(172.16.120.11:3358)..Fri Oct 9 12:35:28 2020 - [info] Executed CHANGE MASTER.Fri Oct 9 12:35:28 2020 - [info] Slave started.Fri Oct 9 12:35:28 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-10390,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.12(172.16.120.12:3358). Executed 0 events.Fri Oct 9 12:35:29 2020 - [info] End of log messages from 172.16.120.12.Fri Oct 9 12:35:29 2020 - [info] -- Slave on host 172.16.120.12(172.16.120.12:3358) started.Fri Oct 9 12:35:29 2020 - [info] All new slave servers recovered successfully.Fri Oct 9 12:35:29 2020 - [info] Fri Oct 9 12:35:29 2020 - [info] * Phase 5: New master cleanup phase..Fri Oct 9 12:35:29 2020 - [info] Fri Oct 9 12:35:29 2020 - [info] Resetting slave info on the new master..Fri Oct 9 12:35:29 2020 - [info] 172.16.120.11: Resetting slave info succeeded.Fri Oct 9 12:35:29 2020 - [info] Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 12:35:29 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.11(172.16.120.11:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.11(172.16.120.11:3358) as a new master.172.16.120.11(172.16.120.11:3358): OK: Applying all logs succeeded.172.16.120.11(172.16.120.11:3358): OK: Activated master IP address.172.16.120.12(172.16.120.12:3358): OK: Slave started, replicating from 172.16.120.11(172.16.120.11:3358)172.16.120.11(172.16.120.11:3358): Resetting slave info succeeded.Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 12:35:29 2020 - [info] Sending mail.. 以下情况都不会failover, 即便是手动failover指定了 –master_state=dead 也不行12345678910111213our @ALIVE_ERROR_CODES = ( 1040, # ER_CON_COUNT_ERROR -- too many connection 1042, # ER_BAD_HOST_ERROR -- Can&#x27;t get hostname for your address 1043, # ER_HANDSHAKE_ERROR -- Bad handshake 1044, # ER_DBACCESS_DENIED_ERROR -- Access denied for user &#x27;%s&#x27;@&#x27;%s&#x27; to database &#x27;%s&#x27; 1045, # ER_ACCESS_DENIED_ERROR -- Access denied for user &#x27;%s&#x27;@&#x27;%s&#x27; (using password: %s) 1129, # ER_HOST_IS_BLOCKED -- Host &#x27;%s&#x27; is blocked because of many connection errors; unblock with &#x27;mysqladmin flush-hosts&#x27; 1130, # ER_HOST_NOT_PRIVILEGED -- Host &#x27;%s&#x27; is not allowed to connect to this MySQL server 1203, # ER_TOO_MANY_USER_CONNECTIONS -- User %s already has more than &#x27;max_user_connections&#x27; active connections 1226, # ER_USER_LIMIT_REACHED -- User &#x27;%s&#x27; has exceeded the &#x27;%s&#x27; resource (current value: %ld) 1251, # ER_NOT_SUPPORTED_AUTH_MODE -- Client does not support authentication protocol requested by server; consider upgrading MySQL client 1275, # ER_SERVER_IS_IN_SECURE_AUTH_MODE -- Server is running in --secure-auth mode, but &#x27;%s&#x27;@&#x27;%s&#x27; has a password in the old format; please change the password to the new format); 详见MHA-为什么too many connection不会failover? [用例测试] master 与 mha manager间网络异常1 Manager &lt;– 不通 –&gt; MasterManager &lt;– 正常 –&gt; S1 &lt;– 正常 –&gt; masterManager &lt;– 正常 –&gt; S2 &lt;– 正常 –&gt; master ping_type=CONNECT1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 结论: 不会failover12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485Fri Oct 9 15:29:50 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 15:29:51 2020 - [info] GTID failover mode = 1Fri Oct 9 15:29:51 2020 - [info] Dead Servers:Fri Oct 9 15:29:51 2020 - [info] Alive Servers:Fri Oct 9 15:29:51 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:29:51 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 15:29:51 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:29:51 2020 - [info] Alive Slaves:Fri Oct 9 15:29:51 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:29:51 2020 - [info] GTID ONFri Oct 9 15:29:51 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:29:51 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:29:51 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:29:51 2020 - [info] GTID ONFri Oct 9 15:29:51 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:29:51 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:29:51 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:29:51 2020 - [info] Checking slave configurations..Fri Oct 9 15:29:51 2020 - [info] Checking replication filtering settings..Fri Oct 9 15:29:51 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 15:29:51 2020 - [info] Replication filtering check ok.Fri Oct 9 15:29:51 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 15:29:51 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 15:29:51 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 15:29:51 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:29:51 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 15:29:51 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 15:29:51 2020 - [info] OK.Fri Oct 9 15:29:51 2020 - [warning] shutdown_script is not defined.Fri Oct 9 15:29:51 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 15:29:51 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 15:29:51 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 15:29:51 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:32:56 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:32:56 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:32:56 2020 - [info] Executing SSH check script: exit 0Master is reachable from 172.16.120.11!Fri Oct 9 15:32:56 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:33:00 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:00 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:33:01 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Fri Oct 9 15:33:03 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:03 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:33:06 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:06 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:33:06 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:33:09 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:09 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:33:09 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:33:09 2020 - [info] Executing SSH check script: exit 0Master is reachable from 172.16.120.11!Fri Oct 9 15:33:09 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:33:12 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:12 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:33:14 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Fri Oct 9 15:33:15 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:15 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:33:18 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:18 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:33:18 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:33:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:21 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:33:21 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:33:21 2020 - [info] Executing SSH check script: exit 0Master is reachable from 172.16.120.11!Fri Oct 9 15:33:21 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:33:24 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:24 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:33:26 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Fri Oct 9 15:33:27 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:27 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:33:30 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:30 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:33:30 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:33:33 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:33:33 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:33:33 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:33:33 2020 - [info] Executing SSH check script: exit 0Master is reachable from 172.16.120.11!Fri Oct 9 15:33:34 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen. ping_type=INSERT1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 此时manager工作正常, 因为ping_type=INSERT是长连接. kill连接 123456789101112131415161718192021root@localhost 15:39:31 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 4 | | NULL | 0 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 22 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 9 | | NULL | 1 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 2 | | NULL | 1 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 19 | | NULL | 0 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 111 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 49 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 4 | | NULL | 1 | 0 || 2409 | repler | 172.16.120.11:32918 | NULL | Binlog Dump GTID | 10898 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2411 | repler | 172.16.120.12:58084 | NULL | Binlog Dump GTID | 10873 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2627 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2836 | mha | 172.16.120.13:35810 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+12 rows in set (0.00 sec)root@localhost 15:39:39 [dbms_monitor]&gt; kill 2836;Query OK, 0 rows affected (0.00 sec) 结论: 不会failover123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172Fri Oct 9 15:37:54 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 15:37:55 2020 - [info] GTID failover mode = 1Fri Oct 9 15:37:55 2020 - [info] Dead Servers:Fri Oct 9 15:37:55 2020 - [info] Alive Servers:Fri Oct 9 15:37:55 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:37:55 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 15:37:55 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:37:55 2020 - [info] Alive Slaves:Fri Oct 9 15:37:55 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:37:55 2020 - [info] GTID ONFri Oct 9 15:37:55 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:37:55 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:37:55 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:37:55 2020 - [info] GTID ONFri Oct 9 15:37:55 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:37:55 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:37:55 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:37:55 2020 - [info] Checking slave configurations..Fri Oct 9 15:37:55 2020 - [info] Checking replication filtering settings..Fri Oct 9 15:37:55 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 15:37:55 2020 - [info] Replication filtering check ok.Fri Oct 9 15:37:55 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 15:37:55 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 15:37:55 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 15:37:55 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:37:55 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 15:37:55 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 15:37:55 2020 - [info] OK.Fri Oct 9 15:37:55 2020 - [warning] shutdown_script is not defined.Fri Oct 9 15:37:55 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 15:37:55 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 15:37:55 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 15:37:55 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:39:46 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 15:39:46 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:39:46 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTMaster is reachable from 172.16.120.11!Fri Oct 9 15:39:47 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:39:51 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Fri Oct 9 15:39:52 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:39:52 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:39:55 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:39:55 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:39:58 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:39:58 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:39:58 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:40:01 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:40:01 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:40:01 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 15:40:01 2020 - [info] Executing SSH check script: exit 0Master is reachable from 172.16.120.11!Fri Oct 9 15:40:01 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:40:04 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:40:04 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:40:06 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Fri Oct 9 15:40:07 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:40:07 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:40:10 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:40:10 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:40:10 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:40:13 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:40:13 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:40:13 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:40:13 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTMaster is reachable from 172.16.120.11!Fri Oct 9 15:40:13 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 15:40:14 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:40:14 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable. [用例测试] master 与 mha manager间网络异常2 Manager &lt;– 不通 –&gt; MasterManager &lt;– 不通 –&gt; S1 &lt;– 正常 –&gt; masterManager &lt;– 正常 –&gt; S2 &lt;– 正常 –&gt; master ping_type=CONNECTslave-1 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 此时manager已经无法连通slave-1 123456789101112#ping centos-2PING centos-2 (172.16.120.11) 56(84) bytes of data.64 bytes from centos-2 (172.16.120.11): icmp_seq=1 ttl=64 time=0.349 ms64 bytes from centos-2 (172.16.120.11): icmp_seq=2 ttl=64 time=0.651 ms^C--- centos-2 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.349/0.500/0.651/0.151 ms[root@centos-4 15:48:55 /usr/local/share/perl5/MHA]#ssh centos-2^C master 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 结论: 不会failover12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576Fri Oct 9 15:48:03 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 15:48:05 2020 - [info] GTID failover mode = 1Fri Oct 9 15:48:05 2020 - [info] Dead Servers:Fri Oct 9 15:48:05 2020 - [info] Alive Servers:Fri Oct 9 15:48:05 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:48:05 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 15:48:05 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:48:05 2020 - [info] Alive Slaves:Fri Oct 9 15:48:05 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:48:05 2020 - [info] GTID ONFri Oct 9 15:48:05 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:48:05 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:48:05 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:48:05 2020 - [info] GTID ONFri Oct 9 15:48:05 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:48:05 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:48:05 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:48:05 2020 - [info] Checking slave configurations..Fri Oct 9 15:48:05 2020 - [info] Checking replication filtering settings..Fri Oct 9 15:48:05 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 15:48:05 2020 - [info] Replication filtering check ok.Fri Oct 9 15:48:05 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 15:48:05 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 15:48:05 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 15:48:05 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:48:05 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 15:48:05 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 15:48:05 2020 - [info] OK.Fri Oct 9 15:48:05 2020 - [warning] shutdown_script is not defined.Fri Oct 9 15:48:05 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 15:48:05 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 15:48:05 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 15:48:05 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:50:40 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:40 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:50:40 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:50:44 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:44 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:50:45 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 15:50:45 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 15:50:47 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:47 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:50:50 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:50 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:50:50 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:50:53 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:53 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:50:53 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:50:53 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:50:56 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:56 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:50:58 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 15:50:58 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 15:50:59 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:50:59 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:51:02 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:51:02 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:51:02 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:51:05 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:51:05 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:51:05 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 15:51:05 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:51:05 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:51:05 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 15:51:09 2020 - [warning] Got timeout on Secondary Check child process and killed it! at /usr/local/share/perl5/MHA/HealthCheck.pm line 435.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable! ping_type=INSERTslave-1 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 此时manager已经无法连通slave-1 123456789101112#ping centos-2PING centos-2 (172.16.120.11) 56(84) bytes of data.64 bytes from centos-2 (172.16.120.11): icmp_seq=1 ttl=64 time=0.349 ms64 bytes from centos-2 (172.16.120.11): icmp_seq=2 ttl=64 time=0.651 ms^C--- centos-2 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.349/0.500/0.651/0.151 ms[root@centos-4 15:48:55 /usr/local/share/perl5/MHA]#ssh centos-2^C master 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 因为ping_type=INSERT是长连接,1 所以此时无异常. kill连接 123456789101112131415161718192021root@localhost 15:39:45 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 3 | | NULL | 1 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 1 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 8 | | NULL | 1 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 11 | | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 8 | | NULL | 0 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 89 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 26 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 3 | | NULL | 0 | 0 || 2409 | repler | 172.16.120.11:32918 | NULL | Binlog Dump GTID | 11837 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2411 | repler | 172.16.120.12:58084 | NULL | Binlog Dump GTID | 11812 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2627 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 2953 | mha | 172.16.120.13:36174 | NULL | Sleep | 0 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+12 rows in set (0.00 sec)root@localhost 15:55:18 [dbms_monitor]&gt; kill 2953;Query OK, 0 rows affected (0.00 sec) 结论: 不会failover1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889Fri Oct 9 15:52:43 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 15:52:44 2020 - [info] GTID failover mode = 1Fri Oct 9 15:52:44 2020 - [info] Dead Servers:Fri Oct 9 15:52:44 2020 - [info] Alive Servers:Fri Oct 9 15:52:44 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:52:44 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 15:52:44 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:52:44 2020 - [info] Alive Slaves:Fri Oct 9 15:52:44 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:52:44 2020 - [info] GTID ONFri Oct 9 15:52:44 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:52:44 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:52:44 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 15:52:44 2020 - [info] GTID ONFri Oct 9 15:52:44 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:52:44 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 15:52:44 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 15:52:44 2020 - [info] Checking slave configurations..Fri Oct 9 15:52:44 2020 - [info] Checking replication filtering settings..Fri Oct 9 15:52:44 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 15:52:44 2020 - [info] Replication filtering check ok.Fri Oct 9 15:52:44 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 15:52:44 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 15:52:45 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 15:52:45 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 15:52:45 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 15:52:45 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 15:52:45 2020 - [info] OK.Fri Oct 9 15:52:45 2020 - [warning] shutdown_script is not defined.Fri Oct 9 15:52:45 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 15:52:45 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 15:52:45 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 15:52:45 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:55:24 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 15:55:24 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 15:55:24 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:55:29 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 15:55:29 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 15:55:30 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:30 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:55:33 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:33 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:55:36 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:36 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:55:36 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:55:39 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:39 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:55:39 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 15:55:39 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:55:42 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:42 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:55:44 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 15:55:44 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 15:55:45 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:45 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:55:48 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:48 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:55:48 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:55:51 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:51 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:55:51 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 15:55:51 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:55:54 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:54 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 15:55:56 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 15:55:56 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 15:55:57 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:55:57 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 15:56:00 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:56:00 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 15:56:00 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 15:56:03 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 15:56:03 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 15:56:03 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 15:56:03 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 15:56:03 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 15:56:03 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Master is reachable from 172.16.120.11!Fri Oct 9 15:56:03 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen. [用例测试] master 与 mha manager间网络异常3 Manager &lt;– 不通 –&gt; MasterManager &lt;– 不通 –&gt; S1 &lt;– 正常 –&gt; masterManager &lt;– 不通 –&gt; S2 &lt;– 正常 –&gt; master ping_type=CONNECTslave-1, slave-2 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 1234567891011121314151617181920212223242526#ping centos-2PING centos-2 (172.16.120.11) 56(84) bytes of data.64 bytes from centos-2 (172.16.120.11): icmp_seq=1 ttl=64 time=0.442 ms64 bytes from centos-2 (172.16.120.11): icmp_seq=2 ttl=64 time=0.441 ms^C--- centos-2 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.441/0.441/0.442/0.021 ms[root@centos-4 16:44:27 /usr/local/share/perl5/MHA]#ssh centos-2 ^C[root@centos-4 16:44:30 /usr/local/share/perl5/MHA]#ping centos-3PING centos-3 (172.16.120.12) 56(84) bytes of data.64 bytes from centos-3 (172.16.120.12): icmp_seq=1 ttl=64 time=0.335 ms64 bytes from centos-3 (172.16.120.12): icmp_seq=2 ttl=64 time=0.575 ms^C--- centos-3 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.335/0.455/0.575/0.120 ms[root@centos-4 16:44:34 /usr/local/share/perl5/MHA]#ssh centos-3^C master 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 结论: 不会failover1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465Fri Oct 9 16:43:25 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 16:43:26 2020 - [info] GTID failover mode = 1Fri Oct 9 16:43:26 2020 - [info] Dead Servers:Fri Oct 9 16:43:26 2020 - [info] Alive Servers:Fri Oct 9 16:43:26 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:43:26 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 16:43:26 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:43:26 2020 - [info] Alive Slaves:Fri Oct 9 16:43:26 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:43:26 2020 - [info] GTID ONFri Oct 9 16:43:26 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:43:26 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:43:26 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:43:26 2020 - [info] GTID ONFri Oct 9 16:43:26 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:43:26 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:43:26 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:43:26 2020 - [info] Checking slave configurations..Fri Oct 9 16:43:26 2020 - [info] Checking replication filtering settings..Fri Oct 9 16:43:26 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 16:43:26 2020 - [info] Replication filtering check ok.Fri Oct 9 16:43:26 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 16:43:26 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 16:43:26 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 16:43:26 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:43:26 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 16:43:26 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 16:43:26 2020 - [info] OK.Fri Oct 9 16:43:26 2020 - [warning] shutdown_script is not defined.Fri Oct 9 16:43:26 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 16:43:26 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 16:43:26 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 16:43:26 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 16:45:55 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:45:55 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:45:55 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 16:45:59 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:45:59 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:46:00 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 16:46:00 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 16:46:02 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:46:02 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:46:05 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:46:05 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 16:46:05 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 16:46:08 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:46:08 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 16:46:08 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:46:08 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 16:46:11 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:46:11 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:46:13 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 16:46:13 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 16:46:14 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:46:14 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:46:15 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond.. ping_type=INSERTslave-1,slave-2 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 1234567891011121314151617181920212223#ping centos-2PING centos-2 (172.16.120.11) 56(84) bytes of data.64 bytes from centos-2 (172.16.120.11): icmp_seq=1 ttl=64 time=0.352 ms^C--- centos-2 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.352/0.352/0.352/0.000 ms[root@centos-4 17:52:38 ~]#ping centos-3PING centos-3 (172.16.120.12) 56(84) bytes of data.64 bytes from centos-3 (172.16.120.12): icmp_seq=1 ttl=64 time=0.221 ms^C--- centos-3 ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.221/0.221/0.221/0.000 ms[root@centos-4 17:52:41 ~]#ssh centos-2 ^C[root@centos-4 17:52:44 ~]#ssh centos-3 master 1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.11 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 由于ping_type=INSERT是长连接, 所以无异常 kill连接 123456789101112131415161718192021root@localhost 17:48:11 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 14 | | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 4 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 32 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 4 | | NULL | 1 | 0 || 2409 | repler | 172.16.120.11:32918 | NULL | Binlog Dump GTID | 18936 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2411 | repler | 172.16.120.12:58084 | NULL | Binlog Dump GTID | 18911 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 3192 | proxysql | 172.16.120.11:33786 | NULL | Sleep | 4 | | NULL | 0 | 0 || 3238 | proxysql | 172.16.120.12:59006 | NULL | Sleep | 4 | | NULL | 1 | 0 || 3245 | proxysql | 172.16.120.11:33888 | NULL | Sleep | 14 | | NULL | 0 | 0 || 3262 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 3268 | mha | 172.16.120.13:36868 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+11 rows in set (0.00 sec)root@localhost 17:53:37 [dbms_monitor]&gt; kill 3268;Query OK, 0 rows affected (0.00 sec) 结论: 不会failover12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879Fri Oct 9 17:50:48 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 17:50:49 2020 - [info] GTID failover mode = 1Fri Oct 9 17:50:49 2020 - [info] Dead Servers:Fri Oct 9 17:50:49 2020 - [info] Alive Servers:Fri Oct 9 17:50:49 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 17:50:49 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 17:50:49 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 17:50:49 2020 - [info] Alive Slaves:Fri Oct 9 17:50:49 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 17:50:49 2020 - [info] GTID ONFri Oct 9 17:50:49 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 17:50:49 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 17:50:49 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 17:50:49 2020 - [info] GTID ONFri Oct 9 17:50:49 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 17:50:49 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 17:50:49 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 17:50:49 2020 - [info] Checking slave configurations..Fri Oct 9 17:50:49 2020 - [info] Checking replication filtering settings..Fri Oct 9 17:50:49 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 17:50:49 2020 - [info] Replication filtering check ok.Fri Oct 9 17:50:49 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 17:50:49 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 17:50:49 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 17:50:49 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 17:50:49 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 17:50:49 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 17:50:49 2020 - [info] OK.Fri Oct 9 17:50:49 2020 - [warning] shutdown_script is not defined.Fri Oct 9 17:50:49 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 17:50:49 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 17:50:49 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 17:50:49 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 17:53:43 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 17:53:43 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 17:53:43 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 17:53:48 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 17:53:48 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 17:53:49 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:53:49 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 17:53:52 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:53:52 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 17:53:55 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:53:55 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 17:53:55 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 17:53:58 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:53:58 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 17:53:58 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 17:53:58 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 17:54:01 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:01 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 17:54:03 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 17:54:03 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 17:54:04 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:04 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 17:54:07 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:07 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 17:54:07 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 17:54:10 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:10 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 17:54:10 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 17:54:10 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 17:54:13 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:13 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 17:54:15 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.ssh: connect to host 172.16.120.11 port 22: Connection timed outMonitoring server 172.16.120.11 is NOT reachable!Fri Oct 9 17:54:15 2020 - [warning] At least one of monitoring servers is not reachable from this script. This is likely a network problem. Failover should not happen.Fri Oct 9 17:54:16 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 17:54:16 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 17:54:16 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond.. [用例测试] master 与 mha manager间网络异常4 Manager &lt;– 不通 –&gt; MasterManager &lt;– 正常 –&gt; S1 &lt;– 不通 –&gt; masterManager &lt;– 正常 –&gt; S2 &lt;– 正常 –&gt; master ping_type=CONNECTmaster 123456IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 结论: 不会failover1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Fri Oct 9 16:05:55 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 16:05:56 2020 - [info] GTID failover mode = 1Fri Oct 9 16:05:56 2020 - [info] Dead Servers:Fri Oct 9 16:05:56 2020 - [info] Alive Servers:Fri Oct 9 16:05:56 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:05:56 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 16:05:56 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:05:56 2020 - [info] Alive Slaves:Fri Oct 9 16:05:56 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:05:56 2020 - [info] GTID ONFri Oct 9 16:05:56 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:05:56 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:05:56 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:05:56 2020 - [info] GTID ONFri Oct 9 16:05:56 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:05:56 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:05:56 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:05:56 2020 - [info] Checking slave configurations..Fri Oct 9 16:05:56 2020 - [info] Checking replication filtering settings..Fri Oct 9 16:05:56 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 16:05:56 2020 - [info] Replication filtering check ok.Fri Oct 9 16:05:56 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 16:05:56 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 16:05:56 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 16:05:56 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:05:56 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 16:05:56 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 16:05:56 2020 - [info] OK.Fri Oct 9 16:05:56 2020 - [warning] shutdown_script is not defined.Fri Oct 9 16:05:56 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 16:05:56 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 16:05:56 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 16:05:56 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 16:06:43 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:43 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:06:43 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 16:06:47 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:47 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:06:48 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Master is reachable from 172.16.120.12!Fri Oct 9 16:06:48 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 16:06:50 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:50 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:06:53 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:53 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 16:06:53 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 16:06:56 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:56 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 16:06:56 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 16:06:56 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:06:59 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:06:59 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:07:01 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Master is reachable from 172.16.120.12!Fri Oct 9 16:07:01 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 16:07:02 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:07:02 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:07:05 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:07:05 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 16:07:05 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 16:07:05 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond.. ping_type=INSERTmaster 123456IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 由于ping_type=INSERT是长连接, 所以无异常 kill连接 123456789101112131415161718192021root@localhost 15:55:23 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 19 | | NULL | 1 | 0 || 1249 | proxysql | 172.16.120.11:59582 | NULL | Sleep | 7 | | NULL | 0 | 0 || 1250 | proxysql | 172.16.120.12:56530 | NULL | Sleep | 4 | | NULL | 0 | 0 || 1254 | proxysql | 172.16.120.11:59592 | NULL | Sleep | 27 | | NULL | 1 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 14 | | NULL | 1 | 0 || 1341 | mha | 172.16.120.12:56698 | information_schema | Sleep | 114 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 51 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 9 | | NULL | 0 | 0 || 2409 | repler | 172.16.120.11:32918 | NULL | Binlog Dump GTID | 12703 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2411 | repler | 172.16.120.12:58084 | NULL | Binlog Dump GTID | 12678 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 2627 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 3022 | mha | 172.16.120.13:36466 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+-------+---------------------------------------------------------------+------------------+-----------+---------------+12 rows in set (0.00 sec)root@localhost 16:09:44 [dbms_monitor]&gt; kill 3022;Query OK, 0 rows affected (0.00 sec) 结论: 不会failover12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576Fri Oct 9 16:08:29 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 16:08:30 2020 - [info] GTID failover mode = 1Fri Oct 9 16:08:30 2020 - [info] Dead Servers:Fri Oct 9 16:08:30 2020 - [info] Alive Servers:Fri Oct 9 16:08:30 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:08:30 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 16:08:30 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:08:30 2020 - [info] Alive Slaves:Fri Oct 9 16:08:30 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:08:30 2020 - [info] GTID ONFri Oct 9 16:08:30 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:08:30 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:08:30 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 16:08:30 2020 - [info] GTID ONFri Oct 9 16:08:30 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:08:30 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 16:08:30 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 16:08:30 2020 - [info] Checking slave configurations..Fri Oct 9 16:08:30 2020 - [info] Checking replication filtering settings..Fri Oct 9 16:08:30 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 16:08:30 2020 - [info] Replication filtering check ok.Fri Oct 9 16:08:30 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 16:08:30 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 16:08:30 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 16:08:30 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 16:08:30 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 16:08:30 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 16:08:30 2020 - [info] OK.Fri Oct 9 16:08:30 2020 - [warning] shutdown_script is not defined.Fri Oct 9 16:08:30 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 16:08:30 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 16:08:30 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 16:08:30 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 16:09:51 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 16:09:51 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 16:09:51 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:09:56 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Master is reachable from 172.16.120.12!Fri Oct 9 16:09:57 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 16:09:57 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:09:57 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:10:00 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:00 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:10:03 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:03 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 16:10:03 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 16:10:06 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:06 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 16:10:06 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 16:10:06 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:10:09 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:09 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:10:11 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Master is reachable from 172.16.120.12!Fri Oct 9 16:10:12 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen.Fri Oct 9 16:10:12 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:12 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 16:10:15 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:15 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 16:10:15 2020 - [warning] Secondary network check script returned errors. Failover should not start so checking server status again. Check network settings for details.Fri Oct 9 16:10:18 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:18 2020 - [warning] Connection failed 1 time(s)..Fri Oct 9 16:10:18 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 16:10:18 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 16:10:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 16:10:21 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 16:10:21 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 16:10:21 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Master is reachable from 172.16.120.11!Fri Oct 9 16:10:21 2020 - [warning] Master is reachable from at least one of other monitoring servers. Failover should not happen. [用例测试] master 与 mha manager间网络异常5 Manager &lt;– 不通 –&gt; MasterManager &lt;– 正常 –&gt; S1 &lt;– 不通 –&gt; masterManager &lt;– 正常 –&gt; S2 &lt;– 不通 –&gt; master ping_type=CONNECTmaster 12345IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 结论: 会failover123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230Fri Oct 9 18:21:13 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 18:21:14 2020 - [info] GTID failover mode = 1Fri Oct 9 18:21:14 2020 - [info] Dead Servers:Fri Oct 9 18:21:14 2020 - [info] Alive Servers:Fri Oct 9 18:21:14 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:21:14 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:21:14 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:21:14 2020 - [info] Alive Slaves:Fri Oct 9 18:21:14 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:21:14 2020 - [info] GTID ONFri Oct 9 18:21:14 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:21:14 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:21:14 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:21:14 2020 - [info] GTID ONFri Oct 9 18:21:14 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:21:14 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:21:14 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:21:14 2020 - [info] Checking slave configurations..Fri Oct 9 18:21:14 2020 - [info] Checking replication filtering settings..Fri Oct 9 18:21:14 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 18:21:14 2020 - [info] Replication filtering check ok.Fri Oct 9 18:21:14 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 18:21:14 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 18:21:14 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 18:21:14 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:21:14 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 18:21:14 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 18:21:14 2020 - [info] OK.Fri Oct 9 18:21:14 2020 - [warning] shutdown_script is not defined.Fri Oct 9 18:21:14 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 18:21:14 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 18:21:14 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 18:21:14 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 18:22:07 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:22:07 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 18:22:07 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTFri Oct 9 18:22:11 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:22:11 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 18:22:12 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Fri Oct 9 18:22:14 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:22:14 2020 - [warning] Connection failed 3 time(s)..Fri Oct 9 18:22:17 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:22:17 2020 - [warning] Connection failed 4 time(s)..Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Fri Oct 9 18:22:18 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Fri Oct 9 18:22:18 2020 - [warning] Master is not reachable from health checker!Fri Oct 9 18:22:18 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Fri Oct 9 18:22:18 2020 - [warning] SSH is NOT reachable.Fri Oct 9 18:22:18 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Fri Oct 9 18:22:18 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Fri Oct 9 18:22:18 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 18:22:18 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 18:22:19 2020 - [info] GTID failover mode = 1Fri Oct 9 18:22:19 2020 - [info] Dead Servers:Fri Oct 9 18:22:19 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:19 2020 - [info] Alive Servers:Fri Oct 9 18:22:19 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:22:19 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:22:19 2020 - [info] Alive Slaves:Fri Oct 9 18:22:19 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:19 2020 - [info] GTID ONFri Oct 9 18:22:19 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:19 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:19 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:19 2020 - [info] GTID ONFri Oct 9 18:22:19 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:19 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:19 2020 - [info] Checking slave configurations..Fri Oct 9 18:22:19 2020 - [info] Checking replication filtering settings..Fri Oct 9 18:22:19 2020 - [info] Replication filtering check ok.Fri Oct 9 18:22:19 2020 - [info] Master is down!Fri Oct 9 18:22:19 2020 - [info] Terminating monitoring script.Fri Oct 9 18:22:19 2020 - [info] Got exit code 20 (Master dead).Fri Oct 9 18:22:19 2020 - [info] MHA::MasterFailover version 0.58.Fri Oct 9 18:22:19 2020 - [info] Starting master failover.Fri Oct 9 18:22:19 2020 - [info] Fri Oct 9 18:22:19 2020 - [info] * Phase 1: Configuration Check Phase..Fri Oct 9 18:22:19 2020 - [info] Fri Oct 9 18:22:20 2020 - [info] GTID failover mode = 1Fri Oct 9 18:22:20 2020 - [info] Dead Servers:Fri Oct 9 18:22:20 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:20 2020 - [info] Checking master reachability via MySQL(double check)...Fri Oct 9 18:22:21 2020 - [info] ok.Fri Oct 9 18:22:21 2020 - [info] Alive Servers:Fri Oct 9 18:22:21 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:22:21 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:22:21 2020 - [info] Alive Slaves:Fri Oct 9 18:22:21 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] Starting GTID based failover.Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] ** Phase 1: Configuration Check Phase completed.Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] Forcing shutdown so that applications never connect to the current master..Fri Oct 9 18:22:21 2020 - [info] Executing master IP deactivation script:Fri Oct 9 18:22:21 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stop Disabling the VIP on old master: 172.16.120.10 Fake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Fri Oct 9 18:22:21 2020 - [info] done.Fri Oct 9 18:22:21 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Fri Oct 9 18:22:21 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 3: Master Recovery Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000009:3084318Fri Oct 9 18:22:21 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:10391-19970Fri Oct 9 18:22:21 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Fri Oct 9 18:22:21 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000009:3084318Fri Oct 9 18:22:21 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:10391-19970Fri Oct 9 18:22:21 2020 - [info] Oldest slaves:Fri Oct 9 18:22:21 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 3.3: Determining New Master Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] Searching new master from slaves..Fri Oct 9 18:22:21 2020 - [info] Candidate masters from the configuration file:Fri Oct 9 18:22:21 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:22:21 2020 - [info] GTID ONFri Oct 9 18:22:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:22:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:22:21 2020 - [info] Non-candidate masters:Fri Oct 9 18:22:21 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Fri Oct 9 18:22:21 2020 - [info] New master is 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:22:21 2020 - [info] Starting master failover..Fri Oct 9 18:22:21 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.11(172.16.120.11:3358) (new master) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 3.3: New Master Recovery Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] Waiting all logs to be applied.. Fri Oct 9 18:22:21 2020 - [info] done.Fri Oct 9 18:22:21 2020 - [info] Getting new master&#x27;s binlog name and position..Fri Oct 9 18:22:21 2020 - [info] mysql-bin.000008:3052407Fri Oct 9 18:22:21 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.11&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Fri Oct 9 18:22:21 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000008, 3052407, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-19970,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Fri Oct 9 18:22:21 2020 - [info] Executing master IP activate script:Fri Oct 9 18:22:21 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.11 --new_master_ip=172.16.120.11 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.11 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Fri Oct 9 18:22:21 2020 - [info] OK.Fri Oct 9 18:22:21 2020 - [info] ** Finished master recovery successfully.Fri Oct 9 18:22:21 2020 - [info] * Phase 3: Master Recovery Phase completed.Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 4: Slaves Recovery Phase..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Fri Oct 9 18:22:21 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] -- Slave recovery on host 172.16.120.12(172.16.120.12:3358) started, pid: 68999. Check tmp log /masterha/cls_new//172.16.120.12_3358_20201009182219.log if it takes time..Fri Oct 9 18:22:22 2020 - [info] Fri Oct 9 18:22:22 2020 - [info] Log messages from 172.16.120.12 ...Fri Oct 9 18:22:22 2020 - [info] Fri Oct 9 18:22:21 2020 - [info] Resetting slave 172.16.120.12(172.16.120.12:3358) and starting replication from the new master 172.16.120.11(172.16.120.11:3358)..Fri Oct 9 18:22:21 2020 - [info] Executed CHANGE MASTER.Fri Oct 9 18:22:21 2020 - [info] Slave started.Fri Oct 9 18:22:21 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-19970,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.12(172.16.120.12:3358). Executed 0 events.Fri Oct 9 18:22:22 2020 - [info] End of log messages from 172.16.120.12.Fri Oct 9 18:22:22 2020 - [info] -- Slave on host 172.16.120.12(172.16.120.12:3358) started.Fri Oct 9 18:22:22 2020 - [info] All new slave servers recovered successfully.Fri Oct 9 18:22:22 2020 - [info] Fri Oct 9 18:22:22 2020 - [info] * Phase 5: New master cleanup phase..Fri Oct 9 18:22:22 2020 - [info] Fri Oct 9 18:22:22 2020 - [info] Resetting slave info on the new master..Fri Oct 9 18:22:22 2020 - [info] 172.16.120.11: Resetting slave info succeeded.Fri Oct 9 18:22:22 2020 - [info] Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 18:22:22 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.11(172.16.120.11:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.11(172.16.120.11:3358) as a new master.172.16.120.11(172.16.120.11:3358): OK: Applying all logs succeeded.172.16.120.11(172.16.120.11:3358): OK: Activated master IP address.172.16.120.12(172.16.120.12:3358): OK: Slave started, replicating from 172.16.120.11(172.16.120.11:3358)172.16.120.11(172.16.120.11:3358): Resetting slave info succeeded.Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 18:22:22 2020 - [info] Sending mail.. ping_type=INSERT由于ping_type=INSERT是长连接, 所以无异常 kill连接 1234567891011121314151617181920root@localhost 18:24:52 [dbms_monitor]&gt; show processlist;+------+----------+---------------------+--------------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+------+----------+---------------------+--------------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| 1248 | proxysql | 172.16.120.10:58672 | NULL | Sleep | 1 | | NULL | 0 | 0 || 1264 | proxysql | 172.16.120.12:56552 | NULL | Sleep | 2 | | NULL | 0 | 0 || 1343 | mha | 172.16.120.11:59758 | information_schema | Sleep | 74 | | NULL | 0 | 0 || 1452 | proxysql | 172.16.120.10:59046 | NULL | Sleep | 2 | | NULL | 1 | 0 || 3192 | proxysql | 172.16.120.11:33786 | NULL | Sleep | 3 | | NULL | 1 | 0 || 3238 | proxysql | 172.16.120.12:59006 | NULL | Sleep | 1 | | NULL | 1 | 0 || 3245 | proxysql | 172.16.120.11:33888 | NULL | Sleep | 2 | | NULL | 0 | 0 || 3262 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 3357 | repler | 172.16.120.11:34036 | NULL | Binlog Dump GTID | 142 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 3359 | repler | 172.16.120.12:59166 | NULL | Binlog Dump GTID | 123 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 3364 | mha | 172.16.120.13:37512 | NULL | Sleep | 2 | | NULL | 0 | 0 |+------+----------+---------------------+--------------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+11 rows in set (0.00 sec)root@localhost 18:26:25 [dbms_monitor]&gt; kill 3364;Query OK, 0 rows affected (0.01 sec) 结论: 长连接断开后才会failover, 否则不会failover123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227Fri Oct 9 18:25:33 2020 - [info] MHA::MasterMonitor version 0.58.Fri Oct 9 18:25:34 2020 - [info] GTID failover mode = 1Fri Oct 9 18:25:34 2020 - [info] Dead Servers:Fri Oct 9 18:25:34 2020 - [info] Alive Servers:Fri Oct 9 18:25:34 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:25:34 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:25:34 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:25:34 2020 - [info] Alive Slaves:Fri Oct 9 18:25:34 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:25:34 2020 - [info] GTID ONFri Oct 9 18:25:34 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:25:34 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:25:34 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:25:34 2020 - [info] GTID ONFri Oct 9 18:25:34 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:25:34 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:25:34 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:25:34 2020 - [info] Checking slave configurations..Fri Oct 9 18:25:34 2020 - [info] Checking replication filtering settings..Fri Oct 9 18:25:34 2020 - [info] binlog_do_db= , binlog_ignore_db= Fri Oct 9 18:25:34 2020 - [info] Replication filtering check ok.Fri Oct 9 18:25:34 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Fri Oct 9 18:25:34 2020 - [info] Checking SSH publickey authentication settings on the current master..Fri Oct 9 18:25:35 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Fri Oct 9 18:25:35 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:25:35 2020 - [info] Checking master_ip_failover_script status:Fri Oct 9 18:25:35 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Fri Oct 9 18:25:35 2020 - [info] OK.Fri Oct 9 18:25:35 2020 - [warning] shutdown_script is not defined.Fri Oct 9 18:25:35 2020 - [info] Set master ping interval 3 seconds.Fri Oct 9 18:25:35 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Fri Oct 9 18:25:35 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Fri Oct 9 18:25:35 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..Fri Oct 9 18:26:44 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Fri Oct 9 18:26:44 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTFri Oct 9 18:26:44 2020 - [info] Executing SSH check script: exit 0Fri Oct 9 18:26:49 2020 - [warning] HealthCheck: Got timeout on checking SSH connection to 172.16.120.10! at /usr/local/share/perl5/MHA/HealthCheck.pm line 344.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Fri Oct 9 18:26:50 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:26:50 2020 - [warning] Connection failed 2 time(s)..Fri Oct 9 18:26:53 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:26:53 2020 - [warning] Connection failed 3 time(s)..Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Fri Oct 9 18:26:54 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Fri Oct 9 18:26:56 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (4))Fri Oct 9 18:26:56 2020 - [warning] Connection failed 4 time(s)..Fri Oct 9 18:26:56 2020 - [warning] Master is not reachable from health checker!Fri Oct 9 18:26:56 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Fri Oct 9 18:26:56 2020 - [warning] SSH is NOT reachable.Fri Oct 9 18:26:56 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Fri Oct 9 18:26:56 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Fri Oct 9 18:26:56 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 18:26:56 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Fri Oct 9 18:26:57 2020 - [info] GTID failover mode = 1Fri Oct 9 18:26:57 2020 - [info] Dead Servers:Fri Oct 9 18:26:57 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:57 2020 - [info] Alive Servers:Fri Oct 9 18:26:57 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:26:57 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:26:57 2020 - [info] Alive Slaves:Fri Oct 9 18:26:57 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:57 2020 - [info] GTID ONFri Oct 9 18:26:57 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:57 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:57 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:57 2020 - [info] GTID ONFri Oct 9 18:26:57 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:57 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:57 2020 - [info] Checking slave configurations..Fri Oct 9 18:26:57 2020 - [info] Checking replication filtering settings..Fri Oct 9 18:26:57 2020 - [info] Replication filtering check ok.Fri Oct 9 18:26:57 2020 - [info] Master is down!Fri Oct 9 18:26:57 2020 - [info] Terminating monitoring script.Fri Oct 9 18:26:57 2020 - [info] Got exit code 20 (Master dead).Fri Oct 9 18:26:57 2020 - [info] MHA::MasterFailover version 0.58.Fri Oct 9 18:26:57 2020 - [info] Starting master failover.Fri Oct 9 18:26:57 2020 - [info] Fri Oct 9 18:26:57 2020 - [info] * Phase 1: Configuration Check Phase..Fri Oct 9 18:26:57 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] GTID failover mode = 1Fri Oct 9 18:26:58 2020 - [info] Dead Servers:Fri Oct 9 18:26:58 2020 - [info] 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Alive Servers:Fri Oct 9 18:26:58 2020 - [info] 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:26:58 2020 - [info] 172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:26:58 2020 - [info] Alive Slaves:Fri Oct 9 18:26:58 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] Starting GTID based failover.Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] ** Phase 1: Configuration Check Phase completed.Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] Forcing shutdown so that applications never connect to the current master..Fri Oct 9 18:26:58 2020 - [info] Executing master IP deactivation script:Fri Oct 9 18:26:58 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stop Disabling the VIP on old master: 172.16.120.10 Fake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Fri Oct 9 18:26:58 2020 - [info] done.Fri Oct 9 18:26:58 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Fri Oct 9 18:26:58 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 3: Master Recovery Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000009:3101017Fri Oct 9 18:26:58 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:19971-20041Fri Oct 9 18:26:58 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Fri Oct 9 18:26:58 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000009:3101017Fri Oct 9 18:26:58 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:19971-20041Fri Oct 9 18:26:58 2020 - [info] Oldest slaves:Fri Oct 9 18:26:58 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 3.3: Determining New Master Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] Searching new master from slaves..Fri Oct 9 18:26:58 2020 - [info] Candidate masters from the configuration file:Fri Oct 9 18:26:58 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledFri Oct 9 18:26:58 2020 - [info] GTID ONFri Oct 9 18:26:58 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Fri Oct 9 18:26:58 2020 - [info] Primary candidate for the new Master (candidate_master is set)Fri Oct 9 18:26:58 2020 - [info] Non-candidate masters:Fri Oct 9 18:26:58 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Fri Oct 9 18:26:58 2020 - [info] New master is 172.16.120.11(172.16.120.11:3358)Fri Oct 9 18:26:58 2020 - [info] Starting master failover..Fri Oct 9 18:26:58 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.11(172.16.120.11:3358) (new master) +--172.16.120.12(172.16.120.12:3358)Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 3.3: New Master Recovery Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] Waiting all logs to be applied.. Fri Oct 9 18:26:58 2020 - [info] done.Fri Oct 9 18:26:58 2020 - [info] Getting new master&#x27;s binlog name and position..Fri Oct 9 18:26:58 2020 - [info] mysql-bin.000008:3068991Fri Oct 9 18:26:58 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.11&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Fri Oct 9 18:26:58 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000008, 3068991, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20041,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Fri Oct 9 18:26:58 2020 - [info] Executing master IP activate script:Fri Oct 9 18:26:58 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.11 --new_master_ip=172.16.120.11 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.11 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Fri Oct 9 18:26:58 2020 - [info] OK.Fri Oct 9 18:26:58 2020 - [info] ** Finished master recovery successfully.Fri Oct 9 18:26:58 2020 - [info] * Phase 3: Master Recovery Phase completed.Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 4: Slaves Recovery Phase..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Fri Oct 9 18:26:58 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] -- Slave recovery on host 172.16.120.12(172.16.120.12:3358) started, pid: 69850. Check tmp log /masterha/cls_new//172.16.120.12_3358_20201009182657.log if it takes time..Fri Oct 9 18:26:59 2020 - [info] Fri Oct 9 18:26:59 2020 - [info] Log messages from 172.16.120.12 ...Fri Oct 9 18:26:59 2020 - [info] Fri Oct 9 18:26:58 2020 - [info] Resetting slave 172.16.120.12(172.16.120.12:3358) and starting replication from the new master 172.16.120.11(172.16.120.11:3358)..Fri Oct 9 18:26:58 2020 - [info] Executed CHANGE MASTER.Fri Oct 9 18:26:58 2020 - [info] Slave started.Fri Oct 9 18:26:58 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20041,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.12(172.16.120.12:3358). Executed 0 events.Fri Oct 9 18:26:59 2020 - [info] End of log messages from 172.16.120.12.Fri Oct 9 18:26:59 2020 - [info] -- Slave on host 172.16.120.12(172.16.120.12:3358) started.Fri Oct 9 18:26:59 2020 - [info] All new slave servers recovered successfully.Fri Oct 9 18:26:59 2020 - [info] Fri Oct 9 18:26:59 2020 - [info] * Phase 5: New master cleanup phase..Fri Oct 9 18:26:59 2020 - [info] Fri Oct 9 18:26:59 2020 - [info] Resetting slave info on the new master..Fri Oct 9 18:26:59 2020 - [info] 172.16.120.11: Resetting slave info succeeded.Fri Oct 9 18:26:59 2020 - [info] Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 18:26:59 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.11(172.16.120.11:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.11(172.16.120.11:3358) as a new master.172.16.120.11(172.16.120.11:3358): OK: Applying all logs succeeded.172.16.120.11(172.16.120.11:3358): OK: Activated master IP address.172.16.120.12(172.16.120.12:3358): OK: Slave started, replicating from 172.16.120.11(172.16.120.11:3358)172.16.120.11(172.16.120.11:3358): Resetting slave info succeeded.Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Fri Oct 9 18:26:59 2020 - [info] Sending mail.. [用例测试] master挂了, 且slave也有问题1(部分slave宕机) master挂了, 在此之前slave-1宕机了 ping_type=CONNECT启动manager后, 关闭slave-1 12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 10:28:35 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 10:28:37 2020 - [info] GTID failover mode = 1Sat Oct 10 10:28:37 2020 - [info] Dead Servers:Sat Oct 10 10:28:37 2020 - [info] Alive Servers:Sat Oct 10 10:28:37 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:28:37 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 10:28:37 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:28:37 2020 - [info] Alive Slaves:Sat Oct 10 10:28:37 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:28:37 2020 - [info] GTID ONSat Oct 10 10:28:37 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:28:37 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:28:37 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:28:37 2020 - [info] GTID ONSat Oct 10 10:28:37 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:28:37 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:28:37 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:28:37 2020 - [info] Checking slave configurations..Sat Oct 10 10:28:37 2020 - [info] Checking replication filtering settings..Sat Oct 10 10:28:37 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 10:28:37 2020 - [info] Replication filtering check ok.Sat Oct 10 10:28:37 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 10:28:37 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 10:28:37 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 10:28:37 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:28:37 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 10:28:37 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 10:28:37 2020 - [info] OK.Sat Oct 10 10:28:37 2020 - [warning] shutdown_script is not defined.Sat Oct 10 10:28:37 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 10:28:37 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 10:28:37 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 10:28:37 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond.. 关闭slave-1 123456789101112131415161718[root@centos-2 10:19:38 ~]#mysql -uroot -p -S /data/mysql_3358/run/mysql.sock dbms_monitor mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2118Server version: 5.7.31-34-log Percona Server (GPL), Release 34, Revision 2e68637Copyright (c) 2009-2020 Percona LLC and/or its affiliatesCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.root@localhost 10:20:52 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.03 sec) 关闭后, mha manager仍然是正常的 关闭master 123456789101112131415161718[root@centos-1 10:20:35 ~]#mysql -uroot -p -S /data/mysql_3358/run/mysql.sock dbms_monitor mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3413Server version: 5.7.31-34-log Percona Server (GPL), Release 34, Revision 2e68637Copyright (c) 2009-2020 Percona LLC and/or its affiliatesCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.root@localhost 10:20:47 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.03 sec) 结论: 会触发failover, 但failover失败12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758Sat Oct 10 10:50:38 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 10:50:38 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTSat Oct 10 10:50:38 2020 - [info] Executing SSH check script: exit 0Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Sat Oct 10 10:50:38 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 10:50:38 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 10:50:41 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 10:50:41 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 10:50:44 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 10:50:44 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 10:50:47 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 10:50:47 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 10:50:47 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 10:50:47 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 10:50:47 2020 - [warning] SSH is reachable.Sat Oct 10 10:50:47 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 10:50:47 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 10:50:47 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 10:50:47 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 10:50:48 2020 - [info] GTID failover mode = 1Sat Oct 10 10:50:48 2020 - [info] Dead Servers:Sat Oct 10 10:50:48 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:50:48 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 10:50:48 2020 - [info] Alive Servers:Sat Oct 10 10:50:48 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:50:48 2020 - [info] Alive Slaves:Sat Oct 10 10:50:48 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:50:48 2020 - [info] GTID ONSat Oct 10 10:50:48 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:50:48 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:50:48 2020 - [info] Checking slave configurations..Sat Oct 10 10:50:48 2020 - [info] Checking replication filtering settings..Sat Oct 10 10:50:48 2020 - [info] Replication filtering check ok.Sat Oct 10 10:50:48 2020 - [info] Master is down!Sat Oct 10 10:50:48 2020 - [info] Terminating monitoring script.Sat Oct 10 10:50:48 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 10:50:48 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 10:50:48 2020 - [info] Starting master failover.Sat Oct 10 10:50:48 2020 - [info] Sat Oct 10 10:50:48 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 10:50:48 2020 - [info] Sat Oct 10 10:50:49 2020 - [info] GTID failover mode = 1Sat Oct 10 10:50:49 2020 - [info] Dead Servers:Sat Oct 10 10:50:49 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:50:49 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 10:50:49 2020 - [info] Checking master reachability via MySQL(double check)...Sat Oct 10 10:50:49 2020 - [info] ok.Sat Oct 10 10:50:49 2020 - [info] Alive Servers:Sat Oct 10 10:50:49 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:50:49 2020 - [info] Alive Slaves:Sat Oct 10 10:50:49 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:50:49 2020 - [info] GTID ONSat Oct 10 10:50:49 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:50:49 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:50:49 2020 - [error][/usr/local/share/perl5/MHA/ServerManager.pm, ln492] Server 172.16.120.11(172.16.120.11:3358) is dead, but must be alive! Check server settings.Sat Oct 10 10:50:49 2020 - [error][/usr/local/share/perl5/MHA/ManagerUtil.pm, ln177] Got ERROR: at /usr/local/share/perl5/MHA/MasterFailover.pm line 269. ping_type=INSERT其实和connect应该是一样的, 不过还是走一遍流程 启动manager后, 关闭slave-1 12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 10:59:07 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 10:59:09 2020 - [info] GTID failover mode = 1Sat Oct 10 10:59:09 2020 - [info] Dead Servers:Sat Oct 10 10:59:09 2020 - [info] Alive Servers:Sat Oct 10 10:59:09 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:59:09 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 10:59:09 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:59:09 2020 - [info] Alive Slaves:Sat Oct 10 10:59:09 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:59:09 2020 - [info] GTID ONSat Oct 10 10:59:09 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:59:09 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:59:09 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 10:59:09 2020 - [info] GTID ONSat Oct 10 10:59:09 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:59:09 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 10:59:09 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 10:59:09 2020 - [info] Checking slave configurations..Sat Oct 10 10:59:09 2020 - [info] Checking replication filtering settings..Sat Oct 10 10:59:09 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 10:59:09 2020 - [info] Replication filtering check ok.Sat Oct 10 10:59:09 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 10:59:09 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 10:59:09 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 10:59:09 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 10:59:09 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 10:59:09 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 10:59:09 2020 - [info] OK.Sat Oct 10 10:59:09 2020 - [warning] shutdown_script is not defined.Sat Oct 10 10:59:09 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 10:59:09 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 10:59:09 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 10:59:09 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond.. 关闭slave-1 12[root@centos-2 10:56:24 /usr/local/Percona-Server-5.7.29-32-Linux.x86_64.ssl101]#2020-10-10T03:00:49.502943Z mysqld_safe mysqld from pid file /data/mysql_3358/run/mysql.pid ended 关闭master 123456789101112131415161718[root@centos-1 11:07:08 ~]#mysql -uroot -p -S /data/mysql_3358/run/mysql.sock dbms_monitor mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 35Server version: 5.7.29-32-log Percona Server (GPL), Release 32, Revision 56bce88Copyright (c) 2009-2020 Percona LLC and/or its affiliatesCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.root@localhost 11:07:09 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会触发failover, 但failover失败12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455Sat Oct 10 11:07:15 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Sat Oct 10 11:07:15 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 11:07:15 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTMonitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Sat Oct 10 11:07:16 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 11:07:16 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 11:07:18 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:07:18 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 11:07:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:07:21 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 11:07:24 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:07:24 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 11:07:24 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 11:07:24 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 11:07:24 2020 - [warning] SSH is reachable.Sat Oct 10 11:07:24 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 11:07:24 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 11:07:24 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:07:24 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:07:25 2020 - [info] GTID failover mode = 1Sat Oct 10 11:07:25 2020 - [info] Dead Servers:Sat Oct 10 11:07:25 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:07:25 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:07:25 2020 - [info] Alive Servers:Sat Oct 10 11:07:25 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:07:25 2020 - [info] Alive Slaves:Sat Oct 10 11:07:25 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:07:25 2020 - [info] GTID ONSat Oct 10 11:07:25 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:07:25 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:07:25 2020 - [info] Checking slave configurations..Sat Oct 10 11:07:25 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:07:25 2020 - [info] Replication filtering check ok.Sat Oct 10 11:07:25 2020 - [info] Master is down!Sat Oct 10 11:07:25 2020 - [info] Terminating monitoring script.Sat Oct 10 11:07:25 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 11:07:25 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 11:07:25 2020 - [info] Starting master failover.Sat Oct 10 11:07:25 2020 - [info] Sat Oct 10 11:07:25 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 11:07:25 2020 - [info] Sat Oct 10 11:07:26 2020 - [info] GTID failover mode = 1Sat Oct 10 11:07:26 2020 - [info] Dead Servers:Sat Oct 10 11:07:26 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:07:26 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:07:26 2020 - [info] Alive Servers:Sat Oct 10 11:07:26 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:07:26 2020 - [info] Alive Slaves:Sat Oct 10 11:07:26 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:07:26 2020 - [info] GTID ONSat Oct 10 11:07:26 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:07:26 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:07:26 2020 - [error][/usr/local/share/perl5/MHA/ServerManager.pm, ln492] Server 172.16.120.11(172.16.120.11:3358) is dead, but must be alive! Check server settings.Sat Oct 10 11:07:26 2020 - [error][/usr/local/share/perl5/MHA/ManagerUtil.pm, ln177] Got ERROR: at /usr/local/share/perl5/MHA/MasterFailover.pm line 269. 总结: 如不希望slave-1宕机影响failover, 需要在配置文件中对slave-1设置ignore_fail=112345[server2]hostname=172.16.120.11port=3358candidate_master=1ignore_fail=1 [用例测试] master挂了, 且slave也有问题2(部分slave io_thread stop) master挂了, 在此之前slave-1 io_thread stop了 ping_type=CONNECT启动manager后, 关闭slave-1 io_thread 12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 11:13:58 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 11:13:59 2020 - [info] GTID failover mode = 1Sat Oct 10 11:13:59 2020 - [info] Dead Servers:Sat Oct 10 11:13:59 2020 - [info] Alive Servers:Sat Oct 10 11:13:59 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:13:59 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:13:59 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:13:59 2020 - [info] Alive Slaves:Sat Oct 10 11:13:59 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:13:59 2020 - [info] GTID ONSat Oct 10 11:13:59 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:13:59 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:13:59 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:13:59 2020 - [info] GTID ONSat Oct 10 11:13:59 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:13:59 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:13:59 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:13:59 2020 - [info] Checking slave configurations..Sat Oct 10 11:13:59 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:13:59 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 11:13:59 2020 - [info] Replication filtering check ok.Sat Oct 10 11:13:59 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 11:13:59 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 11:13:59 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 11:13:59 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:13:59 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 11:13:59 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 11:14:00 2020 - [info] OK.Sat Oct 10 11:14:00 2020 - [warning] shutdown_script is not defined.Sat Oct 10 11:14:00 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 11:14:00 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 11:14:00 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 11:14:00 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond.. 关闭slave-1 123456789101112131415161718192021222324root@localhost 11:17:29 [dbms_monitor]&gt; stop slave io_thread;Query OK, 0 rows affected (0.01 sec)root@localhost 11:17:33 [dbms_monitor]&gt; pager cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;PAGER set to &#x27;cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;&#x27;root@localhost 11:17:49 [dbms_monitor]&gt; show slave status\\G Master_Log_File: mysql-bin.000011 Read_Master_Log_Pos: 194 Relay_Log_File: mysql-relay-bin.000009 Relay_Log_Pos: 407 Relay_Master_Log_File: mysql-bin.000011 Slave_IO_Running: No Slave_SQL_Running: Yes Last_Errno: 0 Last_Error: Exec_Master_Log_Pos: 194 Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 1 row in set (0.00 sec) 关闭io_thread后 manager仍然正常 关闭master 12345678910111213root@localhost 11:21:18 [dbms_monitor]&gt; insert into monitor_delay values(1,now());Query OK, 1 row affected (0.01 sec)root@localhost 11:21:39 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 |+----+---------------------+1 row in set (0.00 sec)root@localhost 11:21:54 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) slave-1 12root@localhost 11:17:49 [dbms_monitor]&gt; select * from monitor_delay;Empty set (0.01 sec) slave-2 1234567root@localhost 10:20:54 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 |+----+---------------------+1 row in set (0.01 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185Sat Oct 10 11:22:12 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:22:12 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTSat Oct 10 11:22:12 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 11:22:12 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 11:22:12 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 11:22:15 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:22:15 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 11:22:18 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:22:18 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 11:22:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:22:21 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 11:22:21 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 11:22:21 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 11:22:21 2020 - [warning] SSH is reachable.Sat Oct 10 11:22:21 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 11:22:21 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 11:22:21 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:22:21 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:22:22 2020 - [info] GTID failover mode = 1Sat Oct 10 11:22:22 2020 - [info] Dead Servers:Sat Oct 10 11:22:22 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:22 2020 - [info] Alive Servers:Sat Oct 10 11:22:22 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:22:22 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:22:22 2020 - [info] Alive Slaves:Sat Oct 10 11:22:22 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:22 2020 - [info] GTID ONSat Oct 10 11:22:22 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:22 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:22 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:22 2020 - [info] GTID ONSat Oct 10 11:22:22 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:22 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:22 2020 - [info] Checking slave configurations..Sat Oct 10 11:22:22 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:22:22 2020 - [info] Replication filtering check ok.Sat Oct 10 11:22:22 2020 - [info] Master is down!Sat Oct 10 11:22:22 2020 - [info] Terminating monitoring script.Sat Oct 10 11:22:22 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 11:22:22 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 11:22:22 2020 - [info] Starting master failover.Sat Oct 10 11:22:22 2020 - [info] Sat Oct 10 11:22:22 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 11:22:22 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] GTID failover mode = 1Sat Oct 10 11:22:23 2020 - [info] Dead Servers:Sat Oct 10 11:22:23 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Checking master reachability via MySQL(double check)...Sat Oct 10 11:22:23 2020 - [info] ok.Sat Oct 10 11:22:23 2020 - [info] Alive Servers:Sat Oct 10 11:22:23 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:22:23 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:22:23 2020 - [info] Alive Slaves:Sat Oct 10 11:22:23 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] Starting GTID based failover.Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 11:22:23 2020 - [info] Executing master IP deactivation script:Sat Oct 10 11:22:23 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 Fake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 11:22:23 2020 - [info] done.Sat Oct 10 11:22:23 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 11:22:23 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000011:486Sat Oct 10 11:22:23 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20042-20531Sat Oct 10 11:22:23 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 11:22:23 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000011:194Sat Oct 10 11:22:23 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20143-20530Sat Oct 10 11:22:23 2020 - [info] Oldest slaves:Sat Oct 10 11:22:23 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] Searching new master from slaves..Sat Oct 10 11:22:23 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 11:22:23 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:22:23 2020 - [info] GTID ONSat Oct 10 11:22:23 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:22:23 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:22:23 2020 - [info] Non-candidate masters:Sat Oct 10 11:22:23 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 11:22:23 2020 - [info] New master is 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:22:23 2020 - [info] Starting master failover..Sat Oct 10 11:22:23 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.12(172.16.120.12:3358) (new master) +--172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 11:22:23 2020 - [info] Sat Oct 10 11:22:23 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 11:22:23 2020 - [info] done.Sat Oct 10 11:22:23 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 11:22:23 2020 - [info] mysql-bin.000007:3182161Sat Oct 10 11:22:23 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.12&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 11:22:23 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000007, 3182161, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20531,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 11:22:23 2020 - [info] Executing master IP activate script:Sat Oct 10 11:22:23 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.12 --new_master_ip=172.16.120.12 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.12 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 11:22:24 2020 - [info] OK.Sat Oct 10 11:22:24 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 11:22:24 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 11:22:24 2020 - [info] Sat Oct 10 11:22:24 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 11:22:24 2020 - [info] Sat Oct 10 11:22:24 2020 - [info] Sat Oct 10 11:22:24 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 11:22:24 2020 - [info] Sat Oct 10 11:22:24 2020 - [info] -- Slave recovery on host 172.16.120.11(172.16.120.11:3358) started, pid: 77208. Check tmp log /masterha/cls_new//172.16.120.11_3358_20201010112222.log if it takes time..Sat Oct 10 11:22:25 2020 - [info] Sat Oct 10 11:22:25 2020 - [info] Log messages from 172.16.120.11 ...Sat Oct 10 11:22:25 2020 - [info] Sat Oct 10 11:22:24 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 11:22:24 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 11:22:24 2020 - [info] Slave started.Sat Oct 10 11:22:24 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20531,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.11(172.16.120.11:3358). Executed 2 events.Sat Oct 10 11:22:25 2020 - [info] End of log messages from 172.16.120.11.Sat Oct 10 11:22:25 2020 - [info] -- Slave on host 172.16.120.11(172.16.120.11:3358) started.Sat Oct 10 11:22:25 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 11:22:25 2020 - [info] Sat Oct 10 11:22:25 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 11:22:25 2020 - [info] Sat Oct 10 11:22:25 2020 - [info] Resetting slave info on the new master..Sat Oct 10 11:22:25 2020 - [info] 172.16.120.12: Resetting slave info succeeded.Sat Oct 10 11:22:25 2020 - [info] Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 11:22:25 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.12(172.16.120.12:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.12(172.16.120.12:3358) as a new master.172.16.120.12(172.16.120.12:3358): OK: Applying all logs succeeded.172.16.120.12(172.16.120.12:3358): OK: Activated master IP address.172.16.120.11(172.16.120.11:3358): OK: Slave started, replicating from 172.16.120.12(172.16.120.12:3358)172.16.120.12(172.16.120.12:3358): Resetting slave info succeeded.Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 11:22:25 2020 - [info] Sending mail.. slave-1正常change到new master slave-2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869root@localhost 11:21:44 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.16.120.12 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000007 Read_Master_Log_Pos: 3182161 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 681 Relay_Master_Log_File: mysql-bin.000007 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 3182161 Relay_Log_Space: 888 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 120123358 Master_UUID: 45e70f96-fcad-11ea-a2f0-0050563108d2 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20531 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20531,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)root@localhost 11:24:39 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 |+----+---------------------+1 row in set (0.00 sec) ping_type=INSERT启动manager后, 关闭slave-1 io_thread 12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 11:29:28 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 11:29:30 2020 - [info] GTID failover mode = 1Sat Oct 10 11:29:30 2020 - [info] Dead Servers:Sat Oct 10 11:29:30 2020 - [info] Alive Servers:Sat Oct 10 11:29:30 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:29:30 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:29:30 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:29:30 2020 - [info] Alive Slaves:Sat Oct 10 11:29:30 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:29:30 2020 - [info] GTID ONSat Oct 10 11:29:30 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:29:30 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:29:30 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:29:30 2020 - [info] GTID ONSat Oct 10 11:29:30 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:29:30 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:29:30 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:29:30 2020 - [info] Checking slave configurations..Sat Oct 10 11:29:30 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:29:30 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 11:29:30 2020 - [info] Replication filtering check ok.Sat Oct 10 11:29:30 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 11:29:30 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 11:29:30 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 11:29:30 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:29:30 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 11:29:30 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 11:29:30 2020 - [info] OK.Sat Oct 10 11:29:30 2020 - [warning] shutdown_script is not defined.Sat Oct 10 11:29:30 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 11:29:30 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 11:29:30 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 11:29:30 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond.. 关闭slave-1 io_thread 123456789101112131415161718192021222324root@localhost 11:30:30 [dbms_monitor]&gt; stop slave io_thread;Query OK, 0 rows affected (0.00 sec)root@localhost 11:30:43 [dbms_monitor]&gt; pager cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;PAGER set to &#x27;cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;&#x27;root@localhost 11:30:45 [dbms_monitor]&gt; show slave status\\G Master_Log_File: mysql-bin.000012 Read_Master_Log_Pos: 18307 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 18480 Relay_Master_Log_File: mysql-bin.000012 Slave_IO_Running: No Slave_SQL_Running: Yes Last_Errno: 0 Last_Error: Exec_Master_Log_Pos: 18307 Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 1 row in set (0.00 sec) 关闭io_thread后 manager仍然正常 关闭master 1234567891011121314root@localhost 11:31:40 [dbms_monitor]&gt; insert into monitor_delay values(2,now());Query OK, 1 row affected (0.00 sec)root@localhost 11:31:45 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 |+----+---------------------+2 rows in set (0.00 sec)root@localhost 11:31:51 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) slave-1 1234567root@localhost 11:30:45 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 |+----+---------------------+1 row in set (0.00 sec) slave-2 12345678root@localhost 11:27:07 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 |+----+---------------------+2 rows in set (0.00 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169Sat Oct 10 11:33:09 2020 - [warning] SSH is reachable.Sat Oct 10 11:33:09 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 11:33:09 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 11:33:09 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:33:09 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:33:10 2020 - [info] GTID failover mode = 1Sat Oct 10 11:33:10 2020 - [info] Dead Servers:Sat Oct 10 11:33:10 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:10 2020 - [info] Alive Servers:Sat Oct 10 11:33:10 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:33:10 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:33:10 2020 - [info] Alive Slaves:Sat Oct 10 11:33:10 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:10 2020 - [info] GTID ONSat Oct 10 11:33:10 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:10 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:10 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:10 2020 - [info] GTID ONSat Oct 10 11:33:10 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:10 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:10 2020 - [info] Checking slave configurations..Sat Oct 10 11:33:10 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:33:10 2020 - [info] Replication filtering check ok.Sat Oct 10 11:33:10 2020 - [info] Master is down!Sat Oct 10 11:33:10 2020 - [info] Terminating monitoring script.Sat Oct 10 11:33:10 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 11:33:10 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 11:33:10 2020 - [info] Starting master failover.Sat Oct 10 11:33:10 2020 - [info] Sat Oct 10 11:33:10 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 11:33:10 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] GTID failover mode = 1Sat Oct 10 11:33:11 2020 - [info] Dead Servers:Sat Oct 10 11:33:11 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Alive Servers:Sat Oct 10 11:33:11 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:33:11 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:33:11 2020 - [info] Alive Slaves:Sat Oct 10 11:33:11 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] Starting GTID based failover.Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 11:33:11 2020 - [info] Executing master IP deactivation script:Sat Oct 10 11:33:11 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 RTNETLINK answers: Cannot assign requested addressFake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 11:33:11 2020 - [info] done.Sat Oct 10 11:33:11 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 11:33:11 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000012:50590Sat Oct 10 11:33:11 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20532-20745Sat Oct 10 11:33:11 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 11:33:11 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000012:18307Sat Oct 10 11:33:11 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20532-20608Sat Oct 10 11:33:11 2020 - [info] Oldest slaves:Sat Oct 10 11:33:11 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] Searching new master from slaves..Sat Oct 10 11:33:11 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 11:33:11 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:33:11 2020 - [info] GTID ONSat Oct 10 11:33:11 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:33:11 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:33:11 2020 - [info] Non-candidate masters:Sat Oct 10 11:33:11 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 11:33:11 2020 - [info] New master is 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:33:11 2020 - [info] Starting master failover..Sat Oct 10 11:33:11 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.12(172.16.120.12:3358) (new master) +--172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 11:33:11 2020 - [info] done.Sat Oct 10 11:33:11 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 11:33:11 2020 - [info] mysql-bin.000007:3232182Sat Oct 10 11:33:11 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.12&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 11:33:11 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000007, 3232182, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20745,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 11:33:11 2020 - [info] Executing master IP activate script:Sat Oct 10 11:33:11 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.12 --new_master_ip=172.16.120.12 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.12 RTNETLINK answers: File existsFake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 11:33:11 2020 - [info] OK.Sat Oct 10 11:33:11 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 11:33:11 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 11:33:11 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] -- Slave recovery on host 172.16.120.11(172.16.120.11:3358) started, pid: 78319. Check tmp log /masterha/cls_new//172.16.120.11_3358_20201010113310.log if it takes time..Sat Oct 10 11:33:12 2020 - [info] Sat Oct 10 11:33:12 2020 - [info] Log messages from 172.16.120.11 ...Sat Oct 10 11:33:12 2020 - [info] Sat Oct 10 11:33:11 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 11:33:11 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 11:33:11 2020 - [info] Slave started.Sat Oct 10 11:33:12 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20745,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.11(172.16.120.11:3358). Executed 2 events.Sat Oct 10 11:33:12 2020 - [info] End of log messages from 172.16.120.11.Sat Oct 10 11:33:12 2020 - [info] -- Slave on host 172.16.120.11(172.16.120.11:3358) started.Sat Oct 10 11:33:12 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 11:33:12 2020 - [info] Sat Oct 10 11:33:12 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 11:33:12 2020 - [info] Sat Oct 10 11:33:12 2020 - [info] Resetting slave info on the new master..Sat Oct 10 11:33:13 2020 - [info] 172.16.120.12: Resetting slave info succeeded.Sat Oct 10 11:33:13 2020 - [info] Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 11:33:13 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.12(172.16.120.12:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.12(172.16.120.12:3358) as a new master.172.16.120.12(172.16.120.12:3358): OK: Applying all logs succeeded.172.16.120.12(172.16.120.12:3358): OK: Activated master IP address.172.16.120.11(172.16.120.11:3358): OK: Slave started, replicating from 172.16.120.12(172.16.120.12:3358)172.16.120.12(172.16.120.12:3358): Resetting slave info succeeded.Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 11:33:13 2020 - [info] Sending mail.. slave-1已经change到了slave-2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970root@localhost 11:32:04 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.16.120.12 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000007 Read_Master_Log_Pos: 3232182 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 32447 Relay_Master_Log_File: mysql-bin.000007 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 3232182 Relay_Log_Space: 32654 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 120123358 Master_UUID: 45e70f96-fcad-11ea-a2f0-0050563108d2 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20609-20745 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20745,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)root@localhost 11:34:29 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 |+----+---------------------+2 rows in set (0.00 sec) [用例测试] master挂了, 且slave也有问题3(部分slave io_thread error) master挂了, 在此之前slave-1 io_thread error了 ping_type=CONNECT启动manager后, 调整master防火墙, 禁止slave-1访问12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 11:58:40 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 11:58:41 2020 - [info] GTID failover mode = 1Sat Oct 10 11:58:41 2020 - [info] Dead Servers:Sat Oct 10 11:58:41 2020 - [info] Alive Servers:Sat Oct 10 11:58:41 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:58:41 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:58:41 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:58:41 2020 - [info] Alive Slaves:Sat Oct 10 11:58:41 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:58:41 2020 - [info] GTID ONSat Oct 10 11:58:41 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:58:41 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:58:41 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:58:41 2020 - [info] GTID ONSat Oct 10 11:58:41 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:58:41 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:58:41 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:58:41 2020 - [info] Checking slave configurations..Sat Oct 10 11:58:41 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:58:41 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 11:58:41 2020 - [info] Replication filtering check ok.Sat Oct 10 11:58:41 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 11:58:41 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 11:58:41 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 11:58:41 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:58:41 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 11:58:41 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 11:58:41 2020 - [info] OK.Sat Oct 10 11:58:41 2020 - [warning] shutdown_script is not defined.Sat Oct 10 11:58:41 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 11:58:41 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 11:58:41 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 11:58:41 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..调整master防火墙, 禁止slave-1访问1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.13 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROPkill slave-1 io_thread12345678910111213141516171819root@localhost 12:04:35 [dbms_monitor]&gt; show processlist;+-----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+-----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| 2 | proxysql | 172.16.120.12:33384 | NULL | Sleep | 9 | | NULL | 0 | 0 || 3 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 4 | proxysql | 172.16.120.10:34072 | NULL | Sleep | 4 | | NULL | 0 | 0 || 6 | proxysql | 172.16.120.11:35090 | NULL | Sleep | 2 | | NULL | 0 | 0 || 7 | repler | 172.16.120.11:35092 | NULL | Binlog Dump GTID | 485 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 8 | repler | 172.16.120.12:33392 | NULL | Binlog Dump GTID | 472 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 110 | proxysql | 172.16.120.10:34094 | NULL | Sleep | 4 | | NULL | 1 | 0 |+-----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+7 rows in set (0.00 sec)root@localhost 12:04:46 [dbms_monitor]&gt; kill 7;Query OK, 0 rows affected (0.00 sec)root@localhost 12:05:20 [dbms_monitor]&gt; insert into monitor_delay values(6,now());Query OK, 1 row affected (0.00 sec) slave-11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374root@localhost 12:05:02 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 || 3 | 2020-10-10 11:51:01 || 4 | 2020-10-10 11:59:15 || 5 | 2020-10-10 12:04:35 |+----+---------------------+5 rows in set (0.00 sec)root@localhost 12:07:32 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Reconnecting after a failed master event read Master_Host: 172.16.120.10 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000014 Read_Master_Log_Pos: 778 Relay_Log_File: mysql-relay-bin.000003 Relay_Log_Pos: 605 Relay_Master_Log_File: mysql-bin.000014 Slave_IO_Running: Connecting Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 778 Relay_Log_Space: 1317 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 2003 Last_IO_Error: error reconnecting to master &#x27;repler@172.16.120.10:3358&#x27; - retry-time: 1 retries: 2 Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 120103358 Master_UUID: 44a4ea53-fcad-11ea-bd16-0050563b7b42 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: 201010 12:06:57 Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20747-20748 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20748,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) slave-2123456789101112root@localhost 12:04:42 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 || 3 | 2020-10-10 11:51:01 || 4 | 2020-10-10 11:59:15 || 5 | 2020-10-10 12:04:35 || 6 | 2020-10-10 12:05:30 |+----+---------------------+6 rows in set (0.00 sec) 关闭master12root@localhost 12:21:24 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186Sat Oct 10 12:11:18 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:11:18 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTSat Oct 10 12:11:18 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 12:11:19 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 12:11:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:11:21 2020 - [warning] Connection failed 2 time(s)..Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 12:11:24 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 12:11:24 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:11:24 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 12:11:27 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:11:27 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 12:11:27 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 12:11:27 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 12:11:27 2020 - [warning] SSH is reachable.Sat Oct 10 12:11:27 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 12:11:27 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 12:11:27 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 12:11:27 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 12:11:28 2020 - [info] GTID failover mode = 1Sat Oct 10 12:11:28 2020 - [info] Dead Servers:Sat Oct 10 12:11:28 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:28 2020 - [info] Alive Servers:Sat Oct 10 12:11:28 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:11:28 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:11:28 2020 - [info] Alive Slaves:Sat Oct 10 12:11:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:28 2020 - [info] GTID ONSat Oct 10 12:11:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:28 2020 - [info] GTID ONSat Oct 10 12:11:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:28 2020 - [info] Checking slave configurations..Sat Oct 10 12:11:28 2020 - [info] Checking replication filtering settings..Sat Oct 10 12:11:28 2020 - [info] Replication filtering check ok.Sat Oct 10 12:11:28 2020 - [info] Master is down!Sat Oct 10 12:11:28 2020 - [info] Terminating monitoring script.Sat Oct 10 12:11:28 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 12:11:28 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 12:11:28 2020 - [info] Starting master failover.Sat Oct 10 12:11:28 2020 - [info] Sat Oct 10 12:11:28 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 12:11:28 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] GTID failover mode = 1Sat Oct 10 12:11:29 2020 - [info] Dead Servers:Sat Oct 10 12:11:29 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Checking master reachability via MySQL(double check)...Sat Oct 10 12:11:29 2020 - [info] ok.Sat Oct 10 12:11:29 2020 - [info] Alive Servers:Sat Oct 10 12:11:29 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:11:29 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:11:29 2020 - [info] Alive Slaves:Sat Oct 10 12:11:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] Starting GTID based failover.Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 12:11:29 2020 - [info] Executing master IP deactivation script:Sat Oct 10 12:11:29 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 Fake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 12:11:29 2020 - [info] done.Sat Oct 10 12:11:29 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 12:11:29 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000014:1070Sat Oct 10 12:11:29 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20747-20749Sat Oct 10 12:11:29 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 12:11:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000014:778Sat Oct 10 12:11:29 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20747-20748Sat Oct 10 12:11:29 2020 - [info] Oldest slaves:Sat Oct 10 12:11:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] Searching new master from slaves..Sat Oct 10 12:11:29 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 12:11:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:11:29 2020 - [info] GTID ONSat Oct 10 12:11:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:11:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:11:29 2020 - [info] Non-candidate masters:Sat Oct 10 12:11:29 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 12:11:29 2020 - [info] New master is 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:11:29 2020 - [info] Starting master failover..Sat Oct 10 12:11:29 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.12(172.16.120.12:3358) (new master) +--172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 12:11:29 2020 - [info] Sat Oct 10 12:11:29 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 12:11:29 2020 - [info] done.Sat Oct 10 12:11:29 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 12:11:29 2020 - [info] mysql-bin.000007:3233250Sat Oct 10 12:11:29 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.12&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 12:11:29 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000007, 3233250, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20749,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 12:11:29 2020 - [info] Executing master IP activate script:Sat Oct 10 12:11:29 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.12 --new_master_ip=172.16.120.12 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.12 RTNETLINK answers: File existsFake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 12:11:30 2020 - [info] OK.Sat Oct 10 12:11:30 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 12:11:30 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 12:11:30 2020 - [info] Sat Oct 10 12:11:30 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 12:11:30 2020 - [info] Sat Oct 10 12:11:30 2020 - [info] Sat Oct 10 12:11:30 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 12:11:30 2020 - [info] Sat Oct 10 12:11:30 2020 - [info] -- Slave recovery on host 172.16.120.11(172.16.120.11:3358) started, pid: 81557. Check tmp log /masterha/cls_new//172.16.120.11_3358_20201010121128.log if it takes time..Sat Oct 10 12:11:31 2020 - [info] Sat Oct 10 12:11:31 2020 - [info] Log messages from 172.16.120.11 ...Sat Oct 10 12:11:31 2020 - [info] Sat Oct 10 12:11:30 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 12:11:30 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 12:11:30 2020 - [info] Slave started.Sat Oct 10 12:11:30 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20749,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.11(172.16.120.11:3358). Executed 2 events.Sat Oct 10 12:11:31 2020 - [info] End of log messages from 172.16.120.11.Sat Oct 10 12:11:31 2020 - [info] -- Slave on host 172.16.120.11(172.16.120.11:3358) started.Sat Oct 10 12:11:31 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 12:11:31 2020 - [info] Sat Oct 10 12:11:31 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 12:11:31 2020 - [info] Sat Oct 10 12:11:31 2020 - [info] Resetting slave info on the new master..Sat Oct 10 12:11:31 2020 - [info] 172.16.120.12: Resetting slave info succeeded.Sat Oct 10 12:11:31 2020 - [info] Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 12:11:31 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.12(172.16.120.12:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.12(172.16.120.12:3358) as a new master.172.16.120.12(172.16.120.12:3358): OK: Applying all logs succeeded.172.16.120.12(172.16.120.12:3358): OK: Activated master IP address.172.16.120.11(172.16.120.11:3358): OK: Slave started, replicating from 172.16.120.12(172.16.120.12:3358)172.16.120.12(172.16.120.12:3358): Resetting slave info succeeded.Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 12:11:31 2020 - [info] Sending mail.. ping_type=INSERT启动manager后, 调整master防火墙, 禁止slave-1访问12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 12:14:59 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 12:15:00 2020 - [info] GTID failover mode = 1Sat Oct 10 12:15:00 2020 - [info] Dead Servers:Sat Oct 10 12:15:00 2020 - [info] Alive Servers:Sat Oct 10 12:15:00 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:15:00 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:15:00 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:15:00 2020 - [info] Alive Slaves:Sat Oct 10 12:15:00 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:15:00 2020 - [info] GTID ONSat Oct 10 12:15:00 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:15:00 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:15:00 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:15:00 2020 - [info] GTID ONSat Oct 10 12:15:00 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:15:00 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:15:00 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:15:00 2020 - [info] Checking slave configurations..Sat Oct 10 12:15:00 2020 - [info] Checking replication filtering settings..Sat Oct 10 12:15:00 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 12:15:00 2020 - [info] Replication filtering check ok.Sat Oct 10 12:15:00 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 12:15:00 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 12:15:00 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 12:15:00 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:15:00 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 12:15:00 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 12:15:00 2020 - [info] OK.Sat Oct 10 12:15:00 2020 - [warning] shutdown_script is not defined.Sat Oct 10 12:15:00 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 12:15:00 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 12:15:00 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 12:15:01 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond.. 调整master防火墙, 禁止slave-1访问1234567IPTABLES=&quot;/sbin/iptables&quot;$IPTABLES -F$IPTABLES -A INPUT -p icmp --icmp-type any -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.10 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.13 -j ACCEPT$IPTABLES -A INPUT -p tcp -s 172.16.120.12 -j ACCEPT$IPTABLES -A INPUT -p tcp --syn -j DROP 在master kill slave-1 io_thread12345678910111213141516171819202122232425262728293031root@localhost 12:13:11 [dbms_monitor]&gt; show processlist;+----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |+----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+| 2 | proxysql | 172.16.120.12:33486 | NULL | Sleep | 9 | | NULL | 0 | 0 || 3 | root | localhost | dbms_monitor | Query | 0 | starting | show processlist | 0 | 0 || 4 | proxysql | 172.16.120.10:34148 | NULL | Sleep | 4 | | NULL | 0 | 0 || 5 | proxysql | 172.16.120.11:35190 | NULL | Sleep | 2 | | NULL | 0 | 0 || 8 | repler | 172.16.120.11:35200 | NULL | Binlog Dump GTID | 428 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 9 | repler | 172.16.120.12:33490 | NULL | Binlog Dump GTID | 379 | Master has sent all binlog to slave; waiting for more updates | NULL | 0 | 0 || 13 | mha | 172.16.120.13:40526 | NULL | Sleep | 0 | | NULL | 0 | 0 || 17 | proxysql | 172.16.120.10:34164 | NULL | Sleep | 14 | | NULL | 1 | 0 |+----+----------+---------------------+--------------+------------------+------+---------------------------------------------------------------+------------------+-----------+---------------+8 rows in set (0.00 sec)root@localhost 12:20:46 [dbms_monitor]&gt; kill 8;Query OK, 0 rows affected (0.00 sec)root@localhost 12:20:59 [dbms_monitor]&gt; truncate table monitor_delay;Query OK, 0 rows affected (0.01 sec)root@localhost 12:21:10 [dbms_monitor]&gt; insert into monitor_delay values(88,now());Query OK, 1 row affected (0.00 sec)root@localhost 12:21:17 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 |+----+---------------------+1 row in set (0.00 sec) slave-11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374root@localhost 12:21:29 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Reconnecting after a failed master event read Master_Host: 172.16.120.10 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000015 Read_Master_Log_Pos: 85472 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 85645 Relay_Master_Log_File: mysql-bin.000015 Slave_IO_Running: Connecting Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 85472 Relay_Log_Space: 85852 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 2003 Last_IO_Error: error reconnecting to master &#x27;repler@172.16.120.10:3358&#x27; - retry-time: 1 retries: 1 Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 120103358 Master_UUID: 44a4ea53-fcad-11ea-bd16-0050563b7b42 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: 201010 12:21:59 Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20750-21111 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-21111,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)root@localhost 12:22:03 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 || 3 | 2020-10-10 11:51:01 || 4 | 2020-10-10 11:59:15 || 5 | 2020-10-10 12:04:35 || 6 | 2020-10-10 12:05:30 |+----+---------------------+6 rows in set (0.00 sec) slave-21234567root@localhost 12:21:32 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 |+----+---------------------+1 row in set (0.00 sec) 关闭master12root@localhost 12:21:24 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184Sat Oct 10 12:22:43 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Sat Oct 10 12:22:43 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 12:22:43 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTSat Oct 10 12:22:43 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 12:22:46 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:22:46 2020 - [warning] Connection failed 2 time(s)..Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 12:22:48 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 12:22:49 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:22:49 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 12:22:52 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 12:22:52 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 12:22:52 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 12:22:52 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 12:22:52 2020 - [warning] SSH is reachable.Sat Oct 10 12:22:52 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 12:22:52 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 12:22:52 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 12:22:52 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 12:22:53 2020 - [info] GTID failover mode = 1Sat Oct 10 12:22:53 2020 - [info] Dead Servers:Sat Oct 10 12:22:53 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:53 2020 - [info] Alive Servers:Sat Oct 10 12:22:53 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:22:53 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:22:53 2020 - [info] Alive Slaves:Sat Oct 10 12:22:53 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:53 2020 - [info] GTID ONSat Oct 10 12:22:53 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:53 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:53 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:53 2020 - [info] GTID ONSat Oct 10 12:22:53 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:53 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:53 2020 - [info] Checking slave configurations..Sat Oct 10 12:22:53 2020 - [info] Checking replication filtering settings..Sat Oct 10 12:22:53 2020 - [info] Replication filtering check ok.Sat Oct 10 12:22:53 2020 - [info] Master is down!Sat Oct 10 12:22:53 2020 - [info] Terminating monitoring script.Sat Oct 10 12:22:53 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 12:22:53 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 12:22:53 2020 - [info] Starting master failover.Sat Oct 10 12:22:53 2020 - [info] Sat Oct 10 12:22:53 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 12:22:53 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] GTID failover mode = 1Sat Oct 10 12:22:54 2020 - [info] Dead Servers:Sat Oct 10 12:22:54 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Alive Servers:Sat Oct 10 12:22:54 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:22:54 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:22:54 2020 - [info] Alive Slaves:Sat Oct 10 12:22:54 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] Starting GTID based failover.Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 12:22:54 2020 - [info] Executing master IP deactivation script:Sat Oct 10 12:22:54 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 RTNETLINK answers: Cannot assign requested addressFake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 12:22:54 2020 - [info] done.Sat Oct 10 12:22:54 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 12:22:54 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000015:110146Sat Oct 10 12:22:54 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20750-21216Sat Oct 10 12:22:54 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 12:22:54 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000015:85472Sat Oct 10 12:22:54 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20750-21111Sat Oct 10 12:22:54 2020 - [info] Oldest slaves:Sat Oct 10 12:22:54 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] Searching new master from slaves..Sat Oct 10 12:22:54 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 12:22:54 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 12:22:54 2020 - [info] GTID ONSat Oct 10 12:22:54 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 12:22:54 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 12:22:54 2020 - [info] Non-candidate masters:Sat Oct 10 12:22:54 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 12:22:54 2020 - [info] New master is 172.16.120.12(172.16.120.12:3358)Sat Oct 10 12:22:54 2020 - [info] Starting master failover..Sat Oct 10 12:22:54 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.12(172.16.120.12:3358) (new master) +--172.16.120.11(172.16.120.11:3358)Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 12:22:54 2020 - [info] done.Sat Oct 10 12:22:54 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 12:22:54 2020 - [info] mysql-bin.000007:3342407Sat Oct 10 12:22:54 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.12&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 12:22:54 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000007, 3342407, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-21216,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 12:22:54 2020 - [info] Executing master IP activate script:Sat Oct 10 12:22:54 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.12 --new_master_ip=172.16.120.12 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.12 RTNETLINK answers: File existsFake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 12:22:54 2020 - [info] OK.Sat Oct 10 12:22:54 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 12:22:54 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 12:22:54 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] -- Slave recovery on host 172.16.120.11(172.16.120.11:3358) started, pid: 82756. Check tmp log /masterha/cls_new//172.16.120.11_3358_20201010122253.log if it takes time..Sat Oct 10 12:22:55 2020 - [info] Sat Oct 10 12:22:55 2020 - [info] Log messages from 172.16.120.11 ...Sat Oct 10 12:22:55 2020 - [info] Sat Oct 10 12:22:54 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 12:22:54 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 12:22:54 2020 - [info] Slave started.Sat Oct 10 12:22:55 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-21216,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.11(172.16.120.11:3358). Executed 2 events.Sat Oct 10 12:22:55 2020 - [info] End of log messages from 172.16.120.11.Sat Oct 10 12:22:55 2020 - [info] -- Slave on host 172.16.120.11(172.16.120.11:3358) started.Sat Oct 10 12:22:55 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 12:22:55 2020 - [info] Sat Oct 10 12:22:55 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 12:22:55 2020 - [info] Sat Oct 10 12:22:55 2020 - [info] Resetting slave info on the new master..Sat Oct 10 12:22:55 2020 - [info] 172.16.120.12: Resetting slave info succeeded.Sat Oct 10 12:22:55 2020 - [info] Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 12:22:55 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.12(172.16.120.12:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.12(172.16.120.12:3358) as a new master.172.16.120.12(172.16.120.12:3358): OK: Applying all logs succeeded.172.16.120.12(172.16.120.12:3358): OK: Activated master IP address.172.16.120.11(172.16.120.11:3358): OK: Slave started, replicating from 172.16.120.12(172.16.120.12:3358)172.16.120.12(172.16.120.12:3358): Resetting slave info succeeded.Master failover to 172.16.120.12(172.16.120.12:3358) completed successfully.Sat Oct 10 12:22:55 2020 - [info] Sending mail.. [用例测试] master挂了, 且slave也有问题4(部分slave sql_thread stop) master挂了, 在此之前slave-1 sql_thread stop了 ping_type=CONNECT启动manager后, 关闭slave-1 sql_thread12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 11:43:34 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 11:43:35 2020 - [info] GTID failover mode = 1Sat Oct 10 11:43:35 2020 - [info] Dead Servers:Sat Oct 10 11:43:35 2020 - [info] Alive Servers:Sat Oct 10 11:43:35 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:43:35 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:43:35 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:43:35 2020 - [info] Alive Slaves:Sat Oct 10 11:43:35 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:43:35 2020 - [info] GTID ONSat Oct 10 11:43:35 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:43:35 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:43:35 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:43:35 2020 - [info] GTID ONSat Oct 10 11:43:35 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:43:35 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:43:35 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:43:35 2020 - [info] Checking slave configurations..Sat Oct 10 11:43:35 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:43:35 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 11:43:35 2020 - [info] Replication filtering check ok.Sat Oct 10 11:43:35 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 11:43:35 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 11:43:35 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 11:43:35 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:43:35 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 11:43:35 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 11:43:35 2020 - [info] OK.Sat Oct 10 11:43:35 2020 - [warning] shutdown_script is not defined.Sat Oct 10 11:43:35 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 11:43:35 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 11:43:35 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 11:43:35 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..关闭slave-1 sql_thread123456789101112131415161718192021222324root@localhost 11:44:26 [dbms_monitor]&gt; stop slave sql_thread;Query OK, 0 rows affected (0.01 sec)root@localhost 11:50:00 [dbms_monitor]&gt; pager cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;PAGER set to &#x27;cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;&#x27;root@localhost 11:50:04 [dbms_monitor]&gt; show slave status\\G Master_Log_File: mysql-bin.000013 Read_Master_Log_Pos: 194 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 367 Relay_Master_Log_File: mysql-bin.000013 Slave_IO_Running: Yes Slave_SQL_Running: No Last_Errno: 0 Last_Error: Exec_Master_Log_Pos: 194 Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Slave_SQL_Running_State: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 1 row in set (0.00 sec)关闭sql_thread后 manager仍然正常 关闭master12345root@localhost 11:40:00 [dbms_monitor]&gt; insert into monitor_delay values(3,now());Query OK, 1 row affected (0.00 sec)root@localhost 11:51:01 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec)slave-112345678root@localhost 11:50:04 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 |+----+---------------------+2 rows in set (0.00 sec)slave-2123456789root@localhost 11:36:17 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 || 3 | 2020-10-10 11:51:01 |+----+---------------------+3 rows in set (0.00 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207Sat Oct 10 11:51:18 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:51:18 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTSat Oct 10 11:51:18 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 11:51:18 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 11:51:18 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 11:51:21 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:51:21 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 11:51:24 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:51:24 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 11:51:27 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 11:51:27 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 11:51:27 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 11:51:27 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 11:51:27 2020 - [warning] SSH is reachable.Sat Oct 10 11:51:27 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 11:51:27 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 11:51:27 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:51:27 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 11:51:28 2020 - [warning] SQL Thread is stopped(no error) on 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:51:28 2020 - [info] GTID failover mode = 1Sat Oct 10 11:51:28 2020 - [info] Dead Servers:Sat Oct 10 11:51:28 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:28 2020 - [info] Alive Servers:Sat Oct 10 11:51:28 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:51:28 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:51:28 2020 - [info] Alive Slaves:Sat Oct 10 11:51:28 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:28 2020 - [info] GTID ONSat Oct 10 11:51:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:28 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:28 2020 - [info] GTID ONSat Oct 10 11:51:28 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:28 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:28 2020 - [info] Checking slave configurations..Sat Oct 10 11:51:28 2020 - [info] Checking replication filtering settings..Sat Oct 10 11:51:28 2020 - [info] Replication filtering check ok.Sat Oct 10 11:51:28 2020 - [info] Master is down!Sat Oct 10 11:51:28 2020 - [info] Terminating monitoring script.Sat Oct 10 11:51:28 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 11:51:28 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 11:51:28 2020 - [info] Starting master failover.Sat Oct 10 11:51:28 2020 - [info] Sat Oct 10 11:51:28 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 11:51:28 2020 - [info] Sat Oct 10 11:51:29 2020 - [warning] SQL Thread is stopped(no error) on 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:51:29 2020 - [info] GTID failover mode = 1Sat Oct 10 11:51:29 2020 - [info] Dead Servers:Sat Oct 10 11:51:29 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Checking master reachability via MySQL(double check)...Sat Oct 10 11:51:29 2020 - [info] ok.Sat Oct 10 11:51:29 2020 - [info] Alive Servers:Sat Oct 10 11:51:29 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:51:29 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:51:29 2020 - [info] Alive Slaves:Sat Oct 10 11:51:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] Starting SQL thread on 172.16.120.11(172.16.120.11:3358) ..Sat Oct 10 11:51:29 2020 - [info] done.Sat Oct 10 11:51:29 2020 - [info] Starting GTID based failover.Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 11:51:29 2020 - [info] Executing master IP deactivation script:Sat Oct 10 11:51:29 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 RTNETLINK answers: Cannot assign requested addressFake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 11:51:29 2020 - [info] done.Sat Oct 10 11:51:29 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 11:51:29 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000013:486Sat Oct 10 11:51:29 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20746Sat Oct 10 11:51:29 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 11:51:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000013:486Sat Oct 10 11:51:29 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:20746Sat Oct 10 11:51:29 2020 - [info] Oldest slaves:Sat Oct 10 11:51:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] Searching new master from slaves..Sat Oct 10 11:51:29 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 11:51:29 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 11:51:29 2020 - [info] GTID ONSat Oct 10 11:51:29 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 11:51:29 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 11:51:29 2020 - [info] Non-candidate masters:Sat Oct 10 11:51:29 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 11:51:29 2020 - [info] New master is 172.16.120.11(172.16.120.11:3358)Sat Oct 10 11:51:29 2020 - [info] Starting master failover..Sat Oct 10 11:51:29 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.11(172.16.120.11:3358) (new master) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 11:51:29 2020 - [info] done.Sat Oct 10 11:51:29 2020 - [info] Replicating from the latest slave 172.16.120.12(172.16.120.12:3358) and waiting to apply..Sat Oct 10 11:51:29 2020 - [info] Waiting all logs to be applied on the latest slave.. Sat Oct 10 11:51:29 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 11:51:29 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 11:51:29 2020 - [info] Slave started.Sat Oct 10 11:51:29 2020 - [info] Waiting to execute all relay logs on 172.16.120.11(172.16.120.11:3358)..Sat Oct 10 11:51:29 2020 - [info] master_pos_wait(mysql-bin.000007:3232449) completed on 172.16.120.11(172.16.120.11:3358). Executed 1 events.Sat Oct 10 11:51:29 2020 - [info] done.Sat Oct 10 11:51:29 2020 - [info] done.Sat Oct 10 11:51:29 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 11:51:29 2020 - [info] mysql-bin.000010:141523Sat Oct 10 11:51:29 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.11&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 11:51:29 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000010, 141523, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20746,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 11:51:29 2020 - [info] Executing master IP activate script:Sat Oct 10 11:51:29 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.11 --new_master_ip=172.16.120.11 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.11 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 11:51:29 2020 - [info] OK.Sat Oct 10 11:51:29 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 11:51:29 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 11:51:29 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] -- Slave recovery on host 172.16.120.12(172.16.120.12:3358) started, pid: 79937. Check tmp log /masterha/cls_new//172.16.120.12_3358_20201010115128.log if it takes time..Sat Oct 10 11:51:30 2020 - [info] Sat Oct 10 11:51:30 2020 - [info] Log messages from 172.16.120.12 ...Sat Oct 10 11:51:30 2020 - [info] Sat Oct 10 11:51:29 2020 - [info] Resetting slave 172.16.120.12(172.16.120.12:3358) and starting replication from the new master 172.16.120.11(172.16.120.11:3358)..Sat Oct 10 11:51:29 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 11:51:29 2020 - [info] Slave started.Sat Oct 10 11:51:29 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-20746,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.12(172.16.120.12:3358). Executed 0 events.Sat Oct 10 11:51:30 2020 - [info] End of log messages from 172.16.120.12.Sat Oct 10 11:51:30 2020 - [info] -- Slave on host 172.16.120.12(172.16.120.12:3358) started.Sat Oct 10 11:51:30 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 11:51:30 2020 - [info] Sat Oct 10 11:51:30 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 11:51:30 2020 - [info] Sat Oct 10 11:51:30 2020 - [info] Resetting slave info on the new master..Sat Oct 10 11:51:30 2020 - [info] 172.16.120.11: Resetting slave info succeeded.Sat Oct 10 11:51:30 2020 - [info] Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Sat Oct 10 11:51:30 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.11(172.16.120.11:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.11(172.16.120.11:3358) as a new master.172.16.120.11(172.16.120.11:3358): OK: Applying all logs succeeded.172.16.120.11(172.16.120.11:3358): OK: Activated master IP address.172.16.120.12(172.16.120.12:3358): OK: Slave started, replicating from 172.16.120.11(172.16.120.11:3358)172.16.120.11(172.16.120.11:3358): Resetting slave info succeeded.Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Sat Oct 10 11:51:30 2020 - [info] Sending mail.. slave-1成了new master123456789101112root@localhost 11:51:06 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-10-10 11:21:39 || 2 | 2020-10-10 11:31:45 || 3 | 2020-10-10 11:51:01 |+----+---------------------+3 rows in set (0.00 sec)root@localhost 11:54:53 [dbms_monitor]&gt; show slave status;Empty set (0.00 sec) ping_type=INSERT启动manager后, 关闭slave-1 sql_thread12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 14:06:09 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 14:06:10 2020 - [info] GTID failover mode = 1Sat Oct 10 14:06:10 2020 - [info] Dead Servers:Sat Oct 10 14:06:10 2020 - [info] Alive Servers:Sat Oct 10 14:06:10 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:06:10 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:06:10 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:06:10 2020 - [info] Alive Slaves:Sat Oct 10 14:06:10 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:06:10 2020 - [info] GTID ONSat Oct 10 14:06:10 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:06:10 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:06:10 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:06:10 2020 - [info] GTID ONSat Oct 10 14:06:10 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:06:10 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:06:10 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:06:10 2020 - [info] Checking slave configurations..Sat Oct 10 14:06:10 2020 - [info] Checking replication filtering settings..Sat Oct 10 14:06:10 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 14:06:10 2020 - [info] Replication filtering check ok.Sat Oct 10 14:06:10 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 14:06:10 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 14:06:10 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 14:06:10 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:06:10 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 14:06:10 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 14:06:10 2020 - [info] OK.Sat Oct 10 14:06:10 2020 - [warning] shutdown_script is not defined.Sat Oct 10 14:06:10 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 14:06:10 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 14:06:10 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 14:06:10 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..关闭slave-1 sql_thread123456789101112131415161718192021222324252627root@localhost 14:10:20 [dbms_monitor]&gt; stop slave sql_thread;Query OK, 0 rows affected (0.03 sec)root@localhost 14:22:34 [dbms_monitor]&gt; pager cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;PAGER set to &#x27;cat - | grep -E &#x27;Master_Log_File|Relay_Master_Log_File|Read_Master_Log_Pos|Exec_Master_Log_Pos|Slave_IO_Running|Slave_SQL_Running|Slave_SQL_Running_State|Last|Relay_Log_File|Relay_Log_Pos&#x27;&#x27;root@localhost 14:22:57 [dbms_monitor]&gt; show slave status\\G Master_Log_File: mysql-bin.000016 Read_Master_Log_Pos: 238476 Relay_Log_File: mysql-relay-bin.000004 Relay_Log_Pos: 211835 Relay_Master_Log_File: mysql-bin.000016 Slave_IO_Running: Yes Slave_SQL_Running: No Last_Errno: 0 Last_Error: Exec_Master_Log_Pos: 211756 Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Slave_SQL_Running_State: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 1 row in set (0.00 sec)root@localhost 14:22:57 [dbms_monitor]&gt; pagerDefault pager wasn&#x27;t set, using stdout.master1234567891011root@localhost 14:22:09 [dbms_monitor]&gt; insert into monitor_delay values(90, now());Query OK, 1 row affected (0.00 sec)root@localhost 14:22:29 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 || 90 | 2020-10-10 14:22:29 |+----+---------------------+2 rows in set (0.01 sec)slave-11234567root@localhost 14:22:57 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 |+----+---------------------+1 row in set (0.00 sec)slave-212345678root@localhost 14:22:36 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 || 90 | 2020-10-10 14:22:29 |+----+---------------------+2 rows in set (0.00 sec) 关闭sql_thread后 manager仍然正常 关闭master12root@localhost 14:23:38 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会failover且成功123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204Sat Oct 10 14:25:05 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Sat Oct 10 14:25:05 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTSat Oct 10 14:25:05 2020 - [info] Executing SSH check script: exit 0Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Sat Oct 10 14:25:06 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 14:25:06 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 14:25:08 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:25:08 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 14:25:11 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:25:11 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 14:25:14 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:25:14 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 14:25:14 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 14:25:14 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 14:25:14 2020 - [warning] SSH is reachable.Sat Oct 10 14:25:14 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 14:25:14 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 14:25:14 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 14:25:14 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 14:25:15 2020 - [warning] SQL Thread is stopped(no error) on 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:25:15 2020 - [info] GTID failover mode = 1Sat Oct 10 14:25:15 2020 - [info] Dead Servers:Sat Oct 10 14:25:15 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:15 2020 - [info] Alive Servers:Sat Oct 10 14:25:15 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:25:15 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:25:15 2020 - [info] Alive Slaves:Sat Oct 10 14:25:15 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:15 2020 - [info] GTID ONSat Oct 10 14:25:15 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:15 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:15 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:15 2020 - [info] GTID ONSat Oct 10 14:25:15 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:15 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:15 2020 - [info] Checking slave configurations..Sat Oct 10 14:25:15 2020 - [info] Checking replication filtering settings..Sat Oct 10 14:25:15 2020 - [info] Replication filtering check ok.Sat Oct 10 14:25:15 2020 - [info] Master is down!Sat Oct 10 14:25:15 2020 - [info] Terminating monitoring script.Sat Oct 10 14:25:15 2020 - [info] Got exit code 20 (Master dead).Sat Oct 10 14:25:15 2020 - [info] MHA::MasterFailover version 0.58.Sat Oct 10 14:25:15 2020 - [info] Starting master failover.Sat Oct 10 14:25:15 2020 - [info] Sat Oct 10 14:25:15 2020 - [info] * Phase 1: Configuration Check Phase..Sat Oct 10 14:25:15 2020 - [info] Sat Oct 10 14:25:16 2020 - [warning] SQL Thread is stopped(no error) on 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:25:16 2020 - [info] GTID failover mode = 1Sat Oct 10 14:25:16 2020 - [info] Dead Servers:Sat Oct 10 14:25:16 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Alive Servers:Sat Oct 10 14:25:16 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:25:16 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:25:16 2020 - [info] Alive Slaves:Sat Oct 10 14:25:16 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] Starting SQL thread on 172.16.120.11(172.16.120.11:3358) ..Sat Oct 10 14:25:16 2020 - [info] done.Sat Oct 10 14:25:16 2020 - [info] Starting GTID based failover.Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] ** Phase 1: Configuration Check Phase completed.Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] Forcing shutdown so that applications never connect to the current master..Sat Oct 10 14:25:16 2020 - [info] Executing master IP deactivation script:Sat Oct 10 14:25:16 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --command=stopssh --ssh_user=root Disabling the VIP on old master: 172.16.120.10 RTNETLINK answers: Cannot assign requested addressFake!!! 原主库 rpl_semi_sync_master_enabled=0 rpl_semi_sync_slave_enabled=1 Sat Oct 10 14:25:16 2020 - [info] done.Sat Oct 10 14:25:16 2020 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Oct 10 14:25:16 2020 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] * Phase 3: Master Recovery Phase..Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] The latest binary log file/position on all slaves is mysql-bin.000016:268346Sat Oct 10 14:25:16 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:21217-22354Sat Oct 10 14:25:16 2020 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Oct 10 14:25:16 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] The oldest binary log file/position on all slaves is mysql-bin.000016:268346Sat Oct 10 14:25:16 2020 - [info] Retrieved Gtid Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:21217-22354Sat Oct 10 14:25:16 2020 - [info] Oldest slaves:Sat Oct 10 14:25:16 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] * Phase 3.3: Determining New Master Phase..Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] Searching new master from slaves..Sat Oct 10 14:25:16 2020 - [info] Candidate masters from the configuration file:Sat Oct 10 14:25:16 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:25:16 2020 - [info] GTID ONSat Oct 10 14:25:16 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:25:16 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:25:16 2020 - [info] Non-candidate masters:Sat Oct 10 14:25:16 2020 - [info] Searching from candidate_master slaves which have received the latest relay log events..Sat Oct 10 14:25:16 2020 - [info] New master is 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:25:16 2020 - [info] Starting master failover..Sat Oct 10 14:25:16 2020 - [info] From:172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)To:172.16.120.11(172.16.120.11:3358) (new master) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] * Phase 3.3: New Master Recovery Phase..Sat Oct 10 14:25:16 2020 - [info] Sat Oct 10 14:25:16 2020 - [info] Waiting all logs to be applied.. Sat Oct 10 14:25:17 2020 - [info] done.Sat Oct 10 14:25:17 2020 - [info] Replicating from the latest slave 172.16.120.12(172.16.120.12:3358) and waiting to apply..Sat Oct 10 14:25:17 2020 - [info] Waiting all logs to be applied on the latest slave.. Sat Oct 10 14:25:17 2020 - [info] Resetting slave 172.16.120.11(172.16.120.11:3358) and starting replication from the new master 172.16.120.12(172.16.120.12:3358)..Sat Oct 10 14:25:17 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 14:25:17 2020 - [info] Slave started.Sat Oct 10 14:25:17 2020 - [info] Waiting to execute all relay logs on 172.16.120.11(172.16.120.11:3358)..Sat Oct 10 14:25:17 2020 - [info] master_pos_wait(mysql-bin.000007:3608644) completed on 172.16.120.11(172.16.120.11:3358). Executed 1 events.Sat Oct 10 14:25:17 2020 - [info] done.Sat Oct 10 14:25:17 2020 - [info] done.Sat Oct 10 14:25:17 2020 - [info] Getting new master&#x27;s binlog name and position..Sat Oct 10 14:25:17 2020 - [info] mysql-bin.000010:517718Sat Oct 10 14:25:17 2020 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;172.16.120.11&#x27;, MASTER_PORT=3358, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;repler&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Sat Oct 10 14:25:17 2020 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: mysql-bin.000010, 517718, 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-22354,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27Sat Oct 10 14:25:17 2020 - [info] Executing master IP activate script:Sat Oct 10 14:25:17 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=start --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 --new_master_host=172.16.120.11 --new_master_ip=172.16.120.11 --new_master_port=3358 --new_master_user=&#x27;mha&#x27; --new_master_password=xxxEnabling the VIP - 172.16.120.128 on the new master - 172.16.120.11 Fake!!! 新主库 rpl_semi_sync_master_enabled=1 rpl_semi_sync_slave_enabled=0 Set read_only=0 on the new master.Creating app user on the new master..Sat Oct 10 14:25:18 2020 - [info] OK.Sat Oct 10 14:25:18 2020 - [info] ** Finished master recovery successfully.Sat Oct 10 14:25:18 2020 - [info] * Phase 3: Master Recovery Phase completed.Sat Oct 10 14:25:18 2020 - [info] Sat Oct 10 14:25:18 2020 - [info] * Phase 4: Slaves Recovery Phase..Sat Oct 10 14:25:18 2020 - [info] Sat Oct 10 14:25:18 2020 - [info] Sat Oct 10 14:25:18 2020 - [info] * Phase 4.1: Starting Slaves in parallel..Sat Oct 10 14:25:18 2020 - [info] Sat Oct 10 14:25:18 2020 - [info] -- Slave recovery on host 172.16.120.12(172.16.120.12:3358) started, pid: 89417. Check tmp log /masterha/cls_new//172.16.120.12_3358_20201010142515.log if it takes time..Sat Oct 10 14:25:19 2020 - [info] Sat Oct 10 14:25:19 2020 - [info] Log messages from 172.16.120.12 ...Sat Oct 10 14:25:19 2020 - [info] Sat Oct 10 14:25:18 2020 - [info] Resetting slave 172.16.120.12(172.16.120.12:3358) and starting replication from the new master 172.16.120.11(172.16.120.11:3358)..Sat Oct 10 14:25:18 2020 - [info] Executed CHANGE MASTER.Sat Oct 10 14:25:18 2020 - [info] Slave started.Sat Oct 10 14:25:18 2020 - [info] gtid_wait(44a4ea53-fcad-11ea-bd16-0050563b7b42:1-22354,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27) completed on 172.16.120.12(172.16.120.12:3358). Executed 0 events.Sat Oct 10 14:25:19 2020 - [info] End of log messages from 172.16.120.12.Sat Oct 10 14:25:19 2020 - [info] -- Slave on host 172.16.120.12(172.16.120.12:3358) started.Sat Oct 10 14:25:19 2020 - [info] All new slave servers recovered successfully.Sat Oct 10 14:25:19 2020 - [info] Sat Oct 10 14:25:19 2020 - [info] * Phase 5: New master cleanup phase..Sat Oct 10 14:25:19 2020 - [info] Sat Oct 10 14:25:19 2020 - [info] Resetting slave info on the new master..Sat Oct 10 14:25:19 2020 - [info] 172.16.120.11: Resetting slave info succeeded.Sat Oct 10 14:25:19 2020 - [info] Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Sat Oct 10 14:25:19 2020 - [info] ----- Failover Report -----cls_new: MySQL Master failover 172.16.120.10(172.16.120.10:3358) to 172.16.120.11(172.16.120.11:3358) succeededMaster 172.16.120.10(172.16.120.10:3358) is down!Check MHA Manager logs at centos-4:/masterha/cls_new/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on 172.16.120.10(172.16.120.10:3358)Selected 172.16.120.11(172.16.120.11:3358) as a new master.172.16.120.11(172.16.120.11:3358): OK: Applying all logs succeeded.172.16.120.11(172.16.120.11:3358): OK: Activated master IP address.172.16.120.12(172.16.120.12:3358): OK: Slave started, replicating from 172.16.120.11(172.16.120.11:3358)172.16.120.11(172.16.120.11:3358): Resetting slave info succeeded.Master failover to 172.16.120.11(172.16.120.11:3358) completed successfully.Sat Oct 10 14:25:19 2020 - [info] Sending mail.. slave-1123456789root@localhost 14:23:56 [dbms_monitor]&gt; select * from monitor_delay;+----+---------------------+| id | ctime |+----+---------------------+| 88 | 2020-10-10 12:21:17 || 90 | 2020-10-10 14:22:29 |+----+---------------------+2 rows in set (0.00 sec) [用例测试] master挂了, 且slave也有问题5(部分slave sql_thread error) master挂了, 在此之前slave-1 sql_thread error了 ping_type=CONNECT启动manager12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 14:34:42 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 14:34:43 2020 - [info] GTID failover mode = 1Sat Oct 10 14:34:43 2020 - [info] Dead Servers:Sat Oct 10 14:34:43 2020 - [info] Alive Servers:Sat Oct 10 14:34:43 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:34:43 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 14:34:43 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:34:43 2020 - [info] Alive Slaves:Sat Oct 10 14:34:43 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:34:43 2020 - [info] GTID ONSat Oct 10 14:34:43 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:34:43 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:34:43 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.31-34-log (oldest major version between slaves) log-bin:enabledSat Oct 10 14:34:43 2020 - [info] GTID ONSat Oct 10 14:34:43 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:34:43 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 14:34:43 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 14:34:43 2020 - [info] Checking slave configurations..Sat Oct 10 14:34:43 2020 - [info] Checking replication filtering settings..Sat Oct 10 14:34:43 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 14:34:43 2020 - [info] Replication filtering check ok.Sat Oct 10 14:34:43 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 14:34:43 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 14:34:43 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 14:34:43 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 14:34:43 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 14:34:43 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 14:34:43 2020 - [info] OK.Sat Oct 10 14:34:43 2020 - [warning] shutdown_script is not defined.Sat Oct 10 14:34:43 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 14:34:43 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 14:34:43 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 14:34:43 2020 - [info] Ping(CONNECT) succeeded, waiting until MySQL doesn&#x27;t respond..制造slave-1 sql_thread error 在master创建表12root@localhost 14:35:55 [dbms_monitor]&gt; create table make_error(id int not null auto_increment primary key);Query OK, 0 rows affected (0.02 sec) 在slave-1删除make_error表1234567891011root@localhost 14:38:10 [dbms_monitor]&gt; set global super_read_only=0;Query OK, 0 rows affected (0.00 sec)root@localhost 14:38:14 [dbms_monitor]&gt; set sql_log_bin=0;Query OK, 0 rows affected (0.00 sec)root@localhost 14:38:17 [dbms_monitor]&gt; drop table make_error;Query OK, 0 rows affected (0.01 sec)root@localhost 14:38:20 [dbms_monitor]&gt; set sql_log_bin=1;Query OK, 0 rows affected (0.00 sec) 在master删除make_error表(slave-1 sql_thread会报错)12root@localhost 14:36:28 [dbms_monitor]&gt; drop table make_error;Query OK, 0 rows affected (0.01 sec) 查看slave-1复制状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root@localhost 14:38:26 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.16.120.10 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000017 Read_Master_Log_Pos: 620 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 589 Relay_Master_Log_File: mysql-bin.000017 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 1051 Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22356&#x27; at master log mysql-bin.000017, end_log_pos 620. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Skip_Counter: 0 Exec_Master_Log_Pos: 416 Relay_Log_Space: 1000 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1051 Last_SQL_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22356&#x27; at master log mysql-bin.000017, end_log_pos 620. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Replicate_Ignore_Server_Ids: Master_Server_Id: 120103358 Master_UUID: 44a4ea53-fcad-11ea-bd16-0050563b7b42 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 201010 14:39:25 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:22355-22356 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-22355,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 此时manager仍然正常运行 关闭master12root@localhost 14:39:25 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会触发failover, 但failover失败12345678910111213141516171819202122232425Sat Oct 10 14:40:59 2020 - [warning] Got error on MySQL connect ping: DBI connect(&#x27;;host=172.16.120.10;port=3358;mysql_connect_timeout=1&#x27;,&#x27;mha&#x27;,...) failed: Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111) at /usr/local/share/perl5/MHA/HealthCheck.pm line 98.2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:40:59 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=CONNECTSat Oct 10 14:40:59 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 14:40:59 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 14:40:59 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 14:41:02 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:41:02 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 14:41:05 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:41:05 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 14:41:08 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 14:41:08 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 14:41:08 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 14:41:08 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 14:41:08 2020 - [warning] SSH is reachable.Sat Oct 10 14:41:08 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 14:41:08 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 14:41:08 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 14:41:08 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 14:41:09 2020 - [error][/usr/local/share/perl5/MHA/Server.pm, ln935] SQL Thread is stopped(error) on 172.16.120.11(172.16.120.11:3358)! Errno:1051, Error:Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22356&#x27; at master log mysql-bin.000017, end_log_pos 620. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any.Sat Oct 10 14:41:09 2020 - [error][/usr/local/share/perl5/MHA/ServerManager.pm, ln703] Server 172.16.120.11(172.16.120.11:3358) is alive, but does not work as a slave!Sat Oct 10 14:41:09 2020 - [warning] Got Error: at /usr/local/share/perl5/MHA/MasterMonitor.pm line 560.Sat Oct 10 14:41:09 2020 - [info] Got exit code 1 (Not master dead). ping_type=INSERT启动manager12345678910111213141516171819202122232425262728293031323334353637Sat Oct 10 15:54:20 2020 - [info] MHA::MasterMonitor version 0.58.Sat Oct 10 15:54:21 2020 - [info] GTID failover mode = 1Sat Oct 10 15:54:21 2020 - [info] Dead Servers:Sat Oct 10 15:54:21 2020 - [info] Alive Servers:Sat Oct 10 15:54:21 2020 - [info] 172.16.120.10(172.16.120.10:3358)Sat Oct 10 15:54:21 2020 - [info] 172.16.120.11(172.16.120.11:3358)Sat Oct 10 15:54:21 2020 - [info] 172.16.120.12(172.16.120.12:3358)Sat Oct 10 15:54:21 2020 - [info] Alive Slaves:Sat Oct 10 15:54:21 2020 - [info] 172.16.120.11(172.16.120.11:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 15:54:21 2020 - [info] GTID ONSat Oct 10 15:54:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 15:54:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 15:54:21 2020 - [info] 172.16.120.12(172.16.120.12:3358) Version=5.7.29-32-log (oldest major version between slaves) log-bin:enabledSat Oct 10 15:54:21 2020 - [info] GTID ONSat Oct 10 15:54:21 2020 - [info] Replicating from 172.16.120.10(172.16.120.10:3358)Sat Oct 10 15:54:21 2020 - [info] Primary candidate for the new Master (candidate_master is set)Sat Oct 10 15:54:21 2020 - [info] Current Alive Master: 172.16.120.10(172.16.120.10:3358)Sat Oct 10 15:54:21 2020 - [info] Checking slave configurations..Sat Oct 10 15:54:21 2020 - [info] Checking replication filtering settings..Sat Oct 10 15:54:21 2020 - [info] binlog_do_db= , binlog_ignore_db= Sat Oct 10 15:54:21 2020 - [info] Replication filtering check ok.Sat Oct 10 15:54:21 2020 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Sat Oct 10 15:54:21 2020 - [info] Checking SSH publickey authentication settings on the current master..Sat Oct 10 15:54:21 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Sat Oct 10 15:54:21 2020 - [info] 172.16.120.10(172.16.120.10:3358) (current master) +--172.16.120.11(172.16.120.11:3358) +--172.16.120.12(172.16.120.12:3358)Sat Oct 10 15:54:21 2020 - [info] Checking master_ip_failover_script status:Sat Oct 10 15:54:21 2020 - [info] /etc/masterha/scripts/master_ip_failover_vip --vip=172.16.120.128 --command=status --ssh_user=root --orig_master_host=172.16.120.10 --orig_master_ip=172.16.120.10 --orig_master_port=3358 Sat Oct 10 15:54:21 2020 - [info] OK.Sat Oct 10 15:54:21 2020 - [warning] shutdown_script is not defined.Sat Oct 10 15:54:21 2020 - [info] Set master ping interval 3 seconds.Sat Oct 10 15:54:21 2020 - [info] Set secondary check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12Sat Oct 10 15:54:21 2020 - [info] Starting ping health check on 172.16.120.10(172.16.120.10:3358)..Sat Oct 10 15:54:21 2020 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..准备工作省略, slave-1 sql_thread报错12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root@localhost 15:55:13 [dbms_monitor]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.16.120.10 Master_User: repler Master_Port: 3358 Connect_Retry: 1 Master_Log_File: mysql-bin.000019 Read_Master_Log_Pos: 16331 Relay_Log_File: mysql-relay-bin.000007 Relay_Log_Pos: 14926 Relay_Master_Log_File: mysql-bin.000019 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 1051 Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22419&#x27; at master log mysql-bin.000019, end_log_pos 14917. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Skip_Counter: 0 Exec_Master_Log_Pos: 14713 Relay_Log_Space: 19342 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1051 Last_SQL_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22419&#x27; at master log mysql-bin.000019, end_log_pos 14917. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Replicate_Ignore_Server_Ids: Master_Server_Id: 120103358 Master_UUID: 44a4ea53-fcad-11ea-bd16-0050563b7b42 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 201010 15:55:16 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:22356-22425 Executed_Gtid_Set: 44a4ea53-fcad-11ea-bd16-0050563b7b42:1-22418,45d1f02a-fcad-11ea-8a44-0050562f2198:1-27 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 关闭master12root@localhost 15:55:16 [dbms_monitor]&gt; shutdown;Query OK, 0 rows affected (0.00 sec) 结论: 会触发failover, 但failover失败123456789101112131415161718192021222324Sat Oct 10 15:56:03 2020 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)Sat Oct 10 15:56:03 2020 - [info] Executing secondary network check script: masterha_secondary_check -s 172.16.120.11 -s 172.16.120.12 --user=root --master_host=172.16.120.10 --master_ip=172.16.120.10 --master_port=3358 --master_user=mha --master_password=xxx --ping_type=INSERTSat Oct 10 15:56:03 2020 - [info] Executing SSH check script: exit 0Sat Oct 10 15:56:04 2020 - [info] HealthCheck: SSH to 172.16.120.10 is reachable.Monitoring server 172.16.120.11 is reachable, Master is not reachable from 172.16.120.11. OK.Monitoring server 172.16.120.12 is reachable, Master is not reachable from 172.16.120.12. OK.Sat Oct 10 15:56:04 2020 - [info] Master is not reachable from all other monitoring servers. Failover should start.Sat Oct 10 15:56:06 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 15:56:06 2020 - [warning] Connection failed 2 time(s)..Sat Oct 10 15:56:09 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 15:56:09 2020 - [warning] Connection failed 3 time(s)..Sat Oct 10 15:56:12 2020 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;172.16.120.10&#x27; (111))Sat Oct 10 15:56:12 2020 - [warning] Connection failed 4 time(s)..Sat Oct 10 15:56:12 2020 - [warning] Master is not reachable from health checker!Sat Oct 10 15:56:12 2020 - [warning] Master 172.16.120.10(172.16.120.10:3358) is not reachable!Sat Oct 10 15:56:12 2020 - [warning] SSH is reachable.Sat Oct 10 15:56:12 2020 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha/conf/masterha_default.cnf and /etc/masterha/conf/cls_new.cnf again, and trying to connect to all servers to check server status..Sat Oct 10 15:56:12 2020 - [info] Reading default configuration from /etc/masterha/conf/masterha_default.cnf..Sat Oct 10 15:56:12 2020 - [info] Reading application default configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 15:56:12 2020 - [info] Reading server configuration from /etc/masterha/conf/cls_new.cnf..Sat Oct 10 15:56:13 2020 - [error][/usr/local/share/perl5/MHA/Server.pm, ln935] SQL Thread is stopped(error) on 172.16.120.11(172.16.120.11:3358)! Errno:1051, Error:Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;44a4ea53-fcad-11ea-bd16-0050563b7b42:22419&#x27; at master log mysql-bin.000019, end_log_pos 14917. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any.Sat Oct 10 15:56:13 2020 - [error][/usr/local/share/perl5/MHA/ServerManager.pm, ln703] Server 172.16.120.11(172.16.120.11:3358) is alive, but does not work as a slave!Sat Oct 10 15:56:13 2020 - [warning] Got Error: at /usr/local/share/perl5/MHA/MasterMonitor.pm line 560.Sat Oct 10 15:56:13 2020 - [info] Got exit code 1 (Not master dead).","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"使用Canal + ClickHouse实时分析MySQL事务信息","slug":"2020-12-31-使用Canal-+-ClickHouse实时分析MySQL事务信息","date":"2020-12-31T05:54:00.000Z","updated":"2020-12-31T05:56:19.701Z","comments":true,"path":"2020/12/31/2020-12-31-使用Canal-+-ClickHouse实时分析MySQL事务信息/","link":"","permalink":"http://fuxkdb.com/2020/12/31/2020-12-31-%E4%BD%BF%E7%94%A8Canal-+-ClickHouse%E5%AE%9E%E6%97%B6%E5%88%86%E6%9E%90MySQL%E4%BA%8B%E5%8A%A1%E4%BF%A1%E6%81%AF/","excerpt":"使用Canal + ClickHouse实时分析MySQL事务信息作为DBA, 有时候我们会希望能够了解线上核心库更具体的”样貌”, 如: 这个库主要的DML类型是什么? 这个库的事务大小, 执行时间, 影响行数大概是什么样的? 以上信息也许没什么价值, 但大事务对复制的影响不用多说, 并且当我们希望升级当前主从架构到MGR/PXC等高可用方案的场景时以上信息就比较重要了(毕竟用数据说话更有力度). 大事务对MGR和PXC都是不友好的, 尤其是MGR(起码在5.7版本)严重时会导致整个集群hang死 在Galera 4.0中新特性Streaming Replication对大事务有了更好的支持 当然, 有人可能会说, 通过分析binlog就可以完成这样的工作, 最简单的方法写个shell脚本就可以, 比如这篇文章中介绍的方法Identifying Useful Info from MySQL Row-Based Binary Logs(这篇文章介绍的方法比较简单, 分析速度也较慢, 可以试试analysis_binlog). 当然还有很多其他工具, 比如infobin. 但个人认为上面的方法从某种角度来看还是比较麻烦, 而且现在ClickHouse越来越流行, 使用ClickHouse去完成这个工作也能帮助我们更好的学习ClickHouse 先看一下最终的成果","text":"使用Canal + ClickHouse实时分析MySQL事务信息作为DBA, 有时候我们会希望能够了解线上核心库更具体的”样貌”, 如: 这个库主要的DML类型是什么? 这个库的事务大小, 执行时间, 影响行数大概是什么样的? 以上信息也许没什么价值, 但大事务对复制的影响不用多说, 并且当我们希望升级当前主从架构到MGR/PXC等高可用方案的场景时以上信息就比较重要了(毕竟用数据说话更有力度). 大事务对MGR和PXC都是不友好的, 尤其是MGR(起码在5.7版本)严重时会导致整个集群hang死 在Galera 4.0中新特性Streaming Replication对大事务有了更好的支持 当然, 有人可能会说, 通过分析binlog就可以完成这样的工作, 最简单的方法写个shell脚本就可以, 比如这篇文章中介绍的方法Identifying Useful Info from MySQL Row-Based Binary Logs(这篇文章介绍的方法比较简单, 分析速度也较慢, 可以试试analysis_binlog). 当然还有很多其他工具, 比如infobin. 但个人认为上面的方法从某种角度来看还是比较麻烦, 而且现在ClickHouse越来越流行, 使用ClickHouse去完成这个工作也能帮助我们更好的学习ClickHouse 先看一下最终的成果 实现方法 canal + kafka 部署canal, 订阅线上库的binlog, 写入到kafka 我这里没有使用flatMessage(canal.mq.flatMessage = false), 写入到kafka的消息是二进制的protobuf格式的, 当然也可以开启flatMessage, 那么写到kafka的消息就是json格式的. 消费binlog, 持久化到clickhouse 具体代码详见 https://github.com/Fanduzi/Use_clickhouse_2_analyze_mysql_binlog clickhouse表 基础表canal消费后直接写入 1234567891011121314151617181920212223242526272829303132333435363738394041-- 本地表CREATE TABLE mysql_monitor.broker_binlog_local( `schema` String COMMENT &#x27;数据库名&#x27;, `table` String COMMENT &#x27;表名&#x27;, `event_type` String COMMENT &#x27;语句类型&#x27;, `is_ddl` UInt8 COMMENT &#x27;DDL 1 else 0&#x27;, `binlog_file` String COMMENT &#x27;binlog文件名&#x27;, `binlog_pos` String COMMENT &#x27;binlog pos&#x27;, `characterset` String COMMENT &#x27;字符集&#x27;, `execute_time` DateTime COMMENT &#x27;执行的时间&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `single_statement_affected_rows` UInt32 COMMENT &#x27;此语句影响行数&#x27;, `single_statement_size` String DEFAULT &#x27;0&#x27; COMMENT &#x27;此语句size,单位bytes&#x27;, `ctime` DateTime DEFAULT now() COMMENT &#x27;写入clickhouse时间&#x27;)ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_binlog&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toDate(execute_time)ORDER BY (execute_time, gtid, table, schema)TTL execute_time + toIntervalMonth(30)SETTINGS index_granularity = 8192-- 分布式表CREATE TABLE mysql_monitor.broker_binlog( `schema` String COMMENT &#x27;数据库名&#x27;, `table` String COMMENT &#x27;表名&#x27;, `event_type` String COMMENT &#x27;语句类型&#x27;, `is_ddl` UInt8 COMMENT &#x27;DDL 1 else 0&#x27;, `binlog_file` String COMMENT &#x27;binlog文件名&#x27;, `binlog_pos` String COMMENT &#x27;binlog pos&#x27;, `characterset` String COMMENT &#x27;字符集&#x27;, `execute_time` DateTime COMMENT &#x27;执行的时间&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `single_statement_affected_rows` UInt32 COMMENT &#x27;此语句影响行数&#x27;, `single_statement_size` String DEFAULT &#x27;0&#x27; COMMENT &#x27;此语句size,单位bytes&#x27;, `ctime` DateTime DEFAULT now() COMMENT &#x27;写入clickhouse时间&#x27;)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_binlog_local&#x27;, rand()) 统计用表 SummingMergeTree ClickHouse会将所有具有相同主键（或更准确地说, 具有相同sorting key）的行替换为包含具有数字数据类型的列的汇总值的一行 每天binlog 各个event_type数量用于统计每日整体binlog event类型占比 1234567891011121314151617181920212223242526272829303132333435363738394041-- 物化视图基表CREATE TABLE mysql_monitor.broker_daily_binlog_event_count_local ON CLUSTER ch_cluster_all( `day` Date, `event_type` String, `event_count` UInt64)ENGINE = ReplicatedSummingMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_daily_binlog_event_count&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY dayORDER BY (day, event_type)TTL day + toIntervalMonth(30)SETTINGS index_granularity = 8192-- 本地物化视图CREATE MATERIALIZED VIEW mysql_monitor.broker_daily_binlog_event_count_mv_local ON CLUSTER ch_cluster_all TO mysql_monitor.broker_daily_binlog_event_count_local( `day` Date, `event_type` String, `event_count` UInt64) ASSELECT toDate(execute_time) AS day, event_type, count(*) AS event_countFROM mysql_monitor.broker_binlog_localGROUP BY day, event_typeORDER BY day ASC, event_type ASC-- 分布式物化视图CREATE TABLE mysql_monitor.broker_daily_binlog_event_count_mv ON CLUSTER ch_cluster_all( `day` Date, `event_type` String, `event_count` UInt64)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_daily_binlog_event_count_mv_local&#x27;, rand()) grafana中查询语句 12345678SELECT day as t, event_type, sum(event_count)FROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_mvWHERE day = yesterday() and event_type!=&#x27;QUERY&#x27; and event_type!=&#x27;EVENTTYPECOMPATIBLEPROTO2&#x27;GROUP BY day, event_type 每日TOP DML表统计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-- 物化视图基表CREATE TABLE mysql_monitor.broker_daily_binlog_event_count_by_table_local ON CLUSTER ch_cluster_all( `day` Date, `schema` String, `table` String, `event_type` String, `event_count` UInt64)ENGINE = ReplicatedSummingMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_daily_binlog_event_count_by_table&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY dayORDER BY (day, table, schema, event_type)TTL day + toIntervalMonth(30)SETTINGS index_granularity = 8192-- 本地物化视图CREATE MATERIALIZED VIEW mysql_monitor.broker_daily_binlog_event_count_by_table_mv_local ON CLUSTER ch_cluster_all TO mysql_monitor.broker_daily_binlog_event_count_by_table_local( `day` Date, `schema` String, `table` String, `event_type` String, `event_count` UInt64) ASSELECT toDate(execute_time) AS day, schema, table, event_type, count(*) AS event_countFROM mysql_monitor.broker_binlog_localGROUP BY day, schema, table, event_typeORDER BY day ASC, schema ASC, table ASC, event_type DESC-- 分布式物化视图CREATE TABLE mysql_monitor.broker_daily_binlog_event_count_by_table_mv ON CLUSTER ch_cluster_all( `day` Date, `schema` String, `table` String, `event_type` String, `event_count` UInt64)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_daily_binlog_event_count_by_table_mv_local&#x27;, rand()) grafana中查询语句 1234567891011SELECT day as t, table, sum(event_count) countFROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_by_table_mvWHERE day = yesterday() and event_type!=&#x27;QUERY&#x27; and event_type!=&#x27;EVENTTYPECOMPATIBLEPROTO2&#x27;GROUP BY day, tableORDER BY count descLIMIT 10 grafana中查询语句 1234567891011121314151617SELECT day as t, table, event_type, sum(event_count) countFROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_by_table_mvWHERE day = yesterday() and event_type!=&#x27;QUERY&#x27; and event_type!=&#x27;EVENTTYPECOMPATIBLEPROTO2&#x27; and table GLOBAL IN ( select table from ( select table, sum(event_count) count FROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_by_table_mv WHERE day = yesterday() GROUP BY table order by count desc limit 3 ))GROUP BY day, table, event_typeORDER BY count DESC 一周执行DML总量情况 1234567891011121314WITH ( SELECT sum(event_count) FROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_mv WHERE (day &gt;= (today() - 6)) AND (event_type != &#x27;QUERY&#x27;) AND (event_type != &#x27;EVENTTYPECOMPATIBLEPROTO2&#x27;) ) AS total_statement_countSELECT multiIf(toDayOfWeek(day) = 1, &#x27;星期一&#x27;, toDayOfWeek(day) = 2, &#x27;星期二&#x27;, toDayOfWeek(day) = 3, &#x27;星期三&#x27;, toDayOfWeek(day) = 4, &#x27;星期四&#x27;, toDayOfWeek(day) = 5, &#x27;星期五&#x27;, toDayOfWeek(day) = 6, &#x27;星期六&#x27;, toDayOfWeek(day) = 7, &#x27;星期日&#x27;, &#x27;N/A&#x27;) AS w, day AS t, sum(event_count) AS statement_count, bar(statement_count, 0, total_statement_count, 500) AS barFROM mysql_monitor.$&#123;prefix&#125;_daily_binlog_event_count_mvWHERE (day &gt;= (today() - 6)) AND (event_type != &#x27;QUERY&#x27;) AND (event_type != &#x27;EVENTTYPECOMPATIBLEPROTO2&#x27;)GROUP BY day ORDER BY toDate(t) DESC 事务情况统计 统计影响行数对多的事务, 产生binlog最大的事务, 执行时间最长的事务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778CREATE TABLE mysql_monitor.broker_largest_transaction_local ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = ReplicatedSummingMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_largest_transaction&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toDate(end_time)ORDER BY gtidTTL toDate(end_time) + toIntervalMonth(30)SETTINGS index_granularity = 8192CREATE TABLE mysql_monitor.broker_largest_transaction ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_largest_transaction_local&#x27;, rand())CREATE TABLE mysql_monitor.broker_most_time_consuming_transaction_local ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = ReplicatedSummingMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_most_time_consuming_transaction&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toDate(end_time)ORDER BY gtidTTL toDate(end_time) + toIntervalMonth(30)SETTINGS index_granularity = 8192CREATE TABLE mysql_monitor.broker_most_time_consuming_transaction ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_most_time_consuming_transaction_local&#x27;, rand())CREATE TABLE mysql_monitor.broker_most_affected_rows_transaction_local ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = ReplicatedSummingMergeTree(&#x27;/clickhouse/mysql_monitor/tables/&#123;layer&#125;-&#123;shard&#125;/broker_most_affected_rows_transaction&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toDate(end_time)ORDER BY gtidTTL toDate(end_time) + toIntervalMonth(30)SETTINGS index_granularity = 8192CREATE TABLE mysql_monitor.broker_most_affected_rows_transaction ON CLUSTER ch_cluster_all( `end_time` DateTime COMMENT &#x27;采集语句中的end_time&#x27;, `invertal` String COMMENT &#x27;采集周期,单位秒&#x27;, `gtid` String COMMENT &#x27;gtid&#x27;, `transaction_spend_time` Int32 COMMENT &#x27;事务用时&#x27;, `transaction_size` Int64 COMMENT &#x27;事务size&#x27;, `single_statement_affected_rows` UInt64 COMMENT &#x27;事务影响行数&#x27;)ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;mysql_monitor&#x27;, &#x27;broker_most_affected_rows_transaction_local&#x27;, rand()) 想了想只能建三张表, 写脚本自己周期性查询size,耗时, 影响行数最多的在插入这些表中 查询语句大致如下, 由于grafana必须需要一个DateTime列, 所以加了一个toDateTime(&#39;&#123;end&#125;&#39;) 取每次采集窗口的高水位. 三个查询只是order by不同 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192昨日最大事物sizeSELECT $timeSeries as t, sum(transaction_size) sum_transaction_sizeFROM mysql_monitor.$&#123;prefix&#125;_largest_transactionWHERE end_time&gt;=toDateTime(yesterday()) and end_time&lt;toDateTime(today())GROUP BY t,gtidORDER BY sum_transaction_size desc limit 1昨日事务最大执行时间SELECT $timeSeries as t, sum(transaction_spend_time) sum_transaction_spend_timeFROM mysql_monitor.$&#123;prefix&#125;_most_time_consuming_transactionWHERE end_time&gt;=toDateTime(yesterday()) and end_time&lt;toDateTime(today())GROUP BY t,gtidORDER BY sum_transaction_spend_time desc limit 1昨日事务影响最大行数SELECT $timeSeries as t, sum(single_statement_affected_rows) transaction_affected_rowsFROM mysql_monitor.$&#123;prefix&#125;_most_affected_rows_transactionWHERE end_time&gt;=toDateTime(yesterday()) and end_time&lt;toDateTime(today())GROUP BY t,gtidORDER BY transaction_affected_rows desc limit 1上周最大事物概览select t,max(sum_transaction_size) max_transaction_size from (SELECT toDate(end_time) as t, gtid, sum(transaction_size) sum_transaction_sizeFROM mysql_monitor.$&#123;prefix&#125;_largest_transactionWHERE end_time &gt;= (today() - 6)GROUP BY t,gtid) group by t order by t desc 上周事务最大执行时间概览select t,max(sum_transaction_spend_time) max_transaction_spend_time from (SELECT toDate(end_time) as t, gtid, sum(transaction_spend_time) sum_transaction_spend_timeFROM mysql_monitor.$&#123;prefix&#125;_most_time_consuming_transactionWHERE end_time &gt;= (today() - 6)GROUP BY t,gtid) group by t order by t desc 上周事务影响最大行数概览select t,max(transaction_affected_rows) max_transaction_affected_rows from (SELECT toDate(end_time) as t, gtid, sum(single_statement_affected_rows) transaction_affected_rowsFROM mysql_monitor.$&#123;prefix&#125;_most_affected_rows_transactionWHERE end_time &gt;= (today() - 6)GROUP BY t,gtid) group by t order by t desc 近实时事务size图SELECT $timeSeries as t, sum(transaction_size) transaction_sizeFROM mysql_monitor.$&#123;prefix&#125;_largest_transactionWHERE $timeFilterGROUP BY t, gtidORDER BY tSELECT $timeSeries as t, sum(transaction_spend_time) transaction_spend_timeFROM mysql_monitor.$&#123;prefix&#125;_most_time_consuming_transactionWHERE $timeFilterGROUP BY t, gtidORDER BY t近实时事务影响行数图SELECT $timeSeries as t, sum(single_statement_affected_rows) transaction_affected_rowsFROM mysql_monitor.$&#123;prefix&#125;_most_affected_rows_transactionWHERE $timeFilterGROUP BY t, gtidORDER BY tSELECT $timeSeries as t, sum(transaction_spend_time) transaction_spend_timeFROM mysql_monitor.$&#123;prefix&#125;_most_time_consuming_transactionWHERE $timeFilterGROUP BY t, gtidORDER BY t 总结实际干下来发现一些查询确实可以通过物化视图优化, 但是grafana每次都要带一个DateTime比较烦, 可能物化视图还有优化空间. 对于如下图所示的实时统计的需求daily_binlog这种天级物化视图就无法实现细粒度的查询了(查询速度太慢, 必须借助物化视图) 那么如何实现更细粒度的物化视图呢? 得看下如何按周期聚合, 比如5分钟一个聚合. 目前clickhouse没有oracle那样的开窗函数 oracle可以 sum over(partition by gtid order by execute_time range between interval ‘1’ day preceding and interval ‘1’ day following) 对于一些不好优化的查询, 如果换马蜂窝那些一天上百G binlog的库可能真的跑不动了. 也许只能多搞几个分片, 不过不知道最后聚合会不会很耗内存, 感觉最主要是这套系统是否值得付出加节点的金钱成本.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Canal","slug":"Canal","permalink":"http://fuxkdb.com/tags/Canal/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"类MHA高可用方案存在的问题","slug":"2020-09-24-类MHA高可用方案存在的问题","date":"2020-09-24T11:00:00.000Z","updated":"2021-03-10T14:36:57.040Z","comments":true,"path":"2020/09/24/2020-09-24-类MHA高可用方案存在的问题/","link":"","permalink":"http://fuxkdb.com/2020/09/24/2020-09-24-%E7%B1%BBMHA%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"类MHA高可用方案存在的问题MHA Generaly Available since 2011? MHA在当时主要解决两个问题: 自动的数据补偿 自动的主从切换 还有两个重要的背景需要交代: 当时主要使用异步复制 当时还没有ProxySQL 所以当时基本使用MHA+VIP作为MySQL复制集的高可用方案. 不谈vip的脑裂问题, 这种架构的一个关键点在于, MHA是作为一个外部机制检测MySQL复制集状态, 并变更复制集拓扑, 变更后漂移vip, 也就是说MHA既控制了集群拓扑的变化, 又控制了app的访问路径(写通过vip)","text":"类MHA高可用方案存在的问题MHA Generaly Available since 2011? MHA在当时主要解决两个问题: 自动的数据补偿 自动的主从切换 还有两个重要的背景需要交代: 当时主要使用异步复制 当时还没有ProxySQL 所以当时基本使用MHA+VIP作为MySQL复制集的高可用方案. 不谈vip的脑裂问题, 这种架构的一个关键点在于, MHA是作为一个外部机制检测MySQL复制集状态, 并变更复制集拓扑, 变更后漂移vip, 也就是说MHA既控制了集群拓扑的变化, 又控制了app的访问路径(写通过vip) 脑裂MHA+ProxySQL架构有一个问题: MHA对MySQL复制集的监控检测逻辑中不包含ProxySQL, 因为MHA不知道用户在MySQL上层会构建什么样的中间件, 同时MHA变更集群拓扑后也并不会通知ProxySQL. 也就是说MHA和ProxySQL有可能会产生不同的”观点” 如上图场景 APP-1, ProxySQL-1和主库在一个网络分区, 正在写入数据(异步复制, 或半同步超时不是无限大) APP-2, ProxySQL-2, 从库和MHA manager在一个网路分区, 无法连通主库 这种情况, MHA会Failover. Failover后变成如下拓扑 这就导致了脑裂(如果是半同步且超时无限大, 那么不会脑裂, 因为数据无法写入Master). 实际上如果在secondary_check_script中配置了ProxySQL地址 1secondary_check_script=masterha_secondary_check -s ProxySQL-1 -s ProxySQL-2 -s Slave-1 -s Slave-2 -s Slave-3 这样配置后, 在图-1的场景中, manager二次检测时无法ssh到ProxySQL-1, 二次检测脚本会以exit_code=2退出, 不会发生Failover, 就不会脑裂(如果是半同步且超时无限大, 那么不会脑裂, 因为数据无法写入Master). 12345在masterha_secondary_check脚本中有如下注释, 这是exit code的含义, 只有0会触发Failover# 0: master is not reachable from all monotoring servers# 1: unknown errors# 2: at least one of monitoring servers is not reachable from this script# 3: master is reachable from at least one of monitoring servers 极端场景无法完成Failover上文的场景就是一种极端场景. 在这种场景(图-1)下, 正确的做法是切断Master流量(关闭APP-1并不太现实, 因为实际可能不只一个应用, 关闭ProxySQL或Master都是可以的), 然后进行切换. 但这正是MHA无法做到的. 想象图-3场景, 使用半同步复制rpl_semi_sync_master_wait_for_slave_count=1 这种情况下, MHA仍然不会Failover(二次检测脚本-s中指定了slave-4), 那么按照正常逻辑, 应该Failover吗? 个人认为要看情况: 如果添加从库的速度很慢, 一旦Slave-4出现问题无法返回ack, Master将不能提供写入, 那么应该Failover, 将Slave1-3组成一个新的复制集, 一主两从, 两个从库出现异常的概率显然要小很多 如果添加从库的速度很快(备份集小, 自动化完善), 并且当时的场景不允许哪怕3-5秒的不可用(切换用时), 那么可以快速为Master添加一个从库Slave-5 为什么类MHA的高可用方案在这种情况下无法完成Failover? ProxySQL是一个”伪集群” ProxySQL集群目前只做到了配置同步, 成员之间并没有使用共识算法实现选举机制, 在图-1的场景中检查ProxySQL-1和ProxySQL-2的runtime_mysql_servers, 你会看到这样的结果 123456789101112131415161718192021ProxySQL-1admin@127.0.0.1 15:10:50 [(none)]&gt; select * from runtime_mysql_servers;+--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+| 10 | 172.16.120.10 | 3358 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | master for backup read || 11 | 172.16.120.10 | 3358 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | master for backup read || 11 | 172.16.120.12 | 3358 | 0 | SHUNNED | 1000 | 0 | 1000 | 120 | 0 | 0 | slave || 11 | 172.16.120.11 | 3358 | 0 | SHUNNED | 1000 | 0 | 1000 | 120 | 0 | 0 | slave |+--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+ProxySQL-2admin@127.0.0.1 15:10:14 [(none)]&gt; select * from runtime_mysql_servers; +--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+| 11 | 172.16.120.11 | 3358 | 0 | ONLINE | 1000 | 0 | 1000 | 120 | 0 | 0 | slave || 11 | 172.16.120.10 | 3358 | 0 | SHUNNED | 1 | 0 | 1000 | 0 | 0 | 0 | master for backup read || 11 | 172.16.120.12 | 3358 | 0 | ONLINE | 1000 | 0 | 1000 | 120 | 0 | 0 | slave |+--------------+---------------+------+-----------+---------+--------+-------------+-----------------+---------------------+---------+----------------+------------------------+3 rows in set (0.00 sec) 如果ProxySQL集群有选举机制, 那么一个集群至少需要三个节点, 当一个节点意识到自己无法与大多数成员通信时, 它应当将自己置为不可用状态. MHA manager单点 这和ProxySQL有一些类似的情况. 如果MHA也是一个集群, 并在MySQL复制集的每个节点部署一个node, 那么在图-1的场景中, Master节点上的MHA node应当意识到自己已经脱离了大多数成员, 它无法发起MySQL拓扑变更, 它要做应当是关闭Master(或打开read_only) 青云开源的Xenon好像实现了类似的功能, 但我也不确定, 这个项目使用人数太少 ​ https://github.com/radondb/xenon/issues/107 ​ 不过同样, xenon是使用vip为应用提供写入通道, 也就是说xenon控制了访问路径 Orchestrator不知道是否实现了这样的功能, 需要调研 MHA的检测逻辑不包含ProxySQL, 变更拓扑后也没有通知ProxySQL MHA的检测机制和ProxySQL并不相同, 两者可能对拓扑情况有不同的判断. 应当改造二次检测脚本, 通过连接ProxySQL判断主库状态, 但即使这样做, 在图-1的场景中, manager会发现自己无法连接ProxySQL-1, 那么二次检测脚本应当以exit_code=2退出, 不会发生Failover, 所以这仍然无法解决极端场景不能Failover的问题. Failover后, MHA应当主动变更ProxySQL中的配置 携程数据库高可用架构实践有类似描述 正确的做法与图-4类似, 在Failover后MHA应当将新的拓扑配置推送给”配置中心”, 保证应用使用新的配置连接数据库 美团也实现了类似的功能踩坑无数，美团点评高可用数据库架构演进 主从复制集并不是集群 即便从库都判定主库不可用, 也无能为力. 而假设是一个PXC或MGR集群(我们先不讨论这两者的问题), 图-1的场景会变成这样(以PXC为例): 实际上图中PXC节点数量为偶数并不合理, 只是为了迎合图-1场景作对比 在PXC中, 所有节点都是主节点, 都可以写入, 只要提交成功就不会丢失数据 MGR也有多主模式, 如果是单主模式, 脱离集群的节点会将自己设为read_only 在图-5的情况下, Master会被踢出集群, 也就是说选举和拓扑变更是MySQL自己控制的, ProxySQL只需要被动接受就好了. (如果无法踢出Master呢? 那只能说遇到了内部bug, 任何软件都可能有bug, 包括MHA ProxySQL, Orchestrator等) 即便是下图的场景, PXC和MGR也能很好地完成选举, 剔除Master和Slave-4 总结对于类MHA高可用方案, 需要进行改造以适应中间件. 对于MHA来说: 要改造masterha_secondary_check脚本, 在二次检测时连接ProxySQL进行主库探活. master_ip_failover也需要改造, 要在failover后删除ProxySQL runtime_mysql_server中原主信息并load 一定要将半同步超时设置无限大, 避免脑裂 如果放弃MHA, 无论选择Orchestrator, PXC亦或是MGR, 都是MySQL架构上的大变动, 需要长时间的技术调研和测试以及业务方的配合, 这三者任何一个都需要DBA花时间学习, 是否升级架构取决于我们面临的问题是不是最迫切需要解决的, 以及风险可控性(扎实的理论基础和完善的测试可以提高可控性). 那么选择其实就两种: 继续使用MHA或类MHA架构, 需要做二次开发适应ProxySQL 使用PXC或MGR这类”原生”集群方案, 需要实际数据论证当前业务是否可以做这样的架构调整. 个人认为两种都可以, 不过MGR必将是未来金融级高可用方案的事实标准(个人拙见, 据我所知网易,腾讯金融目前就是使用MGR,不过它们对源码做了修改, 这并非中小企业所具备的能力), 但目前仍需等待(发展只有4年), 而PXC已经发展8年了, 目前来看相比MGR更可靠(去哪儿网, 马蜂窝和之前的一些p2p企业都是用PXC), 但任何能保证强一致性的集群都必然会有性能损耗, 还要看业务是否可以接受. 如果用长远的眼光看, 调研并在一些边缘系统实践MGR是需要做的. 如CMDB, 明显的读多写少的系统, 美团点评基于MGR的CMDB高可用架构搭建之路","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"ClickHouse到底改写本地表还是分布式表","slug":"2020-09-22-ClickHouse到底改写本地表还是分布式表","date":"2020-09-22T02:30:00.000Z","updated":"2020-09-22T02:31:06.713Z","comments":true,"path":"2020/09/22/2020-09-22-ClickHouse到底改写本地表还是分布式表/","link":"","permalink":"http://fuxkdb.com/2020/09/22/2020-09-22-ClickHouse%E5%88%B0%E5%BA%95%E6%94%B9%E5%86%99%E6%9C%AC%E5%9C%B0%E8%A1%A8%E8%BF%98%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8/","excerpt":"TL;DR如果预估自己的业务数据量不大(日增不到百万行), 那么写分布式表和本地表都可以, 但要注意如果选择写本地表, 请保证每次写入数据都建立新的连接, 且每个连接写入的数据量基本相同 如果预估自己的业务数据量大(日增百万以上, 并发插入大于10), 那么请写本地表 建议每次插入50W行左右数据, 最多不可超过100W行. 总之CH不像MySQL要小事务. 比如1000W行数据, MySQL建议一次插入1W左右, 使用小事务, 执行1000次. CH建议20次,每次50W. 这是MergeTree引擎原理决定的, 频繁少量插入会导致data part过多, 合并不过来. 再有, AP不像TP, TP为了避免建立新连接产生的损耗影响性能, 通常会使用长连接, 连接池等技术做优化. 但AP业务不需要, 因为AP的属性就不会有高并发, 小SQL.","text":"TL;DR如果预估自己的业务数据量不大(日增不到百万行), 那么写分布式表和本地表都可以, 但要注意如果选择写本地表, 请保证每次写入数据都建立新的连接, 且每个连接写入的数据量基本相同 如果预估自己的业务数据量大(日增百万以上, 并发插入大于10), 那么请写本地表 建议每次插入50W行左右数据, 最多不可超过100W行. 总之CH不像MySQL要小事务. 比如1000W行数据, MySQL建议一次插入1W左右, 使用小事务, 执行1000次. CH建议20次,每次50W. 这是MergeTree引擎原理决定的, 频繁少量插入会导致data part过多, 合并不过来. 再有, AP不像TP, TP为了避免建立新连接产生的损耗影响性能, 通常会使用长连接, 连接池等技术做优化. 但AP业务不需要, 因为AP的属性就不会有高并发, 小SQL. 原因请继续看看了一些文档都建议写分布式表, 虽说别人没必要骗我们, 但是搞技术的不能人云亦云, 还是要明白为啥, 说实话我想了很久没想出直接写分布式表有什么致命缺陷, 于是在ClickHouse中文社区提了问题, 内容如下 看了sina高鹏大佬的分享，看了 https://github.com/ClickHouse/ClickHouse/issues/1854 ，还看了一些文章都是建议写本地表而不是分布式表如果我设置 internal_replication=true , 使用 ReplicatedMergeTree 引擎, 除了写本地表更灵活可控外, 写分布式表到底有什么致命缺陷吗?因为要给同事解释, 只说一个大家说最佳实践是这样是不行的.. 我自己也没理解到底写分布式表有啥大缺陷如果说造成数据分布不均匀, sharding key 我设为 rand() 还会有很大不均匀吗? 如果说扩容, 我也可以通过调整 weight 控制数据尽量写入新 shared 啊? 难道是因为： Data is written asynchronously. When inserted in the table, the data block is just written to the local file system. The data is sent to the remote servers in the background as soon as possible. The period for sending data is managed by the distributed_directory_monitor_sleep_time_ms and distributed_directory_monitor_max_sleep_time_ms settings. The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the distributed_directory_monitor_batch_inserts setting. This setting improves cluster performance by better utilizing local server and network resources. You should check whether data is sent successfully by checking the list of files (data waiting to be sent) in the table directory: /var/lib/clickhouse/data/database/table/. &gt; If the server ceased to exist or had a rough restart (for example, after a device failure) after an INSERT to a Distributed table, the inserted data might be lost. If a damaged data part is detected in the table directory, it is transferred to the ‘broken’ subdirectory and no longer used. 上面文档内容我理解意思是说假如我有S1 S2 S3 三个节点,每个节点都有local表和分布式表. 我向S1的分布式表写数据1, 2, 3，1写入S1, 2,3先写到S1本地文件系统, 然后异步发送到S2 S3 , 比如2发给S2, 3发给S3, 如果此时S3宕机了, 则3发到S3失败, 但是1,2还是成功写到S1,S2了? 所以整个过程不能保证原子性? 出现问题还要人为修数据? https://github.com/ClickHouse/ClickHouse/issues/1343 这个issue说S3 come back后S1会尝试重新发送数据给S3. Data blocks are written in /var/lib/clickhouse/data/database/table/ folder. Special thread checks directory periodically and tries to send data. If it can’t, it will try next time. 那么只剩文档最后一句意思是如果S1过程中宕机, 会丢数据?自问自答一下吧, weight 是分片级别的, 不是表级别的, 灵活性差 问了下新浪的高鹏高鹏的回答总结一下就是: 新浪每天增量是千亿级的, INSERT并发和节点数应该比较高, 直接写某个节点的分布式表, 这个节点还需要建立N-1个连接(N是集群节点数)分发数据, 再有就是他说的第3点 通过负载均衡向本地表插入数据要控制尽量每次插入数据建立一次连接, 每个链接插入的数据量要差不多, batch size不能太小, 否则data part过多, merge不过来clickhouse会报错","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"增强半同步会不会丢数据","slug":"2020-09-20-增强半同步会不会丢数据","date":"2020-09-20T14:46:00.000Z","updated":"2020-09-20T14:47:09.090Z","comments":true,"path":"2020/09/20/2020-09-20-增强半同步会不会丢数据/","link":"","permalink":"http://fuxkdb.com/2020/09/20/2020-09-20-%E5%A2%9E%E5%BC%BA%E5%8D%8A%E5%90%8C%E6%AD%A5%E4%BC%9A%E4%B8%8D%E4%BC%9A%E4%B8%A2%E6%95%B0%E6%8D%AE/","excerpt":"只讨论5.7增强半同步和双1的情况 增强半同步会不会丢数据?这里涉及两个过程: 主库 Innodb与Binlog日志的2PC 增强半同步 Innodb与Binlog日志的2PC 在开启Binlog后, MySQL内部会自动将普通事务当做一个XA事务来处理： 自动为每个事务分配一个唯一的ID COMMIT会被自动的分成Prepare和Commit两个阶段 Binlog会被当做事务协调者(Transaction Coordinator), Binlog Event会被当做协调者日志","text":"只讨论5.7增强半同步和双1的情况 增强半同步会不会丢数据?这里涉及两个过程: 主库 Innodb与Binlog日志的2PC 增强半同步 Innodb与Binlog日志的2PC 在开启Binlog后, MySQL内部会自动将普通事务当做一个XA事务来处理： 自动为每个事务分配一个唯一的ID COMMIT会被自动的分成Prepare和Commit两个阶段 Binlog会被当做事务协调者(Transaction Coordinator), Binlog Event会被当做协调者日志 分布式事务ID(XID) 使用2PC时, MySQL会自动的为每一个事务分配一个ID, 叫XID. XID是唯一的, 每个事务的XID都不相同. XID会分别被Binlog和InnoDB记入日志中, 供恢复时使用. MySQ内部的XID由三部分组成: 前缀部分前缀部分是字符串”MySQLXid” Server ID部分当前MySQL的server_id query_id部分为了保证XID的的唯一性, 数字部分使用了query_id. MySQL内部会自动的为每一个语句分配一个query_id, 全局唯一. 事务的协调者BinlogBinlog在2PC中充当了事务的协调者（Transaction Coordinator）. 由Binlog来通知InnoDB引擎来执行prepare, commit或者rollback的步骤. 事务提交的整个过程如下： 协调者准备阶段(Prepare Phase)告诉引擎做Prepare, InnoDB更改事务状态, 并将Redo Log刷入磁盘. 个人理解: innodb写prepare log, 事务标记为prepare状态, 并写入xid 协调者提交阶段(Commit Phase)2.1 记录协调者日志, 即Binlog日志.2.2 告诉引擎做commit. 个人理解: 写Binlog event和xid, 写完后通知innodb commit, innodb写commit log, 事务标记为commit状态 (记得姜成尧说要写binlog file pos到redo) 注意：记录Binlog是在InnoDB引擎Prepare(即Redo Log写入磁盘)之后, 这点至关重要. 协调者日志Xid_log_event作为协调者, Binlog需要将事务的XID记入日志, 供恢复时使用. Xid_log_event有以下几个特点： 仅记录query_id因为前缀部分不变, server_id已经记录在Event Header中, Xid_log_event中只记录query_id部分. 标志事务的结束在Binlog中相当于一个事务的COMMIT语句.一个事务在Binlog中看起来时这样的: 123Query_log_event(&quot;BEGIN&quot;);DML产生的events; Xid_log_event; DDL没有BEGIN, 也没有Xid_log_event. 仅InnoDB的DML会产生Xid_log_event 因为MyISAM不支持2PC所以不能用Xid_log_event, 但会有COMMIT Event. 123Query_log_event(&quot;BEGIN&quot;);DML产生的events;Query_log_event(&quot;COMMIT&quot;); 恢复(Recovery)这个机制是如何保证MySQL的CrashSafe的呢, 我们来分析一下. 这里我们假设用户设置了以下参数来保证可靠性:sync_binlog=1innodb_flush_log_at_trx_commit=1 恢复前事务的状态在恢复开始前事务有以下几种状态: InnoDB中已经提交根据前面2PC的过程, 可知Binlog中也一定记录了该事务的的Events(我感觉说的应该是Xid_log_event). 所以这种事务是一致的不需要处理. InnoDB中是prepared状态, Binlog中有该事务的Events(我感觉说的应该是Xid_log_event).需要通知InnoDB提交这些事务. InnoDB中是prepared状态, Binlog中没有该事务的Events(我感觉说的应该是Xid_log_event).因为Binlog还没记录, 需要通知InnoDB回滚这些事务. Before InnoDB Prepare事务可能还没执行完, 因此InnoDB中的状态还没有prepare. 根据2PC的过程, Binlog中也没有该事务的events(我感觉说的应该是Xid_log_event). 需要通知InnoDB回滚这些事务. 恢复过程从上面的事务状态可以看出: 恢复时事务要提交还是回滚, 是由Binlog来决定的. 事务的Xid_log_event存在, 就要提交. 事务的Xid_log_event不存在, 就要回滚. 恢复的过程非常简单: 从Binlog中读出所有的Xid_log_event 告诉InnoDB提交这些XID的事务 InnoDB回滚其它的事务 了解了MySQL关于Innodb与Binlog的两阶段提交机制后，就可以更深入去探究MySQL在故障恢复时的处理过程。 在MySQL启动时，首先会初始化存储引擎，如本例中的InnoDB引擎，然后InnoDB引擎层会读取redolog进行InnoDB层的故障恢复，回滚未prepared和commit的事务，但对于已经prepared，但未commit的事务，暂时挂起，保存到一个链表中，等待后续读取binlog日志，然后根据binlog日志再对这部分prepared的事务进行处理。 接下来，MySQL会读取最后一个binlog文件。binlog文件通常是以固定的文件名加一组连续的编号来命名的，并且将其记录到一个binlog索引文件中，因此索引文件中的最后一个binlog文件即是MySQL将要读取的最后一个binlog文件。 读取这个binlog文件时，通过文件头上是否存在标记LOG_EVENT_BINLOG_IN_USE_F，通过这个标记可以知道上次MySQL是正常关闭还是异常关闭，如果是异常关闭，则会进入故障恢复过程。 进入故障恢复过程后，会依次读取最后一个binlog文件中的所有log event，并将所有已提交事务的binlog日志中记录的xid提取出来添加到hash表中，以备后续对前述InnoDB故障恢复后遗留的Prepared事务继续处理。另外此处还要定位最后一个完整事务的位置，防止在上次系统异常关闭时有部分binlog日志未刷到磁盘上，即存在写了一半的binlog事务日志，这部分写了一半binlog日志的事务在MySQL中会按事务未提交来处理，后续会将其在存储引擎层回滚。当此文件中的内容全部读出之后，一是得到一个已提交事务的列表，另一个是最后一个完整事务的位置。 然后检查由InnoDB层得到的Prepared事务列表，若Prepared事务在从Binlog中得到的提交事务列表中，则在InnoDB层提交此事务，否则回滚此事务。 出自MySQL · 原理介绍 · 再议MySQL的故障恢复 疑问1：如果事务的Binlog Event只记录了一部分怎么办？只有最后一个事务的Event会发生这样的情况。在恢复时，binlog会自动的将这个不完整的事务Events从Binlog文件中给清除掉。 疑问2：随着长时间的运行，Binlog中会积累了很多Xid_log_event，读取所有的Xid_log_event会不会效率很低？ 当然很低，所以Binlog中有一个机制来保证恢复时只用读取最后一个Binlog文件中的Xid_log_event。这种机制很像一个简单的Xid_log_event的checkpoint机制。 CrashSafe的写盘次数前面说道要想保证CrashSafe就要设置下面两个参数为1:sync_binlog=1innodb_flush_log_at_trx_commit=1下面我们来看看这两个参数的作用. sync_binlogsync_binlog是控制Binlog写盘的, 1表示每次都写. 由于Binlog使用了组提交(Group Commit)的机制, 它代表一组事务提交时必须要将Binlog文件写入硬件存储1次. innodb_flush_log_at_trx_commit的写盘次数这个变量是用来控制InnoDB commit时写盘的方法的. 现在commit被分成了两个阶段, 到底在哪个阶段写盘, 还是两个阶段都要写盘呢？ Prepare阶段时需要写盘2PC要求在Prepare时就要将数据持久化, 只有这样, 恢复时才能提交已经记录了Xid_log_event的事务. Commit阶段时不需要写盘如果Commit阶段不写盘, 会造成什么结果呢？已经Cmmit了的事务, 在恢复时的状态可能是Prepared. 由于恢复时, Prepared的事务可以通过Xid_log_event来提交事务, 所以在恢复后事务的状态就是正确的. 因此在Commit阶段不需要写盘. 总的来说保证MySQL服务的CrashSafe需要写两次盘. 在2PC的过程中, InnoDB只在prepare阶段时, 写一次盘. Binlog在commit阶段, 会设置一个参数告诉InnoDB不要写盘. 这个参数是thd-&gt;durability_property= HA_IGNORE_DURABILITY;代码在sql/binlog.cc的MYSQL_BIN_LOG::ordered_commit()中. 以上出自MySQL的CrashSafe和Binlog的关系 增强半同步对于增强半同步, 主要有两种情况: 主库写binlog落盘后, Binlog dump线程发送binlog给从库, 从库IO thread接收写入relay log, 但是没有写入完, 主库挂了 从库IO thread成功写入relay log后, 还没发送ACK或ACK发送了但是主库收到前, 主库挂了 第一种情况对应用来说, 并没有收到commit ok的信息, 应当认为事务提交失败. 对于主库, 因为已经写了Binlog(写了Xid_log_event), 只是还没有写commit log, crash recover后, 这部分事务又会提交掉 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556假设一主一从半同步root@localhost 19:23:00 [dbms_monitor]&gt; show global variables like &#x27;%semi%&#x27;;+-------------------------------------------+------------+| Variable_name | Value |+-------------------------------------------+------------+| rpl_semi_sync_master_enabled | OFF || rpl_semi_sync_master_timeout | 10000 || rpl_semi_sync_master_trace_level | 32 || rpl_semi_sync_master_wait_for_slave_count | 1 || rpl_semi_sync_master_wait_no_slave | ON || rpl_semi_sync_master_wait_point | AFTER_SYNC || rpl_semi_sync_slave_enabled | OFF || rpl_semi_sync_slave_trace_level | 32 |+-------------------------------------------+------------+8 rows in set (0.00 sec)root@localhost 19:23:02 [dbms_monitor]&gt; truncate table semi_sync;Query OK, 0 rows affected (0.01 sec)root@localhost 19:23:15 [dbms_monitor]&gt; show global status like &#x27;%semi%&#x27;; +--------------------------------------------+-------+| Variable_name | Value |+--------------------------------------------+-------+| Rpl_semi_sync_master_clients | 1 || Rpl_semi_sync_master_net_avg_wait_time | 0 || Rpl_semi_sync_master_net_wait_time | 0 || Rpl_semi_sync_master_net_waits | 18 || Rpl_semi_sync_master_no_times | 1 || Rpl_semi_sync_master_no_tx | 2 || Rpl_semi_sync_master_status | ON || Rpl_semi_sync_master_timefunc_failures | 0 || Rpl_semi_sync_master_tx_avg_wait_time | 555 || Rpl_semi_sync_master_tx_wait_time | 5000 || Rpl_semi_sync_master_tx_waits | 9 || Rpl_semi_sync_master_wait_pos_backtraverse | 0 || Rpl_semi_sync_master_wait_sessions | 0 || Rpl_semi_sync_master_yes_tx | 9 || Rpl_semi_sync_slave_status | OFF |+--------------------------------------------+-------+15 rows in set (0.00 sec)root@localhost 19:23:44 [dbms_monitor]&gt; set global rpl_semi_sync_master_timeout=999999;Query OK, 0 rows affected (0.00 sec)root@localhost 19:23:52 [dbms_monitor]&gt; insert into semi_sync values(1, now());Query OK, 1 row affected (0.00 sec)slave:root@localhost 19:24:10 [(none)]&gt; select * from dbms_monitor.semi_sync;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-09-20 19:24:01 |+----+---------------------+1 row in set (0.00 sec) 可以看到当前是半同步状态, 主库rpl_semi_sync_master_timeout=999999, 写入了一条数据. 我们关闭从库, 之后开三个窗口执行三个insert 12345root@localhost 19:24:50 [dbms_monitor]&gt; insert into semi_sync values(2, now());root@localhost 17:41:52 [dbms_monitor]&gt; insert into semi_sync values(3, now());root@localhost 17:41:52 [dbms_monitor]&gt; insert into semi_sync values(4, now()); 由于rpl_semi_sync_master_wait_for_slave_count=1, 所以三个insert都在等待. 此时kill主库, 然后重启主库 12345678910111213[root@centos-1 mysql5731]# ps -ef| grep mysqldroot 29734 29681 0 17:33 pts/5 00:00:00 tail -f /data/mysql_3358/logs/mysqld.logroot 29743 29626 0 17:33 pts/4 00:00:00 /bin/sh ./bin/mysqld_safe --defaults-file=/data/mysql_3358/my_3358.cnf --user=mysqlmysql 31479 29743 0 17:33 pts/4 00:00:32 /usr/local/mysql5731/bin/mysqld --defaults-file=/data/mysql_3358/my_3358.cnf --basedir=/usr/local/mysql5731 --datadir=/data/mysql_3358/data --plugin-dir=/usr/local/mysql5731/lib/mysql/plugin --user=mysql --log-error=/data/mysql_3358/logs/mysqld.log --open-files-limit=65535 --pid-file=/data/mysql_3358/run/mysql.pid --socket=/data/mysql_3358/run/mysql.sock --port=3358root 32730 29626 0 19:25 pts/4 00:00:00 grep --color=auto mysqld[root@centos-1 mysql5731]# kill -9 29743 31479[root@centos-1 mysql5731]# ./bin/mysqld_safe --defaults-file=/data/mysql_3358/my_3358.cnf --user=mysql &amp;[2] 32735[root@centos-1 mysql5731]# mysqld_safe Adding &#x27;/usr/local/mysql5731/lib/mysql/libjemalloc.so.1&#x27; to LD_PRELOAD for mysqld2020-09-20T11:26:12.801721Z mysqld_safe Logging to &#x27;/data/mysql_3358/logs/mysqld.log&#x27;.2020-09-20T11:26:12.824485Z mysqld_safe Starting mysqld daemon with databases from /data/mysql_3358/data 查看数据 12345678910root@localhost 19:26:44 [dbms_monitor]&gt; select * from semi_sync;+----+---------------------+| id | ctime |+----+---------------------+| 1 | 2020-09-20 19:24:01 || 2 | 2020-09-20 19:25:08 || 3 | 2020-09-20 19:25:10 || 4 | 2020-09-20 19:25:11 |+----+---------------------+4 rows in set (0.00 sec) 可以看到, 三个insert在crash recover后成功提交了 由此可以看出, 主库宕机恢复后是不能作为从库加入原集群的, 需要重做, 否则数据不一致. 第二种情况对应用来说, 并没有收到commit ok的信息, 应当认为事务提交失败. 但是由于这部分事务已经写入了relay log, 从库sql thread应用完relay log中所有binlog event后提升为主库, 此时应用连接到新主库, 准备重试, 那么可能会出现问题: 对于INSERT: 如果INSERT语句中没有显式指定主键或任何唯一键, 那么应用重试插入后, 会出现重复数据. 对于UPDATE 如果采用update set amount=amount-1000 where id的方式重试, 会出现重复扣款, 所以即便重试, where条件中也应当带上要更新列的原值 对于DELETE 没什么影响, 只是影响行数为0 结论不会丢数据. 对于MHA+增强半同步, 主从切换后, 业务不能盲目重试, 而应当做事务执行失败, 按照正常逻辑重新执行完整的事务.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"半同步","slug":"半同步","permalink":"http://fuxkdb.com/tags/%E5%8D%8A%E5%90%8C%E6%AD%A5/"}]},{"title":"innodb_status_file innodb_status_output innodb_status_output_locks和innodb_show_verbose_locks","slug":"2020-09-13-innodb_status_file-innodb_status_output-innodb_status_output_locks和innodb_show_verbose_locks","date":"2020-09-13T02:33:00.000Z","updated":"2020-09-13T02:34:06.383Z","comments":true,"path":"2020/09/13/2020-09-13-innodb_status_file-innodb_status_output-innodb_status_output_locks和innodb_show_verbose_locks/","link":"","permalink":"http://fuxkdb.com/2020/09/13/2020-09-13-innodb_status_file-innodb_status_output-innodb_status_output_locks%E5%92%8Cinnodb_show_verbose_locks/","excerpt":"innodb_status_file这个参数官方文档https://dev.mysql.com/doc/refman/5.7/en/server-system-variable-reference.html 中没有 在https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html 中有 --innodb-status-file Property Value Command-Line Format `–innodb-status-file[={OFF ON}]` Type Boolean Default Value OFF The --innodb-status-file startup option controls whether InnoDB creates a file named innodb_status.*pid* in the data directory and writes SHOW ENGINE INNODB STATUS output to it every 15 seconds, approximately. The innodb_status.*pid* file is not created by default. To create it, start mysqld with the --innodb-status-file option. InnoDB removes the file when the server is shut down normally. If an abnormal shutdown occurs, the status file may have to be removed manually. The --innodb-status-file option is intended for temporary use, as SHOW ENGINE INNODB STATUS output generation can affect performance, and the innodb_status.*pid* file can become quite large over time. For related information, see Section 14.18.2, “Enabling InnoDB Monitors”.","text":"innodb_status_file这个参数官方文档https://dev.mysql.com/doc/refman/5.7/en/server-system-variable-reference.html 中没有 在https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html 中有 --innodb-status-file Property Value Command-Line Format `–innodb-status-file[={OFF ON}]` Type Boolean Default Value OFF The --innodb-status-file startup option controls whether InnoDB creates a file named innodb_status.*pid* in the data directory and writes SHOW ENGINE INNODB STATUS output to it every 15 seconds, approximately. The innodb_status.*pid* file is not created by default. To create it, start mysqld with the --innodb-status-file option. InnoDB removes the file when the server is shut down normally. If an abnormal shutdown occurs, the status file may have to be removed manually. The --innodb-status-file option is intended for temporary use, as SHOW ENGINE INNODB STATUS output generation can affect performance, and the innodb_status.*pid* file can become quite large over time. For related information, see Section 14.18.2, “Enabling InnoDB Monitors”. Section 14.18.2, “Enabling InnoDB Monitors”. Directing Standard InnoDB Monitor Output to a Status FileStandard InnoDB Monitor output can be enabled and directed to a status file by specifying the --innodb-status-file option at startup. When this option is used, InnoDB creates a file named innodb_status.*pid* in the data directory and writes output to it every 15 seconds, approximately. InnoDB removes the status file when the server is shut down normally. If an abnormal shutdown occurs, the status file may have to be removed manually. The --innodb-status-file option is intended for temporary use, as output generation can affect performance, and the innodb_status.*pid* file can become quite large over time. 非动态参数, 可以再my.cnf中添加innodb_status_file=1启用 开启后会在datadir下生成一个innodb_status.pid文件, 周期性15秒向这个文件输出show engine innodb status. 如果异常关闭数据库, 这个文件不会被删除 看文档意思theinnodb_status.pidfile can become quite large over time., 但我看不会表达, 他不是追加写入, 起码percona分支是这样, 上面的图是官方分支, 也没有追加 innodb_status_output开启后会周期性向error log输出show engine innodb status innodb_status_output_locks单独开这个影响show engine innodb status. 这个参数参数配合innodb_status_file或innodb_status_output使用时, 当前两者开启, 则会向前两者输出位置输出锁信息 innodb_status_output_locks=off 1234567891011121314------------TRANSACTIONS------------Trx id counter 5781Purge done for trx&#x27;s n:o &lt; 5776 undo n:o &lt; 0 state: running but idleHistory list length 33LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761960688, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5780, ACTIVE 36 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 13, OS thread handle 140502089910016, query id 257 localhost root innodb_status_output_locks=on, innodb_show_verbose_locks=0 1234567891011121314151617------------TRANSACTIONS------------Trx id counter 5781Purge done for trx&#x27;s n:o &lt; 5776 undo n:o &lt; 0 state: running but idleHistory list length 33LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761960688, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5780, ACTIVE 185 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 13, OS thread handle 140502089910016, query id 257 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5780 lock mode IXRECORD LOCKS space id 133 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5780 lock_mode X locks rec but not gapRECORD LOCKS space id 133 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5780 lock_mode X locks rec but not gap innodb_status_output_locks=on, innodb_show_verbose_locks=1 12345678910111213141516171819202122232425262728293031------------TRANSACTIONS------------Trx id counter 5781Purge done for trx&#x27;s n:o &lt; 5776 undo n:o &lt; 0 state: running but idleHistory list length 33LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761960688, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5780, ACTIVE 48 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 13, OS thread handle 140502089910016, query id 257 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5780 lock mode IXRECORD LOCKS space id 133 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5780 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;RECORD LOCKS space id 133 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5780 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 8; hex 8000000000000001; asc ;; 1: len 6; hex 000000001647; asc G;; 2: len 7; hex b6000000040110; asc ;; 3: len 1; hex 31; asc 1;; 4: len 1; hex 31; asc 1;; 5: len 1; hex 80; asc ;; 6: len 1; hex 61; asc a;; innodb_show_verbose_lockspercona分支参数 variable innodb_show_verbose_locks Command Line:YesConfig File:YesScope:GlobalDynamic:YesVariable Type:ULONGDefault Value:0Range:0 - 1 Specifies to show records locked in SHOW ENGINE INNODB STATUS. The default is 0, which means only the higher-level information about the lock (which table and index is locked, etc.) is printed. If set to 1, then traditional InnoDB behavior is enabled: the records that are locked are dumped to the output. 官方分支没这个参数, 这个参数影响innodb_print_all_deadlocks和前面三个参数. 例如innodb_show_verbose_locks=0, innodb_print_all_deadlocks=1, 则error log中死锁信息为 12345678910111213141516171819202122232425262020-05-15T10:24:29.703878+08:00 4 [Note] InnoDB: Transactions deadlock detected, dumping detailed information.2020-05-15T10:24:29.703949+08:00 4 [Note] InnoDB: *** (1) TRANSACTION:TRANSACTION 5127, ACTIVE 5 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)MySQL thread id 3, OS thread handle 140073055278848, query id 26 localhost root statisticsselect u_c from t8 where d_id=&#x27;1&#x27; and b_id=&#x27;1&#x27; and is_dropped=0 for update2020-05-15T10:24:29.703987+08:00 4 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5127 lock_mode X locks rec but not gap waiting2020-05-15T10:24:29.704003+08:00 4 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 5126, ACTIVE 9 sec starting index readmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 3 row lock(s)MySQL thread id 4, OS thread handle 140073055008512, query id 27 localhost root updatingupdate t8 set u_c=&#x27;b&#x27; where d_id=&#x27;1&#x27; and b_id=&#x27;1&#x27;2020-05-15T10:24:29.704016+08:00 4 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5126 lock_mode X locks rec but not gap2020-05-15T10:24:29.704025+08:00 4 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5126 lock_mode X waiting2020-05-15T10:24:29.704034+08:00 4 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (1) innodb_show_verbose_locks=1, innodb_print_all_deadlocks=1, 则error log中死锁信息为 12345678910111213141516171819202122232425262728293031323334353637383940414243442020-05-15T10:54:23.575868+08:00 4 [Note] InnoDB: Transactions deadlock detected, dumping detailed information.2020-05-15T10:54:23.575923+08:00 4 [Note] InnoDB: *** (1) TRANSACTION:TRANSACTION 5130, ACTIVE 6 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)MySQL thread id 3, OS thread handle 140073055278848, query id 38 localhost root statisticsselect u_c from t8 where d_id=&#x27;1&#x27; and b_id=&#x27;1&#x27; and is_dropped=0 for update2020-05-15T10:54:23.575945+08:00 4 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5130 lock_mode X locks rec but not gap waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;2020-05-15T10:54:23.576779+08:00 4 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 5129, ACTIVE 8 sec starting index readmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 3 row lock(s)MySQL thread id 4, OS thread handle 140073055008512, query id 39 localhost root updatingupdate t8 set u_c=&#x27;b&#x27; where d_id=&#x27;1&#x27; and b_id=&#x27;1&#x27;2020-05-15T10:54:23.576798+08:00 4 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5129 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;2020-05-15T10:54:23.576847+08:00 4 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5129 lock_mode X waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;2020-05-15T10:54:23.576896+08:00 4 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (1) innodb_status_output=1,innodb_show_verbose_locks=0或1, innodb_status_output_locks=off 1234567891011121314------------TRANSACTIONS------------Trx id counter 5134Purge done for trx&#x27;s n:o &lt; 5132 undo n:o &lt; 0 state: running but idleHistory list length 2LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421547726605656, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421547726604528, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5132, ACTIVE 93 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 6, OS thread handle 140073054738176, query id 60 localhost root innodb_status_output=1,innodb_show_verbose_locks=0, innodb_status_output_locks=on 1234567891011121314151617------------TRANSACTIONS------------Trx id counter 5134Purge done for trx&#x27;s n:o &lt; 5132 undo n:o &lt; 0 state: running but idleHistory list length 2LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421547726605656, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421547726604528, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5132, ACTIVE 287 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 6, OS thread handle 140073054738176, query id 60 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5132 lock mode IXRECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5132 lock_mode X locks rec but not gapRECORD LOCKS space id 128 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5132 lock_mode X locks rec but not gap innodb_status_output=1,innodb_show_verbose_locks=1, innodb_status_output_locks=on 12345678910111213141516171819202122232425262728293031------------TRANSACTIONS------------Trx id counter 5134Purge done for trx&#x27;s n:o &lt; 5132 undo n:o &lt; 0 state: running but idleHistory list length 2LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421547726605656, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 421547726604528, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5132, ACTIVE 132 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 6, OS thread handle 140073054738176, query id 60 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5132 lock mode IXRECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5132 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;RECORD LOCKS space id 128 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5132 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 8; hex 8000000000000001; asc ;; 1: len 6; hex 00000000103f; asc ?;; 2: len 7; hex b0000000040110; asc ;; 3: len 1; hex 31; asc 1;; 4: len 1; hex 31; asc 1;; 5: len 1; hex 80; asc ;; 6: len 1; hex 61; asc a;; 对于status_file一样 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162status_file v0 v1 off------------TRANSACTIONS------------Trx id counter 5640Purge done for trx&#x27;s n:o &lt; 0 undo n:o &lt; 0 state: running but idleHistory list length 0LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5639, ACTIVE 301 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3, OS thread handle 140502089639680, query id 23 localhost rootstatus_file v0 on------------TRANSACTIONS------------Trx id counter 5640Purge done for trx&#x27;s n:o &lt; 0 undo n:o &lt; 0 state: running but idleHistory list length 0LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5639, ACTIVE 901 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3, OS thread handle 140502089639680, query id 23 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5639 lock mode IXRECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5639 lock_mode X locks rec but not gapRECORD LOCKS space id 128 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5639 lock_mode X locks rec but not gapstatus_file v1 on------------TRANSACTIONS------------Trx id counter 5640Purge done for trx&#x27;s n:o &lt; 0 undo n:o &lt; 0 state: running but idleHistory list length 0LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421976761961816, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 5639, ACTIVE 561 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3, OS thread handle 140502089639680, query id 23 localhost rootTABLE LOCK table `fanboshi`.`t8` trx id 5639 lock mode IXRECORD LOCKS space id 128 page no 4 n bits 80 index DealerAndBrokerAndDropped of table `fanboshi`.`t8` trx id 5639 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0 0: len 1; hex 31; asc 1;; 1: len 1; hex 31; asc 1;; 2: len 1; hex 80; asc ;; 3: len 8; hex 8000000000000001; asc ;;RECORD LOCKS space id 128 page no 3 n bits 80 index PRIMARY of table `fanboshi`.`t8` trx id 5639 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 7; compact format; info bits 0 0: len 8; hex 8000000000000001; asc ;; 1: len 6; hex 00000000103f; asc ?;; 2: len 7; hex b0000000040110; asc ;; 3: len 1; hex 31; asc 1;; 4: len 1; hex 31; asc 1;; 5: len 1; hex 80; asc ;; 6: len 1; hex 61; asc a;;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"ClickHouse查询分布式表LEFT JOIN改RIGHT JOIN的大坑","slug":"2020-08-28-ClickHouse查询分布式表LEFT-JOIN改RIGHT-JOIN的大坑","date":"2020-08-28T09:06:00.000Z","updated":"2020-08-28T09:17:43.113Z","comments":true,"path":"2020/08/28/2020-08-28-ClickHouse查询分布式表LEFT-JOIN改RIGHT-JOIN的大坑/","link":"","permalink":"http://fuxkdb.com/2020/08/28/2020-08-28-ClickHouse%E6%9F%A5%E8%AF%A2%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8LEFT-JOIN%E6%94%B9RIGHT-JOIN%E7%9A%84%E5%A4%A7%E5%9D%91/","excerpt":"由一个慢查询衍生出的问题我们线上有一个ClickHouse集群, 总共6个服务器, 配置均为16C 64G SSD, 集群配置为三分片两副本 有两个表这里称为small_table和big_table. 都是ReplicatedMergeTree引擎(三个分片两个副本). small_table有79w数据, big_table有5亿数据(数据在之后的示例中没有任何变化), 在下文中small_table和big_table都为分布式表, 可以获取全量数据, small_table_local和big_table_local为各节点上的本地表名称 1234567891011121314SELECT table, formatReadableSize(sum(data_compressed_bytes)) AS tc, formatReadableSize(sum(data_uncompressed_bytes)) AS tu, sum(data_compressed_bytes) / sum(data_uncompressed_bytes) AS ratioFROM system.columnsWHERE (database = currentDatabase()) AND (table IN (&#x27;small_table_local&#x27;, &#x27;big_table_local&#x27;))GROUP BY tableORDER BY table ASC┌─table─────────────────────────┬─tc────────┬─tu────────┬──────────────ratio─┐│ small_table_local │ 12.87 MiB │ 14.91 MiB │ 0.8633041477100831 ││ big_table_local │ 15.46 GiB │ 57.31 GiB │ 0.2697742507036428 │└───────────────────────────────┴───────────┴───────────┴────────────────────┘","text":"由一个慢查询衍生出的问题我们线上有一个ClickHouse集群, 总共6个服务器, 配置均为16C 64G SSD, 集群配置为三分片两副本 有两个表这里称为small_table和big_table. 都是ReplicatedMergeTree引擎(三个分片两个副本). small_table有79w数据, big_table有5亿数据(数据在之后的示例中没有任何变化), 在下文中small_table和big_table都为分布式表, 可以获取全量数据, small_table_local和big_table_local为各节点上的本地表名称 1234567891011121314SELECT table, formatReadableSize(sum(data_compressed_bytes)) AS tc, formatReadableSize(sum(data_uncompressed_bytes)) AS tu, sum(data_compressed_bytes) / sum(data_uncompressed_bytes) AS ratioFROM system.columnsWHERE (database = currentDatabase()) AND (table IN (&#x27;small_table_local&#x27;, &#x27;big_table_local&#x27;))GROUP BY tableORDER BY table ASC┌─table─────────────────────────┬─tc────────┬─tu────────┬──────────────ratio─┐│ small_table_local │ 12.87 MiB │ 14.91 MiB │ 0.8633041477100831 ││ big_table_local │ 15.46 GiB │ 57.31 GiB │ 0.2697742507036428 │└───────────────────────────────┴───────────┴───────────┴────────────────────┘ 123456SELECT count(*)FROM small_table┌─count()─┐│ 794469 │└─────────┘ 1234567SELECT count(*)FROM big_table┌───count()─┐│ 519898780 │└───────────┘ 有如下查询 1SELECT a.UID,B.UID from dwh.small_table a LEFT JOIN dwh.big_table b on a.UID = b.UID 这个查询在ClickHouse中要跑近300秒 12345678910111213#time clickhouse-client --time --progress --query=&quot;SELECT a.UID, B.UIDFROM dwh.small_table a LEFT JOIN dwh.big_table b ON a.UID = b.UID&quot; &gt; /dev/null293.769real 4m53.798suser 0m0.574ssys 0m0.225s 而在TIDB只需要20秒(节点数和配置比CH略好, 数据略多于CH, 未使用TIFlash) 12345678910111213# time mysql -uroot -hxx.xx.xx -P4000 -p dwh -e &quot;SELECT a.UID, B.UIDFROM dwh.small_table a LEFT JOIN dwh.big_table b ON a.UID = b.UID;&quot; &gt; /dev/nullEnter password:real 0m20.955suser 0m11.292ssys 0m2.321s 本人接触ClickHouse不久, 没什么实战经验, 看到这结果就感觉肯定是自己使用姿势不对 JOIN操作时一定要把数据量小的表放在右边一通百度Google, 看到一篇来自携程的文章每天十亿级数据更新，秒出查询结果，ClickHouse在携程酒店的应用, 其中有一段话: JOIN操作时一定要把数据量小的表放在右边，ClickHouse中无论是Left Join 、Right Join还是Inner Join永远都是拿着右表中的每一条记录到左表中查找该记录是否存在，所以右表必须是小表。 有点神奇.. 我们知道在常见的关系型数据库如Oralce、MySQL中, LEFT JOIN和RIGTH JOIN是可以等价改写的, 那么我改成RIGHT JOIN不就”把小表放在右边”了吗, 于是SQL改写为 1SELECT a.UID,B.UID from dwh.big_table b RIGHT JOIN dwh.small_table a on a.UID = b.UID 实测 12345678910111213#time clickhouse-client --time --progress --query=&quot;SELECT a.UID, B.UIDFROM dwh.big_table b RIGHT JOIN dwh.small_table a ON a.UID = b.UIDT&quot; &gt; /dev/null19.588real 0m19.609suser 0m0.742ssys 0m0.293s 没想到还真好使… 难道CH优化器这么弱? 谨慎起见, 我比对了一下结果, 简单count一下吧 LEFT JOIN 1234567891011121314#time clickhouse-client --time --progress --query=&quot;SELECT COUNT(*)FROM dwh.small_table a LEFT JOIN dwh.big_table b ON a.UID = b.UID&quot;6042735 --count917.560 --时间real 15m17.580suser 0m0.253ssys 0m0.489s RIGHT JOIN 1234567891011121314#time clickhouse-client --time --progress --query=&quot;SELECT COUNT(*)FROM dwh.big_table b RIGHT JOIN dwh.small_table a ON a.UID = b.UID&quot;6897617 --count11.655 --时间real 0m11.675suser 0m0.014ssys 0m0.017s RIGHT JOIN数据不对啊 ClickHouse分布式表A LEFT JOIN B != B RIGHT JOIN A创建测试表12345678910111213141516171819202122232425262728293031323334353637383940414243ch-node-05 default@localhost:9000 [dwh]:) show create table t1;SHOW CREATE TABLE t1┌─statement─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ CREATE TABLE dwh.t1 (`I_ID` String, `CTIME` DateTime) ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;dwh&#x27;, &#x27;t1_local&#x27;, rand()) │└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1 rows in set. Elapsed: 0.001 sec. ch-node-05 default@localhost:9000 [dwh]:) show create table t2;SHOW CREATE TABLE t2┌─statement─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ CREATE TABLE dwh.t2 (`I_ID` String, `CTIME` DateTime) ENGINE = Distributed(&#x27;ch_cluster_all&#x27;, &#x27;dwh&#x27;, &#x27;t2_local&#x27;, rand()) │└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1 rows in set. Elapsed: 0.001 sec. ch-node-05 default@localhost:9000 [dwh]:) show create table t1_local;SHOW CREATE TABLE t1_local┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ CREATE TABLE dwh.t1_local (`I_ID` String, `CTIME` DateTime) ENGINE = ReplicatedReplacingMergeTree(&#x27;/clickhouse/dwh/tables/&#123;layer&#125;-&#123;shard&#125;/t1&#x27;, &#x27;&#123;replica&#125;&#x27;) PARTITION BY toDate(CTIME) ORDER BY I_ID SETTINGS index_granularity = 8192 │└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1 rows in set. Elapsed: 0.001 sec. ch-node-05 default@localhost:9000 [dwh]:) show create table t2_local;SHOW CREATE TABLE t2_local┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ CREATE TABLE dwh.t2_local (`I_ID` String, `CTIME` DateTime) ENGINE = ReplicatedReplacingMergeTree(&#x27;/clickhouse/dwh/tables/&#123;layer&#125;-&#123;shard&#125;/t2&#x27;, &#x27;&#123;replica&#125;&#x27;) PARTITION BY toDate(CTIME) ORDER BY I_ID SETTINGS index_granularity = 8192 │└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1 rows in set. Elapsed: 0.001 sec. 数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120ch-node-05 default@localhost:9000 [dwh]:) select * from t1;SELECT *FROM t1┌─I_ID─┬───────────────CTIME─┐│ 1 │ 2020-08-27 15:24:05 ││ 2 │ 2020-08-27 15:24:50 ││ 8 │ 2020-08-27 15:24:50 │└──────┴─────────────────────┘┌─I_ID─┬───────────────CTIME─┐│ 3 │ 2020-08-27 15:24:50 ││ 5 │ 2020-08-27 15:24:50 ││ 9 │ 2020-08-27 15:24:50 │└──────┴─────────────────────┘┌─I_ID─┬───────────────CTIME─┐│ 10 │ 2020-08-27 15:24:50 ││ 3 │ 2020-08-27 15:24:50 ││ 6 │ 2020-08-27 15:24:50 ││ 7 │ 2020-08-27 15:24:50 │└──────┴─────────────────────┘10 rows in set. Elapsed: 0.003 sec. ch-node-05 default@localhost:9000 [dwh]:) select * from t2;SELECT *FROM t2┌─I_ID─┬───────────────CTIME─┐│ 1 │ 2020-08-27 15:25:14 │└──────┴─────────────────────┘┌─I_ID─┬───────────────CTIME─┐│ 2 │ 2020-08-27 15:25:33 ││ 5 │ 2020-08-27 15:25:33 │└──────┴─────────────────────┘┌─I_ID─┬───────────────CTIME─┐│ 3 │ 2020-08-27 15:25:33 ││ 3 │ 2020-08-27 15:25:33 │└──────┴─────────────────────┘5 rows in set. Elapsed: 0.003 sec. ch-node-05 default@localhost:9000 [dwh]:) SELECT :-] _shard_num, :-] count(*):-] FROM :-] (:-] SELECT :-] _shard_num, :-] a.*:-] FROM dwh.t1 AS a:-] ):-] GROUP BY _shard_num:-] WITH ROLLUP;SELECT _shard_num, count(*)FROM ( SELECT _shard_num, a.* FROM dwh.t1 AS a)GROUP BY _shard_num WITH ROLLUP┌─_shard_num─┬─count()─┐│ 3 │ 3 ││ 2 │ 3 ││ 1 │ 4 │└────────────┴─────────┘┌─_shard_num─┬─count()─┐│ 0 │ 10 │└────────────┴─────────┘4 rows in set. Elapsed: 0.004 sec. ch-node-05 default@localhost:9000 [dwh]:) SELECT :-] _shard_num, :-] count(*):-] FROM :-] (:-] SELECT :-] _shard_num, :-] a.*:-] FROM dwh.t2 AS a:-] ):-] GROUP BY _shard_num:-] WITH ROLLUP;SELECT _shard_num, count(*)FROM ( SELECT _shard_num, a.* FROM dwh.t2 AS a)GROUP BY _shard_num WITH ROLLUP┌─_shard_num─┬─count()─┐│ 3 │ 2 ││ 2 │ 1 ││ 1 │ 2 │└────────────┴─────────┘┌─_shard_num─┬─count()─┐│ 0 │ 5 │└────────────┴─────────┘4 rows in set. Elapsed: 0.005 sec. 测试LEFT JOIN RIGHT JOIN12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273ch-node-05 default@localhost:9000 [dwh]:) SELECT :-] a.I_ID, :-] b.I_ID:-] FROM dwh.t2 AS a:-] LEFT JOIN dwh.t1 AS b ON a.I_ID = b.I_ID:-] ORDER BY a.I_ID ASC;SELECT a.I_ID, b.I_IDFROM dwh.t2 AS aLEFT JOIN dwh.t1 AS b ON a.I_ID = b.I_IDORDER BY a.I_ID ASC┌─I_ID─┬─b.I_ID─┐│ 1 │ 1 │└──────┴────────┘┌─I_ID─┬─b.I_ID─┐│ 2 │ 2 ││ 3 │ 3 ││ 3 │ 3 ││ 3 │ 3 ││ 3 │ 3 │└──────┴────────┘┌─I_ID─┬─b.I_ID─┐│ 5 │ 5 │└──────┴────────┘7 rows in set. Elapsed: 0.006 sec. ch-node-05 default@localhost:9000 [dwh]:) SELECT :-] a.I_ID, :-] b.I_ID:-] FROM dwh.t1 AS b:-] RIGHT JOIN dwh.t2 AS a ON a.I_ID = b.I_ID:-] ORDER BY a.I_ID ASC;SELECT a.I_ID, b.I_IDFROM dwh.t1 AS bRIGHT JOIN dwh.t2 AS a ON a.I_ID = b.I_IDORDER BY toUInt32(a.I_ID) ASC┌─a.I_ID─┬─I_ID─┐│ 1 │ ││ 1 │ 1 ││ 1 │ ││ 2 │ ││ 2 │ 2 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 2 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ 3 ││ 3 │ 3 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ ││ 3 │ ││ 3 │ 3 ││ 3 │ 3 ││ 5 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ 5 │└────────┴──────┘ 可以看到RIGHT JOIN返回了一些错误的数据 难道想要对于这个SQL, 就不能用分布式表了吗? 如果我想用RIGHT JOIN就只能用单机? 说好的水平扩展呢? 那看一下单机查询速度吧.. 为此我在一个CH节点创建两个表small_table_total和big_table_toal 他们都不是分布式表, 都拥有全量数据 12345678910111213141516SELECT count(*)FROM `big_table_toal`┌───count()─┐│ 519898780 │└───────────┘1 rows in set. Elapsed: 0.001 sec. SELECT count(*)FROM `small_table_total`┌─count()─┐│ 794469 │└─────────┘ 分布式表只能和分布式表关联, 分布式表无法和本地表关联 1234#clickhouse-client --time --progress --query=&quot;SELECT count(*) from dwh.big_table b RIGHT JOIN dwh.small_table_total a on a.UID = b.UID&quot; → Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 20.3.5):Code: 60. DB::Exception: Received from localhost:9000. DB::Exception: Received from bj2-ch-node-04:9000. DB::Exception: Table dwh.small_table_total doesn&#x27;t exist.. 0.107 测试查询速度 12345678910#clickhouse-client --time --progress --query=&quot;SELECT COUNT(*)FROM dwh.big_table_total b RIGHT JOIN dwh.small_table_total a ON a.UID = b.UID&quot;6042735 --count7.262 --用时 好家伙, 数据准确, 而且比分片了还快 难道只能用本地表?分布式表要想RIGHT JOIN返回正确结果, 只能改写SQL 原始语句123456SELECT a.UID, b.UIDFROM dwh.small_table a LEFT JOIN dwh.big_table b ON a.UID = b.UID 改写为INNER JOIN, 但是没有改表顺序(性能差)123456789101112131415SELECT a.id, b.uidFROM dwh.small_table a GLOBAL INNER JOIN dwh.big_table b ON a.UID = b.UID UNION ALL SELECT a.UID, NULLFROM dwh.small_table aWHERE a.UID GLOBAL NOT IN (SELECT UID FROM dwh.big_table) 这里我还没理解为什么要用GLOBAL JOIN 在我的例子中 ,这个语句根本跑不成功, GLOBAL太耗费内存了 1234567891011121314151617181920212223SELECT a.UID, b.UIDFROM dwh.small_table AS aGLOBAL INNER JOIN dwh.big_table AS b ON a.UID = b.UIDUNION ALLSELECT a.UID, NULLFROM dwh.small_table AS aWHERE a.UID GLOBAL NOT IN ( SELECT UID FROM dwh.big_table)↑ Progress: 220.53 million rows, 29.82 GB (51.24 million rows/s., 6.93 GB/s.) ████████████████████████████████████▋ 20%Received exception from server (version 20.3.5):Code: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 50.00 GiB (attempt to allocate chunk of 4249200 bytes), maximum: 50.00 GiB: (while reading column CH_BILL_USER_TELEPHONE): (while reading from part /data/clickhouse/ch_9000/data/dwh/big_table_local/201910_0_5_1/ from mark 216 with max_rows_to_read = 8192): Code: 241, e.displayText() = DB::Exception: Memory limit (for query) exceeded: would use 50.00 GiB (attempt to allocate chunk of 4227680 bytes), maximum: 50.00 GiB: (while reading column CH_XXX_NAME): (while reading from part /data/clickhouse/ch_9000/data/dwh/big_table_local/202001_6_11_1/ from mark 240 with max_rows_to_read = 8192) (version 20.3.5.21 (official build)): Code: 241, e.displayText() = DB::Exception: Memory limit (for query) exceeded: would use 50.00 GiB (attempt to allocate chunk of 5211280 bytes), maximum: 50.00 GiB: (avg_value_size_hint = 66, avg_chars_size = 69.6, limit = 8192): (while reading column CH_BROKER_NAME): (while reading from part /data/clickhouse/ch_9000/data/dwh/big_table_local/202007_6_11_1/ from mark 24 with max_rows_to_read = 8192) (version 20.3.5.21 (official build)): Code: 241, e.displayText() = DB::Exception: Memory limit (for query) exceeded: would use 50.00 GiB (attempt to allocate chunk of 4572064 bytes), maximum: 50.00 GiB: (avg_value_size_hint = 66.00048828125, avg_chars_size = 69.6005859375, limit = 8192): (while reading column CH_XX_NAME): (while reading from part /data/clickhouse/ch_9000/data/dwh/big_table_local/201805_2_2_0/ from mark 24 with max_rows_to_read = 8192) (version 20.3.5.21 (official build)): While executing CreatingSetsTransform. 0 rows in set. Elapsed: 4.404 sec. Processed 220.53 million rows, 29.82 GB (50.07 million rows/s., 6.77 GB/s.) 如果去掉GLOBAL JOIN, 也不行(GLOBAL IN不能去) 1234567891011121314151617181920SELECT a.UID, b.UIDFROM dwh.small_table AS aINNER JOIN dwh.big_table AS b ON a.UID = b.UIDUNION ALLSELECT a.UID, NULLFROM dwh.small_table AS aWHERE a.UID GLOBAL NOT IN ( SELECT UID FROM dwh.big_table)↑ Progress: 1.91 billion rows, 105.59 GB (6.36 million rows/s., 352.30 MB/s.) ██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ 90%Received exception from server (version 20.3.5):Code: 241. DB::Exception: Received from localhost:9000. DB::Exception: Received from clickhouse-node-01:9000. DB::Exception: Memory limit (total) exceeded: would use 50.10 GiB (attempt to allocate chunk of 133829856 bytes), maximum: 50.00 GiB: While executing CreatingSetsTransform. 0 rows in set. Elapsed: 299.809 sec. Processed 1.91 billion rows, 105.59 GB (6.35 million rows/s., 352.18 MB/s.) 改写表顺序, 让小表在”右边”12345678910SELECT a.UID FROM dwh.big_table bGLOBAL INNER JOIN dwh.small_table aON a.UID = b.UIDUNION ALLSELECT a.UID,null FROM dwh.small_table aWHERE a.UID GLOBAL NOT IN( SELECT UID FROM dwh.big_table WHERE UID GLOBAL IN (SELECT id FROM dwh.small_table)) 实测 12345678910111213141516time clickhouse-client --time --progress --query=&quot;SELECT a.UID,b.UID FROM dwh.big_table bGLOBAL INNER JOIN dwh.small_table aon a.UID = b.UIDUNION ALLSELECT a.UID,null FROM dwh.small_table aWHERE a.UID GLOBAL NOT IN( SELECT UID FROM dwh.big_table WHERE UID GLOBAL IN (SELECT UID FROM dwh.small_table))&quot; &gt;/dev/null21.142real 0m21.164suser 0m1.133ssys 0m0.378s 看一下行数对不对 123456789101112131415161718time clickhouse-client --time --progress --query=&quot;SELECT sum(cnt) FROM (SELECT count(*) cnt FROM dwh.big_table bGLOBAL INNER JOIN dwh.small_table aon a.UID = b.UIDUNION ALLSELECT count(*) cnt FROM dwh.small_table aWHERE a.UID GLOBAL NOT IN( SELECT UID FROM dwh.big_table WHERE UID GLOBAL IN (SELECT UID FROM dwh.small_table)))&quot;6042735 --count12.525 --用时real 0m12.545suser 0m0.018ssys 0m0.012s 最后一个问题不知道你们是否注意到了 ClickHouse 12345678910111213141516171819202122232425262728293031323334SELECT a.I_ID, b.I_IDFROM dwh.t1 AS bRIGHT JOIN dwh.t2 AS a ON a.I_ID = b.I_IDORDER BY a.I_ID ASC┌─a.I_ID─┬─I_ID─┐│ 1 │ ││ 1 │ 1 ││ 1 │ ││ 2 │ ││ 2 │ 2 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 2 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ 3 ││ 3 │ 3 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ ││ 3 │ ││ 3 │ 3 ││ 3 │ 3 ││ 5 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ 5 │└────────┴──────┘ MySQL(例子无关) 123456789101112131415161718192021222324252627root@localhost 15:15:26 [fanboshi]&gt; select a.id,b.id from t1 a left join t3 b on a.id=b.id; +----+------+| id | id |+----+------+| 1 | 1 || 2 | 2 || 3 | 3 || 4 | 4 || 5 | NULL || 6 | NULL || 7 | NULL || 8 | NULL || 9 | NULL || 11 | NULL || 13 | NULL || 15 | NULL || 17 | NULL || 19 | NULL || 21 | NULL || 23 | NULL || 25 | NULL || 27 | NULL || 29 | NULL || 30 | NULL || 31 | NULL |+----+------+21 rows in set (0.00 sec) 在CH里外链接不想MySQL那样”用null补未匹配的数据”而是用该列数据类型的默认值填充 https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.h#L189 join_use_nulls可以在语句,用户profile添加 12345678910111213141516171819202122232425262728293031323334353637SELECT a.I_ID, b.I_IDFROM dwh.t1 AS bRIGHT JOIN dwh.t2 AS a ON a.I_ID = b.I_IDORDER BY toUInt32(a.I_ID) ASCSETTINGS join_use_nulls = 1┌─a.I_ID─┬─I_ID─┐│ 1 │ ᴺᵁᴸᴸ ││ 1 │ 1 ││ 1 │ ᴺᵁᴸᴸ ││ 2 │ ᴺᵁᴸᴸ ││ 2 │ 2 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 2 │ ᴺᵁᴸᴸ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ 3 ││ 3 │ 3 │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 3 │ ᴺᵁᴸᴸ ││ 3 │ ᴺᵁᴸᴸ ││ 3 │ 3 ││ 3 │ 3 ││ 5 │ ᴺᵁᴸᴸ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ ᴺᵁᴸᴸ │└────────┴──────┘┌─a.I_ID─┬─I_ID─┐│ 5 │ 5 │└────────┴──────┘15 rows in set. Elapsed: 0.015 sec. 参考资料我提了issue, 详细原因请看https://github.com/ClickHouse/ClickHouse/issues/14160 总之除了LEFT JOIN 外 For other OUTER JOINs there&#39;s no general solution to return expected result yet","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"译文 ClickHouse Materialized Views Illuminated, Part 2","slug":"2020-08-23-译文-ClickHouse-Materialized-Views-Illuminated,-Part-2","date":"2020-08-23T11:05:00.000Z","updated":"2020-08-24T07:44:14.541Z","comments":true,"path":"2020/08/23/2020-08-23-译文-ClickHouse-Materialized-Views-Illuminated,-Part-2/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-%E8%AF%91%E6%96%87-ClickHouse-Materialized-Views-Illuminated,-Part-2/","excerpt":"ClickHouse Materialized Views Illuminated, Part 2 Read part 1","text":"ClickHouse Materialized Views Illuminated, Part 2 Read part 1 在上一篇关于物化视图的博文中, 我们介绍了一种构造ClickHouse物化视图的方法, 该视图使用SummingMergeTree引擎计算总和和计数. SummingMergeTree可以为这两种类型的聚合使用普通的SQL语法. 我们还让物化视图定义自动为数据创建基础表(.inner表). 这两种技术都很快速, 但对生产系统有限制(都不太适用于生产环境). 在本篇文章中, 我们将展示如何在现有的表上创建一个具有一系列聚合类型的物化视图. 当您需要计算的不仅仅是简单的总和时, 此方法非常适合. 对于表中有大量正在插入的数据(针对Part 1)中的 POPULATE, 使用POPULATE会填充历史数据, 但这期间向原表中新插入数据会被忽略掉而不会写入物化视图中)或必须处理表结构变更的情况, 这也非常方便. 使用State函数和To Tables创建更灵活的物化视图在线面的例子中, 我们将测量设备的读数. 让我们从表定义开始. 1234567CREATE TABLE counter ( when DateTime DEFAULT now(), device UInt32, value Float32) ENGINE=MergeTreePARTITION BY toYYYYMM(when)ORDER BY (device, when) 接下来, 我们添加足够的数据, 以使查询速度变得足够慢: 10个设备的10亿行合成数据. 注意: 如果您要尝试这些操作, 只需输入100万行即可. 无论数据量如何, 示例都可以工作. 12345678910111213141516171819202122232425262728293031323334353637383940INSERT INTO counter SELECT toDateTime(&#x27;2015-01-01 00:00:00&#x27;) + toInt64(number / 10) AS when, (number % 10) + 1 AS device, ((device * 3) + (number / 10000)) + ((rand() % 53) * 0.1) AS valueFROM system.numbersLIMIT 1000000000↓ Progress: 1.00 billion rows, 8.00 GB (5.13 million rows/s., 41.07 MB/s.) Ok.0 rows in set. Elapsed: 194.814 sec. Processed 1.00 billion rows, 8.00 GB (5.13 million rows/s., 41.07 MB/s.) SELECT count(*)FROM counter┌────count()─┐│ 1000000000 │└────────────┘SELECT *FROM counterLIMIT 1┌────────────────when─┬─device─┬─value─┐│ 2015-01-01 00:00:00 │ 1 │ 3.6 │└─────────────────────┴────────┴───────┘[root@bj2-all-clickhouse-test-02 11:21:30 /data/clickhouse/node2/data/duyalan]#du -sh counter/13G counter/[root@bj2-all-clickhouse-test-02 11:21:35 /data/clickhouse/node2/data/duyalan]#du -sh counter/12G counter/[root@bj2-all-clickhouse-test-02 11:26:04 /data/clickhouse/node2/data/duyalan]#du -sh counter/6.5G counter/数据慢慢被压实 现在, 让我们看一下我们希望定期运行的示例查询. 它汇总了整个采样期间所有设备的所有数据. 在这种情况下, 这意味着表中3.25年的数据, 都是在2019年之前. 12345678910111213141516171819202122232425262728293031323334353637SELECT device, count(*) AS count, max(value) AS max, min(value) AS min, avg(value) AS avgFROM counterGROUP BY deviceORDER BY device ASC┌─device─┬─────count─┬────────max─┬─────min─┬────────────────avg─┐│ 1 │ 100000000 │ 100008.15 │ 3.077 │ 50005.599374785554 ││ 2 │ 100000000 │ 100011.164 │ 6.0761 │ 50008.59962170133 ││ 3 │ 100000000 │ 100014.1 │ 9.0022 │ 50011.599634214646 ││ 4 │ 100000000 │ 100017.17 │ 12.0063 │ 50014.59989124005 ││ 5 │ 100000000 │ 100020.164 │ 15.0384 │ 50017.59997032414 ││ 6 │ 100000000 │ 100023.19 │ 18.1045 │ 50020.60019940771 ││ 7 │ 100000000 │ 100026.055 │ 21.0566 │ 50023.60046194672 ││ 8 │ 100000000 │ 100029.14 │ 24.0477 │ 50026.60002471252 ││ 9 │ 100000000 │ 100032.17 │ 27.0218 │ 50029.60008679837 ││ 10 │ 100000000 │ 100035.02 │ 30.0629 │ 50032.60051765903 │└────────┴───────────┴────────────┴─────────┴────────────────────┘10 rows in set. Elapsed: 12.036 sec. Processed 1.00 billion rows, 8.00 GB (83.08 million rows/s., 664.67 MB/s.) bj2-all-clickhouse-test-02 :) select min(when),max(when) from counter;SELECT min(when), max(when)FROM counter┌───────────min(when)─┬───────────max(when)─┐│ 2015-01-01 00:00:00 │ 2018-03-03 09:46:39 │└─────────────────────┴─────────────────────┘1 rows in set. Elapsed: 9.941 sec. Processed 1.00 billion rows, 4.00 GB (100.59 million rows/s., 402.36 MB/s.) 前面的查询很慢, 因为它必须读取表中的所有数据才能获得答案. 我们想要设计一个物化视图, 该视图读取的数据要少得多. 事实证明, 如果我们定义了一个每天汇总数据的视图, 则ClickHouse将正确地在整个时间间隔内汇总每天的数据. 与前面的简单示例(Part 1)不同, 我们将自己定义目标(.inner表)表. 这样做的好处是, 该表现在可见, 这使得加载数据以及进行模式迁移(表结构变更)更加容易. 下面是目标表的定义. 1234567891011CREATE TABLE counter_daily ( day DateTime, device UInt32, count UInt64, max_value_state AggregateFunction(max, Float32), min_value_state AggregateFunction(min, Float32), avg_value_state AggregateFunction(avg, Float32))ENGINE = SummingMergeTree()PARTITION BY tuple()ORDER BY (device, day) 该表定义引入了一种新的数据类型, 称为AggregateFunction, 该数据类型保存部分聚合的数据(which holds partially aggregated data). 这个数据类型用于sum和count以外的聚合需求. 接下来, 我们创建相应的物化视图. 它从counter(源表)中选择数据, 并使用CREATE语句中的特殊TO语法将数据发送到counter_daily(目标表). 该表有聚合函数, SELECT语句有与之相匹配的函数, 如’ maxState ‘. 我们将在详细讨论聚合函数时讨论它们之间的关系. 12345678910111213CREATE MATERIALIZED VIEW counter_daily_mvTO counter_dailyAS SELECT toStartOfDay(when) as day, device, count(*) as count, maxState(value) AS max_value_state, minState(value) AS min_value_state, avgState(value) AS avg_value_stateFROM counterWHERE when &gt;= toDate(&#x27;2019-01-01 00:00:00&#x27;)GROUP BY device, dayORDER BY device, day TO关键字使我们可以指向目标表(存储物化视图数据的表, 在本例中即是counter_daily表), 但有一个缺点. ClickHouse不允许在TO中使用POPULATE关键字. 因此, 物化视图创建后没有任何数据. 我们将手动加载数据. 但是, 我们还将使用一个不错的技巧, 使我们可以避免在同时进行活动数据加载的情况下出现问题. 注意, 视图定义有一个WHERE子句. 这意味着2019年之前的任何数据都应该被忽略. 我们现在有了一种不丢失数据的方法来处理数据加载. 该视图将处理2019年到达的新数据. 同时, 我们可以通过插入加载2018年及之前的旧数据. 让我们通过将新数据加载到counter表中来演示它是如何工作的. 新数据将于2019年开始, 并将自动加载到视图中. 123456INSERT INTO counter SELECT toDateTime(&#x27;2019-01-01 00:00:00&#x27;) + toInt64(number/10) AS when, (number % 10) + 1 AS device, (device * 3) + (number / 10000) + (rand() % 53) * 0.1 AS value FROM system.numbers LIMIT 100000000 现在, 使用以下INSERT手动加载旧数据. 它会加载2018年及之前的所有数据. 123456789101112INSERT INTO counter_dailySELECT toStartOfDay(when) as day, device, count(*) AS count, maxState(value) AS max_value_state, minState(value) AS min_value_state, avgState(value) AS avg_value_stateFROM counterWHERE when &lt; toDateTime(&#x27;2019-01-01 00:00:00&#x27;)GROUP BY device, dayORDER BY device, day 我们终于可以从视图中查询数据了. 与目标表和物化化视图一样, ClickHouse使用专用语法从视图中进行选择. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677物化视图目标表SELECT device, sum(count) AS count, maxMerge(max_value_state) AS max, minMerge(min_value_state) AS min, avgMerge(avg_value_state) AS avgFROM counter_dailyGROUP BY deviceORDER BY device ASC┌─device─┬─────count─┬────────max─┬─────min─┬────────────────avg─┐│ 1 │ 110000000 │ 100008.15 │ 3.051 │ 45914.69035234097 ││ 2 │ 110000000 │ 100011.164 │ 6.0291 │ 45917.69056040798 ││ 3 │ 110000000 │ 100014.1 │ 9.0022 │ 45920.690478928045 ││ 4 │ 110000000 │ 100017.17 │ 12.0063 │ 45923.69086044358 ││ 5 │ 110000000 │ 100020.164 │ 15.0114 │ 45926.69083122718 ││ 6 │ 110000000 │ 100023.19 │ 18.0475 │ 45929.691088042426 ││ 7 │ 110000000 │ 100026.055 │ 21.0566 │ 45932.69135215635 ││ 8 │ 110000000 │ 100029.14 │ 24.0107 │ 45935.690912335944 ││ 9 │ 110000000 │ 100032.17 │ 27.0218 │ 45938.69098338585 ││ 10 │ 110000000 │ 100035.02 │ 30.0429 │ 45941.69140548378 │└────────┴───────────┴────────────┴─────────┴────────────────────┘10 rows in set. Elapsed: 0.019 sec. Processed 13.69 thousand rows, 1.12 MB (729.14 thousand rows/s., 59.84 MB/s.) 物化视图SELECT device, sum(count) AS count, maxMerge(max_value_state) AS max, minMerge(min_value_state) AS min, avgMerge(avg_value_state) AS avgFROM counter_daily_mvGROUP BY deviceORDER BY device ASC┌─device─┬─────count─┬────────max─┬─────min─┬────────────────avg─┐│ 1 │ 110000000 │ 100008.15 │ 3.051 │ 45914.69035234097 ││ 2 │ 110000000 │ 100011.164 │ 6.0291 │ 45917.69056040798 ││ 3 │ 110000000 │ 100014.1 │ 9.0022 │ 45920.690478928045 ││ 4 │ 110000000 │ 100017.17 │ 12.0063 │ 45923.69086044358 ││ 5 │ 110000000 │ 100020.164 │ 15.0114 │ 45926.69083122718 ││ 6 │ 110000000 │ 100023.19 │ 18.0475 │ 45929.691088042426 ││ 7 │ 110000000 │ 100026.055 │ 21.0566 │ 45932.69135215635 ││ 8 │ 110000000 │ 100029.14 │ 24.0107 │ 45935.690912335944 ││ 9 │ 110000000 │ 100032.17 │ 27.0218 │ 45938.69098338585 ││ 10 │ 110000000 │ 100035.02 │ 30.0429 │ 45941.69140548378 │└────────┴───────────┴────────────┴─────────┴────────────────────┘10 rows in set. Elapsed: 0.003 sec. Processed 13.69 thousand rows, 1.12 MB (3.99 million rows/s., 327.87 MB/s.) 源表SELECT device, count(*) AS count, max(value) AS max, min(value) AS min, avg(value) AS avgFROM counterGROUP BY deviceORDER BY device ASC┌─device─┬─────count─┬────────max─┬─────min─┬────────────────avg─┐│ 1 │ 110000000 │ 100008.15 │ 3.051 │ 45914.69035234098 ││ 2 │ 110000000 │ 100011.164 │ 6.0291 │ 45917.69056040798 ││ 3 │ 110000000 │ 100014.1 │ 9.0022 │ 45920.69047892806 ││ 4 │ 110000000 │ 100017.17 │ 12.0063 │ 45923.69086044358 ││ 5 │ 110000000 │ 100020.164 │ 15.0114 │ 45926.690831227155 ││ 6 │ 110000000 │ 100023.19 │ 18.0475 │ 45929.69108804243 ││ 7 │ 110000000 │ 100026.055 │ 21.0566 │ 45932.691352156355 ││ 8 │ 110000000 │ 100029.14 │ 24.0107 │ 45935.690912335944 ││ 9 │ 110000000 │ 100032.17 │ 27.0218 │ 45938.69098338586 ││ 10 │ 110000000 │ 100035.02 │ 30.0429 │ 45941.69140548378 │└────────┴───────────┴────────────┴─────────┴────────────────────┘10 rows in set. Elapsed: 14.041 sec. Processed 1.10 billion rows, 8.80 GB (78.34 million rows/s., 626.72 MB/s.) 该查询正确总结了包括新插入数据在内的所有数据. 您可以通过在counter表上重新运行原始选择来检查计算结果是否一致. 不同之处在于, 物化视图返回数据的速度要快900倍(在我的测试中为4680倍). 由此可见, 学习一些新语法是值得的!! 此时, 我们可以回过头来解释一下在这一切的幕后发生了什么. Aggregate FunctionsAggregate functions类似于收集器(collectors), 允许ClickHouse从分布在多个parts上的数据构建聚合. 下面的图表显示了如何计算平均值. 我们从源表中的一个可选值开始. 物化视图使用avgState函数将数据转换为partial aggregate, avgState函数是一个内部结构. 最后, 在查询数据时, 应用avgMerge将partial aggregates的数据累加为最终的数字. partial aggregate使物化视图能够处理分布在多个节点上的多个parts上的数据. 即使您更改了group by列, merge函数也可以正确地组装聚合. 仅仅结合简单的平均值是行不通的, 因为它们在将每个部分平均值加到总数时缺乏必要的权重. 这种行为有一个重要的后果(这段不会翻译, 原文: It would not work just to combine simple average values, because they would be lacking the weights necessary to scale each partial average as it added to the total. This behavior has an important consequence.). 还记得上面我们提到过, ClickHouse可以使用带有汇总的每日数据的物化视图来回答我们的示例查询吗?这是聚合函数工作的结果. 这意味着我们的daily视图还可以回答关于周、月、年或整个间隔的问题. ClickHouse有点不寻常, 它直接以SQL语法公开了partial aggregates, 但是它们解决问题的方式非常强大. 当您设计实例化视图时, 请尝试使用每日汇总之类的技巧来解决单个视图中的多个问题. 单个视图可以回答很多问题. Table Engines for Materialized ViewsClickHouse有多个对物化视图有用的引擎. AggregatingMergeTree引擎只使用聚合函数. 如果您想做计数或求和, 您需要使用目标表中的AggregateFunction数据类型来定义它们. 您还需要在视图和select语句中使用state和merge函数. 例如, 要处理计数(count), 您需要在上面的示例中使用countState(count)和countMerge(count). 我们建议使用SummingMergeTree引擎在物化视图中进行聚合. 它可以很好地处理聚合函数. 它可以很好地处理聚合函数. 但是, 它会将它们隐藏起来以进行总数和计数, 这对于简单的案例来说非常方便. 在这种情况下, 它不会阻止您使用state和merge函数; 只是你没必要这么做. 同时, 它完成了AggregatingMergeTree的所有工作. Schema Migration在生产系统中, 数据库模式往往会发生变化, 特别是那些正在积极开发的系统. 当使用带有显式目标表的物化视图时, 可以相对容易地管理这些更改. 让我们举一个简单的例子. 假设counter表的名称更改为counter_replicated. 一旦应用了此更改, 物化视图将无法工作. 更糟糕的是, 这个错误将阻止对counter表的插入. 您可以按照以下方式处理更改. 1234567891011121314151617-- Delete view prior to schema change.DROP TABLE counter_daily_mv-- Rename source table.RENAME TABLE counter TO counter_replicated-- Recreate view with correct source table name.CREATE MATERIALIZED VIEW counter_daily_mvTO counter_dailyAS SELECT toStartOfDay(when) as day, device, count(*) as count, maxState(value) AS max_value_state, minState(value) AS min_value_state, avgState(value) AS avg_value_stateFROM counter_replicatedGROUP BY device, dayORDER BY device, day 根据架构迁移中的实际步骤, 您可能必须处理更改物化视图定义时插入到源表的数据(这些数据未插入到物化视图中). 您可以使用过滤条件和手动加载来处理该问题, 如我们在主要示例中所示. Materialized View Plumbing and Data Sizes最后, 让我们再看看数据表和物化视图之间的关系. 目标表是一个普通表. 您可以从目标表或物化视图中选择数据. 没有区别. 此外, 如果您删除物化视图, 目标表扔将保留. 正如我们刚才所展示的, 您可以通过简单地删除和重新创建视图来对其进行模式更改. 如果需要更改目标表本身, 可以像对任何其他表一样运行ALTER table命令. 该图还显示了源表和目标表的数据大小. 物化视图通常远小于其汇总数据的表. 我们的示例就是这样的结果. 以下查询显示了此示例的大小差异. 123456789101112131415SELECT table, formatReadableSize(sum(data_compressed_bytes)) AS tc, formatReadableSize(sum(data_uncompressed_bytes)) AS tu, sum(data_compressed_bytes) / sum(data_uncompressed_bytes) AS ratioFROM system.columnsWHERE database = currentDatabase()GROUP BY tableORDER BY table ASC┌─table────────────────────────────────┬─tc─────────┬─tu─────────┬──────────────ratio─┐│ counter │ 7.14 GiB │ 12.29 GiB │ 0.5805970993939394 ││ counter_daily │ 248.77 KiB │ 494.31 KiB │ 0.503261750004939 ││ counter_daily_mv │ 0.00 B │ 0.00 B │ nan │└──────────────────────────────────────┴────────────┴────────────┴────────────────────┘ 如计算所示, 物化视图目标表大约比物化视图派生的源数据小3万倍. 这种差异极大地加快了查询速度. 如前面所示, 在使用来自物化视图的数据时, 测试查询的运行速度大约快了900x(我这里测试为4680x). Wrap-upClickHouse物化视图非常灵活, 这得益于强大的聚合功能以及源表、物化视图和目标表之间的简单关系. 物化视图允许显式目标表, 这是一个有用的特性, 可以简化模式迁移. 您还可以通过向视图选择定义添加筛选条件并手动加载丢失的数据来减少可能丢失的视图更新. 物化视图还有许多其他方法可以帮助转换数据. 我们已经描述了其中的一些问题, 比如 last point queries, 并且计划将来在这个博客上写一些其他的问题. 欲了解更多信息, 请查看我们最近的网络研讨会ClickHouse and the Magic of Materialized Views. 我们在这里介绍了几个用例示例. 最后, 如果你正在以你认为其他用户会感兴趣的方式使用物化视图, 写一篇文章或者在当地的ClickHouse会议上发言. 我们很乐意在Altinity的博客上发布社区用户的内容, 并在未来的聚会上寻找演讲者. 如果你有什么想与社区分享的, 请让我们知道.","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"},{"name":"物化视图","slug":"物化视图","permalink":"http://fuxkdb.com/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"}]},{"title":"译文 ClickHouse Materialized Views Illuminated, Part 1","slug":"2020-08-23-译文-ClickHouse-Materialized-Views-Illuminated,-Part-1","date":"2020-08-23T11:04:00.000Z","updated":"2020-08-23T11:04:57.497Z","comments":true,"path":"2020/08/23/2020-08-23-译文-ClickHouse-Materialized-Views-Illuminated,-Part-1/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-%E8%AF%91%E6%96%87-ClickHouse-Materialized-Views-Illuminated,-Part-1/","excerpt":"ClickHouse Materialized Views Illuminated, Part 1 Altinity blog的读者们知道我们喜欢ClickHouse的物化视图. 物化视图可以实现聚合计算, 从Kafka读取数据, 实现最后点查询(last point queries)以及重组表主键索引和排序顺序. 除了这些功能之外, 物化视图可以在大量节点上很好地扩缩, 并可以处理大型数据集. 它们是ClickHouse的独特功能之一. 在计算领域, 强大的功能至少意味着一点点复杂性. 这篇由两部分组成的文章通过解释物化视图的工作原理来填补空白, 从而使初学者也可以有效地使用它们. 我们将通过几个详细的示例, 您可以根据自己的使用进行调整. 在此过程中, 我们将探索用于创建视图的语法的确切含义, 并让您深入了解ClickHouse在做什么. 示例是完全自包含的, 因此您可以将它们复制/粘贴到clickhouse-client中并自己运行它们.","text":"ClickHouse Materialized Views Illuminated, Part 1 Altinity blog的读者们知道我们喜欢ClickHouse的物化视图. 物化视图可以实现聚合计算, 从Kafka读取数据, 实现最后点查询(last point queries)以及重组表主键索引和排序顺序. 除了这些功能之外, 物化视图可以在大量节点上很好地扩缩, 并可以处理大型数据集. 它们是ClickHouse的独特功能之一. 在计算领域, 强大的功能至少意味着一点点复杂性. 这篇由两部分组成的文章通过解释物化视图的工作原理来填补空白, 从而使初学者也可以有效地使用它们. 我们将通过几个详细的示例, 您可以根据自己的使用进行调整. 在此过程中, 我们将探索用于创建视图的语法的确切含义, 并让您深入了解ClickHouse在做什么. 示例是完全自包含的, 因此您可以将它们复制/粘贴到clickhouse-client中并自己运行它们. How Materialized Views Work: Computing SumsClickHouse物化视图自动在表之间转换数据. 它们类似于触发器, 对插入的行运行查询并将结果存入第二个表. 让我们看一个基本的例子. 假设我们有一个记录用户下载的表, 如下所示. 1234567CREATE TABLE download ( when DateTime, userid UInt32, bytes Float32) ENGINE=MergeTreePARTITION BY toYYYYMM(when)ORDER BY (userid, when) 我们希望跟踪每个用户的每日下载. 让我们看看如何用一个查询来做到这一点. 首先, 我们需要为单个用户向表中添加一些数据. 1234567INSERT INTO download SELECT now() + number * 60 as when, 25, rand() % 100000000 FROM system.numbers LIMIT 5000 接下来, 让我们运行一个查询来显示该用户的每日下载. 当添加新用户时, 这也将正常工作. 12345678910111213141516171819SELECT toStartOfDay(when) AS day, userid, count() AS downloads, sum(bytes) AS bytesFROM downloadGROUP BY userid, dayORDER BY userid ASC, day ASC┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 736 │ 36722522631 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73305428060 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 73183910537 ││ 2020-08-21 00:00:00 │ 25 │ 1384 │ 70059352697 │└─────────────────────┴────────┴───────────┴─────────────┘ 我们可以通过每次运行查询以交互方式为应用程序计算这些每日总数, 但是对于大型表, 提前计算它们将更快, 更节省资源. 因此, 最好将结果放在单独的表格中, 该表格可以连续跟踪每天每个用户的下载总数. 我们可以使用以下物化视图来做到这一点. 1234567891011CREATE MATERIALIZED VIEW download_daily_mvENGINE = SummingMergeTreePARTITION BY toYYYYMM(day) ORDER BY (userid, day)POPULATEAS SELECT toStartOfDay(when) AS day, userid, count() as downloads, sum(bytes) AS bytesFROM downloadGROUP BY userid, day 这里有三件重要的事情需要注意. 首先, 物化视图定义允许类似于CREATE TABLE的语法, 这是有意义的, 因为这个命令将实际创建一个隐藏的目标表(.inner表)来保存视图数据. 我们使用的ClickHouse引擎旨在使计算和计数变得简单:SummingMergeTree. 它是用于计算聚合的物化视图的推荐引擎. 其次, 视图定义包含关键字POPULATE. 这告诉ClickHouse将download表中的现有数据插入物化视图. 我们稍后会更多地讨论automatic population. 需要注意, 在POPULATE填充历史数据的期间, 新进入的这部分数据会被忽略掉, 所以如果对准确性要求非常高, 应慎用 第三, 视图定义包含一个SELECT语句, 该语句定义在加载视图时如何转换数据. 这个查询在表中的新数据上运行, 以计算每天每个用户id的下载数量和总字节数. 它本质上与我们以交互方式运行的查询相同, 只是在本例中, 结果将放在隐藏的目标表(.inner表)中. 我们可以跳过排序, 因为视图定义已经确保了排序顺序. 现在, 我们从物化视图中查询数据 12345678910SELECT * FROM download_daily_mvORDER BY day, userid LIMIT 5┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 736 │ 36722522631 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73305428060 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 73183910537 ││ 2020-08-21 00:00:00 │ 25 │ 1384 │ 70059352697 │└─────────────────────┴────────┴───────────┴─────────────┘ 这为我们提供了与先前查询完全相同的答案. 原因是上面介绍的POPULATE关键字. 它确保源表中的现有数据自动加载到视图中. 不过, 有一个重要警告：如果在填充视图时插入了新数据, ClickHouse将会丢失它们. 在本系列的第二部分中, 我们将展示如何手动插入数据并避免数据遗漏的问题. 现在, 尝试使用其他用户向表中添加更多数据. 1234567INSERT INTO download SELECT now() + number * 60 as when, 22, rand() % 100000000 FROM system.numbers LIMIT 5000 如果您从实例化视图中进行选择, 您将看到它现在具有用户ID 22和25的总数. 请注意, 一旦INSERT完成, 将立即填充新数据. 这是ClickHouse实例化视图的重要功能, 这使其对于实时分析非常有用. 下面是查询和新结果. 12345678910111213141516171819SELECT *FROM download_daily_mvORDER BY userid ASC, day ASC┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 22 │ 416 │ 21063519801 ││ 2020-08-19 00:00:00 │ 22 │ 1440 │ 71523929305 ││ 2020-08-20 00:00:00 │ 22 │ 1440 │ 70435459582 ││ 2020-08-21 00:00:00 │ 22 │ 1440 │ 70725673036 ││ 2020-08-22 00:00:00 │ 22 │ 264 │ 13826466067 │└─────────────────────┴────────┴───────────┴─────────────┘┌─────────────────day─┬─userid─┬─downloads─┬────────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 1467 │ 75441118166 ││ 2020-08-19 00:00:00 │ 25 │ 2880 │ 144811386193 ││ 2020-08-20 00:00:00 │ 25 │ 2880 │ 145138479865 ││ 2020-08-21 00:00:00 │ 25 │ 2773 │ 138488485955 │└─────────────────────┴────────┴───────────┴──────────────┘ 作为练习, 您可以对源表运行原始查询, 以确认它与物化视图中的总数相匹配. 作为最后一个示例, 让我们使用物化视图按月汇总. 在本例中, 我们将物化视图视为一个普通表, 按月分组, 如下所示. 我们添加了WITH TOTALS子句, 它打印一个方便的聚合的总和. 1234567891011121314151617181920212223SELECT toStartOfMonth(day) AS month, userid, sum(downloads), sum(bytes)FROM download_daily_mvGROUP BY userid, month WITH TOTALSORDER BY userid ASC, month ASC┌──────month─┬─userid─┬─sum(downloads)─┬───sum(bytes)─┐│ 2020-08-01 │ 22 │ 5000 │ 247575047791 ││ 2020-08-01 │ 25 │ 10000 │ 503879470179 │└────────────┴────────┴────────────────┴──────────────┘Extremes:┌──────month─┬─userid─┬─sum(downloads)─┬───sum(bytes)─┐│ 0000-00-00 │ 0 │ 15000 │ 751454517970 │└────────────┴────────┴────────────────┴──────────────┘ 从前面的示例中, 我们可以清楚地看到实例化视图如何正确地汇总源数据中的数据(how the materialized view correctly summarizes data from the source data). 如上例所示, 我们甚至可以“summarize the summaries”. 那么幕后到底发生了什么？ 下图说明了数据的逻辑流. 如图所示, 原表上INSERT的值被转换并应用于隐藏的目标表(.inner表). 要填充视图(To populate the view), 您要做的就是在源表中插入数据. 您可以从隐藏的目标表(.inner表)和物化视图中进行查询. 实际上ClickHouse就是将查询路由到创建物化视图时自动创建的internel表中的. 图中还有另外一件重要的事情需要注意. 物化视图创建一个具有特殊名称私有表来保存数据. 如果您通过输入DROP TABLE download_daily_mv删除物化视图, 则私有表也会被删除. 如果需要更改视图, 则需要将其删除并使用新数据重新创建 12345678&gt;SHOW TABLES&gt;┌─name─────────────────────┐&gt;│ .inner.download_daily_mv │&gt;│ download │&gt;│ download_daily_mv │&gt;└──────────────────────────┘ .inner.download_daily_mv就是 internel表或叫私有表 Wrap-up - 总结我们刚刚审阅的示例使用SummingMergeTree创建一个视图以累加每日用户下载量. 我们从物化视图对SELECT使用了标准SQL语法. 这是SummingMergeTree引擎的特殊功能, 仅适用于总和和计数. 对于其他类型的聚合, 我们需要使用其他方法. 另外, 我们的示例使用POPULATE关键字将现有表数据发布到视图创建的私有目标表(.inner表)中. 如果在填充视图时到达新的INSERT行, ClickHouse将错过它们. 当您是唯一使用数据集的人时, 此限制很容易解决, 但对于不断加载数据的生产系统来说是个问题. 此外, 删除视图后, 专用表也会消失. 这使得很难更改视图以适应源表中的架构更改. 在下一篇文章中, 我们将展示如何创建物化视图来计算其他类型的聚合, 比如平均值或最大值/最小值. 我们还将展示如何显式定义目标表(.inner表), 并使用我们自己的SQL语句手动将数据加载到其中. 我们还将简要介绍模式迁移(schema migration). 同时, 我们希望您喜欢这一简要介绍, 并发现示例有用. 实测及问题删除基表, 物化视图仍然可以查询这是符合预期的, 因为物化视图是存储了数据的 1234567891011121314151617181920212223242526272829303132333435363738394041bj2-all-clickhouse-test-02 :) drop table download;DROP TABLE downloadOk.0 rows in set. Elapsed: 0.016 sec. bj2-all-clickhouse-test-02 :) show tables;SHOW TABLES┌─name─────────────────────┐│ .inner.download_daily_mv ││ download_daily_mv ││ sbtest ││ sbtest_local │└──────────────────────────┘4 rows in set. Elapsed: 0.001 sec. bj2-all-clickhouse-test-02 :) select * from download_daily_mv;SELECT *FROM download_daily_mv┌─────────────────day─┬─userid─┬─downloads─┬────────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 1467 │ 75441118166 ││ 2020-08-19 00:00:00 │ 25 │ 2880 │ 144811386193 ││ 2020-08-20 00:00:00 │ 25 │ 2880 │ 145138479865 ││ 2020-08-21 00:00:00 │ 25 │ 2773 │ 138488485955 │└─────────────────────┴────────┴───────────┴──────────────┘┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 22 │ 416 │ 21063519801 ││ 2020-08-19 00:00:00 │ 22 │ 1440 │ 71523929305 ││ 2020-08-20 00:00:00 │ 22 │ 1440 │ 70435459582 ││ 2020-08-21 00:00:00 │ 22 │ 1440 │ 70725673036 ││ 2020-08-22 00:00:00 │ 22 │ 264 │ 13826466067 │└─────────────────────┴────────┴───────────┴─────────────┘9 rows in set. Elapsed: 0.008 sec. 数据在分区合并时聚合重新创建download表和物化视图 12345678910111213141516171819202122232425262728293031323334SELECT toStartOfDay(when) AS day, userid, count() AS downloads, sum(bytes) AS bytesFROM downloadGROUP BY userid, dayORDER BY userid ASC, day ASC┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 387 │ 19855551536 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73316885071 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 72322018002 ││ 2020-08-21 00:00:00 │ 25 │ 1440 │ 71675677053 ││ 2020-08-22 00:00:00 │ 25 │ 293 │ 14125612942 │└─────────────────────┴────────┴───────────┴─────────────┘SELECT *FROM download_daily_mv┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 387 │ 19855551536 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73316885071 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 72322018002 ││ 2020-08-21 00:00:00 │ 25 │ 1440 │ 71675677053 ││ 2020-08-22 00:00:00 │ 25 │ 293 │ 14125612942 │└─────────────────────┴────────┴───────────┴─────────────┘ 再次向基表中插入数据, 然后查看物化视图中的数据 1234567891011121314151617181920212223242526INSERT INTO download SELECT now() + (number * 60) AS when, 25, rand() % 100000000FROM system.numbersLIMIT 5000SELECT *FROM download_daily_mv┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 387 │ 19855551536 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73316885071 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 72322018002 ││ 2020-08-21 00:00:00 │ 25 │ 1440 │ 71675677053 ││ 2020-08-22 00:00:00 │ 25 │ 293 │ 14125612942 │└─────────────────────┴────────┴───────────┴─────────────┘┌─────────────────day─┬─userid─┬─downloads─┬───────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 385 │ 18771807859 ││ 2020-08-19 00:00:00 │ 25 │ 1440 │ 73675739078 ││ 2020-08-20 00:00:00 │ 25 │ 1440 │ 70555293899 ││ 2020-08-21 00:00:00 │ 25 │ 1440 │ 70773685728 ││ 2020-08-22 00:00:00 │ 25 │ 295 │ 14750186600 │└─────────────────────┴────────┴───────────┴─────────────┘ 可以看到物化视图中的数据并没有”全部聚合完整” 查看物化视图private表分区情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879bj2-all-clickhouse-test-02 :) select * from system.parts where table=&#x27;.inner.download_daily_mv&#x27;\\GSELECT *FROM system.partsWHERE table = &#x27;.inner.download_daily_mv&#x27;Row 1:──────partition: 202008name: 202008_1_1_0part_type: Wideactive: 1marks: 2rows: 5bytes_on_disk: 421data_compressed_bytes: 200data_uncompressed_bytes: 120marks_bytes: 192modification_time: 2020-08-18 17:33:51remove_time: 0000-00-00 00:00:00refcount: 1min_date: 0000-00-00max_date: 0000-00-00min_time: 2020-08-18 00:00:00max_time: 2020-08-22 00:00:00partition_id: 202008min_block_number: 1max_block_number: 1level: 0data_version: 1primary_key_bytes_in_memory: 16primary_key_bytes_in_memory_allocated: 8192is_frozen: 0database: duyalantable: .inner.download_daily_mvengine: SummingMergeTreedisk_name: defaultpath: /data/clickhouse/node2/data/duyalan/%2Einner%2Edownload_daily_mv/202008_1_1_0/hash_of_all_files: f4b55a88dac393d25ffe1c703cca4f6dhash_of_uncompressed_files: 58a4ab29ef36c22884ee7accd528b590uncompressed_hash_of_compressed_files: e18b006f608580db132ed90adb46902fRow 2:──────partition: 202008name: 202008_2_2_0part_type: Wideactive: 1marks: 2rows: 5bytes_on_disk: 421data_compressed_bytes: 200data_uncompressed_bytes: 120marks_bytes: 192modification_time: 2020-08-18 17:35:13remove_time: 0000-00-00 00:00:00refcount: 1min_date: 0000-00-00max_date: 0000-00-00min_time: 2020-08-18 00:00:00max_time: 2020-08-22 00:00:00partition_id: 202008min_block_number: 2max_block_number: 2level: 0data_version: 2primary_key_bytes_in_memory: 16primary_key_bytes_in_memory_allocated: 8192is_frozen: 0database: duyalantable: .inner.download_daily_mvengine: SummingMergeTreedisk_name: defaultpath: /data/clickhouse/node2/data/duyalan/%2Einner%2Edownload_daily_mv/202008_2_2_0/hash_of_all_files: 049d090ea65c24be8544ab86846ea9fahash_of_uncompressed_files: 58a4ab29ef36c22884ee7accd528b590uncompressed_hash_of_compressed_files: 9ffea93e6b9bc375c7f04374b4a4a6a92 rows in set. Elapsed: 0.002 sec. 可以看到, 有两个active分区202008_1_1_0, 202008_2_2_0 我们手动OPTIMIZE尝试合并分区 1234567bj2-all-clickhouse-test-02 :) optimize table download_daily_mv;OPTIMIZE TABLE download_daily_mvOk.0 rows in set. Elapsed: 0.004 sec. 再次查询物化视图数据 123456789101112131415bj2-all-clickhouse-test-02 :) select * from download_daily_mv;SELECT *FROM download_daily_mv┌─────────────────day─┬─userid─┬─downloads─┬────────bytes─┐│ 2020-08-18 00:00:00 │ 25 │ 772 │ 38627359395 ││ 2020-08-19 00:00:00 │ 25 │ 2880 │ 146992624149 ││ 2020-08-20 00:00:00 │ 25 │ 2880 │ 142877311901 ││ 2020-08-21 00:00:00 │ 25 │ 2880 │ 142449362781 ││ 2020-08-22 00:00:00 │ 25 │ 588 │ 28875799542 │└─────────────────────┴────────┴───────────┴──────────────┘5 rows in set. Elapsed: 0.001 sec. 查看private表分区情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115bj2-all-clickhouse-test-02 :) select * from system.parts where table=&#x27;.inner.download_daily_mv&#x27;\\GSELECT *FROM system.partsWHERE table = &#x27;.inner.download_daily_mv&#x27;Row 1:──────partition: 202008name: 202008_1_1_0part_type: Wideactive: 0marks: 2rows: 5bytes_on_disk: 421data_compressed_bytes: 200data_uncompressed_bytes: 120marks_bytes: 192modification_time: 2020-08-18 17:33:51remove_time: 2020-08-18 17:38:24refcount: 1min_date: 0000-00-00max_date: 0000-00-00min_time: 2020-08-18 00:00:00max_time: 2020-08-22 00:00:00partition_id: 202008min_block_number: 1max_block_number: 1level: 0data_version: 1primary_key_bytes_in_memory: 16primary_key_bytes_in_memory_allocated: 8192is_frozen: 0database: duyalantable: .inner.download_daily_mvengine: SummingMergeTreedisk_name: defaultpath: /data/clickhouse/node2/data/duyalan/%2Einner%2Edownload_daily_mv/202008_1_1_0/hash_of_all_files: f4b55a88dac393d25ffe1c703cca4f6dhash_of_uncompressed_files: 58a4ab29ef36c22884ee7accd528b590uncompressed_hash_of_compressed_files: e18b006f608580db132ed90adb46902fRow 2:──────partition: 202008name: 202008_1_2_1part_type: Wideactive: 1marks: 2rows: 5bytes_on_disk: 421data_compressed_bytes: 200data_uncompressed_bytes: 120marks_bytes: 192modification_time: 2020-08-18 17:38:24remove_time: 0000-00-00 00:00:00refcount: 1min_date: 0000-00-00max_date: 0000-00-00min_time: 2020-08-18 00:00:00max_time: 2020-08-22 00:00:00partition_id: 202008min_block_number: 1max_block_number: 2level: 1data_version: 1primary_key_bytes_in_memory: 16primary_key_bytes_in_memory_allocated: 8192is_frozen: 0database: duyalantable: .inner.download_daily_mvengine: SummingMergeTreedisk_name: defaultpath: /data/clickhouse/node2/data/duyalan/%2Einner%2Edownload_daily_mv/202008_1_2_1/hash_of_all_files: fdc534a9b9aa0904cde863ee1deff532hash_of_uncompressed_files: 58a4ab29ef36c22884ee7accd528b590uncompressed_hash_of_compressed_files: df972eeaad3304d9a16bfa8cb46861ceRow 3:──────partition: 202008name: 202008_2_2_0part_type: Wideactive: 0marks: 2rows: 5bytes_on_disk: 421data_compressed_bytes: 200data_uncompressed_bytes: 120marks_bytes: 192modification_time: 2020-08-18 17:35:13remove_time: 2020-08-18 17:38:24refcount: 1min_date: 0000-00-00max_date: 0000-00-00min_time: 2020-08-18 00:00:00max_time: 2020-08-22 00:00:00partition_id: 202008min_block_number: 2max_block_number: 2level: 0data_version: 2primary_key_bytes_in_memory: 16primary_key_bytes_in_memory_allocated: 8192is_frozen: 0database: duyalantable: .inner.download_daily_mvengine: SummingMergeTreedisk_name: defaultpath: /data/clickhouse/node2/data/duyalan/%2Einner%2Edownload_daily_mv/202008_2_2_0/hash_of_all_files: 049d090ea65c24be8544ab86846ea9fahash_of_uncompressed_files: 58a4ab29ef36c22884ee7accd528b590uncompressed_hash_of_compressed_files: 9ffea93e6b9bc375c7f04374b4a4a6a93 rows in set. Elapsed: 0.002 sec. 可以看到只有一个active分区了202008_1_2_1 所以对于本例中的物化视图, 在查询是仍然应该使用聚合函数聚合数据 123456789SELECT day, userid, sum(downloads), sum(bytes)FROM download_no_populate_daily_mvGROUP BY day, userid","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"},{"name":"物化视图","slug":"物化视图","permalink":"http://fuxkdb.com/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"}]},{"title":"为什么pt-osc和gh-osc在拷贝源表数据时要使用insert IGNORE into select lock in share mode","slug":"2020-08-23-为什么pt-osc和gh-osc在拷贝源表数据时要使用insert-IGNORE-into-select-lock-in-share-mode","date":"2020-08-23T10:58:00.000Z","updated":"2020-09-06T08:46:55.930Z","comments":true,"path":"2020/08/23/2020-08-23-为什么pt-osc和gh-osc在拷贝源表数据时要使用insert-IGNORE-into-select-lock-in-share-mode/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-%E4%B8%BA%E4%BB%80%E4%B9%88pt-osc%E5%92%8Cgh-osc%E5%9C%A8%E6%8B%B7%E8%B4%9D%E6%BA%90%E8%A1%A8%E6%95%B0%E6%8D%AE%E6%97%B6%E8%A6%81%E4%BD%BF%E7%94%A8insert-IGNORE-into-select-lock-in-share-mode/","excerpt":"insert IGNORE into select lock in share mode 的作用pt-osc和gh-osc在拷贝旧数据时逻辑是一样的, 都是用insert ignore into 影子表 select * from 原表force index (PRIMARY) where chunk范围 lock in share mode","text":"insert IGNORE into select lock in share mode 的作用pt-osc和gh-osc在拷贝旧数据时逻辑是一样的, 都是用insert ignore into 影子表 select * from 原表force index (PRIMARY) where chunk范围 lock in share mode1234567pt-oscINSERT LOW_PRIORITY IGNORE INTO `sysbench`.`_sbtest1_new` (`id`, `k`, `c`, `pad`, `snum`) SELECT `id`, `k`, `c`, `pad`, `snum` FROM `sysbench`.`sbtest1` FORCE INDEX(`PRIMARY`) WHERE ((`id` &gt;= &#x27;98802&#x27;)) AND ((`id` &lt;= &#x27;98901&#x27;)) LOCK IN SHARE MODE /*pt-online-schema-change 6012 copy nibble*/gh-ostinsert /* gh-ost `sysbench`.`sbtest1` */ ignore into `sysbench`.`_sbtest1_gho` (`id`, `k`, `c`, `pad`, `snum`) (select `id`, `k`, `c`, `pad`, `snum` from `sysbench`.`sbtest1` force index (`PRIMARY`) where (((`id` &gt; _binary&#x27;419601&#x27;)) and ((`id` &lt; _binary&#x27;419701&#x27;) or ((`id` = _binary&#x27;419701&#x27;)))) lock in share mode 为什么需要lock in share mode ?假设修改表t1, t1有两列 id, sname, id为主键, 数据如下 12345678id,sname1,&#x27;foo&#x27;10,&#x27;foo&#x27;20,&#x27;foo&#x27;30,&#x27;foo&#x27;40,&#x27;foo&#x27;...1000万,&#x27;foo&#x27; 开始改表, 需要拷贝原始数据拷贝原表数据语句A: 1insert IGNORE into _t1_影子 select * from t1 where id&gt;1 and id&lt;100万; 我们这个insert正在执行,且执行的很慢, 总是就是放大它的执行时间. 此时另一个会话执行下面的语句B 123insert into t1 values(11,&#x27;foo&#x27;);update t1 set sname=&#x27;hehe&#x27; where id=20;delete from t1 where id=30; 如果是pt-osc ,走触发器, 这些语句会应用到影子表 pt-osc 三个触发器 12345&gt;CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest1_del` AFTER DELETE ON `sysbench`.`sbtest1` FOR EACH ROW DELETE IGNORE FROM `sysbench`.`_sbtest1_new` WHERE `sysbench`.`_sbtest1_new`.`id` &lt;=&gt; OLD.`id`&gt;CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest1_upd` AFTER UPDATE ON `sysbench`.`sbtest1` FOR EACH ROW BEGIN DELETE IGNORE FROM `sysbench`.`_sbtest1_new` WHERE !(OLD.`id` &lt;=&gt; NEW.`id`) AND `sysbench`.`_sbtest1_new`.`id` &lt;=&gt; OLD.`id`;REPLACE INTO `sysbench`.`_sbtest1_new` (`id`, `k`, `c`, `pad`, `snum`) VALUES (NEW.`id`, NEW.`k`, NEW.`c`, NEW.`pad`, NEW.`snum`);END&gt;CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest1_del` AFTER DELETE ON `sysbench`.`sbtest1` FOR EACH ROW DELETE IGNORE FROM `sysbench`.`_sbtest1_new` WHERE `sysbench`.`_sbtest1_new`.`id` &lt;=&gt; OLD.`id` 123REPLACE INTO _t1_影子 values(11,&#x27;foo&#x27;);REPLACE INTO _t1_影子 values(20,&#x27;hehe&#x27;); (如果更新了主键值, 则会delete where id=原主键值, replace into 新主键值)DELETE IGNORE FROM _t1_影子 where id=30; 此时影子表会有如下数据 1211,&#x27;foo&#x27;20,&#x27;hehe&#x27; 之后拷贝原表数据语句A执行完毕, 影子表数据如下 123456781,&#x27;foo&#x27;10,&#x27;foo&#x27;11,&#x27;foo&#x27; --20,&#x27;hehe&#x27; --30,&#x27;foo&#x27; --30又回来了40,&#x27;foo&#x27;...100,&#x27;foo&#x27; 被删除的id=30又会被插入进来 所以数据就出现了问题, 所以需要insert IGNORE into select lock in share mode, 这样语句B就会被阻塞 这也是为什么pt-osc对update不能像gh-ost一样使用update, 而是需要replace into数据还是有问题，所以需要lock in share mode 如果是gh-ost, 走binlog, gh-ost解析binlog应用到影子表 https://github.com/github/gh-ost/blob/e48844de0bee9a8db611a06cd6080cac4dab25cb/go/sql/builder.goinsert就是replace intoupdate还是update (如果更新了主键值, 则会delete where id=原主键值, replace into 新主键值)delete还是delete 123REPLACE INTO _t1_影子 values(11,&#x27;foo&#x27;);update _t1_影子 set sname=&#x27;hehe&#x27; where id=20; --更新不到delete from _t1_影子 where id=30; --删除不到 此时影子表会有如下数据 111,&#x27;foo&#x27; 之后拷贝原表数据语句A执行完毕, 影子表数据如下 123456781,&#x27;foo&#x27;10,&#x27;foo&#x27;11, &#x27;foo&#x27;20,&#x27;foo&#x27; --更新丢了30,&#x27;foo&#x27; --删除也丢了40,&#x27;foo&#x27;...100,&#x27;foo&#x27; 数据还是有问题, 所以需要lock in share mode","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"gh-ost","slug":"gh-ost","permalink":"http://fuxkdb.com/tags/gh-ost/"},{"name":"pt-osc","slug":"pt-osc","permalink":"http://fuxkdb.com/tags/pt-osc/"}]},{"title":"ProxySQL升级时对默认部署的实例的影响","slug":"2020-08-23-ProxySQL升级时对默认部署的实例的影响","date":"2020-08-23T10:54:00.000Z","updated":"2020-08-23T10:55:07.570Z","comments":true,"path":"2020/08/23/2020-08-23-ProxySQL升级时对默认部署的实例的影响/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-ProxySQL%E5%8D%87%E7%BA%A7%E6%97%B6%E5%AF%B9%E9%BB%98%E8%AE%A4%E9%83%A8%E7%BD%B2%E7%9A%84%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%BD%B1%E5%93%8D/","excerpt":"公司有使用默认安装目录的ProxySQL跑业务. 在升级2.0.8 至 2.0.12的时候发现卸载2.0.8后这些使用默认方式安装的ProxySQL实例就会被关闭. 原因是rpm包安装和卸载前后执行了一些动作 123456789101112131415161718192021222324252627282930313233343536373839404142[root@bj1-mysql-manager-prod-01 tmp]# rpm --scripts -qp /tmp/proxysql-2.0.8-1-centos7.x86_64.rpm preinstall scriptlet (using /bin/sh):# Cleanup artifactsif [ -f /var/lib/proxysql/PROXYSQL_UPGRADE ]; then rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfipostinstall scriptlet (using /bin/sh):# Create relevant user, directories and configuration filesif [ ! -d /var/run/proxysql ]; then /bin/mkdir /var/run/proxysql ; fiif [ ! -d /var/lib/proxysql ]; then /bin/mkdir /var/lib/proxysql ; fiif ! id -u proxysql &gt; /dev/null 2&gt;&amp;1; then useradd -r -U -s /bin/false -d /var/lib/proxysql -c &quot;ProxySQL Server&quot; proxysql; fi/bin/chown -R proxysql: /var/lib/proxysql /var/run/proxysql/bin/chown root:proxysql /etc/proxysql.cnf/bin/chmod 640 /etc/proxysql.cnf# Configure systemd appropriately./bin/systemctl daemon-reload/bin/systemctl enable proxysql.service# Notify that a package update is in progress in order to start service.if [ $1 -eq 2 ]; then /bin/touch /var/lib/proxysql/PROXYSQL_UPGRADE ; fipreuninstall scriptlet (using /bin/sh):# When uninstalling always try stop the service, ignore failures/bin/systemctl stop proxysql || truepostuninstall scriptlet (using /bin/sh):if [ $1 -eq 0 ]; then # This is a pure uninstall, systemd unit file removed # only daemon-reload is needed. /bin/systemctl daemon-reloadelse # This is an upgrade, ProxySQL should be started. This # logic works for packages newer than 2.0.7 and ensures # a faster restart time. /bin/systemctl start proxysql.service /bin/rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfiposttrans scriptlet (using /bin/sh):if [ -f /var/lib/proxysql/PROXYSQL_UPGRADE ]; then # This is a safeguard to start the service after an update # which supports legacy &quot;preun&quot; / &quot;postun&quot; logic and will # only execute for packages before 2.0.7. /bin/systemctl start proxysql.service /bin/rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfi","text":"公司有使用默认安装目录的ProxySQL跑业务. 在升级2.0.8 至 2.0.12的时候发现卸载2.0.8后这些使用默认方式安装的ProxySQL实例就会被关闭. 原因是rpm包安装和卸载前后执行了一些动作 123456789101112131415161718192021222324252627282930313233343536373839404142[root@bj1-mysql-manager-prod-01 tmp]# rpm --scripts -qp /tmp/proxysql-2.0.8-1-centos7.x86_64.rpm preinstall scriptlet (using /bin/sh):# Cleanup artifactsif [ -f /var/lib/proxysql/PROXYSQL_UPGRADE ]; then rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfipostinstall scriptlet (using /bin/sh):# Create relevant user, directories and configuration filesif [ ! -d /var/run/proxysql ]; then /bin/mkdir /var/run/proxysql ; fiif [ ! -d /var/lib/proxysql ]; then /bin/mkdir /var/lib/proxysql ; fiif ! id -u proxysql &gt; /dev/null 2&gt;&amp;1; then useradd -r -U -s /bin/false -d /var/lib/proxysql -c &quot;ProxySQL Server&quot; proxysql; fi/bin/chown -R proxysql: /var/lib/proxysql /var/run/proxysql/bin/chown root:proxysql /etc/proxysql.cnf/bin/chmod 640 /etc/proxysql.cnf# Configure systemd appropriately./bin/systemctl daemon-reload/bin/systemctl enable proxysql.service# Notify that a package update is in progress in order to start service.if [ $1 -eq 2 ]; then /bin/touch /var/lib/proxysql/PROXYSQL_UPGRADE ; fipreuninstall scriptlet (using /bin/sh):# When uninstalling always try stop the service, ignore failures/bin/systemctl stop proxysql || truepostuninstall scriptlet (using /bin/sh):if [ $1 -eq 0 ]; then # This is a pure uninstall, systemd unit file removed # only daemon-reload is needed. /bin/systemctl daemon-reloadelse # This is an upgrade, ProxySQL should be started. This # logic works for packages newer than 2.0.7 and ensures # a faster restart time. /bin/systemctl start proxysql.service /bin/rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfiposttrans scriptlet (using /bin/sh):if [ -f /var/lib/proxysql/PROXYSQL_UPGRADE ]; then # This is a safeguard to start the service after an update # which supports legacy &quot;preun&quot; / &quot;postun&quot; logic and will # only execute for packages before 2.0.7. /bin/systemctl start proxysql.service /bin/rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfi 安装preinstall 123if [ -f /var/lib/proxysql/PROXYSQL_UPGRADE ]; then rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfi 这就是看如果有PROXYSQL_UPGRADE这个文件就删除掉,这个文件主要是rpm -Uvh升级时会用到 postinstall 123456789101112# Create relevant user, directories and configuration filesif [ ! -d /var/run/proxysql ]; then /bin/mkdir /var/run/proxysql ; fiif [ ! -d /var/lib/proxysql ]; then /bin/mkdir /var/lib/proxysql ; fiif ! id -u proxysql &gt; /dev/null 2&gt;&amp;1; then useradd -r -U -s /bin/false -d /var/lib/proxysql -c &quot;ProxySQL Server&quot; proxysql; fi/bin/chown -R proxysql: /var/lib/proxysql /var/run/proxysql/bin/chown root:proxysql /etc/proxysql.cnf/bin/chmod 640 /etc/proxysql.cnf# Configure systemd appropriately./bin/systemctl daemon-reload/bin/systemctl enable proxysql.service# Notify that a package update is in progress in order to start service.if [ $1 -eq 2 ]; then /bin/touch /var/lib/proxysql/PROXYSQL_UPGRADE ; fi 创建目录, 创建用户, 设置开机启动proxysql.service. 这里主要就是一点, 用systemd管理rpm包安装的proxysql 卸载preuninstall 12# When uninstalling always try stop the service, ignore failures/bin/systemctl stop proxysql || true 这里就是把rpm包安装的proxysql实例给关了. 所有如果用默认方式安装的ProxySQL实例, 卸载rpm包的时候这个实例就会被关闭, 所以不建议用这个实例, 应该自己创建, 并且建议把rpm包生成的配置文件, systemd都mv改名, 否则机器重启默认实例启动会占用6032,6033端口 preuninstall 1234567891011121314preuninstall scriptlet (using /bin/sh):postuninstall scriptlet (using /bin/sh):if [ $1 -eq 0 ]; then # This is a pure uninstall, systemd unit file removed # only daemon-reload is needed. /bin/systemctl daemon-reloadelse # This is an upgrade, ProxySQL should be started. This # logic works for packages newer than 2.0.7 and ensures # a faster restart time. /bin/systemctl start proxysql.service /bin/rm -fr /var/lib/proxysql/PROXYSQL_UPGRADEfi 这里$1我不知道咋取的, 总之就是如果是写在就删了proxysql.service文件后systemctl daemon-reload 另外，为提供操作中可参考的信息，rpm还提供了一种信号机制：不同的操作会返回不同的信息，并放到默认变量$1中。 0代表卸载、1代表安装、2代表升级 如果是升级则会再启动 可以使用rpm -Uvh升级吗?ansible yum可以完成升级工作, 但不要使用ansible yum升级, 使用此方式升级2.0.8 - 2.0.12会自动安装并启动一个proxysql12proxysql 125223 1 0 21:17 ? 00:00:00 /usr/bin/proxysql --idle-threads -c /etc/proxysql.cnfproxysql 125225 125223 0 21:17 ? 00:00:00 /usr/bin/proxysql --idle-threads -c /etc/proxysql.cnf 这个proxysql会占用6032 和 6033 端口. 这种方式实际上应该是使用rpm -Uvh完成的升级, 我手动使用rpm -Uvh升级效果和ansible yum是一致的 应该通过shell rpm -e 2.0.8 再rpm -ivh 2.0.12进行升级, 这种方式不影响(影响rpm默认安装的proxysql实例,因为会关闭)已经运行的proxysql进程 (文件句柄没有释放)1234567[root@centos-1 data]# lsof | grep delete| grep proxysqlproxysql 118312 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)proxysql 118313 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)proxysql 118313 118315 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)proxysql 118313 118316 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)proxysql 118313 118317 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)proxysql 118313 118318 root txt REG 253,0 36156384 101426928 /usr/bin/proxysql (deleted)另外升级后请123mv /etc/proxysql.cnf /etc/proxysql.cnf.bakmv /etc/systemd/system/proxysql.service /etc/systemd/system/proxysql.service.bakmv /etc/systemd/system/proxysql-initial.service /etc/systemd/system/proxysql-initial.service.bak目前上海的proxysql我都是采用这种方式升级 结论检查是否有rpm默认安装的proxysql实例, 如果有, 请先升级这个proxysql实例!","categories":[],"tags":[{"name":"ProxySQL","slug":"ProxySQL","permalink":"http://fuxkdb.com/tags/ProxySQL/"},{"name":"升级","slug":"升级","permalink":"http://fuxkdb.com/tags/%E5%8D%87%E7%BA%A7/"}]},{"title":"MySQL动态行转列","slug":"2020-08-23-MySQL动态行转列","date":"2020-08-23T10:51:00.000Z","updated":"2020-08-23T10:51:27.868Z","comments":true,"path":"2020/08/23/2020-08-23-MySQL动态行转列/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-MySQL%E5%8A%A8%E6%80%81%E8%A1%8C%E8%BD%AC%E5%88%97/","excerpt":"通过max行转列不是动态的, 你还是要知道所有列名才可以12345678910111213141516171819root@localhost 17:46:56 [fanboshi]&gt; select * from t3;+----+---------+------+-----------+| id | title | env | progress |+----+---------+------+-----------+| 1 | 工单1 | 1 | 完成 || 2 | 工单1 | 2 | 完成 || 3 | 工单1 | 3 | 待审核 || 4 | 工单2 | 1 | 待审核 |+----+---------+------+-----------+4 rows in set (0.00 sec)root@localhost 17:48:20 [fanboshi]&gt; select title,max(if(env=1,progress,-1)) &#x27;RC&#x27;,max(if(env=2,progress,-1)) &#x27;Stage&#x27;,max(if(env=3,progress,-1)) &#x27;Prod&#x27; from t3 group by title;+---------+-----------+--------+-----------+| title | RC | Stage | Prod |+---------+-----------+--------+-----------+| 工单1 | 完成 | 完成 | 待审核 || 工单2 | 待审核 | -1 | -1 |+---------+-----------+--------+-----------+2 rows in set (0.00 sec)","text":"通过max行转列不是动态的, 你还是要知道所有列名才可以12345678910111213141516171819root@localhost 17:46:56 [fanboshi]&gt; select * from t3;+----+---------+------+-----------+| id | title | env | progress |+----+---------+------+-----------+| 1 | 工单1 | 1 | 完成 || 2 | 工单1 | 2 | 完成 || 3 | 工单1 | 3 | 待审核 || 4 | 工单2 | 1 | 待审核 |+----+---------+------+-----------+4 rows in set (0.00 sec)root@localhost 17:48:20 [fanboshi]&gt; select title,max(if(env=1,progress,-1)) &#x27;RC&#x27;,max(if(env=2,progress,-1)) &#x27;Stage&#x27;,max(if(env=3,progress,-1)) &#x27;Prod&#x27; from t3 group by title;+---------+-----------+--------+-----------+| title | RC | Stage | Prod |+---------+-----------+--------+-----------+| 工单1 | 完成 | 完成 | 待审核 || 工单2 | 待审核 | -1 | -1 |+---------+-----------+--------+-----------+2 rows in set (0.00 sec) oracle有pivot函数可以解决这个问题,MySQL并没有这样的函数, 一种退而求其次的办法是使用JSON_OBJECTAGG1234567&gt; select title,JSON_OBJECTAGG(env,progress) val from t3 group by title;+---------+--------------------------------------------------+| title | val |+---------+--------------------------------------------------+| 工单1 | &#123;&quot;1&quot;: &quot;完成&quot;, &quot;2&quot;: &quot;完成&quot;, &quot;3&quot;: &quot;待审核&quot;&#125; || 工单2 | &#123;&quot;1&quot;: &quot;待审核&quot;&#125; |+---------+--------------------------------------------------+ 然后再程序循环val中的key,value自己做处理 使用prepare statement可以实现, 本例需要构造一个env值与env名字的对应表123456789root@localhost 17:42:07 [fanboshi]&gt; select * from t_env;+----+--------+----------+| id | env_id | env_name |+----+--------+----------+| 1 | 1 | RC || 2 | 2 | Stage || 3 | 3 | Prod |+----+--------+----------+3 rows in set (0.00 sec) 然后 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647SELECT GROUP_CONCAT(DISTINCT CONCAT( &#x27;MAX(IF(pa.env = &#x27;&#x27;&#x27;, env, &#x27;&#x27;&#x27;, pa.progress, -1)) AS &#x27;, &quot;&#x27;&quot;,e.env_name,&quot;&#x27;&quot; ) ) INTO @sqlFROM t3,(select @sql:= NULL) tmp, t_env e where env = e.env_id;root@localhost 17:44:40 [fanboshi]&gt; select @sql;+---------------------------------------------------------------------------------------------------------------------------------------------------------+| @sql |+---------------------------------------------------------------------------------------------------------------------------------------------------------+| MAX(IF(pa.env = &#x27;1&#x27;, pa.progress, NULL)) AS &#x27;RC&#x27;,MAX(IF(pa.env = &#x27;2&#x27;, pa.progress, NULL)) AS &#x27;Stage&#x27;,MAX(IF(pa.env = &#x27;3&#x27;, pa.progress, NULL)) AS &#x27;Prod&#x27; |+---------------------------------------------------------------------------------------------------------------------------------------------------------+select @sql:=CONCAT(&#x27;SELECT pa.title , &#x27;, @sql, &#x27; FROM t3 pa GROUP BY pa.title&#x27;); +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| @sql:=CONCAT(&#x27;SELECT pa.title , &#x27;, @sql, &#x27; FROM t3 pa GROUP BY pa.title&#x27;) |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| SELECT pa.title , MAX(IF(pa.env = &#x27;1&#x27;, pa.progress, -1)) AS &#x27;RC&#x27;,MAX(IF(pa.env = &#x27;2&#x27;, pa.progress, -1)) AS &#x27;Stage&#x27;,MAX(IF(pa.env = &#x27;3&#x27;, pa.progress, -1)) AS &#x27;Prod&#x27; FROM t3 pa GROUP BY pa.title |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)其实这里已经把拼接好的sql查出来了, 程序直接拿着个结果去执行应该也是可以的, 可能不用后面在使用prepare statement了PREPARE stmt FROM @sql;EXECUTE stmt;+---------+-----------+--------+-----------+| title | RC | Stage | Prod |+---------+-----------+--------+-----------+| 工单1 | 完成 | 完成 | 待审核 || 工单2 | 待审核 | -1 | -1 |+---------+-----------+--------+-----------+DEALLOCATE PREPARE stmt; 那么基于上面的思路, 我寻思还用得着prepare statement吗, 直接拼个sql不就完事了吗, 用SQL造SQL1234567891011121314151617181920212223242526272829303132SELECT CONCAT(&#x27;SELECT pa.title, &#x27;, GROUP_CONCAT(DISTINCT CONCAT(&#x27;MAX(IF(pa.env = \\&#x27;&#x27;, env, &#x27;\\&#x27;, pa.progress, -1)) AS &#x27;, &#x27;\\&#x27;&#x27;, e.env_name, &#x27;\\&#x27;&#x27;)), &#x27; FROM t3 pa group by pa.title&#x27;) final_sqlFROM t3, t_env eWHERE env = e.env_id;+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| final_sql |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| SELECT pa.title, MAX(IF(pa.env = &#x27;1&#x27;, pa.progress, -1)) AS &#x27;RC&#x27;,MAX(IF(pa.env = &#x27;2&#x27;, pa.progress, -1)) AS &#x27;Stage&#x27;,MAX(IF(pa.env = &#x27;3&#x27;, pa.progress, -1)) AS &#x27;Prod&#x27; FROM t3 pa group by pa.title |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)&gt; SELECT pa.title, MAX(IF(pa.env = &#x27;1&#x27;, pa.progress, -1)) AS &#x27;RC&#x27;,MAX(IF(pa.env = &#x27;2&#x27;, pa.progress, -1)) AS &#x27;Stage&#x27;,MAX(IF(pa.env = &#x27;3&#x27;, pa.progress, -1)) AS &#x27;Prod&#x27; FROM t3 pa group by pa.title;+---------+-----------+--------+-----------+| title | RC | Stage | Prod |+---------+-----------+--------+-----------+| 工单1 | 完成 | 完成 | 待审核 || 工单2 | 待审核 | -1 | -1 |+---------+-----------+--------+-----------+2 rows in set (0.00 sec) 参考:https://stackoverflow.com/questions/12598120/mysql-pivot-table-query-with-dynamic-columns","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"死锁案例2 两个UPDATE死锁","slug":"2020-08-23-死锁案例2-两个UPDATE死锁","date":"2020-08-23T10:44:00.000Z","updated":"2020-08-23T10:45:15.485Z","comments":true,"path":"2020/08/23/2020-08-23-死锁案例2-两个UPDATE死锁/","link":"","permalink":"http://fuxkdb.com/2020/08/23/2020-08-23-%E6%AD%BB%E9%94%81%E6%A1%88%E4%BE%8B2-%E4%B8%A4%E4%B8%AAUPDATE%E6%AD%BB%E9%94%81/","excerpt":"问题描述2020.05.18 16:12 收到报警 登录主机, 查看error log( 开启了innodb_print_all_deadlocks参数) 123456789101112131415161718192021222324*** (1) TRANSACTION:TRANSACTION 1261729280, ACTIVE 0 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3645828, OS thread handle 139681739994880, query id 1522438991 172.16.23.82 yos_rw Searching rows for updateUPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 12020-05-18T16:11:11.696061+08:00 3056367 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1208 page no 29439 n bits 160 index PRIMARY of table `yos`.`f_log` trx id 1261729280 lock_mode X locks rec but not gap waiting2020-05-18T16:11:11.696079+08:00 3056367 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 1261729278, ACTIVE 0 sec updating or deletingmysql tables in use 1, locked 110 lock struct(s), heap size 1136, 7 row lock(s), undo log entries 3MySQL thread id 3056367, OS thread handle 139682403342080, query id 1522438990 172.16.23.82 yos_rw updatingUPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;)2020-05-18T16:11:11.696099+08:00 3056367 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 1208 page no 29439 n bits 160 index PRIMARY of table `yos`.`f_log` trx id 1261729278 lock_mode X locks rec but not gap2020-05-18T16:11:11.696112+08:00 3056367 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1208 page no 17 n bits 1120 index idx_status_type of table `yos`.`f_log` trx id 1261729278 lock_mode X locks rec but not gap waiting2020-05-18T16:11:11.696223+08:00 3056367 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (1) 简单解释一下上面的死锁日志","text":"问题描述2020.05.18 16:12 收到报警 登录主机, 查看error log( 开启了innodb_print_all_deadlocks参数) 123456789101112131415161718192021222324*** (1) TRANSACTION:TRANSACTION 1261729280, ACTIVE 0 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3645828, OS thread handle 139681739994880, query id 1522438991 172.16.23.82 yos_rw Searching rows for updateUPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 12020-05-18T16:11:11.696061+08:00 3056367 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1208 page no 29439 n bits 160 index PRIMARY of table `yos`.`f_log` trx id 1261729280 lock_mode X locks rec but not gap waiting2020-05-18T16:11:11.696079+08:00 3056367 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 1261729278, ACTIVE 0 sec updating or deletingmysql tables in use 1, locked 110 lock struct(s), heap size 1136, 7 row lock(s), undo log entries 3MySQL thread id 3056367, OS thread handle 139682403342080, query id 1522438990 172.16.23.82 yos_rw updatingUPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;)2020-05-18T16:11:11.696099+08:00 3056367 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 1208 page no 29439 n bits 160 index PRIMARY of table `yos`.`f_log` trx id 1261729278 lock_mode X locks rec but not gap2020-05-18T16:11:11.696112+08:00 3056367 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1208 page no 17 n bits 1120 index idx_status_type of table `yos`.`f_log` trx id 1261729278 lock_mode X locks rec but not gap waiting2020-05-18T16:11:11.696223+08:00 3056367 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (1) 简单解释一下上面的死锁日志 TRANSACTION 1261729280, MySQL thread id 3645828, 最后一条执行的sql为 1UPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 1 它在等待获取主键上的记录锁 1RECORD LOCKS index PRIMARY of table `yos`.`f_log` lock_mode X locks rec but not gap waiting TRANSACTION 1261729278, MySQL thread id 3056367, 最后一条执行的sql为 1UPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;) 持有主键记录锁 1RECORD LOCKS index PRIMARY of table `yos`.`f_log` lock_mode X locks rec but not gap 等待在二级索引idx_status_type上加记录锁 1RECORD LOCKS index idx_status_type of table `yos`.`f_log` lock_mode X locks rec but not gap waiting 表结构 1234567891011121314151617root@localhost 16:18:24 [yos]&gt; show create table f_log\\G*************************** 1. row *************************** Table: f_logCreate Table: CREATE TABLE `f_log` ( `I_ID` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;自增主键&#x27;, `I_CHANNEL_ID` bigint(20) NOT NULL COMMENT &#x27;通道ID&#x27;, `I_FILE_ID` varchar(100) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;文件ID&#x27;, `I_TYPE` tinyint(4) NOT NULL COMMENT &#x27;日志类型1 同步 2 删除&#x27;, `I_STATUS` tinyint(1) NOT NULL COMMENT &#x27;同步标志 0 未同步 1 同步中 2 已同步 3&#x27;, `D_CREATED_AT` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `D_UPDATED_AT` datetime DEFAULT NULL COMMENT &#x27;修改时间&#x27;, PRIMARY KEY (`I_ID`), KEY `idx_updatedat_status_type` (`D_UPDATED_AT`,`I_STATUS`,`I_TYPE`), KEY `idx_status_type` (`I_STATUS`,`I_TYPE`), KEY `idx_file_id` (`I_FILE_ID`)) ENGINE=InnoDB AUTO_INCREMENT=2011346 DEFAULT CHARSET=utf8 COMMENT=&#x27;文件日志表&#x27;1 row in set (0.00 sec) 事务隔离级别为READ-COMMITTED 死锁这块我比较弱, 其实厉害的话根据上面的信息已经可以看出原因了. 排查过程我当时去翻审计日志, 通过语句中的一些关键信息, 如 `thread id 3645828/3056367’ 时间范围, 找出了如下信息 TRANSACTION (2) 也就是 12345TRANSACTION 1261729278, ACTIVE 0 sec updating or deletingmysql tables in use 1, locked 110 lock struct(s), heap size 1136, 7 row lock(s), undo log entries 3MySQL thread id 3056367, OS thread handle 139682403342080, query id 1522438990 172.16.23.82 yos_rw updatingUPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;) 执行语句如下: 123456789START TRANSACTIONSELECT I_FILE_ID,I_FILE_COUNT,I_FILE_STATUS,I_IS_EXPIRED,I_FILE_VALIDDAY,I_FILE_ACCESS,CH_FILE_NAME,CH_FILE_TYPE,I_BUSINESS_ID,D_CREATED_AT,D_UPDATED_AT,I_YOS_VERSION,D_EXPIRED_AT FROM f_file_info WHERE I_FILE_ID = &#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27; FOR UPDATESELECT i_channel_id FROM f_file_storage WHERE i_file_id = &#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27; AND i_storage_status = 1UPDATE f_file_info SET i_file_count = 2, d_updated_at = &#x27;2020-05-18 16:11:11.687696&#x27; WHERE i_file_id = &#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;SELECT I_ID,I_FILE_ID,I_STORAGE_STATUS,I_CHANNEL_ID,D_CREATED_AT,D_UPDATED_AT FROM f_file_storage WHERE i_file_id = &#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27; AND i_channel_id = 1 FOR UPDATEUPDATE f_file_storage SET i_storage_status = 1 , d_updated_at = &#x27;2020-05-18 16:11:11.690879&#x27; WHERE i_file_id = &#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27; AND i_channel_id = 1SELECT count(*) FROM f_file_storage WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27; AND I_STORAGE_STATUS=1)UPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;)COMMIT TRANSACTION (1) 也就是 12345TRANSACTION 1261729280, ACTIVE 0 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 3645828, OS thread handle 139681739994880, query id 1522438991 172.16.23.82 yos_rw Searching rows for updateUPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 1 执行语句如下: 12PrepareUPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 1 到这里想不出原因了, 因为两个会话实际上就是update语句死锁了. 我但是没想明白为啥UPDATE f_log SET D_UPDATED_AT=&#39;2020-05-18 16:11:11.693998&#39;,I_STATUS=3 WHERE (I_FILE_ID=&#39;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#39;) 要请求在二级索引idx_status_type上加记录锁 后来还是和研发讨论, 研发提醒了我update要锁被修改列二级索引. 我一开始还说不对… 后来想想有道理 https://www.aneasystone.com/archives/2017/12/solving-dead-locks-three.html 那么到这里就说通了, 这就是为啥UPDATE f_log SET D_UPDATED_AT=&#39;2020-05-18 16:11:11.693998&#39;,I_STATUS=3 WHERE (I_FILE_ID=&#39;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#39;) 要请求在二级索引idx_status_type上加记录锁 那么实际这个死锁是这样产生的 事务1,2 开始时 I_FILE_ID=’92b52d6589ce11a9a3c985e7c5f37462efb66ac7’ 对应记录的 I_STATUS = 1 , I_TYPE = 1 事务2执行语句 1UPDATE f_log SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;92b52d6589ce11a9a3c985e7c5f37462efb66ac7&#x27;) 走I_FILE_ID列二级索引idx_file_id, 根据二级索引中的主键值找到主键索引对应记录加记录锁, 因为是update语句更新了I_STATUS列, 且I_STATUS列有二级索引idx_status_type所以还要去修改二级索引中对应记录, 于是也要锁idx_status_type中的记录I_STATUS = 1 , I_TYPE = 1, I_ID=2010712(原值)和I_STATUS = 3 , I_TYPE = 1, I_ID=2010712(新值). 但此时事务2还未在idx_status_type对应记录加上锁, 事务1执行了语句 1UPDATE f_log SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 1 这个sql走二级索引idx_status_type, 于是先对二级索引idx_status_type对应记录I_STATUS = 1 , I_TYPE = 1, I_ID=2010712加锁, 加锁成功, 然后需要根据二级索引idx_status_type中记录的主键值I_ID=2010712去主键加锁, 但此锁已经被事务2持有 事务2持有主键锁, 等待在二级索引idx_status_type加记录锁 事务1持有idx_status_type记录锁, 等待在主键加记录锁 至此陷入死锁. 问题复现这个死锁可以说是很巧合了, 手工无法模拟 我是这样的模拟的, 开三个窗口, 分别执行 1while true;do mysql -uroot -p&#x27;123.com&#x27; -S /data/mysql_3306/run/mysql.sock fanboshi -e&quot;begin;UPDATE f_log_2 SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;250181420422912&#x27;);commit;&quot;;done 1while true;do mysql -uroot -p&#x27;123.com&#x27; -S /data/mysql_3306/run/mysql.sock fanboshi -e&quot;UPDATE f_log_2 SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 1&quot;;done 1while true;do mysql -uroot -p&#x27;123.com&#x27; -S /data/mysql_3306/run/mysql.sock fanboshi -e&quot;UPDATE f_log_2 SET I_STATUS = 1, I_TYPE=1 WHERE I_ID=9656 &quot;;done 第三个窗口目的只是将I_STATUS和I_TYPE列值更新回去, 制造条件, 不然前两个窗口语句不满足条件更新不到记录 复现出的死锁日志如下, 开起了innodb_show_verbose_locks参数: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061622020-05-19T14:15:29.074829+08:00 11644 [Note] InnoDB: Transactions deadlock detected, dumping detailed information.2020-05-19T14:15:29.074841+08:00 11644 [Note] InnoDB: *** (1) TRANSACTION:TRANSACTION 19924, ACTIVE 0 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 11643, OS thread handle 140502089639680, query id 40057 localhost root Searching rows for updateUPDATE f_log_2 SET D_UPDATED_AT = NOW() , I_STATUS = 17 WHERE I_STATUS = 1 AND I_TYPE = 12020-05-19T14:15:29.074858+08:00 11644 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED: --等待获取如下锁RECORD LOCKS space id 137 page no 136 n bits 312 index PRIMARY of table `fanboshi`.`f_log_2` trx id 19924 lock_mode X locks rec but not gap waiting --主键记录锁Record lock, heap no 217 PHYSICAL RECORD: n_fields 9; compact format; info bits 0 0: len 8; hex 80000000000025b8; asc % ;; 第一列, 根据表结构知道这列是主键, 16进制转10进制 9656 1: len 6; hex 000000004dd5; asc M ;; --该字段为6个字节的事务id，这个id表示最近一次被更新的事务id 000000004dd5 16进制转10进制结果为19925正是下面事务2的的事务号&quot;TRANSACTION 19925&quot; 2: len 7; hex 34000000051016; asc 4 ;; --该字段为7个字节的回滚指针，用于mvcc 3: len 8; hex 8000000000000001; asc ;; I_CHANNEL_ID = 1 4: len 15; hex 323530313831343230343232393132; asc 250181420422912;; I_FILE_ID 这一列是varchar类型. 0x32 0x35 0x30 0x31 0x38 0x31 0x34 0x32 0x30 0x34 0x32 0x32 0x39 0x31 0x32 转过来就是250181420422912 5: len 1; hex 81; asc ;; I_TYPE=1 6: len 1; hex 83; asc ;; I_STATUS=3 7: len 5; hex 99a2d8fd14; asc ;; D_CREATED_AT 不知道怎么转换 8: len 5; hex 99a66502cc; asc e ;; D_UPDATED_AT 不知道怎么转换到这里已经能定位是哪一行了mysql&gt; select * from f_log_2 WHERE I_ID=9656;+------+--------------+-----------------+--------+----------+---------------------+---------------------+| I_ID | I_CHANNEL_ID | I_FILE_ID | I_TYPE | I_STATUS | D_CREATED_AT | D_UPDATED_AT |+------+--------------+-----------------+--------+----------+---------------------+---------------------+| 9656 | 1 | 250181420422912 | 1 | 1 | 2019-04-12 15:52:20 | 2020-05-18 16:11:12 |+------+--------------+-----------------+--------+----------+---------------------+---------------------+1 row in set (0.00 sec)2020-05-19T14:15:29.074996+08:00 11644 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 19925, ACTIVE 0 sec updating or deletingmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 1MySQL thread id 11644, OS thread handle 140502025062144, query id 40060 localhost root updatingUPDATE f_log_2 SET D_UPDATED_AT=&#x27;2020-05-18 16:11:11.693998&#x27;,I_STATUS=3 WHERE (I_FILE_ID=&#x27;250181420422912&#x27;)2020-05-19T14:15:29.075010+08:00 11644 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 137 page no 136 n bits 312 index PRIMARY of table `fanboshi`.`f_log_2` trx id 19925 lock_mode X locks rec but not gap --持有主键记录锁Record lock, heap no 217 PHYSICAL RECORD: n_fields 9; compact format; info bits 0 0: len 8; hex 80000000000025b8; asc % ;; 1: len 6; hex 000000004dd5; asc M ;; 2: len 7; hex 34000000051016; asc 4 ;; 3: len 8; hex 8000000000000001; asc ;; 4: len 15; hex 323530313831343230343232393132; asc 250181420422912;; 5: len 1; hex 81; asc ;; 6: len 1; hex 83; asc ;; 7: len 5; hex 99a2d8fd14; asc ;; 8: len 5; hex 99a66502cc; asc e ;;持有TRANSACTION (1)等待获取的哪一行主键锁2020-05-19T14:15:29.075141+08:00 11644 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 137 page no 17 n bits 1120 index idx_status_type of table `fanboshi`.`f_log_2` trx id 19925 lock_mode X locks rec but not gap waiting --等待二级索引列加锁Record lock, heap no 526 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 1; hex 81; asc ;; I_STATUS = 1 1: len 1; hex 81; asc ;; I_TYPE = 1 2: len 8; hex 80000000000025b8; asc % ;; 这是主键值 12020-05-19T14:15:29.075197+08:00 11644 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (1) 可以看到和线上出现的死锁日志是吻合的 解决办法将事务1语句改为先根据条件查询出主键值, 然后根据主键值更新.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"死锁","slug":"死锁","permalink":"http://fuxkdb.com/tags/%E6%AD%BB%E9%94%81/"}]},{"title":"MGR单主到底做不做冲突检测?","slug":"2020-06-18-MGR单主到底做不做冲突检测","date":"2020-06-18T14:23:00.000Z","updated":"2020-06-18T14:34:48.314Z","comments":true,"path":"2020/06/18/2020-06-18-MGR单主到底做不做冲突检测/","link":"","permalink":"http://fuxkdb.com/2020/06/18/2020-06-18-MGR%E5%8D%95%E4%B8%BB%E5%88%B0%E5%BA%95%E5%81%9A%E4%B8%8D%E5%81%9A%E5%86%B2%E7%AA%81%E6%A3%80%E6%B5%8B/","excerpt":"和同事探讨一个问题, MGR单主做不做冲突检测. 我理解是不需要做的, 因为已经明确只有主节点才能写入数据了, 那么必然不会有数据冲突的可能, 没必要再做冲突检测浪费性能了. 看了下官方文档 1In single-primary mode, Group Replication enforces that only a single server writes to the group, so compared to multi-primary mode, consistency checking can be less strict and DDL statements do not need to be handled with any extra care","text":"和同事探讨一个问题, MGR单主做不做冲突检测. 我理解是不需要做的, 因为已经明确只有主节点才能写入数据了, 那么必然不会有数据冲突的可能, 没必要再做冲突检测浪费性能了. 看了下官方文档 1In single-primary mode, Group Replication enforces that only a single server writes to the group, so compared to multi-primary mode, consistency checking can be less strict and DDL statements do not need to be handled with any extra care 这里less strict让人很迷惑, 意思是还有冲突检测呗, 但是和多主区别是啥没说 之前看MGR的时候看过网易温正湖的文章, 路上搜了下, 发现两个文章: MySQL MGR事务认证机制优化 MySQL事务在MGR中的漫游记 - 事务认证 其实他文章哪些源码我也看不懂. 我是不想别人说啥我就信啥, 所以想找到知识源头 到家我搜了下conflict_detection_enable 搜到这个网站https://s0dev0mysql0com.icopy.site/doc/dev/mysql-server/latest/classCertifier.html 那么这个网站源头又是啥呢, 又搜了下 https://dev.mysql.com/doc/dev/mysql-server/latest/classCertifier.html这里面说的就很清楚了 就是说单主, 主库挂了, 新主库应用原主库事务的时候才做冲突检测 想起以前做过实验压测2 5.7MGR是否要应用完所有binlog才会选举出新主 实际上5.7官方文档有描述 单主模式下: 当选择一个新的主数据库时，它只有在处理完所有来自旧主数据库的事务后才可写。 这样可以避免旧的主事务中的旧事务与在该成员上执行的新事务之间可能发生的并发问题。 在新的主数据库重新路由客户端应用程序之前，最好等待新的主数据库应用其复制相关的中继日志。 When a new primary is elected, it is only writable once it has processed all of the transactions that came from the old primary. This avoids possible concurrency issues between old transactions from the old primary and the new ones being executed on this member. It is a good practice to wait for the new primary to apply its replication related relay-log before re-routing client applications to it. https://dev.mysql.com/doc/refman/5.7/en/group-replication-single-primary-mode.html 在8.0文档中是这样写的 选举或任命新的主库时，可能会积压已应用于旧的主库但尚未在此服务器上应用的更改。 在这种情况下，直到新的主数据库赶上旧的主数据库，读写事务可能会导致冲突并回滚，而只读事务可能会导致陈旧的读取。 When a new primary is elected or appointed, it might have a backlog of changes that had been applied on the old primary but have not yet been applied on this server. In this situation, until the new primary catches up with the old primary, read-write transactions might result in conflicts and be rolled back, and read-only transactions might result in stale reads. https://dev.mysql.com/doc/refman/8.0/en/group-replication-single-primary-mode.html 这其实很合理, 假设一个单主模式MGR集群 三个节点A, B, C A是主库, app向T1表插入三条数据, 主键值分别为 1,2,3 B,C收到binlog event, 但还未应用, 此时A宕机, B当选为新主库, 那么B需要应用在A产生的三个插入1,2,3. 如果没有冲突检测, 在B应用1,2,3前,业务有插入了新数据1,2,3, 那么就明显有问题, 所以此阶段一定要做冲突检测. 仔细看感觉5.7和8.0的描述有了”很大区别” 5.7说 When a new primary is elected, it is only writable once it has processed all of the transactions that came from the old primary. 新主必须应用完原主所有事物才可写 8.0说In this situation, until the new primary catches up with the old primary, read-write transactions might result in conflicts and be rolled back, and read-only transactions might result in stale reads. 在新主应用完原主所有事物前, 写可能会冲突回滚, 而读可能会读到旧数据 我猜测这是说5.7单主是彻底关闭了冲突检测, 新主应用原主完原主事务前是不可写的, 通过不可写避免了冲突, 还需要继续做实验测试, 新主应用原主完原主事务前, 是否可以执行不冲突的事务(比如我们像T1表写大量数据制造transactions_behind, 新主当选后, 我们想T2表写数据, 这明显是不冲突的.) 那么看来8.0比5.7有了改进, 在新主应用原主完原主事务期间开启冲突检测, 那么按照上面的实验例子, 业务就可以执行”不冲突的事务了” 但是是否这样对业务来说是可接受的呢? 也许业务希望新主应用原主完原主事务后才可写是合理的, 所以有了下面的参数 8.0.14后增加了参数group_replication_consistency, 从根本上解决了读旧数据的问题(写操作无需设置参数也会等待应用完所有backlog才可以执行) 123BEFORE_ON_PRIMARY_FAILOVERNew RO or RW transactions with a newly elected primary that is applying backlog from the old primary are held (not applied) until any backlog has been applied. This ensures that when a primary failover happens, intentionally or not, clients always see the latest value on the primary. This guarantees consistency, but means that clients must be able to handle the delay in the event that a backlog is being applied. Usually this delay should be minimal, but does depend on the size of the backlog. 在发生切换时，连到新主的事务会被阻塞，等待先序提交的事务回放完成；这样确保在故障切换时客户端都能读取到主服务器上的最新数据，保证了一致性 上面的描述不严谨, 出自知数堂田鹏的文章MySQL MGR”一致性读写”特性解读. 事实上只读事务也会等待, 除了以下只读事务(参考此译文https://cloud.tencent.com/developer/article/1478455) SHOW commands SET option DO EMPTY USE SELECTing from performance_schema database SELECTing from table PROCESSLIST on database infoschema SELECTing from sys database SELECT command that don’t use tables SELECT command that don’t execute user defined functions STOP GROUP_REPLICATION command SHUTDOWN command RESET PERSIST 个人理解如果设置BEFORE_ON_PRIMARY_FAILOVER虽然会保证一致性, 如果节点新主与原主延迟过大, 新主应用差异日志时间过长, 那么会导致大量连接进来处于等待状态, 导致Threads_running暴涨, 甚至连接数打满新主崩溃 至8.0.18MGR选主算法是 1.选版本最小的 2.选权重最大的 3.选uuid排序最小的 所以并没有判断哪个节点延迟最小 https://dev.mysql.com/doc/refman/8.0/en/group-replication-single-primary-mode.html 那么多主如何处理? 从参数说明上来看BEFORE_ON_PRIMARY_FAILOVER应该只是针对单主, 所以除非应用显示指定了group_replication_consistency, 否则多主还是会读到旧数据. 对于写入, 因为多主是要做冲突检测的, 所以我们假设一个场景 多主MGR, N1,N2,N3, 单写N1, T1(id int primary key, sname varchar(10))表插入 123451,fan GTID: GROUP_UUID:12,bo GTID: GROUP_UUID:23,shi GTID: GROUP_UUID:3目前GTID: GROUP_UUID:1-3 N1宕机, N2应用到1, 未执行2,3. 此时client像N2插入(2,hehe) 那么此时N2这个插入版本是GROUP_UUID:1 而冲突检测数据库中id=2这一行的版本是GROUP_UUID:1-2, 所以这条插入会冲突检测失败回滚掉 以上是我的理解, 目前没有测试","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"}]},{"title":"译文 New Feature in Percona XtraDB Cluster 8.0 – Streaming Replication","slug":"2020-05-16-译文-New-Feature-in-Percona-XtraDB-Cluster-8.0-–-Streaming-Replication","date":"2020-05-16T14:56:00.000Z","updated":"2020-05-17T03:10:38.334Z","comments":true,"path":"2020/05/16/2020-05-16-译文-New-Feature-in-Percona-XtraDB-Cluster-8.0-–-Streaming-Replication/","link":"","permalink":"http://fuxkdb.com/2020/05/16/2020-05-16-%E8%AF%91%E6%96%87-New-Feature-in-Percona-XtraDB-Cluster-8.0-%E2%80%93-Streaming-Replication/","excerpt":"New Feature in Percona XtraDB Cluster 8.0 – Streaming ReplicationPercona XtraDB Cluster 8.0附带了一个升级的Galera 4.0库, 它提供了一个新特性—Streaming Replication.让我们回顾一下它是什么, 什么时候可能有用 以前版本的Percona XtraDB集群和Galera 3.x在处理大事务方面有限制. 让我们看一下sysbench-tpcc工作负载下的性能, 与此同时, 我们对一个表执行了一个大的更新语句, 该更新甚至与主工作负载中的表都不相关.","text":"New Feature in Percona XtraDB Cluster 8.0 – Streaming ReplicationPercona XtraDB Cluster 8.0附带了一个升级的Galera 4.0库, 它提供了一个新特性—Streaming Replication.让我们回顾一下它是什么, 什么时候可能有用 以前版本的Percona XtraDB集群和Galera 3.x在处理大事务方面有限制. 让我们看一下sysbench-tpcc工作负载下的性能, 与此同时, 我们对一个表执行了一个大的更新语句, 该更新甚至与主工作负载中的表都不相关. Without Streaming ReplicationLet’s run two workloads. sysbench-tpcc workload with 1 sec resolution In parallel run UPDATE oltp.sbtest SET k=k+1 LIMIT 1000000 Running update: 123mysql&gt; update sbtest1 set k=k+1 limit 1000000;Query OK, 1000000 rows affected (34.48 sec)Rows matched: 1000000 Changed: 1000000 Warnings: 0 Check what is happening in sysbench-tpcc: 123456789101112131415161718192021222324252627[ 77s ] thds: 100 tps: 7011.97 qps: 198248.21 (r/w/o: 90469.64/93758.63/14019.94) lat (ms,95%): 25.28 err/s 31.00 reconn/s: 0.00[ 78s ] thds: 100 tps: 6779.94 qps: 196129.34 (r/w/o: 89462.24/93103.21/13563.88) lat (ms,95%): 26.20 err/s 30.00 reconn/s: 0.00[ 79s ] thds: 100 tps: 6948.01 qps: 199157.35 (r/w/o: 90878.16/94383.16/13896.02) lat (ms,95%): 26.20 err/s 28.00 reconn/s: 0.00[ 80s ] thds: 100 tps: 3920.09 qps: 113882.48 (r/w/o: 51940.13/54102.18/7840.17) lat (ms,95%): 27.17 err/s 15.00 reconn/s: 0.00[ 81s ] thds: 100 tps: 67.00 qps: 1956.02 (r/w/o: 899.01/923.01/134.00) lat (ms,95%): 623.33 err/s 0.00 reconn/s: 0.00[ 82s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 83s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 84s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 85s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 86s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 87s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 88s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 89s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 90s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 91s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 92s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 93s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 94s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 95s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 96s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 97s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 98s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 99s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 100s ] thds: 100 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00[ 101s ] thds: 100 tps: 3501.85 qps: 99695.66 (r/w/o: 45473.02/47218.94/7003.70) lat (ms,95%): 257.95 err/s 14.00 reconn/s: 0.00[ 102s ] thds: 100 tps: 6980.06 qps: 197777.73 (r/w/o: 90228.79/93588.82/13960.12) lat (ms,95%): 25.74 err/s 33.00 reconn/s: 0.00[ 103s ] thds: 100 tps: 6745.15 qps: 196518.25 (r/w/o: 89717.94/93310.02/13490.29) lat (ms,95%): 26.68 err/s 46.00 reconn/s: 0.00 更新本身花了34秒. 在这种情况下, 主工作负载停止了22秒. 基本上所有语句在这段时间都被暂停/阻塞了 With Streaming Replication如何通过streaming replication改进这一点? 让我们在执行更新语句的会话中启用streaming replication 12SET SESSION wsrep_trx_fragment_unit=&#x27;rows&#x27;;SET SESSION wsrep_trx_fragment_size=1000; 基本上, 我们说集群应该将大事务分割成多个块, 每个块1000行, 然后在这些较小的块中进行复制.wsrep_trx_fragment_unit还可以设置为 ‘rows’ , ‘bytes’ 或 ‘statements’, 就是可以按行数, binlog字节数, 语句数量进行控制 1&gt;wsrep_trx_fragment_unit Defines the replication unit type to use in Streaming Replication. Command-line Format --wsrep-trx-fragment-unit System Variable wsrep_trx_fragment_unit Variable Scope Session Dynamic Variable Yes Permitted Values String Default Value bytes Valid Values bytes, events, rows, statements Initial Version Version 4.0 In Streaming Replication, the node breaks transactions down into fragments, then replicates and certifies them while the transaction is in progress. Once certified, a fragment can no longer be aborted due to conflicting transactions. This parameter determines the unit to use in determining the size of the fragment. To define the number of replication units to use in the fragment, use wsrep_trx_fragment_size. Supported replication units are: bytes: Refers to the fragment size in bytes. events: Refers to the number of binary log events in the fragment. rows: Refers to the number of rows updated in the fragment. statements: Refers to the number of SQL statements in the fragment. And run the query: 123mysql&gt; update sbtest1 set k=k+1 limit 1000000;Query OK, 1000000 rows affected (39.76 sec)Rows matched: 1000000 Changed: 1000000 Warnings: 0 In sysbench-tpcc: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[ 81s ] thds: 100 tps: 6682.94 qps: 188552.70 (r/w/o: 85967.65/89221.16/13363.88) lat (ms,95%): 26.68 err/s 32.98 reconn/s: 0.00[ 82s ] thds: 100 tps: 6700.92 qps: 192216.77 (r/w/o: 87715.23/91103.70/13397.84) lat (ms,95%): 27.17 err/s 27.01 reconn/s: 0.00[ 83s ] thds: 100 tps: 3835.05 qps: 108387.43 (r/w/o: 49408.65/51302.68/7676.10) lat (ms,95%): 82.96 err/s 15.00 reconn/s: 0.00[ 84s ] thds: 100 tps: 2210.13 qps: 63161.58 (r/w/o: 28852.64/29888.70/4420.25) lat (ms,95%): 95.81 err/s 9.00 reconn/s: 0.00[ 85s ] thds: 100 tps: 2558.00 qps: 72592.08 (r/w/o: 33093.04/34383.04/5116.01) lat (ms,95%): 87.56 err/s 9.00 reconn/s: 0.00[ 86s ] thds: 100 tps: 2617.99 qps: 75127.81 (r/w/o: 34299.91/35591.91/5235.99) lat (ms,95%): 78.60 err/s 9.00 reconn/s: 0.00[ 87s ] thds: 100 tps: 2887.75 qps: 81760.97 (r/w/o: 37312.79/38672.68/5775.50) lat (ms,95%): 73.13 err/s 15.00 reconn/s: 0.00[ 88s ] thds: 100 tps: 3024.00 qps: 84461.96 (r/w/o: 38606.98/39806.98/6048.00) lat (ms,95%): 69.29 err/s 15.00 reconn/s: 0.00[ 89s ] thds: 100 tps: 3119.27 qps: 91128.99 (r/w/o: 41566.65/43323.80/6238.55) lat (ms,95%): 63.32 err/s 9.00 reconn/s: 0.00[ 90s ] thds: 100 tps: 3385.74 qps: 98314.42 (r/w/o: 44883.54/46659.40/6771.48) lat (ms,95%): 56.84 err/s 14.00 reconn/s: 0.00[ 91s ] thds: 100 tps: 3641.08 qps: 103916.20 (r/w/o: 47422.00/49212.04/7282.15) lat (ms,95%): 54.83 err/s 21.00 reconn/s: 0.00[ 92s ] thds: 100 tps: 3850.12 qps: 106013.43 (r/w/o: 48296.56/50021.62/7695.25) lat (ms,95%): 57.87 err/s 23.00 reconn/s: 0.00[ 93s ] thds: 100 tps: 3828.07 qps: 111682.90 (r/w/o: 51005.87/53015.90/7661.13) lat (ms,95%): 54.83 err/s 22.00 reconn/s: 0.00[ 94s ] thds: 100 tps: 4358.95 qps: 122173.63 (r/w/o: 55746.37/57709.35/8717.90) lat (ms,95%): 42.61 err/s 14.00 reconn/s: 0.00[ 95s ] thds: 100 tps: 4367.09 qps: 123297.63 (r/w/o: 56193.20/58370.24/8734.19) lat (ms,95%): 44.98 err/s 16.00 reconn/s: 0.00[ 96s ] thds: 100 tps: 4272.92 qps: 118822.67 (r/w/o: 54076.94/56201.90/8543.83) lat (ms,95%): 46.63 err/s 24.00 reconn/s: 0.00[ 97s ] thds: 100 tps: 4697.88 qps: 133071.68 (r/w/o: 60676.49/62997.43/9397.77) lat (ms,95%): 38.25 err/s 17.00 reconn/s: 0.00[ 98s ] thds: 100 tps: 4742.21 qps: 135167.87 (r/w/o: 61693.68/63989.78/9484.41) lat (ms,95%): 37.56 err/s 21.00 reconn/s: 0.00[ 99s ] thds: 100 tps: 4949.89 qps: 139343.00 (r/w/o: 63616.63/65826.58/9899.79) lat (ms,95%): 36.24 err/s 21.00 reconn/s: 0.00[ 100s ] thds: 100 tps: 4766.10 qps: 139554.99 (r/w/o: 63695.37/66327.42/9532.20) lat (ms,95%): 36.89 err/s 18.00 reconn/s: 0.00[ 101s ] thds: 100 tps: 5069.91 qps: 143318.44 (r/w/o: 65310.83/67867.79/10139.82) lat (ms,95%): 35.59 err/s 13.00 reconn/s: 0.00[ 102s ] thds: 100 tps: 4947.06 qps: 140053.63 (r/w/o: 63820.74/66338.77/9894.12) lat (ms,95%): 36.24 err/s 23.00 reconn/s: 0.00[ 103s ] thds: 100 tps: 5045.00 qps: 145397.93 (r/w/o: 66328.97/68978.97/10090.00) lat (ms,95%): 34.33 err/s 18.00 reconn/s: 0.00[ 104s ] thds: 100 tps: 5139.02 qps: 141954.54 (r/w/o: 64723.25/66953.25/10278.04) lat (ms,95%): 36.24 err/s 28.00 reconn/s: 0.00[ 105s ] thds: 100 tps: 5214.90 qps: 147582.10 (r/w/o: 67371.68/69780.63/10429.80) lat (ms,95%): 34.33 err/s 25.00 reconn/s: 0.00[ 106s ] thds: 100 tps: 4924.08 qps: 139603.33 (r/w/o: 63673.06/66082.10/9848.16) lat (ms,95%): 36.24 err/s 23.00 reconn/s: 0.00[ 107s ] thds: 100 tps: 5202.97 qps: 147199.09 (r/w/o: 67176.58/69616.57/10405.94) lat (ms,95%): 34.33 err/s 30.00 reconn/s: 0.00[ 108s ] thds: 100 tps: 5219.91 qps: 147677.47 (r/w/o: 67416.84/69820.80/10439.82) lat (ms,95%): 33.72 err/s 28.00 reconn/s: 0.00[ 109s ] thds: 100 tps: 5018.99 qps: 143211.61 (r/w/o: 65365.82/67808.81/10036.97) lat (ms,95%): 36.24 err/s 23.00 reconn/s: 0.00[ 110s ] thds: 100 tps: 5070.16 qps: 142049.54 (r/w/o: 64817.07/67091.15/10141.32) lat (ms,95%): 34.95 err/s 17.00 reconn/s: 0.00[ 111s ] thds: 100 tps: 4954.87 qps: 141476.26 (r/w/o: 64529.29/67037.23/9909.74) lat (ms,95%): 35.59 err/s 25.00 reconn/s: 0.00[ 112s ] thds: 100 tps: 4827.12 qps: 140426.46 (r/w/o: 64103.58/66668.64/9654.24) lat (ms,95%): 35.59 err/s 19.00 reconn/s: 0.00[ 113s ] thds: 100 tps: 5027.00 qps: 145229.08 (r/w/o: 66179.04/68996.04/10054.01) lat (ms,95%): 34.33 err/s 26.00 reconn/s: 0.00[ 114s ] thds: 100 tps: 5099.87 qps: 144585.36 (r/w/o: 65976.34/68409.28/10199.74) lat (ms,95%): 34.33 err/s 26.00 reconn/s: 0.00[ 115s ] thds: 100 tps: 5010.11 qps: 143316.08 (r/w/o: 65356.40/67939.46/10020.22) lat (ms,95%): 34.95 err/s 26.00 reconn/s: 0.00[ 116s ] thds: 100 tps: 5056.00 qps: 143686.98 (r/w/o: 65621.99/67952.99/10112.00) lat (ms,95%): 34.95 err/s 31.00 reconn/s: 0.00[ 117s ] thds: 100 tps: 4908.95 qps: 141669.49 (r/w/o: 64653.31/67198.28/9817.90) lat (ms,95%): 36.24 err/s 21.00 reconn/s: 0.00[ 118s ] thds: 100 tps: 5039.07 qps: 142667.01 (r/w/o: 65056.92/67531.95/10078.14) lat (ms,95%): 34.33 err/s 24.00 reconn/s: 0.00[ 119s ] thds: 100 tps: 5076.89 qps: 143205.79 (r/w/o: 65195.54/67856.48/10153.77) lat (ms,95%): 35.59 err/s 18.00 reconn/s: 0.00[ 120s ] thds: 100 tps: 4909.09 qps: 137380.48 (r/w/o: 62539.13/65024.17/9817.18) lat (ms,95%): 34.95 err/s 13.00 reconn/s: 0.00[ 121s ] thds: 100 tps: 5024.93 qps: 144610.91 (r/w/o: 66027.05/68533.01/10050.85) lat (ms,95%): 35.59 err/s 23.00 reconn/s: 0.00[ 122s ] thds: 100 tps: 4874.10 qps: 138066.96 (r/w/o: 62942.35/65376.40/9748.21) lat (ms,95%): 35.59 err/s 16.00 reconn/s: 0.00[ 123s ] thds: 100 tps: 6745.83 qps: 187288.34 (r/w/o: 85354.88/88441.80/13491.66) lat (ms,95%): 28.67 err/s 27.00 reconn/s: 0.00[ 124s ] thds: 100 tps: 6132.03 qps: 172854.73 (r/w/o: 78867.33/81723.34/12264.05) lat (ms,95%): 29.19 err/s 18.00 reconn/s: 0.00[ 125s ] thds: 100 tps: 6114.99 qps: 175777.68 (r/w/o: 80098.85/83448.85/12229.98) lat (ms,95%): 30.26 err/s 39.00 reconn/s: 0.00[ 126s ] thds: 100 tps: 6206.87 qps: 179830.22 (r/w/o: 82043.28/85374.21/12412.74) lat (ms,95%): 29.72 err/s 29.00 reconn/s: 0.00[ 127s ] thds: 100 tps: 6441.25 qps: 181759.03 (r/w/o: 82799.20/86076.33/12883.50) lat (ms,95%): 28.67 err/s 28.00 reconn/s: 0.00[ 128s ] thds: 100 tps: 5925.87 qps: 169978.30 (r/w/o: 77565.31/80561.24/11851.74) lat (ms,95%): 30.81 err/s 32.00 reconn/s: 0.00[ 129s ] thds: 100 tps: 6614.92 qps: 186834.71 (r/w/o: 85216.96/88388.92/13228.84) lat (ms,95%): 27.66 err/s 24.00 reconn/s: 0.00 so 这里发生了什么: 更新查询花了更长的时间(39秒而不是34秒).主要工作负载也受到了一些影响(从6700 tps下降到最坏时期的2210 tps), 但并没有完全停止, 这是一个巨大的改进. 为什么我们不应该默认为所有事务启用streaming replication?原因是它可能会对常规的小事务产生负面影响, 所以建议只对大型或长时间运行的事务使用streaming replication.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"},{"name":"MySQL 8.0","slug":"MySQL-8-0","permalink":"http://fuxkdb.com/tags/MySQL-8-0/"},{"name":"Galera 4.0","slug":"Galera-4-0","permalink":"http://fuxkdb.com/tags/Galera-4-0/"}]},{"title":"译文 Galera 4 Streaming Replication in Percona XtraDB Cluster 8.0","slug":"2020-05-16-[译文]Galera-4-Streaming-Replication-in-Percona-XtraDB-Cluster-8.0","date":"2020-05-16T14:40:00.000Z","updated":"2020-05-16T15:06:34.937Z","comments":true,"path":"2020/05/16/2020-05-16-[译文]Galera-4-Streaming-Replication-in-Percona-XtraDB-Cluster-8.0/","link":"","permalink":"http://fuxkdb.com/2020/05/16/2020-05-16-[%E8%AF%91%E6%96%87]Galera-4-Streaming-Replication-in-Percona-XtraDB-Cluster-8.0/","excerpt":"Galera 4 Streaming Replication in Percona XtraDB Cluster 8.0我正在测试最新的Percona XtraDB Cluster 8.0 (PXC)版本, 其中包含了Galera 4插件, 我想分享一下到目前为止我在Streaming Replication特性方面的经验和想法, What Is Streaming Replication, in One Sentence?在Galera 4中, 大型事务可以分割成更小的片段, 甚至在提交之前, 这些片段就已经被复制到其他节点, 并且已经开始了认证(certification)和应用(apply)过程. 手册描述了所有的优点和缺点, 但是让我们看看它是如何工作的. 我已经创建了一个包含1千万行的表, 我将在这个表上运行一些大的update语句, 首先, 我在不使用Streaming Replication的情况下运行更新, 由于默认情况下Streaming Replication是disabled, 所以我们不需要做任何事情, 只需运行更新即可. 在node1上, 我记录更新之前和之后的时间, 在node2上, 我每秒钟运行一次select, 以查看这个更新在其他节点上实际提交的时间.","text":"Galera 4 Streaming Replication in Percona XtraDB Cluster 8.0我正在测试最新的Percona XtraDB Cluster 8.0 (PXC)版本, 其中包含了Galera 4插件, 我想分享一下到目前为止我在Streaming Replication特性方面的经验和想法, What Is Streaming Replication, in One Sentence?在Galera 4中, 大型事务可以分割成更小的片段, 甚至在提交之前, 这些片段就已经被复制到其他节点, 并且已经开始了认证(certification)和应用(apply)过程. 手册描述了所有的优点和缺点, 但是让我们看看它是如何工作的. 我已经创建了一个包含1千万行的表, 我将在这个表上运行一些大的update语句, 首先, 我在不使用Streaming Replication的情况下运行更新, 由于默认情况下Streaming Replication是disabled, 所以我们不需要做任何事情, 只需运行更新即可. 在node1上, 我记录更新之前和之后的时间, 在node2上, 我每秒钟运行一次select, 以查看这个更新在其他节点上实际提交的时间.On the writer: 1234567891011121314151617node1-T1 mysql&gt; select now();update sbtest1 set k=1 where id &lt; 1000000;select now();+---------------------+| now() |+---------------------+| 2020-03-24 14:25:20 |+---------------------+1 row in set (0.00 sec)Query OK, 999997 rows affected (34.53 sec)Rows matched: 999997 Changed: 999997 Warnings: 0+---------------------+| now() |+---------------------+| 2020-03-24 14:25:54 |+---------------------+1 row in set (0.00 sec) 运行和这个update语句花费了大约34秒 Check the other node: 12345678910+---------------------+| now() |+---------------------+| 2020-03-24 14:26:22 |+---------------------++---+| k |+---+| 1 |+---+ 正如我所说, 我每秒钟都在运行一个查询, 以查看k何时变为1. node2花了28秒的时间来验证、应用和提交这个更新. 重要的是要理解, 当节点正在验证这个大更新时, 任何其他表上的所有其他更改都将被阻塞, 因此该节点将停滞/冻结近30秒. Will Streaming Replication Decrease This Delay?理论上, 其他节点应该可以更快地进行更新, 因为当更新在node1上运行时, 片段(fragments 指事务的部分binlog)已经被复制并应用到其他节点上, 我将把片段大小设置为1MB, 12345678910111213141516171819set session wsrep_trx_fragment_size=1048576; node1-T1 mysql&gt; select now();update sbtest1 set k=2 where id &lt; 1000000;select now();+---------------------+| now() |+---------------------+| 2020-03-24 14:44:08 |+---------------------+1 row in set (0.00 sec) Query OK, 999997 rows affected (40.08 sec)Rows matched: 999997 Changed: 999997 Warnings: 0 +---------------------+| now() |+---------------------+| 2020-03-24 14:44:48 |+---------------------+1 row in set (0.00 sec) 现在这个更新要跑40s, 我们看看其他节点. 12345678910+---------------------+| now() |+---------------------+| 2020-03-24 14:44:48 |+---------------------++---+| k |+---+| 2 |+---+ 在同一秒内, k在node2上也已经是2了, 我们不再有28秒的延迟, 这很好, 但是这个uupdate语句本身变慢了一点, 我多次运行这些更新, 在没有Streaming Replication的情况下总是更快, 但是对于大型事务, 我更喜欢流复制, 因为其他节点不会与写节点(执行更新语句的节点)差的太远. Why Could It Be Slower With Streaming Replication?其中一个原因可能是, 使用Streaming Replication时, Galera基本上会写两份write-sets.它在mysql.wsrep_streaming_log中记录write-sets, 以确保在发生崩溃的情况下也可以进行Streaming Replication updates.这就是为什么为所有查询启用流复制不是一个好主意的原因. Does the Fragment Size Matter?Let’s change the fragment size to 0.1MB: 1234567891011121314151617181920node1-T1 mysql&gt; set session wsrep_trx_fragment_size=104857;Query OK, 0 rows affected (0.01 sec) node1-T1 mysql&gt; select now();update sbtest1 set k=5 where id &lt; 1000000;select now();+---------------------+| now() |+---------------------+| 2020-03-24 15:28:30 |+---------------------+1 row in set (0.00 sec) Query OK, 999997 rows affected (51.21 sec)Rows matched: 999997 Changed: 999997 Warnings: 0 +---------------------+| now() |+---------------------+| 2020-03-24 15:29:21 |+---------------------+1 row in set (0.00 sec) 现在花了50多秒钟, 因此我们可以看到gragment size很重要. 当我将片段大小增加到100MB时, 我也进行了测试, 在这种情况下, 查询又花了40秒钟左右, 但其他节点需要更多时间才能赶上, 差不多10秒钟左右. 找到正确的片段大小很重要, 在我的测试中1MB是可以接受的, 但是您也可以定义行数, 如每10000行之后创建片段. Locking在Galera 4之前, 锁没有传播到其他节点. 如果您在node1上运行查询, 则InnoDB仅在node1上锁定了必要的行, 但是node2和node3不知道哪些行被锁定. 如果有人在写node2和node3, 那么Galera必须处理这些冲突, 基本上是先提交的事务获胜. 如果要在多个节点进行写入, 则可能会有很多冲突. 但随着Streaming Replication的出现, 这种情况也发生了变化.片段被复制到其他节点, 它们也将在那里持有必要的锁. Let’s see how this works, first without Streaming Replication, step by step. 在node1和node2上启动事务, 在node1上运行大型更新, 在node2上删除单个行, 然后在node2上提交事务.当我在node1上提交时, 我得到一个死锁: 1234567891011121314node1-T1 mysql&gt; begin;node2-T1 mysql&gt; begin;node1-T1 mysql&gt; update sbtest1 set k=8 where id &lt; 1000000;Query OK, 999997 rows affected (27.87 sec)Rows matched: 999997 Changed: 999997 Warnings: 0node2-T1 mysql&gt; delete from sbtest1 where id=12;Query OK, 1 row affected (0.01 sec)node2-T1 mysql&gt; commit;Query OK, 0 rows affected (0.00 sec)node1-T1 mysql&gt; commit;ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction 因为没有集群范围的锁定, 所以在本例中, 它将使用乐观锁定, 并在认证过程中获得一个错误. How Does This Work With Streaming Replication? Repeat the same steps: 123456789101112131415161718node1-T1 mysql&gt; set session wsrep_trx_fragment_size=1048570;Query OK, 0 rows affected (0.00 sec)node1-T1 mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)node2-T1 mysql&gt; begin;Query OK, 0 rows affected (0.00 sec)node2-T1 mysql&gt; update sbtest1 set k=9 where id &lt; 1000000;Query OK, 999996 rows affected (38.15 sec)Rows matched: 999996 Changed: 999996 Warnings: 0node2-T1 mysql&gt; delete from sbtest1 where id=13;Waiting.....node1-T1 mysql&gt; commit;Query OK, 0 rows affected (2.46 sec)node2-T1 mysql&gt; commit;Query OK, 0 rows affected (0.00 sec) 同样的步骤, 但是当我们试图在node2上运行delete时, 它只是hang在那里, 等待获取锁, 但是被来自node1的更新语句产生的锁阻塞了, 这正是因为Streaming Replication.当我在node1上提交事务时, node2可以继续运行delete语句了.这可以将我们从许多僵局和冲突中解救出来.查询可能必须等待锁, 但最终它们仍然会完成. 这听起来很好, 但我仍然不建议将Streaming Replication作为默认设置启用, 因为它会影响性能, 而且它会将所有内容重复写入mysql.wsrep_streaming_log表.此外, 如果您想回滚一个大事务, 那么它必须在所有节点上执行, 而不是只在一个节点上执行. Metrics/Monitoring有一个表叫做mysql.wsrep_streaming_log, 其中包含关于当前正在运行的流的信息. 123456node1-T1 mysql&gt; SELECT node_uuid, trx_id, seqno, flags FROM mysql.wsrep_streaming_log;+--------------------------------------+--------+-------+-------+| node_uuid | trx_id | seqno | flags |+--------------------------------------+--------+-------+-------+| 9d7f7e09-6d20-11ea-842b-26b6d24e606d | 8571 | 24712 | 1 |+--------------------------------------+--------+-------+-------+ 但是要小心, 因为名为frag的列包含整个二进制日志复制事件, 这个事件可能非常大.在我意识到这一点之前, 我得到了以下错误: 12node1-T1 mysql&gt; select * from mysql.wsrep_streaming_log;ERROR 2020 (HY000): Got packet bigger than &#x27;max_allowed_packet&#x27; bytes 不幸的是, 据我所知, 没有计数器来监视通过Streaming Replication复制了多少语句.为此, 我创建了一个feature request. Conclusion到目前为止, Streaming Replication效果很好, 我认为它具有巨大的潜力. 因为我们可以在会话级启用它, 所以您自己或应用程序可以在需要时轻松打开它, but I am also waiting to see real-life experiences with it.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"},{"name":"MySQL 8.0","slug":"MySQL-8-0","permalink":"http://fuxkdb.com/tags/MySQL-8-0/"},{"name":"Galera 4.0","slug":"Galera-4-0","permalink":"http://fuxkdb.com/tags/Galera-4-0/"}]},{"title":"译文-A Simple Approach to Troubleshooting High CPU in MySQL","slug":"2020-05-03-A-Simple-Approach-to-Troubleshooting-High-CPU-in-MySQL","date":"2020-05-03T04:21:00.000Z","updated":"2020-05-03T04:27:52.057Z","comments":true,"path":"2020/05/03/2020-05-03-A-Simple-Approach-to-Troubleshooting-High-CPU-in-MySQL/","link":"","permalink":"http://fuxkdb.com/2020/05/03/2020-05-03-A-Simple-Approach-to-Troubleshooting-High-CPU-in-MySQL/","excerpt":"A Simple Approach to Troubleshooting High CPU in MySQL - 在MySQL中对高CPU进行故障排除的简单方法我们的一位客户最近问，是否有可能从MySQL方面识别导致系统CPU使用率高的查询。 PostgreSQL和Oracle DBA长期以来一直使用简单的OS工具查找罪魁祸首，但是它对MySQL无效，因为历史上我们一直缺乏将OS线程与内部线程进行匹配的工具 - 直到最近。 Percona添加了支持，可从针对MySQL 5.6.27的Percona Server开始，通过information_schema.processlist表的TID列将进程列表ID映射到OS线程ID。 在5.7发行版中，MySQL扩展了PERFORMANCE_SCHEMA.THREADS表并添加了一个名为THREAD_OS_ID的新列，从而实现了自己的实现，Percona Server for MySQL代替了自己的列，因为它通常尽可能地靠近上游。 对于在其他内核正常运行时查询使一个特定CPU过载的情况，以下方法很有用。 对于一般的CPU使用问题，可以使用不同的方法，例如另一篇博客文章Reducing High CPU on MySQL: A Case Study的方法.","text":"A Simple Approach to Troubleshooting High CPU in MySQL - 在MySQL中对高CPU进行故障排除的简单方法我们的一位客户最近问，是否有可能从MySQL方面识别导致系统CPU使用率高的查询。 PostgreSQL和Oracle DBA长期以来一直使用简单的OS工具查找罪魁祸首，但是它对MySQL无效，因为历史上我们一直缺乏将OS线程与内部线程进行匹配的工具 - 直到最近。 Percona添加了支持，可从针对MySQL 5.6.27的Percona Server开始，通过information_schema.processlist表的TID列将进程列表ID映射到OS线程ID。 在5.7发行版中，MySQL扩展了PERFORMANCE_SCHEMA.THREADS表并添加了一个名为THREAD_OS_ID的新列，从而实现了自己的实现，Percona Server for MySQL代替了自己的列，因为它通常尽可能地靠近上游。 对于在其他内核正常运行时查询使一个特定CPU过载的情况，以下方法很有用。 对于一般的CPU使用问题，可以使用不同的方法，例如另一篇博客文章Reducing High CPU on MySQL: A Case Study的方法. 我们如何使用这个新列来找出哪个会话使用了数据库中最多的CPU资源?让我们举个例子： 要解决CPU问题，我们可以使用几种工具，例如top或pidstat（需要sysstat程序包）。 在下面的示例中，我们将使用pidstat。 该工具有一个选项（-t），可将其视图从进程（默认值）更改为线程，在其中显示给定进程内的关联线程。 我们可以使用它来找出哪个线程在服务器中消耗了最多的CPU。 将-p参数与mysql进程ID一起添加，以便该工具仅显示MySQL线程，从而使我们更容易进行故障排除。 最后一个参数（1）是每秒显示一个样本： 命令为pidstat -t -p &lt;mysqld_pid&gt; 1： 12345678910111213141516171819202122shell&gt; pidstat -t -p 31258 103:31:06 PM UID TGID TID %usr %system %guest %CPU CPU Command[...]03:31:07 PM 10014 - 32039 5.00 1.00 0.00 6.00 22 |__mysqld03:31:07 PM 10014 - 32040 5.00 1.00 0.00 6.00 23 |__mysqld03:31:07 PM 10014 - 32042 6.00 1.00 0.00 7.00 8 |__mysqld03:31:07 PM 10014 - 32047 5.00 1.00 0.00 6.00 6 |__mysqld03:31:07 PM 10014 - 32048 5.00 1.00 0.00 6.00 15 |__mysqld03:31:07 PM 10014 - 32049 5.00 1.00 0.00 6.00 14 |__mysqld03:31:07 PM 10014 - 32052 5.00 1.00 0.00 6.00 14 |__mysqld03:31:07 PM 10014 - 32053 94.00 0.00 0.00 94.00 9 |__mysqld03:31:07 PM 10014 - 32055 4.00 1.00 0.00 5.00 10 |__mysqld03:31:07 PM 10014 - 4275 5.00 1.00 0.00 6.00 10 |__mysqld03:31:07 PM 10014 - 4276 5.00 1.00 0.00 6.00 7 |__mysqld03:31:07 PM 10014 - 4277 6.00 1.00 0.00 7.00 15 |__mysqld03:31:07 PM 10014 - 4278 5.00 1.00 0.00 6.00 18 |__mysqld03:31:07 PM 10014 - 4279 5.00 1.00 0.00 6.00 10 |__mysqld03:31:07 PM 10014 - 4280 5.00 1.00 0.00 6.00 12 |__mysqld03:31:07 PM 10014 - 4281 5.00 1.00 0.00 6.00 11 |__mysqld03:31:07 PM 10014 - 4282 4.00 1.00 0.00 5.00 2 |__mysqld03:31:07 PM 10014 - 35261 0.00 0.00 0.00 0.00 4 |__mysqld03:31:07 PM 10014 - 36153 0.00 0.00 0.00 0.00 5 |__mysqld 我们可以看到线程32053在很大程度上消耗了最多的CPU，并且我们确保验证了在pidstat的多个样本之间的消耗是否恒定。 利用这些信息，我们可以登录数据库，并使用以下查询来找出哪个MySQL Thread是罪魁祸首： 1234567891011121314151617181920mysql &gt; select * from performance_schema.threads where THREAD_OS_ID = 32053 \\G*************************** 1. row *************************** THREAD_ID: 686 NAME: thread/sql/one_connection TYPE: FOREGROUND PROCESSLIST_ID: 590 PROCESSLIST_USER: msandbox PROCESSLIST_HOST: localhost PROCESSLIST_DB: NULLPROCESSLIST_COMMAND: Query PROCESSLIST_TIME: 0 PROCESSLIST_STATE: Sending data PROCESSLIST_INFO: select * from test.joinit where b = &#x27;a a eveniet ut.&#x27; PARENT_THREAD_ID: NULL ROLE: NULL INSTRUMENTED: YES HISTORY: YES CONNECTION_TYPE: SSL/TLS THREAD_OS_ID: 320531 row in set (0.00 sec) 好了！ 现在我们知道，CPU高消耗来自joinit表中的查询，该查询由数据库测试中来自本地主机的用户msandbox执行。 使用此信息，我们可以对查询进行故障排除，并使用EXPLAIN命令检查执行计划，以查看是否还有任何改进的余地。 123456789101112131415mysql &gt; explain select * from test.joinit where b = &#x27;a a eveniet ut.&#x27; \\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: joinit partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 7170836 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) In this case, it was a simple index that was missing! 123mysql &gt; alter table test.joinit add index (b) ;Query OK, 0 rows affected (15.18 sec)Records: 0 Duplicates: 0 Warnings: 0 After we create the index, we are no longer seeing CPU spikes: 12345678910111213141516171819202122shell&gt; pidstat -t -p 31258 103:37:53 PM UID TGID TID %usr %system %guest %CPU CPU Command[...]03:37:54 PM 10014 - 32039 25.00 6.00 0.00 31.00 0 |__mysqld03:37:54 PM 10014 - 32040 25.00 5.00 0.00 30.00 21 |__mysqld03:37:54 PM 10014 - 32042 25.00 6.00 0.00 31.00 20 |__mysqld03:37:54 PM 10014 - 32047 25.00 4.00 0.00 29.00 23 |__mysqld03:37:54 PM 10014 - 32048 25.00 7.00 0.00 32.00 22 |__mysqld03:37:54 PM 10014 - 32049 23.00 6.00 0.00 29.00 4 |__mysqld03:37:54 PM 10014 - 32052 23.00 7.00 0.00 30.00 14 |__mysqld03:37:54 PM 10014 - 32053 10.00 2.00 0.00 12.00 11 |__mysqld03:37:54 PM 10014 - 32055 24.00 6.00 0.00 30.00 1 |__mysqld03:37:54 PM 10014 - 4275 25.00 6.00 0.00 31.00 7 |__mysqld03:37:54 PM 10014 - 4276 25.00 6.00 0.00 31.00 1 |__mysqld03:37:54 PM 10014 - 4277 24.00 5.00 0.00 29.00 14 |__mysqld03:37:54 PM 10014 - 4278 24.00 6.00 0.00 30.00 9 |__mysqld03:37:54 PM 10014 - 4279 25.00 5.00 0.00 30.00 6 |__mysqld03:37:54 PM 10014 - 4280 26.00 5.00 0.00 31.00 14 |__mysqld03:37:54 PM 10014 - 4281 24.00 6.00 0.00 30.00 10 |__mysqld03:37:54 PM 10014 - 4282 25.00 6.00 0.00 31.00 10 |__mysqld03:37:54 PM 10014 - 35261 0.00 0.00 0.00 0.00 4 |__mysqld03:37:54 PM 10014 - 36153 0.00 0.00 0.00 0.00 5 |__mysqld 为什么不使用这种方法来解决IO和内存问题?从OS端测量线程IO的问题在于，大多数MySQL IO操作是由后台线程完成的，例如读取，写入和页面清理程序线程。 要测量线程IO，您可以使用带有-d（IO而不是CPU）选项的pidstat或带有-H（每个线程）的iostat之类的工具。 如果您有一个非常消耗IO的线程，您也许可以看到它，但是要警告，由于后台线程操作，结果可能会产生误导。 内存消耗是要从OS端衡量的一个更加棘手的资源，因为所有内存都是在MySQL进程下分配的，并且由于是MySQL管理其内存访问，因此对于OS来说哪个线程消耗最多的内存是透明的。 为此，我们可以使用从5.7开始使用perfomance_schema memory instrumentation available starting in 5.7. 结论有很多方法可以解决CPU使用率过高的问题，但是从5.7版开始，我们在Oracle和PostgreSQL数据库上提供了一种简单且广泛使用的方法，该方法可以适应MySQL。 通过从OS线程消耗跟踪到数据库端，我们可以快速检测到影响系统性能的CPU密集型查询。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"译文-Group Replication and Percona XtraDB Cluster: Overview of Common Operations","slug":"2020-05-03-译文-Group-Replication-and-Percona-XtraDB-Cluster-Overview-of-Common-Operations","date":"2020-05-03T04:20:00.000Z","updated":"2020-05-03T04:28:19.223Z","comments":true,"path":"2020/05/03/2020-05-03-译文-Group-Replication-and-Percona-XtraDB-Cluster-Overview-of-Common-Operations/","link":"","permalink":"http://fuxkdb.com/2020/05/03/2020-05-03-%E8%AF%91%E6%96%87-Group-Replication-and-Percona-XtraDB-Cluster-Overview-of-Common-Operations/","excerpt":"Group Replication and Percona XtraDB Cluster: Overview of Common Operations在这篇博客文章中，我将概述使用MySQL Group Replication 8.0.19 (aka GR 国内爱叫MGR发现国外还是习惯叫GR)和Percona XtraDB Cluster 8 (PXC)(基于Galera)时最常见的故障转移场景和操作，并解释每种技术如何处理每种情况。我已经创建了一个包含三个节点的集群，使用一个主节点和一个包含三个节点的PXC进行组复制，它们均使用默认参数配置。 我还将使用ProxySQL与两个群集进行交互。 在这两个群集中，节点的名称分别为mysql1，mysql2和mysql3。 在组复制中，如果我们使用单主模式，则主节点处理写请求。 在PXC中，我也将使用相同的术语，并将在发送写操作的节点称为Primary。 请注意，在PXC中没有主节点的概念，所有节点都是相等的。","text":"Group Replication and Percona XtraDB Cluster: Overview of Common Operations在这篇博客文章中，我将概述使用MySQL Group Replication 8.0.19 (aka GR 国内爱叫MGR发现国外还是习惯叫GR)和Percona XtraDB Cluster 8 (PXC)(基于Galera)时最常见的故障转移场景和操作，并解释每种技术如何处理每种情况。我已经创建了一个包含三个节点的集群，使用一个主节点和一个包含三个节点的PXC进行组复制，它们均使用默认参数配置。 我还将使用ProxySQL与两个群集进行交互。 在这两个群集中，节点的名称分别为mysql1，mysql2和mysql3。 在组复制中，如果我们使用单主模式，则主节点处理写请求。 在PXC中，我也将使用相同的术语，并将在发送写操作的节点称为Primary。 请注意，在PXC中没有主节点的概念，所有节点都是相等的。这是两种解决方案的设置示意图。 Primary Server Crashes Group Replication – Writing在此测试中，我仅向集群发送写请求。 当我杀死GR上的主节点时，重新组织拓扑需要花费5到15秒的时间，ProxySQL识别新的拓扑结构后将写入发送到新的主节点。 启动旧的主数据库并将其重新添加到集群中不会导致任何中断(不会影响业务)。 Group Replication – Reading如果我仅向集群发送读取请求，主节点崩溃将导致读取中断吗？ ProxySQL只会将流量重定向到其他节点。 重组期间不会阻塞查询。 Percona XtraDB Cluster – Writing/Reading在PXC中，读取和写入之间没有区别，一旦节点崩溃/消失/分离(crashes/goes away/gets separated)，群集必须重新创建群集视图并检查仲裁。 这样做时，它不接受任何读取或写入。 通常，这需要3到10秒，在此时间范围内，应用程序会受到影响。 Removing/Adding Node如果我们删除或添加一个新节点，集群会如何做。 Group Replication在GR中，添加或删除节点不会影响或导致应用程序中断。 如果我们使用克隆插件添加新节点，则集群会将数据传播到新节点。 Percona XtraDB Cluster删除或添加节点不会导致任何中断。 类似地，就像在GR中添加新节点时一样，它将执行SST（State Snapshot Transfer状态快照传输）以从另一个节点获取所有数据。 Partial Network Failure如果读节点与主节点分离，但仍能够看到其他节点，则群集会发生什么情况？ 在这种情况下，mysql2（主）和mysql3之间存在网络中断。 Group Replication在我之前的一篇博文MySQL Group Replication – Partial Network Failure Performance Impact中我更详细地解释了这个特殊情况。基本上，部分网络中断会严重影响集群中的写性能，从而导致应用程序问题和/或停机。 8.0.19有bug,官方已经复现并确认bug, 但是一个朋友没有复现 目前最新版本为8.0.20, 在release note没有看到修复此bug Percona XtraDB Cluster在PXC中，当群集重新创建群集视图并开始将流量中继到可看到该服务器的节点时，将发生3-5秒的中断。 之后，它将像以前一样继续工作，而不会对性能造成任何严重影响。 In PXC there is going to be a 3-5s outage while the cluster re-creates the cluster view and begins relaying the traffic to a node that sees that server. After that, it will continue working just like before without any serious performance impact. Total Network Isolation 现在，mysql3与所有其他节点完全分离。 Group Replication群集可以接受读取和写入而不会发生任何中断，ProxySQL会将读取重定向到其他节点。 Percona XtraDB Cluster在PXC上，将有3-5秒的停机时间，而群集意识到一个节点不可用，并将如上所述重新创建群集视图。 在那之后，它能够处理读写。 Local Applications 如果一个节点或部分节点与集群分开(网络分区, 5节点集群,形成3, 2结构), 那个对于分区中的2个节点, 可能应用仍然能和它们通信, 此时会发生什么情况? What happens if a node or part of the nodes are separated and they do not have the quorum, but they have the application server in the same network segments which could still connect to the server. Group Replication分离的节点仍将接受读取流量，因此应用程序可以根据过时的数据做出决策。 这是默认设置，但是您可以使用名为group_replication_exit_state_action的参数做出调整 group_replication_exit_state_action Property Value Command-Line Format --group-replication-exit-state-action=value Introduced 8.0.12 System Variable group_replication_exit_state_action Scope Global Dynamic Yes SET_VAR Hint Applies No Type Enumeration Default Value (≥ 8.0.16) READ_ONLY Default Value (≥ 8.0.12, ≤ 8.0.15) ABORT_SERVER Valid Values (≥ 8.0.18) ABORT_SERVER``OFFLINE_MODE``READ_ONLY Valid Values (≥ 8.0.12, ≤ 8.0.17) ABORT_SERVER``READ_ONLY ABORT_SERVER节点实例会被shutdown OFFLINE_MODE连接的客户端用户在下一个请求时断开连接，不再接受连接 READ_ONLY 实例变为只读模式 Percona XtraDB Cluster在PXC中，如果节点分离，则它将不接受任何读取或写入。 优先级是数据一致性，只有具有quorum的segment才能接受任何读取和写入。 In PXC, if a node gets separated, it is not going to accept any reads or writes. The priority is the data consistency and only the segment which has the quorum will accept any reads and writes. Changing PrimaryGroup ReplicationIf you would like to use a new Primary node you have to promote a reader to be the new primary: 1cluster.setPrimaryInstance(&quot;mysql2:3306&quot;) ProxySQL将遵循这些更改，但是在群集重新组织自身时，它将导致几秒钟的中断。 Percona XtraDB Cluster在PXC上没有Primary的概念，任何节点都可以在任何时间进行写操作，因此我们只需要将流量重定向到负载均衡器中的另一个节点（即ProxySQL）。 PXC中还有一个pxc_maint_mode变量。 将其更改为MAINTENANCE可以从节点上软删除该连接，即使该连接是Primary，也是如此，但是ProxySQL Native Galera支持对此的支持很差。 I would recommend using the 1.4 scheduler which respects this variable. Summary Group Replication Percona XtraDB Cluster Primary Crashes 5-15s outage 5-10s outage Reader Crashes No impact 3-5s outage Adding a Node No impact No impact Removing a Node No impact No impact Partial Network Failure Performance Impact 3-5s outage, than normal performance Total Network Isolation No impact 3-5s outage Changing Primary 1-3s outage No impact on the cluster 如果读取节点发生故障或分离，则组复制的影响较小。 在PXC中，由于所有节点都是相同的，因此没有专用的主节点。 如果任何节点发生任何事情，则集群必须投票并重新创建集群视图，这可能会对应用程序产生一些影响。 但是，PXC可以更好地处理主节点切换(primary promotions)和网络故障。 如我们所见，这两种集群解决方案都有各自的优缺点。 希望本摘要将帮助您对它们有更多的了解，并使您对使用哪种技术的决策更加容易。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"}]},{"title":"译文-ClickHouse In the Storm. Part 2 - Maximum QPS for key-value lookups","slug":"2020-05-03-[译文]ClickHouse-In-the-Storm.-Part-2-Maximum-QPS-for-key-value-lookups","date":"2020-05-03T03:43:00.000Z","updated":"2020-05-03T04:08:38.999Z","comments":true,"path":"2020/05/03/2020-05-03-[译文]ClickHouse-In-the-Storm.-Part-2-Maximum-QPS-for-key-value-lookups/","link":"","permalink":"http://fuxkdb.com/2020/05/03/2020-05-03-[%E8%AF%91%E6%96%87]ClickHouse-In-the-Storm.-Part-2-Maximum-QPS-for-key-value-lookups/","excerpt":"ClickHouse In the Storm. Part 2: Maximum QPS for key-value lookups上一篇文章调查了ClickHouse的连接性基准，以估计服务器并发的总体性能。 在本文中，我们将以实际示例为例，并在涉及实际数据时探讨并发性能。 MergeTree: Key-value lookup让我们看看MergeTree引擎表如何处理高并发性，以及它能够处理多少个QPS用于键值查找。 我们将使用2个综合数据集： ‘fs12M’-具有1200万条记录的表，id为FixedString(16)和5个不同类型的参数： 123456789CREATE TABLE default.fs12M ( id FixedString(16), value1 Float64, value2 Float64, value3 UInt32, value4 UInt64, str_value String) ENGINE = MergeTree PARTITION BY tuple() ORDER BY idSETTINGS index_granularity = 256","text":"ClickHouse In the Storm. Part 2: Maximum QPS for key-value lookups上一篇文章调查了ClickHouse的连接性基准，以估计服务器并发的总体性能。 在本文中，我们将以实际示例为例，并在涉及实际数据时探讨并发性能。 MergeTree: Key-value lookup让我们看看MergeTree引擎表如何处理高并发性，以及它能够处理多少个QPS用于键值查找。 我们将使用2个综合数据集： ‘fs12M’-具有1200万条记录的表，id为FixedString(16)和5个不同类型的参数： 123456789CREATE TABLE default.fs12M ( id FixedString(16), value1 Float64, value2 Float64, value3 UInt32, value4 UInt64, str_value String) ENGINE = MergeTree PARTITION BY tuple() ORDER BY idSETTINGS index_granularity = 256 ‘int20M’-具有2000万条记录的表，id为UInt64和一个Float32参数： 12345CREATE TABLE default.int20M ( id UInt64, value Float64) ENGINE = MergeTree PARTITION BY tuple() ORDER BY idSETTINGS index_granularity = 256 You can find the create table statements here. 我们将在下面使用两个查询来检查在4种情况下对这些表的lookup查询性能： 通过id查询单条记录 以1:3 hit/miss比率查询100条随机记录 此外，还要为上面的每个测试检查两种形式的ClickHouse过滤——WHERE和PREWHERE (PREWHERE子句与WHERE语法相同，但内部工作方式略有不同)。 12SELECT * FROM table WHERE id = … SELECT * FROM table WHERE id IN (... SELECT 100 random ids ...) You can find all the select statements here. CHOOSING INDEX_GRANULARITYMergeTree的最重要参数之一是index_granularity。 让我们尝试为每种情况找到最佳的index_granularity： 由于上面的表总结了几个不同测试的结果，所以该图显示了规范化到最佳运行的QPS(即1 =表示所有测试都给出了最大QPS)。如您所见，相比’int20M’, ‘fs12M’在index_granularity较小时性能较好, 因为键更大，涉及的列更多(包括一个相当大的字符串)。 For fs12M the best QPS performance gives index_granularity 64 &amp; 128 For int20M - 128 &amp; 256. 小查询的性能可以通过使用uncompressed cache提高. ClickHouse通常不缓存数据, 但是对于快速的简短查询, 可以使用internal cache来保存未压缩的数据库块. 可以使用配置文件级别的use_uncompressed_cache设置将其打开。 默认缓存大小为8GB，但可以增加。 目前use_uncompressed_cache参数在我们的生产环境没有打开, 我查阅了一些资料发现可能是参考了国内的博客, 而国内的博客可能都是互相借鉴都没有自己看这个参数, 也可能是我臆测的 use_uncompressed_cache默认0关闭, 但在文档中有如下描述 For queries that read at least a somewhat large volume of data (one million rows or more), the uncompressed cache is disabled automatically to save space for truly small queries. This means that you can keep the ‘use_uncompressed_cache’ setting always set to 1. 不过在本文结论在, 作者建议只对特定的查询和特定的用户开启use_uncompressed_cache, 但是use_uncompressed_cache其实好像无法针对查询开启但可以针对会话 Let’s see how QPS looks for 100 random ID lookups: 正如我们预期的那样，对’fs12’的查找要比对’int20M’的查找慢。 使用PREWHERE而不是WHERE可以显着改善性能。 Here are QPS numbers for single value lookup: 启用use_uncompressed_cache最多可提高50％性能。 对于’int20M’单点查询，它的效果似乎较低，因为在所有测试案例中，它的运行速度都非常快。 但是，如果该QPS还不够，该怎么办？ ClickHouse可以做得更好吗？ 可以，但是我们必须切换到其他数据结构以确保数据始终在内存中。 ClickHouse为此提供了多种选择：我们将尝试使用外部Dictionary和Join引擎。 Dictionary通常使用字典来与外部数据集成。 请参阅有关此主题的博客文章。 但是，也可以使用字典来为已经存储在ClickHouse中的数据建立内存中的缓存。 让我们看看如何做。 首先，让我们为”int20M”表配置字典： 12345678910111213141516171819202122232425&lt;dictionaries&gt;&lt;dictionary&gt; &lt;name&gt;int20M&lt;/name&gt; &lt;source&gt; &lt;clickhouse&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;db&gt;default&lt;/db&gt; &lt;table&gt;int20M&lt;/table&gt; &lt;/clickhouse&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;layout&gt;&lt;hashed&gt;&lt;/hashed&gt;&lt;/layout&gt; &lt;structure&gt; &lt;id&gt;&lt;name&gt;id&lt;/name&gt;&lt;/id&gt; &lt;attribute&gt; &lt;name&gt;value&lt;/name&gt; &lt;type&gt;Float32&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt;&lt;/dictionary&gt;&lt;/dictionaries&gt; 您可以从此处下载我们为您准备好文件，该文件需要放置在ClickHouse配置文件夹中。 不幸的是，目前无法使用FixedString键创建字典，因此我们仅测试’int20M’情况。 为了通过键获取值，我们可以使用字典get函数对其进行查询： 1select dictGetFloat32(&#x27;test100M&#x27;,&#x27;value&#x27;,9221669071414979782) You can find all the test selects here 好吧，它的QPS约为9.2K：比MergeTree更好. 根据经验对于single point select MySQL可以轻松达到90k qps. 这也说明ClickHouse并不是用来做高并发短查询的. 另一个选项是使用“缓存”字典布局(‘cached’ dictionaries layout)，它提示ClickHouse只将部分行保存在内存中。如果您的缓存命中率很高，那么它可以很好地工作。 如果您不需要过于频繁地更新字典，则字典效果很好。 更新是异步的； 字典会定期检查源表，以确定是否需要刷新它。 有时可能不方便。 另一个不便之处是XML配置。 应该很快用字典DDL修复它，但是暂时我们必须坚持使用XML。 Join table EngineClickHouse内部使用联接表引擎来处理SQL联接。 但是您可以像这样显式创建一个表： 12345678CREATE TABLE fs12M ( id FixedString(16), value1 Float64, value2 Float64, value3 UInt32, value4 UInt64, str_value String) ENGINE = Join(ANY, LEFT, id) 然后可以像插入其他表一样插入该表。理想情况下，您可以用一个INSERT填充整个表，但是也可以用parts来完成(但是仍然需要足够大的parts，最好不要有太多的parts)。联接表在内存中持久存在(就像字典一样，但是以一种更紧凑的方式)，并且也在磁盘上刷新。也就是说，它将在服务器重启后恢复。 无法直接查询Join table； 您始终需要将其用作联接的正确部分。 有两个选项可从联接表中提取数据。 第一个类似于使用joinGet函数的字典语法（它是由一个贡献者添加的，自18.6开始可用）。 或者，您可以进行真正的联接（例如，使用system.one表）。 我们将使用以下查询来测试这两种方法： 12SELECT joinGet(&#x27;int20M&#x27;, &#x27;value&#x27;, 9221669071414979782) as valueSELECT 9221669071414979782 as id, * FROM system.one ANY LEFT JOIN int20M USING (id); Results, QPS: 当您需要为每次查找进行大量的joinGet调用时，对于’fs12M’来说，joinGet和实际联接之间的区别显而易见。 当您只需要调用一次时-没有区别。 与字典相比，Join表的性能稍差。 但是，由于它已经在数据库架构中，因此维护起来更容易。 与字典相比，联接使用的内存更少，您可以轻松添加新数据，等等。 不幸的是，无法进行更新。 （That would be too good. :)） 让我们总结一下所有QPS结果并将其放在一张图表中： Latencies (90th percentile) Conclusions - 结论在本文的过程中，我们已经研究了并发基础知识。这是您可以用于自己的应用程序的一些主要结论。 ClickHouse is not a key-value database (surprise! :) ) 如果您需要在Clickhouse内部模拟键值查找方案-连接引擎和字典可提供最佳性能 如果每秒有很多查询，则禁用日志（或降低日志级别）可以提高性能 启用uncompressed cache有助于提高查询性能（对于“小的”查询仅返回几行）。最好仅针对特定查询和特定用户profile启用它 在高并发方案中使用max_thread = 1 尽量保持同时连接的数量足够少，以获得最大的QPS性能。具体数字当然取决于硬件。使用我们的低端机器进行测试，16-32范围内的连接显示了最好的QPS性能对于UInt64键的键值类型场景，index_granularity= 256看起来是最好的选择，对于FixedString(16)键，也要考虑index_granularity= 128。 然而默认index_granularity为8192 使用PREWHERE而不是WHERE进行点查找。 Summary - 总结ClickHouse不是key-value stroe, 但是根据我们的结果证实ClickHouse在不同并发的高负载下表现稳定, 并且能够在MergeTree表上（当数据位于文件系统缓存中时）每秒提供约4K(每秒)的查询, 当使用Dictionary或Join engine是甚至可以达到10k(每秒). 当然，在更好的硬件上，您会有更好的数字. 这些结果远非键值数据库（例如Redis，在相同的硬件上可提供约125K QPS），但在某些情况下，即使这样的QPS速率也可以令人满意。 例如-通过复杂的OLAP计算在实时创建的数据中进行基于id的查找/通过id从物化视图中提取一些聚合等。并且，ClickHouse还可以水平和垂直缩放。 简单ping和普通select之间的QPS差异表明，未来的优化具有一定的潜力。另外，将ClickHouse与键值数据库结合起来，这样就可以在两个方向上与ClickHouse无缝合作，这个想法听起来很有前途。如果你关注ClickHouse的pull requests，你可能已经看到了一些与Cassandra和Redis的草案整合。 请继续关注，并订阅我们的博客！ 原文链接:https://www.altinity.com/blog/clickhouse-in-the-storm-part-2","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"译文-ClickHouse In the Storm. Part 1 - Maximum QPS estimation - 最大QPS估算","slug":"2020-05-03-[译文]ClickHouse-In-the-Storm.-Part-1-Maximum-QPS-estimation---最大QPS估算","date":"2020-05-03T03:33:00.000Z","updated":"2020-05-03T04:08:44.568Z","comments":true,"path":"2020/05/03/2020-05-03-[译文]ClickHouse-In-the-Storm.-Part-1-Maximum-QPS-estimation---最大QPS估算/","link":"","permalink":"http://fuxkdb.com/2020/05/03/2020-05-03-[%E8%AF%91%E6%96%87]ClickHouse-In-the-Storm.-Part-1-Maximum-QPS-estimation---%E6%9C%80%E5%A4%A7QPS%E4%BC%B0%E7%AE%97/","excerpt":"ClickHouse In the Storm. Part 1: Maximum QPS estimation - 最大QPS估算ClickHouse是一个用于分析的OLAP数据库, 因此典型的使用场景是处理相对少量的请求: 每小时几个查询到每秒几十甚至几百个查询 影响大量数据(gigabytes/millions of rows) 但是它在其他情况下表现如何? Let’s try to use a steam-hammer to crack nuts, 并检查ClickHouse每秒将如何处理数千个小请求。 这将帮助我们更好地理解可能的用例范围和限制。 这篇文章分为两个部分。 第一部分介绍连接性基准测试和测试设置。 下一部分将介绍涉及实际数据的方案中的最大QPS。","text":"ClickHouse In the Storm. Part 1: Maximum QPS estimation - 最大QPS估算ClickHouse是一个用于分析的OLAP数据库, 因此典型的使用场景是处理相对少量的请求: 每小时几个查询到每秒几十甚至几百个查询 影响大量数据(gigabytes/millions of rows) 但是它在其他情况下表现如何? Let’s try to use a steam-hammer to crack nuts, 并检查ClickHouse每秒将如何处理数千个小请求。 这将帮助我们更好地理解可能的用例范围和限制。 这篇文章分为两个部分。 第一部分介绍连接性基准测试和测试设置。 下一部分将介绍涉及实际数据的方案中的最大QPS。 Environment在最初的测试中，我选择了现有的旧工作站： 4-cores Intel(R) Core(TM) i5-2400 CPU @ 3.10GHz 8Gb of RAM SSD disk CentOS 7 本文介绍了从该机器收集的结果，但是当然，尝试在功能更强大的硬件上重复这些测试非常有趣。 我将这项任务留给我们的读者，因此您可以在自己的硬件上的不同情况下测试ClickHouse的最大QPS。 如果可以，请发布结果！ 为了运行基准测试，我还创建了一套可在Altinity github中免费使用的脚本：https://github.com/Altinity/clickhouse-sts/。 这些脚本需要Docker（I have v18.09）和Bash。 要运行测试套件，只需克隆GitHub存储库并在根文件夹中运行“ make test”命令。 它将在您的主机上执行所有测试（将花费几个小时），并将结果放入一个CSV文件中，以后可以在Excel，Pandas或ClickHouse本身中进行分析。 当然，您可以共享您的发现以将它们与本文的结果进行比较。 Under the hood those scripts use: https://github.com/wg/wrk，一种轻量级且快速的HTTP基准测试工具，允许创建不同的HTTP工作负载 ClickHouse发行版中包含的clickhouse-benchmark工具-用于本机协议ClickHouse测试 这两种工具都允许您创建所需的并发负载（模拟不同数量的并发客户端），并测量每秒服务的查询数量和延迟百分位数。 A few words about handling concurrent requests in ClickHouse默认情况下ClickHouse最多可以处理4096个inbound connections (max_connections setting in server config file), 但只会同时执行100个查询(max_concurrent_queries.), 所以所有其他client会在队列中等待so all other clients will just wait in the queue.客户端请求可以保持多久排队最长持续时间通过设置queue_max_wait_ms定义(这个参数文档中没有找到, 只有issue中搜到了)(5000 or 5 sec by default) 译者注: 这里说queue_max_wait_ms默认5000 or 5 sec 有点描述不清, 5000估计单位是ms. 但是我查询默认值并不是5ms, 应该是版本差异 1234567SELECT *FROM settingsWHERE name = &#x27;queue_max_wait_ms&#x27;┌─name──────────────┬─value─┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐│ queue_max_wait_ms │ 0 │ 0 │ The wait time in the request queue, if the number of concurrent requests exceeds the maximum. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │└───────────────────┴───────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘ It is a user/profile setting, so users can define some smaller value to prompt an exception in cases where the queue is too long. Keepalive timeout for http connection is relatively low by default - it’s 3 seconds (keep_alive_timeout setting). There are also a lot of advanced network-related settings to fine-tune different timeouts, poll intervals, listen_backlog size etc. HTTP ping: theoretically possible maximum throughput of HTTP server - HTTP ping: 理论上可能的最大HTTP服务器吞吐量首先，让我们检查一下ClickHouse本身使用的HTTP服务器有多快。 换句话说，服务器可以处理多少个”do nothing”请求。 对于HTTP，两个主要的场景很重要: 使用keepalive(使用持久连接来处理多个请求，无需重新连接) 没有keepalive(为每个请求建立新连接)。 另外，ClickHouse在默认情况下有一个非常详细的日志级别(“trace”)。对于每个查询，它都会向日志文件写入几行代码，这对于调试来说很好，但是当然会造成一些额外的延迟。因此，我们还检查了禁用日志的两个相同场景。 我们在不同的并发级别上检查了这些场景，以模拟不同数量的同时连接的客户机(一个接一个地发送请求)。每个测试执行15秒，然后取每秒处理请求的平均值。 Results: 在X轴上，您可以看到同时连接的客户端数量。在Y轴上是每个特定场景中每秒处理的请求的平均数量。 Well, results look good: 在每种情况下，该server上QPS的最大值为8到64个并发连接。 启用keepalive和禁用日志后，最大吞吐量约为97K QPS。 启用日志后，速度降低了约30％，并提供了约71K QPS。 两种非Keepalive变体都慢得多（约18.5 kqps），甚至在此处看不到日志记录开销。 这是可以预期的，因为有了Keepalive，ClickHouse当然可以处理更多的ping命令，这是因为跳过了为每个请求建立连接的额外费用。 现在，我们对ClickHouse web-server理论上可能达到的最大吞吐量和并发级别有了一种感觉。事实上，ClickHouse HTTP-server实现非常快。例如，在同一台机器上使用默认设置的NGINX每秒可以处理大约30K个请求。 SELECT 1让我们进一步检查一个普通的”SELECT 1”请求. 这样的查询是在查询解析阶段“执行”的，这样就会显示“网络+授权+查询解析+格式化结果”(‘network + authorization + query parser + formatting result’)的理论最大吞吐量，也就是说实际的请求永远不会比这个速度快。 We will test http and https using keepalive and no-keepalive options, and native client (both secure and non-secure). Results: 最佳情况约为14K QPS: http &amp; keepalive 在https＆keepalive情况下, 性能稍差一些(13K QPS). 在这种情况下, Https开销并不重要 10.7 kqps for http no-keepalive. 10.1 kqps for native (no secure). 9.3 kqps for native (secure) And quite poor 4.3 kqps for https no-keepalive 在最高并发级别上，我们记录了几十个连接错误（即小于0.01％），这很可能是由操作系统级别上的套接字重用问题引起的。 ClickHouse在该测试中的运行情况稳定，我没有发现任何可见的问题。 令人惊讶的是，本机协议的性能比http差，但实际上这是预期的:本机TCP/IP更复杂，并且有许多额外的协议特性。它不适用于高QPS，而是用于来回传输大数据块。 当本机客户端中的并发增长时，QPS也会显着下降，并发级别更高（&gt; 3000）。 此时，系统变得无响应，并且不返回任何结果。 这很可能是由于clickhouse-benchmark工具为每个连接使用了一个单独的线程，并且线程和上下文切换的数量对于系统来说太多了。 现在让我们看一下等待时间，即每个客户等待结果集的时间。 该数字在每个请求中有所不同，因此该图显示了每种情况下延迟的90％(90th percentile of the latency)。 这意味着90％的用户比显示的数字更快地得到答案。 LATENCIES (90TH PERCENTILE) - 1-256 CONCURRENCY LEVELS 随着并发性的增长，延迟会降低。现在看起来还不错:如果您的并发用户少于256个，那么延迟可能会低于50毫秒(注意这里指的是’network + authorization + query parser + formatting result’用时)。 让我们看看它是如何处理更高的并发性的。 LATENCIES (90TH PERCENTILE) - &gt;256 CONCURRENCY LEVELS 现在，延迟退化变得更加严重，并且本机协议native protocol再次显示出最差的结果。 有趣的是，没有keepalive的http请求的行为非常稳定，即使有2K个并发用户，其延迟也低于50ms。如果没有keepalive，延迟将更加可预测，并且stdev将随着并发性的增加而保持较小的值，但是QPS的速率会降低一些。它可能与webserver的实现细节有关:例如，当每个连接使用一个线程时，线程上下文切换会使服务器变慢，并在一定的并发级别后增加延迟。 我们还检查了其他设置，如max_concurrent_queries、queue_max_wait_ms、max_threads、network_compression_method、enable_http_compression和一些输出格式。在这种情况下，调整它们的效果基本上可以忽略不计。 Effects of multithreading默认情况下，ClickHouse使用多个线程来处理更大的查询，从而有效地使用所有的CPU核心。然而，如果您有大量的并发连接，多线程将在上下文切换、重新连接线程和工作同步方面产生额外的成本。 为了衡量并发连接和多线程的交互作用, 我们看看在使用默认的multithreading设置(我认为这里作者是指max_threads默认值, 即为the number of physical CPU cores)和指定max_threads=1两种情况下运行查询”synthetic select for finding maximum of 100K random numbers”(这里应该是指查询system.numbers_mt表)的性能差异 To measure the interaction of concurrent connections and multithreading let’s look at the difference in a synthetic select for finding maximum of 100K random numbers with default multithreading settings and with max_threads=1. 结论很简单:要在高并发场景中实现更高的QPS，使用max_threads=1设置。 To Be Continued…本文介绍了ClickHouse的一般连接测试。我们检查了服务器本身的速度有多快，它可以处理多少个简单的查询，以及在高并发性场景中哪些设置会影响QPS。请参阅后续文章，其中我们深入研究了在键值场景中估计实际查询的最大QPS，这将向测试用例添加数据。 原文链接:https://www.altinity.com/blog/clickhouse-in-the-storm-part-1","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"ClickHouse多实例部署","slug":"2020-05-02-ClickHouse多实例部署","date":"2020-05-02T15:46:00.000Z","updated":"2020-09-22T02:35:42.616Z","comments":true,"path":"2020/05/02/2020-05-02-ClickHouse多实例部署/","link":"","permalink":"http://fuxkdb.com/2020/05/02/2020-05-02-ClickHouse%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2/","excerpt":"ClickHouse多实例部署本人刚接触ClickHouse两周, 难免有错误之处, 感谢指正. 另外一些参数我也不甚理解, 大家也可以先不必纠结参数, 先部署起来再说, 我的感触是部署后就会对整体结构有了一遍认识. 如果多实例都可以部署完毕, 那么生产单实例部署当然就不成问题了. 生产环境并不建议多实例部署, ClickHouse一个查询可以用到多个CPU, 本例只适用于测试环境","text":"ClickHouse多实例部署本人刚接触ClickHouse两周, 难免有错误之处, 感谢指正. 另外一些参数我也不甚理解, 大家也可以先不必纠结参数, 先部署起来再说, 我的感触是部署后就会对整体结构有了一遍认识. 如果多实例都可以部署完毕, 那么生产单实例部署当然就不成问题了. 生产环境并不建议多实例部署, ClickHouse一个查询可以用到多个CPU, 本例只适用于测试环境 部署规划 集群部署关系如下: 逻辑结构图如下: 编辑三台主机/etc/hosts添加如下内容: 123172.16.120.10 centos-1172.16.120.11 centos-2172.16.120.12 centos-3 依赖组件安装JDK下载openjdk12wget https://github.com/AdoptOpenJDK/openjdk8-binaries/releases/download/jdk8u242-b08/OpenJDK8U-jdk_x64_linux_hotspot_8u242b08.tar.gztar -zxvf OpenJDK8U-jdk_x64_linux_hotspot_8u242b08.tar.gz -C /usr/local/ 做软链1ln -s /usr/local/jdk8u242-b08 /usr/local/java 配置环境变量123456#vi ~/.bashrc export JAVA_HOME=/usr/local/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH ZooKeeper3.6.0有bug 所以改用稳定版本3.4.14 下载安装包12wget https://downloads.apache.org/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gztar -zxvf zookeeper-3.4.14.tar.gz -C /usr/local/ 做软链1ln -s /usr/local/zookeeper-3.4.14 /usr/local/zookeeper 配置环境变量1234#vi ~/.bashrc export ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 修改配置文件1234567891011121314151617181920212223242526272829303132333435363738394041cd /usr/local/zookeeper/conf #参考官方#https://clickhouse.tech/docs/en/operations/tips/#zookeeper#vim zoo.cfg tickTime=2000initLimit=30000syncLimit=10maxClientCnxns=2000maxSessionTimeout=60000000dataDir=/data/zookeeper/datadataLogDir=/data/zookeeper/logsautopurge.snapRetainCount=10autopurge.purgeInterval=1preAllocSize=131072snapCount=3000000leaderServes=yesclientPort=2181 集群配置部分三个节点分别为:# centos-1server.1=0.0.0.0:2888:3888server.2=172.16.120.11:2888:3888server.3=172.16.120.12:2888:3888 # centos-2server.1=172.16.120.10:2888:3888server.2=0.0.0.0:2888:3888server.3=172.16.120.12:2888:3888 # centos-3server.1=172.16.120.10:2888:3888server.2=172.16.120.11:2888:3888server.3=0.0.0.0:2888:3888 创建目录12mkdir -p /data/zookeeper/&#123;data,logs&#125;mkdir -p /usr/local/zookeeper/logs myid12345678# centos-1echo &quot;1&quot;&gt;/data/zookeeper/data/myid # centos-2echo &quot;2&quot;&gt;/data/zookeeper/data/myid # centos-3echo &quot;3&quot;&gt;/data/zookeeper/data/myid 配置zk日志默认zk日志输出到一个文件,且不会自动清理,所以,一段时间后zk日志会非常大 1.zookeeper-env.sh ./conf目录下新建zookeeper-env.sh文件,修改到sudo chmod 755 zookeeper-env.sh权限12345678#cat conf/zookeeper-env.sh #!/usr/bin/env bash#tip:custom configurationfile，do not amend the zkEnv.sh file#chang the log dir and output of rolling file ZOO_LOG_DIR=&quot;/usr/local/zookeeper/logs&quot;ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot; 2.log4j.properties 修改日志的输入形式12345678zookeeper.root.logger=INFO, ROLLINGFILE#zookeeper.root.logger=INFO, CONSOLE # Max log file size of 10MBlog4j.appender.ROLLINGFILE.MaxFileSize=128MB# uncomment the next line to limit number of backup fileslog4j.appender.ROLLINGFILE.MaxBackupIndex=10 配置运行zk的JVM./conf目录下新建java.env文件,修改到sudo chmod 755 java.env权限,主要用于GC log,RAM等的配置. 1234567#!/usr/bin/env bash#config the jvm parameter in a reasonable#note that the shell is source in so that do not need to use export#set java classpath#CLASSPATH=&quot;&quot;#set jvm start parameter , also can set JVMFLAGS variableSERVER_JVMFLAGS=&quot;-Xms1024m -Xmx2048m $JVMFLAGS&quot; 启动zookeeper服务(所有节点)1234# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 验证zk12345678910111213141516171819202122232425262728# zkServer.sh status #bin/zkCli.sh -server 127.0.0.1:2181Connecting to 127.0.0.1:2181 [zk: 127.0.0.1:2181(CONNECTED) 0][zk: 127.0.0.1:2181(CONNECTED) 0] ls /[zookeeper] [zk: 127.0.0.1:2181(CONNECTED) 1] create /zk_test mydataCreated /zk_test [zk: 127.0.0.1:2181(CONNECTED) 2] ls /[zk_test, zookeeper] [zk: 127.0.0.1:2181(CONNECTED) 3] get /zk_testmydata [zk: 127.0.0.1:2181(CONNECTED) 4] set /zk_test junk[zk: 127.0.0.1:2181(CONNECTED) 5] get /zk_testjunk [zk: 127.0.0.1:2181(CONNECTED) 6] delete /zk_test[zk: 127.0.0.1:2181(CONNECTED) 7] ls /[zookeeper] [zk: 127.0.0.1:2181(CONNECTED) 8] ClickHouse安装yum安装123456yum install yum-utilsrpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPGyum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64 yum install clickhouse-server clickhouse-client 创建目录12345678centos-1 创建目录：mkdir -p /data/clickhouse/&#123;node1,node4&#125;/&#123;data,tmp,logs&#125; centos-2 创建目录：mkdir -p /data/clickhouse/&#123;node2,node5&#125;/&#123;data,tmp,logs&#125; centos-3 创建目录：mkdir -p /data/clickhouse/&#123;node3,node6&#125;/&#123;data,tmp,logs&#125; 创建配置文件配置clickhouse参数文件，账户文件，分片配置文件 这部分会啰嗦一些, 节点间有一些重复的配置为了方便也没有省略, 都贴出来了, 照着配置就可以了 node1配置文件如下config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node1/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node1/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8123&lt;/http_port&gt; &lt;tcp_port&gt;9000&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-1&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node1/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node1/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node1/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8001&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node1/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node1&lt;/replica&gt; &lt;!--当前节点IP--&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; node2配置文件如下:config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node2/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node2/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8123&lt;/http_port&gt; &lt;tcp_port&gt;9000&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-2&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node2/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node2/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node2/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8001&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node2/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node2&lt;/replica&gt; &lt;!--当前节点IP--&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; node3配置文件如下:config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node3/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node3/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8123&lt;/http_port&gt; &lt;tcp_port&gt;9000&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-3&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node3/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node3/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node3/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8001&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node3/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;03&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node3&lt;/replica&gt; &lt;!--当前节点IP--&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; node4配置文件如下:config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node4/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node4/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8124&lt;/http_port&gt; &lt;tcp_port&gt;9002&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9010&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-1&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node4/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node4/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node4/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8002&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node4/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node4&lt;/replica&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; node5配置文件如下:config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node5/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node5/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8124&lt;/http_port&gt; &lt;tcp_port&gt;9002&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9010&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-2&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node5/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node5/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node5/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8002&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node5/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;03&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node5&lt;/replica&gt; &lt;!--当前节点IP--&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; node6配置文件如下:config.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--日志--&gt; &lt;logger&gt; &lt;level&gt;warning&lt;/level&gt; &lt;log&gt;/data/clickhouse/node6/logs/clickhouse.log&lt;/log&gt; &lt;errorlog&gt;/data/clickhouse/node6/logs/error.log&lt;/errorlog&gt; &lt;size&gt;500M&lt;/size&gt; &lt;count&gt;5&lt;/count&gt; &lt;/logger&gt; &lt;!--本地节点信息--&gt; &lt;http_port&gt;8124&lt;/http_port&gt; &lt;tcp_port&gt;9002&lt;/tcp_port&gt; &lt;interserver_http_port&gt;9010&lt;/interserver_http_port&gt; &lt;interserver_http_host&gt;centos-3&lt;/interserver_http_host&gt; &lt;!--本机域名或IP--&gt; &lt;!--本地配置--&gt; &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; &lt;max_connections&gt;2048&lt;/max_connections&gt; &lt;receive_timeout&gt;800&lt;/receive_timeout&gt; &lt;send_timeout&gt;800&lt;/send_timeout&gt; &lt;keep_alive_timeout&gt;3&lt;/keep_alive_timeout&gt; &lt;max_concurrent_queries&gt;64&lt;/max_concurrent_queries&gt; &lt;uncompressed_cache_size&gt;4294967296&lt;/uncompressed_cache_size&gt; &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt; &lt;path&gt;/data/clickhouse/node6/&lt;/path&gt; &lt;tmp_path&gt;/data/clickhouse/node6/tmp/&lt;/tmp_path&gt; &lt;users_config&gt;/data/clickhouse/node6/users.xml&lt;/users_config&gt; &lt;default_profile&gt;default&lt;/default_profile&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt; &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8002&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt; &lt;default_database&gt;default&lt;/default_database&gt; &lt;timezone&gt;Asia/Shanghai&lt;/timezone&gt; &lt;!--集群相关配置--&gt; &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt; &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt; &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt; &lt;max_session_timeout&gt;3600&lt;/max_session_timeout&gt; &lt;default_session_timeout&gt;300&lt;/default_session_timeout&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;merge_tree&gt; &lt;parts_to_delay_insert&gt;300&lt;/parts_to_delay_insert&gt; &lt;parts_to_throw_insert&gt;600&lt;/parts_to_throw_insert&gt; &lt;max_delay_to_insert&gt;2&lt;/max_delay_to_insert&gt; &lt;/merge_tree&gt; &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt; &lt;max_partition_size_to_drop&gt;0&lt;/max_partition_size_to_drop&gt; &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;/distributed_ddl&gt; &lt;include_from&gt;/data/clickhouse/node6/metrika.xml&lt;/include_from&gt;&lt;/yandex&gt; users.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;!-- 请根据自己机器实际内存配置 --&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/readonly&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;put_your_passwordsha256hex_here&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;/users&gt;&lt;/yandex&gt; metrika.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!--ck集群节点--&gt; &lt;clickhouse_remote_servers&gt; &lt;ch_cluster_all&gt; &lt;!--分片1--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集1--&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片2--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集2--&gt; &lt;replica&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;!--分片3--&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;!--复制集3--&gt; &lt;replica&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;9002&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;supersecrect&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/ch_cluster_all&gt; &lt;/clickhouse_remote_servers&gt; &lt;!--zookeeper相关配置--&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;centos-1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;centos-2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;centos-3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;layer&gt;01&lt;/layer&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--分片号--&gt; &lt;replica&gt;node6&lt;/replica&gt; &lt;!--当前节点IP--&gt; &lt;/macros&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;!--压缩相关配置--&gt; &lt;clickhouse_compression&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;lz4&lt;/method&gt; &lt;!--压缩算法lz4压缩比zstd快, 更占磁盘--&gt; &lt;/case&gt; &lt;/clickhouse_compression&gt;&lt;/yandex&gt; 用户密码生成方式密码生成方式： https://clickhouse.tech/docs/en/operations/settings/settings_users/ shell命令行执行： 1PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d &#x27;-&#x27; 输出结果 12X1UNizCS（明文密码）379444c5e109e2f7e5f284831db54cd955011e6ae27d6e7a572cca635fbc7b1d （加密长密码, 即password_sha256_hex） 修改属主1cd /data &amp;&amp; chown -R clickhouse.clickhouse clickhouse 进程守护使用systemd管理 以node1为例 12345678910111213141516171819202122232425# vim /etc/systemd/system/clickhouse_node1.service [Unit]Description=ClickHouse Server (analytic DBMS for big data)Requires=network-online.targetAfter=network-online.target [Service]#Type=simpleType=forkingUser=clickhouseGroup=clickhouseRestart=alwaysRestartSec=30RuntimeDirectory=clickhouse-server#ExecStart=/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --pid-file=/run/clickhouse-server/clickhouse-server.pidExecStart=/usr/bin/clickhouse-server --daemon --config=/data/clickhouse/ch_9000/config.xml --pid-file=/data/clickhouse/node1/clickhouse-server.pid#PIDFile=/data/clickhouse/node1/clickhouse-server.pidLimitCORE=infinityLimitNOFILE=500000CapabilityBoundingSet=CAP_NET_ADMIN CAP_IPC_LOCK CAP_SYS_NICE [Install]WantedBy=multi-user.target 以上面的配置作为模板创建 node1~node6 的systemd配置文件 ClickHouse服务启动123456789101112131415161718192021222324252627centos-1主机进行如下操作：systemctl start clickhouse_node1.servicesystemctl start clickhouse_node4.service centos-2主机进行如下操作：systemctl start clickhouse_node2.servicesystemctl start clickhouse_node5.service centos-3主机进行如下操作：systemctl start clickhouse_node3.servicesystemctl start clickhouse_node6.service 验证如下端口是否被监听：netstat -anlp|grep 9000 （clickhouse tcp端口）netstat -anlp|grep 9002 （clickhouse tcp端口）netstat -anlp|grep 8123 (clickhouse http端口)netstat -anlp|grep 8124 (clickhouse http端口)netstat -anlp|grep 9009 (clickhouse 数据交互端口)netstat -anlp|grep 9010 (clickhouse 数据交互端口) 登陆方式：clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;show databases&quot; 测试集群功能是否正常创建数据库testdb1234567891011121314151617181920# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1ClickHouse client version 20.3.4.10 (official build).Connecting to localhost:9000 as user default.Connected to ClickHouse server version 20.3.4 revision 54433.centos-1 :) create database testdb on cluster ch_cluster_all;CREATE DATABASE testdb ON CLUSTER ck_cluster┌─host─────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐│ centos-3 │ 9000 │ 0 │ │ 5 │ 0 ││ centos-2 │ 9000 │ 0 │ │ 4 │ 0 ││ centos-1 │ 9002 │ 0 │ │ 3 │ 0 ││ centos-3 │ 9002 │ 0 │ │ 2 │ 0 ││ centos-1 │ 9000 │ 0 │ │ 1 │ 0 ││ centos-2 │ 9002 │ 0 │ │ 0 │ 0 │└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘6 rows in set. Elapsed: 0.107 sec. 创建本地表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-01/sbtest&#x27;,&#x27;node1&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-1 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-02/sbtest&#x27;,&#x27;node4&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot;clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-2 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-02/sbtest&#x27;,&#x27;node2&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-2 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-03/sbtest&#x27;,&#x27;node5&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot;clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-3 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-03/sbtest&#x27;,&#x27;node3&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-3 --query=&quot;CREATE TABLE testdb.sbtest_local(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01-01/sbtest&#x27;,&#x27;node6&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SETTINGS index_granularity = 8192;&quot; 创建分布式表1234567891011121314151617181920212223242526272829303132333435363738394041clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-1 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot;clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-2 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-2 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot;clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-3 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot;clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-3 --query=&quot;CREATE TABLE testdb.sbtest (EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = Distributed(ck_cluster,testdb, sbtest_local, rand())&quot; 验证数据复制状态：写入分片1组的写入节点的local表1# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;insert into testdb.sbtest_local VALUES (now(), 10000, 10000)&quot; 在分片1组local表可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:02:31 10000 10000 在分片1 副本节点local读取可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-3 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:02:31 10000 10000 写入分片2组的写入节点的local表1# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-2 --query=&quot;insert into testdb.sbtest_local VALUES (now(), 20000, 20000)&quot; 在分片2组local表可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-2 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:03:31 20000 20000 在分片2 副本节点local读取可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-1 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:03:31 20000 20000 写入分片3组的写入节点的local表1# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-3 --query=&quot;insert into testdb.sbtest_local VALUES (now(), 30000, 30000)&quot; 在分片3组local表可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-3 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:04:39 30000 30000 在分片3 副本节点local读取可见一条记录12# clickhouse-client -u default --password xxxxxx --port 9002 -hcentos-2 --query=&quot;select * from testdb.sbtest_local&quot;2020-05-02 23:04:39 30000 30000 Distributed表验证1234# clickhouse-client -u default --password xxxxxx --port 9000 -hcentos-1 --query=&quot;select * from testdb.sbtest&quot;2020-05-02 23:02:31 10000 100002020-05-02 23:03:31 20000 200002020-05-02 23:04:39 30000 30000 可以看到结果是之前写入的3条记录 结论，clickhouse基本功能正常 macros建表集群后期规模较大，创建分布式表时很复杂，clickhouse提供了在配置文件定义macros，创建表使用&#123;&#125;替代具体参数的方式，让表创建更透明方便。 可以在config.xml和metrika.xml中搜索一下macros找到相应的配置 使用以下语句连接一个节点即可再集群所有节点创建好对应的表 12345678910111213# ReplicatedMergeTree表示例：CREATE TABLE testdb.sbtest_local on cluster ch_cluster_all(EventDate DateTime,CounterID UInt32,UserID UInt32) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/sbtest_local&#x27;, &#x27;&#123;replica&#125;&#x27;)PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID)); # Distributed表示例：CREATE TABLE testdb.sbtest ON CLUSTER ch_cluster_all AS testdb.sbtest_local engine = Distributed(ch_cluster_all, testdb, sbtest_local, rand()); ClickHouse用户管理据我了解ClickHouse没有创建用户的语句(现在已经有了), 创建用户的方式是增加配置参数, ClickHouse会自动检测配置文件变化, 所以无需重启服务 在之前的配置中我们制定了如下配置 1&lt;users_config&gt;/data/clickhouse/node1/users.xml&lt;/users_config&gt; 这表示用户配置在这个文件下. 其实根据官方文档介绍, 我们可以在/data/clickhouse/node1目录下创建users.d目录, 为每个用户创建单独的配置文件, 这样就不必在一个users.xml中维护所有用户信息了, ClickHouse会自动合并/data/clickhouse/node1/users.xml和/data/clickhouse/node1/users.d/*.xml, 生产一个”运行时”配置/data/clickhouse/node1/preprocessed_configs/users.xml 例如我在users.d创建了两个用户 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@centos-1 node1]# cd users.d/[root@centos-1 users.d]# lltotal 12-rwxr-xr-x 1 clickhouse clickhouse 1025 Apr 29 17:46 fanboshi.xml-rwxr-xr-x 1 clickhouse clickhouse 1035 Apr 30 16:35 monitor_ro.xml[root@centos-1 users.d]# cat fanboshi.xml &lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;fanboshi&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;/fanboshi&gt; &lt;/profiles&gt; &lt;users&gt; &lt;fanboshi&gt; &lt;password_sha256_hex&gt;fanboshi_user_password_sha256_hex&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;fanboshi&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/fanboshi&gt; &lt;/users&gt;&lt;/yandex&gt;[root@centos-1 users.d]# cat monitor_ro.xml &lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;monitor_ro&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;/monitor_ro&gt; &lt;/profiles&gt; &lt;users&gt; &lt;monitor_ro&gt; &lt;password_sha256_hex&gt;monitor_ro_user_password_sha256_hex&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;monitor_ro&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/monitor_ro&gt; &lt;/users&gt;&lt;/yandex&gt; 目前我个人建议将一个用户的profile, quota等信息都配置到一个配置文件中, 避免混乱 如此配置之后, /data/clickhouse/node1/preprocessed_configs/users.xml就会自动合并我们的所有用户配置, 生成如下配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;!-- This file was generated automatically. Do not edit it: it is likely to be discarded and generated again before it&#x27;s read next time. Files used to generate this file: /data/clickhouse/node1/users.xml /data/clickhouse/node1/users.d/fanboshi.xml --&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;default&gt; &lt;max_memory_usage&gt;4294967296&lt;/max_memory_usage&gt; &lt;!-- 每个 session 的内存限制 --&gt; &lt;max_bytes_before_external_group_by&gt;2147483648&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;2147483648&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;1&lt;/max_threads&gt; &lt;/default&gt; &lt;readonly&gt; &lt;max_threads&gt;1&lt;/max_threads&gt; &lt;max_memory_usage&gt;4294967296&lt;/max_memory_usage&gt; &lt;max_bytes_before_external_group_by&gt;2147483648&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;2147483648&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;/readonly&gt; &lt;fanboshi&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;/fanboshi&gt; &lt;/profiles&gt; &lt;quotas&gt; &lt;default&gt; &lt;interval&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; &lt;/quotas&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;default_user_password_sha256_hex&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;ch_ro&gt; &lt;password_sha256_hex&gt;ch_ro_user_password_sha256_hex&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/ch_ro&gt; &lt;fanboshi&gt; &lt;password_sha256_hex&gt;fanboshi_user_password_sha256_hex&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;fanboshi&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/fanboshi&gt; &lt;/users&gt; &lt;/yandex&gt; 注意看这里 12345&lt;!-- This file was generated automatically. Do not edit it: it is likely to be discarded and generated again before it&#x27;s read next time. Files used to generate this file: /data/clickhouse/node1/users.xml /data/clickhouse/node1/users.d/fanboshi.xml --&gt; 创建用户还是比较麻烦的, 要在所有节点增加配置文件, 建议还是使用ansible之类的工具配合, 这里我给出我们现在使用的ansible role的关键部分吧, 关键就是template和var吧 template 123456789101112131415161718192021222324252627282930313233#jinja2: lstrip_blocks:True&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;&#123;&#123; user.name &#125;&#125;&gt; &#123;% for key, value in profile.settings.items() %&#125; &lt;&#123;&#123; key &#125;&#125;&gt;&#123;&#123; value &#125;&#125;&lt;/&#123;&#123; key &#125;&#125;&gt; &#123;% endfor %&#125; &lt;/&#123;&#123; user.name &#125;&#125;&gt; &lt;/profiles&gt; &lt;users&gt; &lt;&#123;&#123; user.name &#125;&#125;&gt; &lt;password_sha256_hex&gt;&#123;&#123; password_sha256_hex &#125;&#125;&lt;/password_sha256_hex&gt; &lt;networks&gt; &#123;% for key, values in user.networks.items() %&#125; &#123;% for value in values %&#125; &lt;&#123;&#123; key &#125;&#125;&gt;&#123;&#123; value &#125;&#125;&lt;/&#123;&#123; key &#125;&#125;&gt; &#123;% endfor %&#125; &#123;% endfor %&#125; &lt;/networks&gt; &lt;profile&gt;&#123;&#123; user.name|default(&#x27;default&#x27;)&#125;&#125;&lt;/profile&gt; &lt;quota&gt;&#123;&#123;quota|default(&#x27;default&#x27;)&#125;&#125;&lt;/quota&gt; &#123;% if user.allow_databases|default(&#x27;&#x27;)|length &gt; 0 %&#125; &lt;allow_databases&gt; &#123;% for database in user.allow_databases %&#125; &lt;database&gt;&#123;&#123; database &#125;&#125;&lt;/database&gt; &#123;% endfor %&#125; &lt;/allow_databases&gt; &#123;% endif %&#125; &lt;/&#123;&#123; user.name &#125;&#125;&gt; &lt;/users&gt;&lt;/yandex&gt; var 12345678910111213141516171819202122232425---# vars file for clickhouse_add_usersprofile: settings: max_memory_usage: 54975581388# max_memory_usage_for_all_queries: 61847529062 max_bytes_before_external_group_by: 21474836480 max_bytes_before_external_sort: 21474836480 use_uncompressed_cache: 0 load_balancing: random distributed_aggregation_memory_efficient: 1 max_threads: 8 log_queries: 1 readonly: 0user: name: test password: &quot;&quot;# 不要定义password_sha256_hex, password_sha256_hex应该根据password生成 networks: ip: - ::/0 quota: default allow_databases: [] clickhouse-client prompt调整根据官方文档描述: clickhouse-client uses the first existing file of the following: Defined in the --config-file parameter. ./clickhouse-client.xml ~/.clickhouse-client/config.xml /etc/clickhouse-client/config.xml 查看/etc/clickhouse-client/config.xml(yum安装会自动创建这个配置), 发现有示例配置 12345&lt;prompt_by_server_display_name&gt; &lt;default&gt;&#123;display_name&#125; :) &lt;/default&gt; &lt;test&gt;&#123;display_name&#125; \\x01\\e[1;32m\\x02:)\\x01\\e[0m\\x02 &lt;/test&gt; &lt;!-- if it matched to the substring &quot;test&quot; in the server display name - --&gt; &lt;production&gt;&#123;display_name&#125; \\x01\\e[1;31m\\x02:)\\x01\\e[0m\\x02 &lt;/production&gt; &lt;!-- if it matched to the substring &quot;production&quot; in the server display name --&gt;&lt;/prompt_by_server_display_name&gt; 于是调整一下, 比如default改成 1&lt;default&gt;&#123;display_name&#125; &#123;user&#125;@&#123;host&#125;:&#123;port&#125; [&#123;database&#125;]\\n\\x01\\e[1;31m\\x02:)\\x01\\e[0m\\x02 &lt;/default&gt; 12345678#clickhouse-client -ufanboshi --ask-password ClickHouse client version 20.3.5.21 (official build).Password for user (fanboshi): Connecting to localhost:9000 as user fanboshi.Connected to ClickHouse server version 20.3.5 revision 54433.bj2-clickhouse-all-prod-01 fanboshi@localhost:9000 [default]:) --其实笑脸是红色的.. 其实这个配置还不是很理解, 因为你可以注意到我这里使用的用户是fanboshi, 而非default但却也应用到了这个prompt, 我怀疑是因为profile是default 123456789101112131415161718192021222324252627#cat /data/clickhouse/ch_9000/users.d/fanboshi.xml &lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;profiles&gt; &lt;fanboshi&gt; &lt;max_memory_usage&gt;54975581388&lt;/max_memory_usage&gt; &lt;max_memory_usage_for_all_queries&gt;61847529062&lt;/max_memory_usage_for_all_queries&gt; &lt;max_bytes_before_external_group_by&gt;21474836480&lt;/max_bytes_before_external_group_by&gt; &lt;max_bytes_before_external_sort&gt;21474836480&lt;/max_bytes_before_external_sort&gt; &lt;use_uncompressed_cache&gt;0&lt;/use_uncompressed_cache&gt; &lt;load_balancing&gt;random&lt;/load_balancing&gt; &lt;distributed_aggregation_memory_efficient&gt;1&lt;/distributed_aggregation_memory_efficient&gt; &lt;max_threads&gt;10&lt;/max_threads&gt; &lt;log_queries&gt;1&lt;/log_queries&gt; &lt;/fanboshi&gt; &lt;/profiles&gt; &lt;users&gt; &lt;fanboshi&gt; &lt;password_sha256_hex&gt;supersecurt&lt;/password_sha256_hex&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/fanboshi&gt; &lt;/users&gt;&lt;/yandex&gt; 实现类似mysql免输用户密码登录我们都知道mysql cli可以通过在my.cnf添加如下参数实现快速登录 123[client]user=xxpassword=xxx clickhouse-client也可以 根据官方文档描述, 在client config文件中添加如下参数即可 1234567# vim ~/.clickhouse-client/config.xml &lt;config&gt; &lt;user&gt;username&lt;/user&gt; &lt;password&gt;password&lt;/password&gt; &lt;secure&gt;False&lt;/secure&gt;&lt;/config&gt; 疑问看了很多大佬的文章, 大家都是建议给应用提供一个负载均衡写本地表读分布式表, 而不直接通过分布式表写数据 我们搞技术的肯定不愿意人云亦云, 别人说什么就信什么, 当然前任经验肯定是有价值的 不过我想来想去还是有些没想明白直接写分布式表有什么致命缺陷, 于是在ClickHouse中文社区提了一个问题), 内容如下: 不过至今无人答复 看了sina高鹏大佬的分享 看了 https://github.com/ClickHouse/ClickHouse/issues/1854 还看了一些文章都是建议写本地表而不是分布式表 如果我设置internal_replication=true, 使用ReplicatedMergeTree引擎, 除了写本地表更灵活可控外, 写分布式表到底有什么致命缺陷吗? 因为要给同事解释, 只说一个大家说最佳实践是这样是不行的… 我自己也没理解到底写分布式表有啥大缺陷 如果说造成数据分布不均匀, sharding key我设为rand()还会有很大不均匀吗? 如果说扩容, 我也可以通过调整weight控制数据尽量写入新shared啊? 难道是因为文档中这段: 123Data is written asynchronously. When inserted in the table, the data block is just written to the local file system. The data is sent to the remote servers in the background as soon as possible. The period for sending data is managed by the distributed_directory_monitor_sleep_time_ms and distributed_directory_monitor_max_sleep_time_ms settings. The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the distributed_directory_monitor_batch_inserts setting. This setting improves cluster performance by better utilizing local server and network resources. You should check whether data is sent successfully by checking the list of files (data waiting to be sent) in the table directory: /var/lib/clickhouse/data/database/table/.If the server ceased to exist or had a rough restart (for example, after a device failure) after an INSERT to a Distributed table, the inserted data might be lost. If a damaged data part is detected in the table directory, it is transferred to the ‘broken’ subdirectory and no longer used. 上面文档内容我理解意思是说假如我有S1 S2 S3 三个节点,每个节点都有local表和分布式表. 我向S1的分布式表写数据1, 2, 3 1写入S1, 2,3先写到S1本地文件系统, 然后异步发送到S2 S3 , 比如2发给S2, 3发给S3, 如果此时S3宕机了, 则3发到S3失败, 但是1,2还是成功写到S1,S2了? 所以整个过程不能保证原子性? 出现问题还要人为修数据? https://github.com/ClickHouse/ClickHouse/issues/1343 这个issue说S3 come back后S1会尝试重新发送数据给S3. Data blocks are written in /var/lib/clickhouse/data/database/table/ folder. Special thread checks directory periodically and tries to send data. If it can’t, it will try next time. 那么只剩文档最后一句意思是如果S1过程中宕机, 会丢数据? 再就是weight是分片级别的, 不是表级别的, 灵活性差? 已经有答案了, 详见ClickHouse到底改写本地表还是分布式表","categories":[],"tags":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://fuxkdb.com/tags/ClickHouse/"}]},{"title":"Zookeeper3.6.0集群部署文档","slug":"2020-04-15-Zookeeper3.6.0集群部署文档","date":"2020-04-15T15:32:00.000Z","updated":"2020-04-15T15:33:28.928Z","comments":true,"path":"2020/04/15/2020-04-15-Zookeeper3.6.0集群部署文档/","link":"","permalink":"http://fuxkdb.com/2020/04/15/2020-04-15-Zookeeper3.6.0%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/","excerpt":"Zookeeper3.6.0集群部署文档一点一点完善…","text":"Zookeeper3.6.0集群部署文档一点一点完善… 下载安装包https://zookeeper.apache.org/releases.html 12345cd /tmp &amp;&amp; \\wget https://downloads.apache.org/zookeeper/zookeeper-3.6.0/apache-zookeeper-3.6.0-bin.tar.gztar -zxvf apache-zookeeper-3.6.0-bin.tar.gz -C /usr/local/ln -s /usr/local/apache-zookeeper-3.6.0-bin /usr/local/zookeeper 配置环境变量1234vi ~/.bashrcexport ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 修改配置文件zoo.cfg12345678910111213141516171819202122cat zoo.cfg# 抄的clickhouse官网# https://clickhouse.tech/docs/en/operations/tips/#zookeepertickTime=2000initLimit=30000syncLimit=10maxClientCnxns=2000maxSessionTimeout=60000000autopurge.snapRetainCount=10autopurge.purgeInterval=1preAllocSize=131072snapCount=3000000leaderServes=yesstandaloneEnabled=falsereconfigEnabled=true4lw.commands.whitelist=*dataDir=/data/zookeeper/test/datadataLogDir=/data/zookeeper/test/logsdynamicConfigFile=/usr/local/apache-zookeeper-3.6.0-bin/conf/zoo_replicated1.cfg.dynamic4lw.commands.whitelist=* dynamicConfigFilevim /usr/local/apache-zookeeper-3.6.0-bin/conf/zoo_replicated1.cfg.dynamic 三个节点一样 1234#cat zoo_replicated1.cfg.dynamicserver.1=172.16.24.2:2888:3888:participant;2181server.2=172.16.24.13:2888:3888:participant;2181server.3=172.16.24.109:2888:3888:participant;2181 注意不能用0.0.0.0, 否则有bug 用于客户端连接的端口clientPort: 2181用于节点间通信的TCP端口peerPort: 2888用于首领选举的TCP端口leaderPort: 3888 participant代表参与者 myid12345678#masterecho &quot;1&quot;&gt;/data/zookeeper/ch_9000/data/myid#slave1echo &quot;2&quot;&gt;/data/zookeeper/ch_9000/data/myid#slave2echo &quot;3&quot;&gt;/data/zookeeper/ch_9000/data/myid 配置zk日志的滚动输入 看bin/zkEnv.sh 里面 默认zk日志输出到一个文件,且不会自动清理,所以,一段时间后zk日志会非常大!这里配置zk日志滚动输出,且每个文件10M限制,最多保留10个文件. zookeeper-env.sh./conf目录下新建zookeeper-env.sh文件,修改到sudo chmod 755 zookeeper-env.sh权限 1234567#cat conf/zookeeper-env.sh#!/usr/bin/env bash#tip:custom configurationfile，do not amend the zkEnv.sh file#chang the log dir and output of rolling fileZOO_LOG_DIR=&quot;/usr/local/zookeeper/logs&quot;ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot; log4j.properties 修改日志的输入形式 1234567891011zookeeper.root.logger=INFO, ROLLINGFILE#zookeeper.root.logger=INFO, CONSOLEzookeeper.console.threshold=INFOzookeeper.log.dir=/usr/local/zookeeper/logszookeeper.log.file=zookeeper.logzookeeper.log.threshold=INFOzookeeper.log.maxfilesize=256MB --要改就改这个zookeeper.log.maxbackupindex=20 --要改就改这个 mkdir /usr/local/zookeeper/logs 123456789### 配置运行zk的jvm&gt; 看bin/zkEnv.sh 里面`./conf`目录下新建`java.env`文件,修改到`sudo chmod 755 java.env`权限,主要用于`GC log`,`RAM`等的配置. #!/usr/bin/env bash #config the jvm parameter in a reasonable #note that the shell is source in so that do not need to use export #set java classpath #CLASSPATH=&quot;&quot; #set jvm start parameter , also can set JVMFLAGS variable SERVER_JVMFLAGS=&quot;-Xms1024m -Xmx2048m $JVMFLAGS&quot; 12345## 启动zookeeper服务(所有节点) # zkServer.sh start ZooKeeper JMX enabled by default Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED 12345678910## 遇到问题### 问题1使用stat验证zookeeper服务时报错 #telnet 127.0.0.1 2181 Trying 127.0.0.1... Connected to 127.0.0.1. Escape character is &apos;^]&apos;. stat stat is not executed because it is not in the whitelist. Connection closed by foreign host. 12345678910这里出问题了. 3.5.3以后新增参数`4lw.commands.whitelist`https://zookeeper.apache.org/doc/r3.6.0/zookeeperAdmin.html### 问题2之前我是这样配置的dynamicConfigFile的三个节点, 自己都是`0.0.0.0` #cat zoo_ch_9000.cfg.dynamic server.1=0.0.0.0:2888:3888:participant;0.0.0.0:2181 server.2=172.16.24.13:2888:3888:participant;0.0.0.0:2181 server.3=172.16.24.109:2888:3888:participant;0.0.0.0:2181 1 #cat zoo_ch_9000.cfg.dynamic server.1=172.16.24.2:2888:3888:participant;0.0.0.0:2181 server.2=0.0.0.0:2888:3888:participant;0.0.0.0:2181 server.3=172.16.24.109:2888:3888:participant;0.0.0.0:2181 1 #cat zoo_ch_9000.cfg.dynamic server.1=172.16.24.2:2888:3888:participant;0.0.0.0:2181 server.2=172.16.24.13:2888:3888:participant;0.0.0.0:2181 server.3=0.0.0.0:2888:3888:participant;0.0.0.0:2181 123这样装完以后能用, 但是myid=1挂掉重启后一直无法加入 2020-04-15 16:03:24,420 [myid:1] - INFO [WorkerSender[myid=1]:QuorumCnxManager@462] - Have smaller server identifier, so dropping the connection: (3, 1) 2020-04-15 16:03:24,622 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumCnxManager@462] - Have smaller server identifier, so dropping the connection: (2, 1) 2020-04-15 16:03:24,623 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumCnxManager@462] - Have smaller server identifier, so dropping the connection: (3, 1) 2020-04-15 16:03:24,623 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 400 2020-04-15 16:03:25,024 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumCnxManager@462] - Have smaller server identifier, so dropping the connection: (2, 1) 2020-04-15 16:03:25,025 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):QuorumCnxManager@462] - Have smaller server identifier, so dropping the connection: (3, 1) 2020-04-15 16:03:25,025 [myid:1] - INFO [QuorumPeer[myid=1](plain=0.0.0.0:2181)(secure=disabled):FastLeaderElection@966] - Notification time out: 800 貌似是bug https://issues.apache.org/jira/browse/ZOOKEEPER-2938 用3.4.14的配置启动3.6.0 仍然有此问题, 说明可能不是配置问题反复测过几次就是0.0.0.0的问题, 实际上我在来云账户之前从没有用过0.0.0.0, 之前马蜂窝的服务器也是双网卡, 我看过运维的kafka和大数据的kafka都没有使用过0.0.0.0这种方式, 来到这边才看到这种用法, 本着”可能有坑、与线上统一”的原则继承了这样的配置, 实际对这种配置我还是不太理解, 虽然百度了一下说是ECS或Docker不这样配有问题","categories":[],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://fuxkdb.com/tags/ZooKeeper/"}]},{"title":"Kafka版本升级","slug":"2020-04-15-Kafka版本升级","date":"2020-04-15T15:29:00.000Z","updated":"2020-04-16T02:46:15.915Z","comments":true,"path":"2020/04/15/2020-04-15-Kafka版本升级/","link":"","permalink":"http://fuxkdb.com/2020/04/15/2020-04-15-Kafka%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7/","excerpt":"Kafka版本升级本文档介绍kafka_2.11_2.1.1升级到2.12-2.4.1的具体操作方法. 其他版本大同小异, 详见1.5 Upgrading From Previous Versions","text":"Kafka版本升级本文档介绍kafka_2.11_2.1.1升级到2.12-2.4.1的具体操作方法. 其他版本大同小异, 详见1.5 Upgrading From Previous Versions 升级前检查项1.确认是否有副本因子是1的Topic1bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep &quot;ReplicationFactor:[[:space:]]*1&quot; 单副本Topic在rolling restart过程中必定会受到影响 2.确认是否有只有一个Isr的Topic12345678910111213bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep Replicas|while read line;do echo &quot;$(awk -F&#x27;Topic:&#x27; &#x27;&#123;print $NF&#125;&#x27; &lt;&lt;&lt; &quot;$&#123;line&#125;&quot;|awk -F &#x27;Partition:&#x27; &#x27;&#123;print $1&#125;&#x27;|sed &#x27;s/[ \\t]*//g&#x27; ) Isr: $(awk -F&#x27;Isr:&#x27; &#x27;&#123;print $NF&#125;&#x27; &lt;&lt;&lt; &quot;$&#123;line&#125;&quot;|tr &quot;,&quot; &quot;\\n&quot;|sort -n|tr &quot;\\n&quot; &quot;,&quot;|sed &#x27;s/,$/\\n/g;&#x27;|sed &#x27;s/[ \\t]*//g&#x27; )&quot;;done|awk &#x27;&#123;printf &quot;%-64s%-4s%-23s\\n&quot;,$1,$2,$3&#125;&#x27;test.t_batch Isr:8,13,174 test.t_template Isr:8,13,174 test.t_user_apply Isr:8,13,174 test.t_user_order Isr:8,13,174 test.t_user Isr:8,13,174 test.t_user_job_record Isr:8,13,174 使用如下语句输出只有一个Isr的Topic. (if (length(a) == 1)bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep Replicas|awk &#x27;&#123;split($NF,a,&quot;,&quot;);slen=asort(a,tA);&#125;&#123;if (length(a) == 1)&#123;&#123;printf $2&quot; : &quot;;for (i = 1; i &lt;= length(a); i++)&#123;if (i==length(a)&amp;&amp;i==1)&#123;printf tA[i];&#125;else&#123;printf tA[i]&quot;,&quot;;&#125;&#125;&#125;&#123;printf &quot;\\n&quot;;&#125;&#125;&#125;&#x27;|awk &#x27;&#123;printf &quot;%-64s%-23s\\n&quot;,$1,$3&#125;&#x27; 只有一个Isr的在rolling restart过程中必定会受到影响 增加Topic副本数当上一步检查步骤存在问题Topic时, 需要针对这些Topic增加副本, 或等待Isr&gt;1. 本次线上升级我们已知所有Topic都只有一个副本, 所以全部需要增加副本数 增加副本会触发数据复制, 可能导致网卡打满, IO增高, 还是会影响业务 增加副本数Kafka 支持为已有 topic 的分区增加副本因子(replication factor)，具体的方法就是使用kafka-reassign-partitions.sh并且为 topic分区增加额外的副本 理论上不影响生产和消费, 实测也确实没影响 生成json文件我们先为__consumer_offsets增加副本, 先describe一下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#bin/kafka-topics.sh --zookeeper localhost:2182 --topic __consumer_offsets --describeTopic: __consumer_offsets PartitionCount: 50 ReplicationFactor: 1 Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600 Topic: __consumer_offsets Partition: 0 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 1 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 2 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 3 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 4 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 5 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 6 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 7 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 8 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 9 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 10 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 11 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 12 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 13 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 14 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 15 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 16 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 17 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 18 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 19 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 20 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 21 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 22 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 23 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 24 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 25 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 26 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 27 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 28 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 29 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 30 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 31 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 32 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 33 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 34 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 35 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 36 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 37 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 38 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 39 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 40 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 41 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 42 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 43 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 44 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 45 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 46 Leader: 174 Replicas: 174 Isr: 174 Topic: __consumer_offsets Partition: 47 Leader: 8 Replicas: 8 Isr: 8 Topic: __consumer_offsets Partition: 48 Leader: 13 Replicas: 13 Isr: 13 Topic: __consumer_offsets Partition: 49 Leader: 174 Replicas: 174 Isr: 174 目前副本因子是1, 现在我们要把它重设成3, 创建json文件/tmp/incr_replica.json 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[13,174,8]&#125;]&#125; 执行重分配工作1234567[root@bj1-mysql-dba-prod-01 kafka_2.11-2.1.1]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2182 --reassignment-json-file /tmp/incr_replica.json --executeCurrent partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[13],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[8],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[174],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions. 验证重分配是否成功12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152bin/kafka-reassign-partitions.sh --zookeeper localhost:2182 --reassignment-json-file /tmp/incr_replica.json --verifyStatus of partition reassignment: Reassignment of partition __consumer_offsets-22 completed successfullyReassignment of partition __consumer_offsets-30 completed successfullyReassignment of partition __consumer_offsets-8 completed successfullyReassignment of partition __consumer_offsets-21 completed successfullyReassignment of partition __consumer_offsets-4 completed successfullyReassignment of partition __consumer_offsets-27 completed successfullyReassignment of partition __consumer_offsets-7 completed successfullyReassignment of partition __consumer_offsets-9 completed successfullyReassignment of partition __consumer_offsets-46 completed successfullyReassignment of partition __consumer_offsets-25 completed successfullyReassignment of partition __consumer_offsets-35 completed successfullyReassignment of partition __consumer_offsets-41 completed successfullyReassignment of partition __consumer_offsets-33 completed successfullyReassignment of partition __consumer_offsets-23 completed successfullyReassignment of partition __consumer_offsets-49 completed successfullyReassignment of partition __consumer_offsets-47 completed successfullyReassignment of partition __consumer_offsets-16 completed successfullyReassignment of partition __consumer_offsets-28 completed successfullyReassignment of partition __consumer_offsets-31 completed successfullyReassignment of partition __consumer_offsets-36 completed successfullyReassignment of partition __consumer_offsets-42 completed successfullyReassignment of partition __consumer_offsets-3 completed successfullyReassignment of partition __consumer_offsets-18 completed successfullyReassignment of partition __consumer_offsets-37 completed successfullyReassignment of partition __consumer_offsets-15 completed successfullyReassignment of partition __consumer_offsets-24 completed successfullyReassignment of partition __consumer_offsets-38 completed successfullyReassignment of partition __consumer_offsets-17 completed successfullyReassignment of partition __consumer_offsets-48 completed successfullyReassignment of partition __consumer_offsets-19 completed successfullyReassignment of partition __consumer_offsets-11 completed successfullyReassignment of partition __consumer_offsets-13 completed successfullyReassignment of partition __consumer_offsets-2 completed successfullyReassignment of partition __consumer_offsets-43 completed successfullyReassignment of partition __consumer_offsets-6 completed successfullyReassignment of partition __consumer_offsets-14 completed successfullyReassignment of partition __consumer_offsets-20 completed successfullyReassignment of partition __consumer_offsets-0 completed successfullyReassignment of partition __consumer_offsets-44 completed successfullyReassignment of partition __consumer_offsets-39 completed successfullyReassignment of partition __consumer_offsets-12 completed successfullyReassignment of partition __consumer_offsets-45 completed successfullyReassignment of partition __consumer_offsets-1 completed successfullyReassignment of partition __consumer_offsets-5 completed successfullyReassignment of partition __consumer_offsets-26 completed successfullyReassignment of partition __consumer_offsets-29 completed successfullyReassignment of partition __consumer_offsets-34 completed successfullyReassignment of partition __consumer_offsets-10 completed successfullyReassignment of partition __consumer_offsets-32 completed successfullyReassignment of partition __consumer_offsets-40 completed successfully 再describe确认一下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@bj1-mysql-dba-prod-01 kafka_2.11-2.1.1]# bin/kafka-topics.sh --zookeeper localhost:2182 --topic __consumer_offsets --describeTopic:__consumer_offsets PartitionCount:50 ReplicationFactor:3 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer Topic: __consumer_offsets Partition: 0 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 1 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 2 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 3 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 4 Leader: 174 Replicas: 13,174,8 Isr: 174,13,8 Topic: __consumer_offsets Partition: 5 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 6 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 7 Leader: 174 Replicas: 13,174,8 Isr: 174,13,8 Topic: __consumer_offsets Partition: 8 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 9 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 10 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 11 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 12 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 13 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 14 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 15 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 16 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 17 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 18 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 19 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 20 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 21 Leader: 13 Replicas: 13,174,8 Isr: 13,174,8 Topic: __consumer_offsets Partition: 22 Leader: 174 Replicas: 13,174,8 Isr: 174,13,8 Topic: __consumer_offsets Partition: 23 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 24 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 25 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 26 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 27 Leader: 13 Replicas: 13,174,8 Isr: 13,174,8 Topic: __consumer_offsets Partition: 28 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 29 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 30 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 31 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 32 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 33 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 34 Leader: 174 Replicas: 13,174,8 Isr: 174,13,8 Topic: __consumer_offsets Partition: 35 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 36 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 37 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 38 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 39 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 40 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 41 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 42 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 43 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 44 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 45 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 46 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 Topic: __consumer_offsets Partition: 47 Leader: 8 Replicas: 13,174,8 Isr: 8,174,13 Topic: __consumer_offsets Partition: 48 Leader: 13 Replicas: 13,174,8 Isr: 13,8,174 Topic: __consumer_offsets Partition: 49 Leader: 174 Replicas: 13,174,8 Isr: 174,8,13 副本是3, 都是Isr, 这样就OK了 为其他topic增加副本1bin/kafka-topics.sh --zookeeper localhost:2182 --describe| grep &quot;Topic: &quot;| grep -v &quot;__consumer_offsets&quot;|awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27; |awk &#x27;&#123;print $1&#125;&#x27; 12345678910111213&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;pd.t1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;pd.t2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;pd.t3&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;pd.t4&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t5&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t6&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t7&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t8&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t9&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;,&#123;&quot;topic&quot;:&quot;bk.t10&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[13,174,8]&#125;]&#125; 观察副本增加情况123456789101112返回必须为空bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep &quot;ReplicationFactor:[[:space:]]*1&quot;输出Isr需要&gt;1bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep Replicas|while read line;do echo &quot;$(awk -F&#x27;Topic:&#x27; &#x27;&#123;print $NF&#125;&#x27; &lt;&lt;&lt; &quot;$&#123;line&#125;&quot;|awk -F &#x27;Partition:&#x27; &#x27;&#123;print $1&#125;&#x27;|sed &#x27;s/[ \\t]*//g&#x27; ) Isr: $(awk -F&#x27;Isr:&#x27; &#x27;&#123;print $NF&#125;&#x27; &lt;&lt;&lt; &quot;$&#123;line&#125;&quot;|tr &quot;,&quot; &quot;\\n&quot;|sort -n|tr &quot;\\n&quot; &quot;,&quot;|sed &#x27;s/,$/\\n/g;&#x27;|sed &#x27;s/[ \\t]*//g&#x27; )&quot;;done|awk &#x27;&#123;printf &quot;%-64s%-4s%-23s\\n&quot;,$1,$2,$3&#125;&#x27;使用如下语句输出只有一个Isr的Topic. (if (length(a) == 1)bin/kafka-topics.sh --zookeeper localhost:2182 --describe|grep Replicas|awk &#x27;&#123;split($NF,a,&quot;,&quot;);slen=asort(a,tA);&#125;&#123;if (length(a) == 1)&#123;&#123;printf $2&quot; : &quot;;for (i = 1; i &lt;= length(a); i++)&#123;if (i==length(a)&amp;&amp;i==1)&#123;printf tA[i];&#125;else&#123;printf tA[i]&quot;,&quot;;&#125;&#125;&#125;&#123;printf &quot;\\n&quot;;&#125;&#125;&#125;&#x27;|awk &#x27;&#123;printf &quot;%-64s%-23s\\n&quot;,$1,$3&#125;&#x27;以下两个返回都需要为空bin/kafka-topics.sh --zookeeper localhost:2182 --describe --under-replicated-partitionsbin/kafka-topics.sh --zookeeper localhost:2182 --describe --unavailable-partitions 分区重分配增加副本数以后, prefferred replica是不变的, 在我这个例子里, 由于之前所有topic所有分区都只有一个副本112, 所以增加副本以后prefferred replica仍然是112. 虽然auto.leader.rebalance.enable默认为true, 但是由于kafka计算leader不均衡的方法是: Kafka计算不均衡程度的逻辑实际上非常简单——该broker上的leader不是preferred replica的分区数/ broker上总的分区数。 胡夕. Apache Kafka实战 (Kindle 位置 4563-4564). 电子工业出版社. Kindle 版本. 所以没有卵用 我们需要进行分区重分配, 目的是更改prefferred replica 以往进行重分配的场景是kafka集群增加了新的broker, 比如源集群k1 k2 k3, 新增k4 k5. 原有Topic不会自动迁移到新节点上, 为了让k4 k5也能干活, 进行重分配, 将一部分topic迁移到k4 k5 重分配后应该也会产生一些leader选举, 对客户端也会造成影响 构造topics-to-move-json-file我们需要指明一个需要进行分区重分配的topic列表(建议少量多次) 12# cat /tmp/topics-to-move.json&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;:&quot;ReconChannelMatch&quot;&#125;, &#123;&quot;topic&quot;:&quot;ReconImportBill&quot;&#125;, &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;&#125;], &quot;version&quot;:1&#125; 生产分配方案12345678# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file /tmp/topics-to-move.json --broker-list &quot;112,167,226&quot; --generateCurrent partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;ReconChannelMatch&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;ReconImportBill&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;Proposed partition reassignment configuration&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;ReconImportBill&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;ReconChannelMatch&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[112,226,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[167,112,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[112,167,226],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[226,112,167],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[167,226,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[226,167,112],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;[root@bj3-canal-canal-prod-01 kafka]# cat /tmp/topics-to-move.json&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;:&quot;ReconChannelMatch&quot;&#125;, &#123;&quot;topic&quot;:&quot;ReconImportBill&quot;&#125;, &#123;&quot;topic&quot;:&quot;__consumer_offsets&quot;&#125;], &quot;version&quot;:1&#125; 保存Proposed partition reassignment configuration后面的部分到 执行重分配1bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file /tmp/move_prefferred.json --execute 执行后可以使用--verify参数来查看分区重分配执行情况 1bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file /tmp/move_prefferred.json --verify 滚动升级以下步骤每个broker依次执行, 全部执行完成再进行下一个broker的操作 部署新版本kafka过程略, 只解压软件包, 复制旧版的配置文件server.properties, 增加如下参数配置 1234567inter.broker.protocol.version=2.1default.replication.factor=3offsets.topic.replication.factor=3transaction.state.log.replication.factor=3transaction.state.log.min.isr=2min.insync.replicas=2 逐一升级broker开两个窗口,分别启动一个producer和consumer 123bin/kafka-console-producer.sh --broker-list 172.16.x.x:9092,172.16.x.x:9092,172.16.x.x:9092 --topic test_upgradebin/kafka-console-consumer.sh --bootstrap-server 172.16.x.x:9092,172.16.x.x:9092,172.16.x.x:9092 --topic test_upgrade 测试一下producer写数据consumer是否可以消费到, 我们在下文中称这个步骤为T1 关闭旧版本broker12345678910# jps -m2224 WrapperSimpleApp CloudResetPwdUpdateAgent2432 ZooKeeperMain -server localhost:21829204 Jps -m21351 QuorumPeerMain /usr/local/zookeeper/bin/../conf/zoo_test.cfg32542 QuorumPeerMain /usr/local/zookeeper/bin/../conf/zoo.cfg28335 ZooKeeperMain -server localhost:218227279 Kafka config/server.properties# kill 27279 观察日志, 确认shutdown complete 123tail -f /usr/local/kafka_2.11-2.1.1/logs/kafkaServer.out INFO [ReplicaFetcher replicaId=8, leaderId=174, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread) 执行T1测试, 如果生产或消费失败, 则启动旧版本broker, 排查好问题后再次升级 使用新版本启动确认一下命令无输出 1bin/kafka-topics.sh --zookeeper localhost:2182 --describe --unavailable-partitions 启动 12345cd new_kafka_path./bin/kafka-server-start.sh -daemon config/server.properties确认无输出bin/kafka-topics.sh --zookeeper localhost:2182 --describe --under-replicated-partitions 执行T1测试, 如果生产或消费失败, 则启动旧版本broker, 排查好问题后再次升级 切换至新协议修改配置参数 1inter.broker.protocol.version=2.4 逐一重启broker","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://fuxkdb.com/tags/Kafka/"}]},{"title":"使用Ansible自动部署MGR中生成group_replication_group_seeds值的方法","slug":"2017-08-22-使用Ansible自动部署MGR中生成group_replication_group_seeds值的方法","date":"2020-03-20T04:23:00.000Z","updated":"2020-03-20T04:56:09.423Z","comments":true,"path":"2020/03/20/2017-08-22-使用Ansible自动部署MGR中生成group_replication_group_seeds值的方法/","link":"","permalink":"http://fuxkdb.com/2020/03/20/2017-08-22-%E4%BD%BF%E7%94%A8Ansible%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2MGR%E4%B8%AD%E7%94%9F%E6%88%90group_replication_group_seeds%E5%80%BC%E7%9A%84%E6%96%B9%E6%B3%95/","excerpt":"生成group_replication_group_seeds值的方法网卡不统一之前我是通过groups[&#39;mfw_test&#39;] | map(&#39;extract&#39;, hostvars, [&#39;ansible_all_ipv4_addresses&#39;])这种形式, 去一个inventory group下每一个host的ansible_all_ipv4_addresses ansible_all_ipv4_address是所有网卡上的ip, 包括手动添加的ip(例如vip) 123456&quot;msg&quot;: &#123; &quot;centos-1&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;192.168.124.136&quot;, &quot;172.16.120.10&quot; ],","text":"生成group_replication_group_seeds值的方法网卡不统一之前我是通过groups[&#39;mfw_test&#39;] | map(&#39;extract&#39;, hostvars, [&#39;ansible_all_ipv4_addresses&#39;])这种形式, 去一个inventory group下每一个host的ansible_all_ipv4_addresses ansible_all_ipv4_address是所有网卡上的ip, 包括手动添加的ip(例如vip) 123456&quot;msg&quot;: &#123; &quot;centos-1&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;192.168.124.136&quot;, &quot;172.16.120.10&quot; ], 我们可以通过下面的playbook查看hostvars 12- debug: msg: &quot;&#123;&#123; hostvars &#125;&#125;&quot; 上面的情况是没有vip, 也就是 1234567891011121314151617181920212223[root@centos-1 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b2:18:33 brd ff:ff:ff:ff:ff:ff inet 172.16.120.10/24 brd 172.16.120.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::e880:2501:cdab:b11d/64 scope link tentative dadfailed valid_lft forever preferred_lft forever inet6 fe80::ada9:2128:e605:334a/64 scope link tentative dadfailed valid_lft forever preferred_lft forever inet6 fe80::2f9b:31f4:73e6:e438/64 scope link tentative dadfailed valid_lft forever preferred_lft forever3: ens37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:2c:1a:a8 brd ff:ff:ff:ff:ff:ff inet 192.168.124.166/24 brd 192.168.124.255 scope global dynamic ens37 valid_lft 1452sec preferred_lft 1452sec inet6 fe80::5c28:fb1e:459:1b9/64 scope link valid_lft forever preferred_lft forever 如果有vip则是 1234567&quot;msg&quot;: &#123; &quot;centos-1&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;192.168.124.136&quot;, &quot;172.16.120.10&quot;, &quot;172.16.120.100&quot; ], 之前我为了解决现实服务器网卡名称不统一的问题(现实有叫em2的有叫p6p1的还有team0, eth0等等, 而cmdb中又没有这些信息), 采用了ansible_all_ipv4_addresses的方法, 取所有ip, 然后根据规则自己过滤(比如过滤掉vip等等). 时间长了就成了这么一坨 123456789101112131415&#123;%- for ip_list in (groups[group] | map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_all_ipv4_addresses&#x27;])) -%&#125; &#123;%- if loop.last -%&#125; &#123;%- for ip in ip_list -%&#125; &#123;%- if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;)) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125; &#123;&#123; ip &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125; &#123;%- endif -%&#125; &#123;%- endfor %&#125; &#123;%- else -%&#125; &#123;%- for ip in ip_list -%&#125; &#123;%- if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;)) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125; &#123;&#123; ip &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125;, &#123;%- endif -%&#125; &#123;%- endfor -%&#125; &#123;%- endif -%&#125;&#123;%- endfor -%&#125; 写到vars/main.yaml里 1group_replication_group_seeds: &quot;&#123;%- for ip_list in (groups[group] | map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_all_ipv4_addresses&#x27;])) -%&#125;&#123;%- if loop.last -%&#125;&#123;%- for ip in ip_list -%&#125;&#123;%- if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;)) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125;&#123;&#123; ip &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125;&#123;%- endif -%&#125;&#123;%- endfor %&#125;&#123;%- else -%&#125;&#123;%- for ip in ip_list -%&#125;&#123;%- if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;)) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125;&#123;&#123; ip &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125;,&#123;%- endif -%&#125;&#123;%- endfor -%&#125;&#123;%- endif -%&#125;&#123;%- endfor -%&#125;&quot; 网卡统一如果网卡是统一的, 或者我们能知道要部署服务器的网卡叫什么. 一个部署任务的机器网卡名称要一致, 否则下面的方法也用不了 观察hostvars可以发现 ansible_ens33无vip 12345678910111213141516171819202122232425&quot;msg&quot;: &#123; &quot;centos-1&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;192.168.124.136&quot;, &quot;172.16.120.10&quot; ], ...省略 &quot;ansible_ens33&quot;: &#123; &quot;active&quot;: true, &quot;device&quot;: &quot;ens33&quot;, ...省略 &quot;hw_timestamp_filters&quot;: [], &quot;ipv4&quot;: &#123; &quot;address&quot;: &quot;172.16.120.10&quot;, &quot;broadcast&quot;: &quot;172.16.120.255&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;172.16.120.0&quot; &#125;, &quot;ipv6&quot;: [ &#123; &quot;address&quot;: &quot;fe80::e880:2501:cdab:b11d&quot;, &quot;prefix&quot;: &quot;64&quot;, &quot;scope&quot;: &quot;link&quot; &#125; ], ansible_ens33有vip 12345678910111213141516171819202122232425262728293031323334353637383940&quot;msg&quot;: &#123; &quot;centos-1&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;192.168.124.136&quot;, &quot;172.16.120.10&quot;, &quot;172.16.120.100&quot; ], ...省略 &quot;ansible_ens33&quot;: &#123; &quot;active&quot;: true, &quot;device&quot;: &quot;ens33&quot;, ...省略 &quot;hw_timestamp_filters&quot;: [], &quot;ipv4&quot;: &#123; &quot;address&quot;: &quot;172.16.120.10&quot;, &quot;broadcast&quot;: &quot;172.16.120.255&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;172.16.120.0&quot; &#125;, &quot;ipv4_secondaries&quot;: [ --不知道为什么有俩 &#123; &quot;address&quot;: &quot;172.16.120.100&quot;, &quot;broadcast&quot;: &quot;global&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;172.16.120.0&quot; &#125;, &#123; &quot;address&quot;: &quot;172.16.120.100&quot;, &quot;broadcast&quot;: &quot;global&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;172.16.120.0&quot; &#125; ], &quot;ipv6&quot;: [ &#123; &quot;address&quot;: &quot;fe80::e880:2501:cdab:b11d&quot;, &quot;prefix&quot;: &quot;64&quot;, &quot;scope&quot;: &quot;link&quot; &#125; ], 可以这样取map(&#39;extract&#39;, hostvars, [&#39;ansible_ens33&#39;, &#39;ipv4&#39;, &#39;address&#39;]) 于是 1234567891011121314151617181920[root@centos-4 ansible]# cat hosts[mysql]172.16.120.10 mysql_repl_role=master172.16.120.11 mysql_repl_role=slave172.16.120.12 mysql_repl_role=slave[mha]172.16.120.13[proxysql]172.16.120.10 weight=100 comment=PRIMARY172.16.120.11 weight=99 comment=SECONDARY172.16.120.12 weight=99 comment=SECONDARY[all:vars]ansible_user=root#ansible_password=vagrantansible_port=22ansible_become=true playbook 1234567---- name: &#x27;测试&#x27; hosts: mysql tasks: - name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])|list&quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])|list&#125;&#125;&quot; 其中172.16.120.10还是有vip 172.16.120.100的 结果 12345678910111213141516171819202122TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])|list] *********************************************************************************************************************************************************************ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10&quot;, &quot;172.16.120.11&quot;, &quot;172.16.120.12&quot; ]&#125;ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10&quot;, &quot;172.16.120.11&quot;, &quot;172.16.120.12&quot; ]&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10&quot;, &quot;172.16.120.11&quot;, &quot;172.16.120.12&quot; ]&#125; 到这一步这是取到了所有ip, 还没完成拼接出group_replication_group_seeds 接下来要用到zip_longest 拿官方文档的例子 1234567891011121314- name: &#x27;测试&#x27; hosts: 172.16.120.10 tasks: - name: give me list combo of two lists debug: msg: &quot;&#123;&#123; [1,2,3,4,5] | zip([&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;,&#x27;f&#x27;]) | list &#125;&#125;&quot; - name: give me shortest combo of two lists debug: msg: &quot;&#123;&#123; [1,2,3] | zip([&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;,&#x27;f&#x27;], [&#x27;!&#x27;,&#x27;@&#x27;,&#x27;#&#x27;,&#x27;$&#x27;]) | list &#125;&#125;&quot; - name: give me longest combo of three lists , fill with X debug: msg: &quot;&#123;&#123; [1,2,3] | zip_longest([&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;,&#x27;f&#x27;], [21, 22, 23], fillvalue=&#x27;X&#x27;) | list &#125;&#125;&quot; 结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182TASK [give me list combo of two lists] *************************************************************************************************************************************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ [ 1, &quot;a&quot; ], [ 2, &quot;b&quot; ], [ 3, &quot;c&quot; ], [ 4, &quot;d&quot; ], [ 5, &quot;e&quot; ] ]&#125;TASK [give me shortest combo of two lists] *********************************************************************************************************************************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ [ 1, &quot;a&quot;, &quot;!&quot; ], [ 2, &quot;b&quot;, &quot;@&quot; ], [ 3, &quot;c&quot;, &quot;#&quot; ] ]&#125;TASK [give me longest combo of three lists , fill with X] ******************************************************************************************************************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ [ 1, &quot;a&quot;, 21 ], [ 2, &quot;b&quot;, 22 ], [ 3, &quot;c&quot;, 23 ], [ &quot;X&quot;, &quot;d&quot;, &quot;X&quot; ], [ &quot;X&quot;, &quot;e&quot;, &quot;X&quot; ], [ &quot;X&quot;, &quot;f&quot;, &quot;X&quot; ] ]&#125; 可以看到在这前两个task中, “多余”的会被舍弃掉, 而且可以zip多个list, 最后一个task使用fillvalue取”补齐” 于是我们就可以 123- name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) |list&quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) |list&#125;&#125;&quot; 这里为了看到结果所以list一下,否则是一个&lt;itertools.izip_longest object at 0x24468e8&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) |list] **********************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ [ &quot;172.16.120.10&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.11&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.12&quot;, &quot;:23306&quot; ] ]&#125;ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: [ [ &quot;172.16.120.10&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.11&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.12&quot;, &quot;:23306&quot; ] ]&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: [ [ &quot;172.16.120.10&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.11&quot;, &quot;:23306&quot; ], [ &quot;172.16.120.12&quot;, &quot;:23306&quot; ] ]&#125; 然后使用map(‘join’),就是将列表里的每一个元素使用join拼接 123- name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) | map(&#x27;join&#x27;)|list &quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) | map(&#x27;join&#x27;) |list&#125;&#125;&quot; 12345678910111213141516171819202122TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) | map(&#x27;join&#x27;)|list] *********************************************************************************************ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10:23306&quot;, &quot;172.16.120.11:23306&quot;, &quot;172.16.120.12:23306&quot; ]&#125;ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10:23306&quot;, &quot;172.16.120.11:23306&quot;, &quot;172.16.120.12:23306&quot; ]&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: [ &quot;172.16.120.10:23306&quot;, &quot;172.16.120.11:23306&quot;, &quot;172.16.120.12:23306&quot; ]&#125; 最后 123456789---- name: &#x27;测试&#x27; hosts: mysql vars: mysql_port: 3306 tasks: - name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) | map(&#x27;join&#x27;) | join(&#x27;,&#x27;)&quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip_longest([], fillvalue=&#x27;:2&#x27;+mysql_port|string) | map(&#x27;join&#x27;) | join(&#x27;,&#x27;)&#125;&#125;&quot; 12345678910TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=:3306) | map(&#x27;join&#x27;) | join(&#x27;,&#x27;)] *******************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10:23306,172.16.120.11:23306,172.16.120.12:23306&quot;&#125;ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10:23306,172.16.120.11:23306,172.16.120.12:23306&quot;&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10:23306,172.16.120.11:23306,172.16.120.12:23306&quot;&#125; 附1: 生产group_replication_local_address的方法使用hostvars[inventory_hostname] 123- name: &quot;hostvars[inventory_hostname][&#x27;ansible_ens33&#x27;][&#x27;ipv4&#x27;][&#x27;address&#x27;]:23306&quot; debug: msg: &quot;&#123;&#123; hostvars[inventory_hostname][&#x27;ansible_ens33&#x27;][&#x27;ipv4&#x27;][&#x27;address&#x27;] &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125;&quot; 12345678910TASK [hostvars[inventory_hostname][&#x27;ansible_ens33&#x27;][&#x27;ipv4&#x27;][&#x27;address&#x27;]:23306] **********************************************************************************************************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10:23306&quot;&#125;ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.11:23306&quot;&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.12:23306&quot;&#125; 同样, 如果网卡不统一, 只能… 12345&#123;% for ip in ansible_all_ipv4_addresses -%&#125; &#123;% if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;)) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125; loose-group_replication_local_address = &#123;&#123; ip &#125;&#125;:2&#123;&#123; mysql_port &#125;&#125; &#123;% endif %&#125;&#123;% endfor %&#125; 附2: /etc/hosts网卡不统一 123456- name: 设置/etc/hosts blockinfile: path: /etc/hosts block: | &#123;&#123; item &#125;&#125; with_items: &quot;&#123;&#123; etc_hosts &#125;&#125;&quot; 123456789101112etc_hosts: | &#123;% for ip_list in (groups[group] | map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_all_ipv4_addresses&#x27;])) -%&#125; &#123;% set iploop = loop %&#125; &#123;% for ip in ip_list -%&#125; &#123;% for hostname in (groups[group] | map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_hostname&#x27;])) -%&#125; &#123;% if loop.index == iploop.index -%&#125; &#123;% if (ip.startswith(&#x27;192.168&#x27;) or ip.startswith(&#x27;10.163&#x27;) or ip.startswith(&#x27;10.152&#x27;) or ip.startswith(&#x27;10.132&#x27;) or ip.startswith(&#x27;10.133&#x27;) ) and not ip.startswith(&#x27;192.168.3&#x27;) and not ip.startswith(&#x27;192.168.8&#x27;) and not ip.startswith(&#x27;192.168.16&#x27;) and not ip.startswith(&#x27;10.62&#x27;) and not ip.startswith(&#x27;10.32&#x27;) and not ip.startswith(&#x27;10.52&#x27;) and not ip.startswith(&#x27;10.33&#x27;) and not (ip.startswith(&#x27;10.133.1.2&#x27;) and ip.split(&#x27;.&#x27;)[3]|int&gt;=200) -%&#125; &#123;&#123; ip &#125;&#125; &#123;&#123; hostname &#125;&#125; &#123;% endif %&#125; &#123;% endif %&#125; &#123;% endfor %&#125; &#123;% endfor %&#125; &#123;% endfor %&#125; 网卡统一 123456789101112131415---- name: &#x27;测试&#x27; hosts: mysql vars: etc_hosts: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip(groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_hostname&#x27;])) | map(&#x27;join&#x27;,&#x27; &#x27;) | join(&#x27;\\n&#x27;) &#125;&#125;&quot; tasks: - name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip(groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_hostname&#x27;])) | map(&#x27;join&#x27;,&#x27; &#x27;) | join(&#x27;\\n&#x27;) &quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip(groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_hostname&#x27;])) | map(&#x27;join&#x27;,&#x27; &#x27;) | join(&#x27;\\n&#x27;) &#125;&#125;&quot; - name: 设置/etc/hosts blockinfile: path: /tmp/hosts.test block: | &#123;&#123; item &#125;&#125; with_items: &quot;&#123;&#123; etc_hosts &#125;&#125;&quot; 12345678910111213141516171819TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip(groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_hostname&#x27;])) | map(&#x27;join&#x27;,&#x27; &#x27;) | join(&#x27;&#x27;)] ***************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-1\\n172.16.120.11 centos-2\\n172.16.120.12 centos-3&quot;&#125;ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-1\\n172.16.120.11 centos-2\\n172.16.120.12 centos-3&quot;&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-1\\n172.16.120.11 centos-2\\n172.16.120.12 centos-3&quot;&#125;[root@centos-1 ~]# cat /tmp/hosts.test # BEGIN ANSIBLE MANAGED BLOCK172.16.120.10 centos-1172.16.120.11 centos-2172.16.120.12 centos-3# END ANSIBLE MANAGED BLOCK 注意hostname要用groups[&#39;mysql&#39;]|map(&#39;extract&#39;, hostvars, [&#39;ansible_hostname&#39;])取, 不能直接ansible_hostname 123- name: &quot; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27; &#x27;+ansible_hostname) | map(&#x27;join&#x27;)| join(&#x27;\\n&#x27;) &quot; debug: msg: &quot;&#123;&#123; groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;]) | list | zip_longest([], fillvalue=&#x27; &#x27;+ansible_hostname) | map(&#x27;join&#x27;) | join(&#x27;\\n&#x27;)&#125;&#125;&quot; 看输出的结果hostname部分, 每个主机都只取到自己的hostname 123456789101112TASK [groups[&#x27;mysql&#x27;]|map(&#x27;extract&#x27;, hostvars, [&#x27;ansible_ens33&#x27;, &#x27;ipv4&#x27;, &#x27;address&#x27;])| list | zip_longest([], fillvalue=&#x27; &#x27;+ansible_hostname) | map(&#x27;join&#x27;)| join(&#x27;&#x27;)] ****************************************************************************************ok: [172.16.120.10] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-1\\n172.16.120.11 centos-1\\n172.16.120.12 centos-1&quot;&#125;ok: [172.16.120.11] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-2\\n172.16.120.11 centos-2\\n172.16.120.12 centos-2&quot;&#125;ok: [172.16.120.12] =&gt; &#123; &quot;msg&quot;: &quot;172.16.120.10 centos-3\\n172.16.120.11 centos-3\\n172.16.120.12 centos-3&quot;&#125;","categories":[],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://fuxkdb.com/tags/Ansible/"}]},{"title":"使用python消费canal protobuf格式数据","slug":"2020-03-19-使用python消费canal-protobuf格式数据","date":"2020-03-19T15:23:00.000Z","updated":"2020-03-19T14:55:55.042Z","comments":true,"path":"2020/03/19/2020-03-19-使用python消费canal-protobuf格式数据/","link":"","permalink":"http://fuxkdb.com/2020/03/19/2020-03-19-%E4%BD%BF%E7%94%A8python%E6%B6%88%E8%B4%B9canal-protobuf%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE/","excerpt":"canal -&gt; kafka -&gt; consumer. flatMessage=False参考 canal Python客户端.由于canal Python客户端是作为canal的client直连canal 11111端口消费数据而非消费kafka数据, 所以example不能照搬, 需要做一些修改 Python3.7.4 requriments 12345678910111213141516171819202122232425262728293031backcall==0.1.0bleach==3.1.0canal-python==0.4certifi==2019.6.16chardet==3.0.4confluent-kafka==1.3.0decorator==4.4.2docopt==0.6.2docutils==0.15.2idna==2.8ipython==7.13.0ipython-genutils==0.2.0jedi==0.16.0parso==0.6.2pexpect==4.8.0pickleshare==0.7.5pkginfo==1.5.0.1prompt-toolkit==3.0.4protobuf==3.9.1ptyprocess==0.6.0Pygments==2.4.2readme-renderer==24.0requests==2.22.0requests-toolbelt==0.9.1six==1.12.0tqdm==4.34.0traitlets==4.3.3twine==1.13.0urllib3==1.25.3wcwidth==0.1.8webencodings==0.5.1","text":"canal -&gt; kafka -&gt; consumer. flatMessage=False参考 canal Python客户端.由于canal Python客户端是作为canal的client直连canal 11111端口消费数据而非消费kafka数据, 所以example不能照搬, 需要做一些修改 Python3.7.4 requriments 12345678910111213141516171819202122232425262728293031backcall==0.1.0bleach==3.1.0canal-python==0.4certifi==2019.6.16chardet==3.0.4confluent-kafka==1.3.0decorator==4.4.2docopt==0.6.2docutils==0.15.2idna==2.8ipython==7.13.0ipython-genutils==0.2.0jedi==0.16.0parso==0.6.2pexpect==4.8.0pickleshare==0.7.5pkginfo==1.5.0.1prompt-toolkit==3.0.4protobuf==3.9.1ptyprocess==0.6.0Pygments==2.4.2readme-renderer==24.0requests==2.22.0requests-toolbelt==0.9.1six==1.12.0tqdm==4.34.0traitlets==4.3.3twine==1.13.0urllib3==1.25.3wcwidth==0.1.8webencodings==0.5.1 主要参考 https://github.com/haozi3156666/canal-python/blob/master/canal/client.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183# -*- coding: utf8 -*-# __author__ = &#x27;Fan()&#x27;# Date: 2020-03-18&#x27;&#x27;&#x27;Usage: canal_kafka_protobuf_consume.py --bootstrap-servers=&lt;host:port,host2:port2..&gt; [--k_user=&lt;user&gt; ] [--from-beginning=&lt;false&gt; | --from-end=&lt;false&gt;] --topic=&lt;topic_name&gt; [--partition=&lt;partition_number&gt;] [--verbose=&lt;0&gt;] canal_kafka_protobuf_consume.py -h | --help canal_kafka_protobuf_consume.py --versionOptions: -h --help 打印帮助信息. --version 版本信息. --bootstrap_servers=&lt;host:port,host2:port2..&gt; kafka servers --from-beginning=&lt;false&gt; 从头开始消费 [default: False] --from-end=&lt;false&gt; 从最后开始消费 [default: True] --k_user=&lt;user&gt; kafka用户, 可选项 --topic=&lt;topic_name&gt; topic名称 --partition=&lt;partition_number&gt; topic分区号 [default: 0] --verbose=&lt;0&gt; 输出详细信息0,1,2 默认0不输出 [default: 0]&#x27;&#x27;&#x27;import getpassfrom docopt import docoptfrom canal.protocol import CanalProtocol_pb2from canal.protocol import EntryProtocol_pb2from confluent_kafka import Consumer, KafkaError, TopicPartition, OFFSET_END, OFFSET_BEGINNINGclass DocOptArgs: def __init__(self, args): self.topic = args[&#x27;--topic&#x27;] self.k_user = args[&#x27;--k_user&#x27;] self.verbose = int(args[&#x27;--verbose&#x27;]) self.partition = int(args[&#x27;--partition&#x27;]) self.bootstrap_servers = args[&#x27;--bootstrap-servers&#x27;] self.from_end = eval(args[&#x27;--from-end&#x27;].capitalize()) self.from_beginning = eval(args[&#x27;--from-beginning&#x27;].capitalize()) if not self.k_user: self.k_password = None elif self.k_user == &#x27;admin&#x27;: self.k_password = &#x27;superSecurt&#x27; else: self.k_password = getpass.getpass(&quot;please enter kafka password: &quot;)class MyConsumer(DocOptArgs): def __init__(self, docopt_args): self.args = docopt_args DocOptArgs.__init__(self, self.args) if self.verbose &gt;= 1: print(self.args) def _on_send_response(self, err, partations): pt = partations[0] if isinstance(err, KafkaError): print(&#x27;Topic &#123;&#125; 偏移量 &#123;&#125; 提交异常. &#123;&#125;&#x27;.format(pt.topic, pt.offset, err)) raise Exception(err) def messages(self, offset_end=True): config = &#123;&#x27;bootstrap.servers&#x27;: self.bootstrap_servers, &quot;group.id&quot;: self.topic, &#x27;enable.auto.commit&#x27;: True, &quot;fetch.wait.max.ms&quot;: 3000, &quot;max.poll.interval.ms&quot;: 60000, &#x27;session.timeout.ms&#x27;: 60000, &quot;on_commit&quot;: self._on_send_response, &quot;default.topic.config&quot;: &#123;&quot;auto.offset.reset&quot;: &quot;latest&quot;&#125;&#125; if self.k_user and self.k_password: config[&#x27;security.protocol&#x27;] = &#x27;SASL_PLAINTEXT&#x27; config[&#x27;sasl.mechanism&#x27;] = &#x27;SCRAM-SHA-256&#x27; config[&#x27;sasl.username&#x27;] = self.k_user config[&#x27;sasl.password&#x27;] = self.k_password consumer = Consumer(config) offset = OFFSET_END if offset_end else OFFSET_BEGINNING pt = TopicPartition(self.topic, 0, offset) consumer.assign([pt]) # consumer.seek(pt) try: while True: ret = consumer.consume(num_messages=100, timeout=0.1) if ret is None: print(&quot;No message Continue!&quot;) continue for msg in ret: if msg.error() is None: # protobuf binary yield msg.value() elif msg.error(): if msg.error().code() == KafkaError._PARTITION_EOF: continue else: raise Exception(msg.error()) except Exception as e: print(e) consumer.close() except KeyboardInterrupt: consumer.close()class Decoder: @staticmethod def create_canal_message(kafka_message): data = kafka_message packet = CanalProtocol_pb2.Packet() packet.MergeFromString(data) message = dict(id=0, entries=[]) # 因为从kafka获取的canal写入的消息, 所以这个条件应该永远成立 # if packet.type == CanalProtocol_pb2.PacketType.MESSAGES: messages = CanalProtocol_pb2.Messages() messages.MergeFromString(packet.body) for item in messages.messages: entry = EntryProtocol_pb2.Entry() entry.MergeFromString(item) message[&#x27;entries&#x27;].append(entry) return messageif __name__ == &#x27;__main__&#x27;: version = &#x27;canal_kafka_protobuf_consume 0.1.0&#x27; arguments = docopt(__doc__, version=version) consumer = MyConsumer(arguments) for message in consumer.messages(): canal_message = Decoder.create_canal_message(message) entries = canal_message[&#x27;entries&#x27;] for entry in entries: entry_type = entry.entryType if entry_type in [EntryProtocol_pb2.EntryType.TRANSACTIONBEGIN, EntryProtocol_pb2.EntryType.TRANSACTIONEND]: continue row_change = EntryProtocol_pb2.RowChange() row_change.MergeFromString(entry.storeValue) # event_type = row_change.eventType header = entry.header database = header.schemaName table = header.tableName binlog_file = header.logfileName binlog_pos = header.logfileOffset characterset = header.serverenCode es = header.executeTime gtid = header.gtid event_type = header.eventType for row in row_change.rowDatas: format_data = dict() if event_type == EntryProtocol_pb2.EventType.DELETE: for column in row.beforeColumns: format_data.update(&#123; column.name: column.value &#125;) elif event_type == EntryProtocol_pb2.EventType.INSERT: for column in row.afterColumns: format_data.update(&#123; column.name: column.value &#125;) else: format_data[&#x27;before&#x27;] = dict() format_data[&#x27;after&#x27;] = dict() for column in row.beforeColumns: format_data[&#x27;before&#x27;][column.name] = column.value for column in row.afterColumns: format_data[&#x27;after&#x27;][column.name] = column.value data = dict( db=database, table=table, event_type=EntryProtocol_pb2.EventType.Name(event_type), is_ddl=row_change.isDdl, binlog_file=binlog_file, binlog_pos=binlog_pos, characterset=characterset, es=es, gtid=header.gtid, data=format_data, ) print(data) 使用效果 123#python canal_kafka_protobuf_consume.py --bootstrap-servers=172.16.xx.xx:9092,172.16.xx.xx:9092,172.16.xx.xx:9092 --topic=fanboshi.monitor_delay&#123;&#x27;db&#x27;: &#x27;fanboshi&#x27;, &#x27;table&#x27;: &#x27;monitor_delay&#x27;, &#x27;event_type&#x27;: &#x27;INSERT&#x27;, &#x27;is_ddl&#x27;: False, &#x27;binlog_file&#x27;: &#x27;mysql-bin.000006&#x27;, &#x27;binlog_pos&#x27;: 469896982, &#x27;characterset&#x27;: &#x27;UTF-8&#x27;, &#x27;es&#x27;: 1584535911000, &#x27;gtid&#x27;: &#x27;c30c6a02-4e32-11ea-84ec-fa163edcd14e:1-2940100&#x27;, &#x27;data&#x27;: &#123;&#x27;id&#x27;: &#x27;5&#x27;, &#x27;ctime&#x27;: &#x27;2020-03-18 20:51:51&#x27;&#125;&#125;&#123;&#x27;db&#x27;: &#x27;fanboshi&#x27;, &#x27;table&#x27;: &#x27;monitor_delay&#x27;, &#x27;event_type&#x27;: &#x27;INSERT&#x27;, &#x27;is_ddl&#x27;: False, &#x27;binlog_file&#x27;: &#x27;mysql-bin.000006&#x27;, &#x27;binlog_pos&#x27;: 469897261, &#x27;characterset&#x27;: &#x27;UTF-8&#x27;, &#x27;es&#x27;: 1584535912000, &#x27;gtid&#x27;: &#x27;c30c6a02-4e32-11ea-84ec-fa163edcd14e:1-2940101&#x27;, &#x27;data&#x27;: &#123;&#x27;id&#x27;: &#x27;6&#x27;, &#x27;ctime&#x27;: &#x27;2020-03-18 20:51:52&#x27;&#125;&#125;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Canal","slug":"Canal","permalink":"http://fuxkdb.com/tags/Canal/"}]},{"title":"Canal dynamicTopic问题续","slug":"2020-03-07-Canal-dynamicTopic问题续","date":"2020-03-07T13:23:00.000Z","updated":"2020-03-07T13:23:10.407Z","comments":true,"path":"2020/03/07/2020-03-07-Canal-dynamicTopic问题续/","link":"","permalink":"http://fuxkdb.com/2020/03/07/2020-03-07-Canal-dynamicTopic%E9%97%AE%E9%A2%98%E7%BB%AD/","excerpt":"最近在新公司搭了一套canal. 按照&lt;&gt;设置了canal.mq.topic和canal.mq.dynamicTopic 意图将一些不符合dynamicTopic匹配的语句的消息发送到一个默认的topic而避免报错INVALID_TOPIC_EXCEPTION 123456789101112131415161718# table regexcanal.instance.filter.regex=fanboshi\\\\..*,sysbench\\\\..*# table black regexcanal.instance.filter.black.regex=.*\\\\.\\\\_.*\\\\_ghc,.*\\\\.\\\\_.*\\\\_gho,.*\\\\.\\\\_.*\\\\_del# table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch# table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch# mq configcanal.mq.topic=default_topic# dynamic topic route by schema or table regex#canal.mq.dynamicTopic=mytest1.user,mytest2\\\\..*,.*\\\\..*canal.mq.dynamicTopic=.*\\\\..*#canal.mq.partition=0# hash partition config#canal.mq.partitionsNum=3#canal.mq.partitionHash=test.table:id^name,.*\\\\..*","text":"最近在新公司搭了一套canal. 按照&lt;&gt;设置了canal.mq.topic和canal.mq.dynamicTopic 意图将一些不符合dynamicTopic匹配的语句的消息发送到一个默认的topic而避免报错INVALID_TOPIC_EXCEPTION 123456789101112131415161718# table regexcanal.instance.filter.regex=fanboshi\\\\..*,sysbench\\\\..*# table black regexcanal.instance.filter.black.regex=.*\\\\.\\\\_.*\\\\_ghc,.*\\\\.\\\\_.*\\\\_gho,.*\\\\.\\\\_.*\\\\_del# table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch# table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch# mq configcanal.mq.topic=default_topic# dynamic topic route by schema or table regex#canal.mq.dynamicTopic=mytest1.user,mytest2\\\\..*,.*\\\\..*canal.mq.dynamicTopic=.*\\\\..*#canal.mq.partition=0# hash partition config#canal.mq.partitionsNum=3#canal.mq.partitionHash=test.table:id^name,.*\\\\..* 因为和前几天同事说之前使dynamicTopic遇到了bug. 今天正准备自己搞些语句测一测, 于是想看下default_topic能否消费到消息(如果消费到了就说明我执行的语句无法被dynamicTopic规则匹配到而发送到了这个”默认Topic”) 结果发现大量消息 123456&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581779000,&quot;id&quot;:655469,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;UPDATE sbtest1 SET c=? WHERE id=?&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;sbtest1&quot;,&quot;ts&quot;:1583581779149,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581779000,&quot;id&quot;:655469,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;DELETE FROM sbtest8 WHERE id=?&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;sbtest8&quot;,&quot;ts&quot;:1583581779149,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581779000,&quot;id&quot;:655469,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;INSERT INTO sbtest8 (id, k, c, pad) VALUES (?, ?, ?, ?)&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;sbtest8&quot;,&quot;ts&quot;:1583581779149,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581779000,&quot;id&quot;:655470,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;insert into fanboshi.monitor_delay(ctime) values(now())&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;monitor_delay&quot;,&quot;ts&quot;:1583581779465,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581780000,&quot;id&quot;:655471,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;insert into fanboshi.monitor_delay(ctime) values(now())&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;monitor_delay&quot;,&quot;ts&quot;:1583581780570,&quot;type&quot;:&quot;QUERY&quot;&#125;&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;&quot;,&quot;es&quot;:1583581781000,&quot;id&quot;:655472,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;insert into fanboshi.monitor_delay(ctime) values(now())&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;monitor_delay&quot;,&quot;ts&quot;:1583581781475,&quot;type&quot;:&quot;QUERY&quot;&#125; 一开始有点懵, 后来细看发现这些居然都是”statement”格式的, 是原始语句 于是搜了下github https://github.com/alibaba/canal/issues/1361 参考一下FAQ里的解释：https://github.com/alibaba/canal/wiki/FAQ 问1：INSERT/UPDATE/DELETE被解析为Query或DDL语句？ 答1：出现这类情况主要原因为收到的binlog就为Query事件，比如: binlog格式为非row模式，通过show variables like ‘binlog_format’可以查看. 针对statement/mixed模式，DML语句都会是以SQL语句存在 mysql5.6+之后，在binlog为row模式下，针对DML语句通过一个开关(binlog-rows-query-log-events=true, show variables里也可以看到该变量)，记录DML的原始SQL，对应binlog事件为RowsQueryLogEvent，同时也有对应的row记录. ps. canal可以通过properties设置来过滤：canal.instance.filter.query.dml = true 我这里肯定是row格式了. 那么可能就是binlog_rows_query_log_events = 1的原因, 看了下还真是设置成了on 至于作者说的 1ps. canal可以通过properties设置来过滤：canal.instance.filter.query.dml = true https://github.com/alibaba/canal/wiki/AdminGuide canal.instance.filter.query.dml 是否忽略dml语句(mysql5.6之后，在row模式下每条DML语句也会记录SQL到binlog中,可参考MySQL文档) false 这解释不是很清楚的样子, 到底是过滤啥啊, 不管了反正用不着 现在怀疑那次同事遇到问题是不是就是因为就是开启了binlog_rows_query_log_events","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Canal","slug":"Canal","permalink":"http://fuxkdb.com/tags/Canal/"}]},{"title":"canal.mq.flatMessage参数","slug":"2020-03-06-canal.mq.flatMessage参数","date":"2020-03-06T14:23:00.000Z","updated":"2020-03-07T13:33:32.116Z","comments":true,"path":"2020/03/06/2020-03-06-canal.mq.flatMessage参数/","link":"","permalink":"http://fuxkdb.com/2020/03/06/2020-03-06-canal.mq.flatMessage%E5%8F%82%E6%95%B0/","excerpt":"canal.mq.flatMessage 是否为json格式如果设置为false,对应MQ收到的消息为protobuf格式需要通过CanalMessageDeserializer进行解码 canal.mq.flatMessage = true 生产到kafka的消息就是json的, 否则就是protobuf二进制的","text":"canal.mq.flatMessage 是否为json格式如果设置为false,对应MQ收到的消息为protobuf格式需要通过CanalMessageDeserializer进行解码 canal.mq.flatMessage = true 生产到kafka的消息就是json的, 否则就是protobuf二进制的12345678910111213141516171819202122232425262728$bin/kafka-console-consumer.sh --bootstrap-server 172.16.23.174:9092 --topic fanboshi.t1 --from-beginning&#123;&quot;data&quot;:null,&quot;database&quot;:&quot;fanboshi&quot;,&quot;es&quot;:1583414454000,&quot;id&quot;:2,&quot;isDdl&quot;:true,&quot;mysqlType&quot;:null,&quot;old&quot;:null,&quot;pkNames&quot;:null,&quot;sql&quot;:&quot;create table t1(id int not null auto_increment primary key,sname varchar(10))&quot;,&quot;sqlType&quot;:null,&quot;table&quot;:&quot;t1&quot;,&quot;ts&quot;:1583414454111,&quot;type&quot;:&quot;CREATE&quot;&#125;&#123;&quot;data&quot;:[&#123;&quot;id&quot;:&quot;1&quot;,&quot;sname&quot;:&quot;fan&quot;&#125;],&quot;database&quot;:&quot;fanboshi&quot;,&quot;es&quot;:1583414520000,&quot;id&quot;:3,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:&#123;&quot;id&quot;:&quot;int(11)&quot;,&quot;sname&quot;:&quot;varchar(10)&quot;&#125;,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;sqlType&quot;:&#123;&quot;id&quot;:4,&quot;sname&quot;:12&#125;,&quot;table&quot;:&quot;t1&quot;,&quot;ts&quot;:1583414520403,&quot;type&quot;:&quot;INSERT&quot;&#125;&#123;&quot;data&quot;:[&#123;&quot;id&quot;:&quot;1&quot;,&quot;sname&quot;:&quot;fan&quot;&#125;],&quot;database&quot;:&quot;fanboshi&quot;,&quot;es&quot;:1583414520000,&quot;id&quot;:1,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:&#123;&quot;id&quot;:&quot;int(11)&quot;,&quot;sname&quot;:&quot;varchar(10)&quot;&#125;,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;sqlType&quot;:&#123;&quot;id&quot;:4,&quot;sname&quot;:12&#125;,&quot;table&quot;:&quot;t1&quot;,&quot;ts&quot;:1583414603231,&quot;type&quot;:&quot;INSERT&quot;&#125;&#123;&quot;data&quot;:[&#123;&quot;id&quot;:&quot;2&quot;,&quot;sname&quot;:&quot;bo&quot;&#125;],&quot;database&quot;:&quot;fanboshi&quot;,&quot;es&quot;:1583414621000,&quot;id&quot;:2,&quot;isDdl&quot;:false,&quot;mysqlType&quot;:&#123;&quot;id&quot;:&quot;int(11)&quot;,&quot;sname&quot;:&quot;varchar(10)&quot;&#125;,&quot;old&quot;:null,&quot;pkNames&quot;:[&quot;id&quot;],&quot;sql&quot;:&quot;&quot;,&quot;sqlType&quot;:&#123;&quot;id&quot;:4,&quot;sname&quot;:12&#125;,&quot;table&quot;:&quot;t1&quot;,&quot;ts&quot;:1583414621766,&quot;type&quot;:&quot;INSERT&quot;&#125;*\u0001mysql-bin.000002 *UTF-80֊.8fanboshiJt1P,Xb6curtGtid*c30c6a02-4e32-11ea-84ec-fa163edcd14e:65460bcurtGtidSn64398bcurtGtidLct64397b _-+_C-+++1+,c30c6a02-4e32-11ea-84ec-fa163edcd14e:1-65460\u0001Pbid (0B1Ri++(11)# _+a+e (0Bfa+R +a_cha_(10)\u0002+y_-+-bi+.000002\u0003 *UTF-80֊.8fa+b-_hiJ+1P+Xb6c+_+G+id*c30c6a02-4e32-11ea-84ec-fa163edcd14e:65461bc+_+G+idS+64399bc+_+G+idLc+64398b _-+_C-+++1+,c30c6a02-4e32-11ea-84ec-fa163edcd14e:1-65461\u0001Pbid (0B2Ri++(11)&quot; _+a+e (0Bb-R +a_cha_(10)*\u0002\u0002 乱码部分就是protobuf的 protobuf性能要好很多. 1.1.5性能有很大提升. 即便canal.mq.flatMessage = true性能也比以前好了很多很多 canal.mq.flatMessage = true默认的格式没带GTID https://github.com/alibaba/canal/wiki/Canal-MQ-Performance","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Canal","slug":"Canal","permalink":"http://fuxkdb.com/tags/Canal/"}]},{"title":"mlanuch文档翻译","slug":"2020-01-21-mlanuch文档翻译","date":"2020-01-21T10:09:00.000Z","updated":"2020-01-21T10:09:59.000Z","comments":true,"path":"2020/01/21/2020-01-21-mlanuch文档翻译/","link":"","permalink":"http://fuxkdb.com/2020/01/21/2020-01-21-mlanuch%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91/","excerpt":"[TOC] mlanuch使用此工具，您可以快速启动并监视本地计算机上的MongoDB环境。 它支持独立服务器，副本集和分片群集的各种配置。 单个节点或节点组可以轻松停止并重新启动。 除了下面列出的mlaunch的所有列出的参数之外，您还可以传递mongos或mongod二进制文件可以理解的任意选项，mlaunch会传递正确的参数给mongos或mongod二进制文件。 这包括-f或--config选项，以传递带有更多选项的配置文件。","text":"[TOC] mlanuch使用此工具，您可以快速启动并监视本地计算机上的MongoDB环境。 它支持独立服务器，副本集和分片群集的各种配置。 单个节点或节点组可以轻松停止并重新启动。 除了下面列出的mlaunch的所有列出的参数之外，您还可以传递mongos或mongod二进制文件可以理解的任意选项，mlaunch会传递正确的参数给mongos或mongod二进制文件。 这包括-f或--config选项，以传递带有更多选项的配置文件。 Usage12mlaunch [-h] [--version] [--no-progressbar] &#123;init,start,stop,restart,list,kill&#125; ... General Parameters 以下参数适用于所有命令. Help -h, --help 显示帮助文本并退出. Version --version 显示版本号并退出. Verbosity --verbose 这将打印其他信息，具体取决于每个命令. Data directory --dir DIR 此参数更改mlaunch存储其数据和日志文件的目录。默认情况下，该目录是本地目录./data，位于当前工作目录下。 Commandsmlaunch uses different commands to initialize, stop, start and list test environments. The general syntax is: 1mlaunch &lt;command&gt; [--parameters ...] 其中，command是以下选择之一: init: 创建一个初始环境并启动所有节点 stop: 停止当前环境中的部分或所有节点 start: 启动当前环境中的部分或所有节点 list: 显示当前环境的列表 kill: 向当前环境中的节点发送终止信号（或其他信号） 对于给定的环境（由带有--dir参数的数据目录指定，默认值为./data），init命令只需调用一次。 mlaunch将配置存储在数据目录中的配置文件中，该文件名为.mlaunch_startup。 使用此文件，mlaunch可以记住配置，并可以在需要时启动和停止节点。 init这个命令初始化并启动MongoDB stand-anloneinstance, replica sets, 或shared cluster. 每个环境只需要调用一次（由带有--dir参数的数据目录指定，默认为./data）。 Usage1234567mlaunch init [-h] (--single | --replicaset) [--nodes NUM] [--arbiter] [--name NAME] [--priority] [--sharded N [N ...]] [--config NUM] [--csrs] [--mongos NUM] [--verbose] [--port PORT] [--binarypath PATH] [--dir DIR] [--hostname HOSTNAME] [--auth] [--username USERNAME] [--password PASSWORD] [--auth-db DB] [--auth-roles [ROLE [ROLE ...]]] 为了方便和向后兼容，init命令是默认命令，可以省略。 Required Parametersinit命令需要以下两个参数之一才能运行：--single或--replicaset。它们是互斥的，并且必须为指定一个。 --single 此参数将创建一个stand-alone node。如果还指定了--sharded，则此参数会将每个分片创建为stand-alone node。 例如, 要在端口27017上启动一个mongod实例: 1mlaunch --single --replicaset 此参数将创建副本集，而不是单个节点。其他副本集参数适用，并可修改副本集的属性以启动。如果还指定了--sharded，则此参数将为每个分片创建一个这样的副本集。 例如，要使用端口27017、27018、27019 启动3个节点的副本集： 1mlaunch --replicaset Replica Set Parameters以下参数更改了副本集的设置方式。这些参数要求您从所需参数中选择--replicaset选项。 The following parameters change how a replica set is set up. These parameters require that you picked the --replicaset option from the required parameters. --nodes N 指定此副本集的数据承载节点（不包括仲裁器）的数量。预设值为3 例如: 1mlaunch --replicaset --nodes 5 这个命令启动5个mongod实例并配置为一个副本集 --arbiter 如果存在此参数，则将附加arbiter添加到副本集。目前，mlaunch仅支持添加一个arbiter。您也可以启动其他arbiter并将其手动添加到副本集 例如: 1mlaunch --replicaset --nodes 2 --arbiter 此命令启动2个承载数据的mongod实例，并将一个arbiter添加到副本集，总共3个投票节点。 --name NAME 此选项使您可以修改副本集的名称。这将更改dbpath的名称和子目录。此选项仅允许用于单个副本集，而不适用于分片设置（副本集名称与分片名称等效）的分片设置中。默认名称是replset For example: 1mlaunch --replicaset --name &quot;my_rs_1&quot; 此命令将创建一个名为my_rs_1的副本集，并将dbpath和日志文件存储在./data/my_rs_1下 Sharding Parameters以下参数会影响分片环境的设置。每个分片将是先前指定设置的副本，无论是单个实例还是副本集。 --sharded S [S ...] 如果指定此参数，则启用分片，并且mlaunch将创建指定数量的分片，并将这些分片一起添加到分片群集中。该参数可以通过两种方式工作：通过指定单个数字（即分片的数量），或通过指定分片的名称列表。 例如: 1mlaunch --single --sharded 3 该命令将创建一个包含3个分片的环境，每个分片由一个独立节点组成。碎片名称为shard0001，shard0002，shard0003。它还将默认创建1个配置服务器和1个mongos 例如: 1mlaunch --replicaset --sharded tic tac toe 此命令将创建3个分片，分别名为tic，tac和toe. 每个分片将包含一个副本集（默认情况下）为3个节点. 它还将默认创建1个配置服务器和1个mongos. --config N 此参数确定在分片环境中启动了多少个配置服务器。默认数字是1。N的唯一有效选项是1或3 --csrs 此参数可将配置服务器用作副本集（CSRS），而不是较早的同步集群连接配置（SCCC） MongoDB 3.2+支持CSRS部署选项，而从MongoDB 3.4开始，默认（唯一）支持的选项是CSRS部署选项。 If you are using MongoDB 3.4 and greater, mlaunch will use CSRS by default. Changed in version 1.2.3 CSRS config servers will no longer include incompatible settings, such as: --storageEngine – CSRS config servers will always use WiredTiger. --arbiter – CSRS config servers cannot have any arbiter. --mongos N 此参数确定在分片环境中启动了多少个mongos实例。默认数字为1。使用此设置，默认值可以更改为N个mongos实例。 Authentication Parameters --auth 此参数在您的设置上启用身份验证。它可以透明地用于单个实例（需要--auth）以及副本集和分片环境（需要--keyFile）。无需另外指定keyfile，mlaunch将为您生成一个随机keyfile。 用户名和密码也将被设置，或者在mongos为分片的环境中，或对副本集主节点或在一个节点上。 A username and password will also be set up, either on the mongos for sharded environments, or on the primary node for replica sets or on a single node. --username 此参数将默认用户名用户更改为指定用户 --password 此参数将默认密码password更改为指定的密码 --auth-db 此参数从admin更改默认数据库，将在其中创建用户。(不要改这个参数默认值) 默认密码是故意在选择易于记忆或猜测。 mlaunch是为测试和问题再现，而不是用于生产。 因为用户名和密码都以明文形式包含在data/.mlaunch_startup文件中，所以即使是强密码也无法保证在mlaunch生成的环境中的安全性。 --auth-roles This parameter changes the initial default roles that the user will receive. The default roles are dbAdminAnyDatabase, readWriteAnyDatabase, userAdminAnyDatabase and clusterAdmin. You can provide different roles with this parameter, separated by spaces. If you change the default roles, it may not be possible for mlaunch to execute certain commands due to missing privileges. This may lead to unexpected behavior for some mlaunch operations, like for example mlaunch stop, which uses the internal shutdown command. If this is the case, use mlaunch kill instead. 例如: 1mlaunch --sharded 2 --single --auth --auth-user thomas --auth-password my_s3cr3t_p4ssw0rd This command would start a sharded cluster with 2 single shards, 1 config server, 1 mongos, and create the user thomas with password my_s3cr3t_p4ssw0rd. It will use the default roles and place the user in the admin database. mlaunch will Optional Parameters --port PORT 使用指定的PORT值作为第一个实例的起始端口号，其余实例（mongod / mongos）以PORT值递增作为自己的算口号使用。 默认情况下，MongoDB起始端口值为标准端口27017。使用此参数可以在不同端口范围上并行启动多个设置。 例如: 1mlaunch --replicaset --nodes 3 --port 30000 此命令将使用端口30000、30001和30002启动3个节点的副本集。 --binarypath PATH 将设置mlaunch在其中查找mongod和mongos的二进制文件的路径到提供的PATH。 默认情况下，$PATH环境变量用于确定启动哪个二进制文件。 您可以使用此选项覆盖默认设置。 例如，如果您编译自己的源代码并希望mlaunch使用已编译的版本，则这很有用。 例如： 1mlaunch --single --binarypath ./build/bin 此命令将在./build/bin/mongod中查找mongod二进制文件，而不是默认位置。 kill通过发送SIGTERM(15)信号，kill命令根据指定的标签停止当前环境中的部分或所有正在运行的节点。 如果未指定标签，则mlaunch kill将杀死所有节点。 如果指定了一个或多个标签，则mlaunch kill将仅杀死具有所有给定标签（设置交集）的节点。 即使没有具有clusterAdmin角色的管理员用户，此方法也有效。 可以使用--signal参数来指定其他信号，而不是SIGTERM信号。 （在Windows上不可用） Usage1mlaunch kill [TAG [TAG ...]] [--signal S] [--dir DIR] [--verbose] Tag ParametersThe following tags are used with mlaunch, although not all tags are present in every environment: all: 当前环境中的所有节点 running: 当前正在运行的节点 down: 所有关闭的节点 mongos: 所有mongos进程 mongod: 所有mongod进程(包括arbiters和config servers). config: 所有config servers shard: 该标签仅用于标识特定的分片号（请参见下文） &lt;shard name&gt;: 对于分片环境，分片的每个成员都将分片名称作为标签，例如“shard-a” primary: 所有正在运行的主节点 secondary: 所有正在运行的secondary节点 arbiter: 所有arbiter &lt;port number&gt;: 每个节点都将其端口号作为标记 如果为kill命令指定单个标签，则匹配该标签的节点将被杀死。 如果指定了多个标签，则仅杀死与所有标签匹配的节点。 每个标签都会进一步缩小匹配范围。(貌似意思是取连个标签的交集) 例: 1mlaunch kill 该命令将杀死当前环境中的所有正在运行的节点。 例: 1mlaunch kill mongos 该命令将杀死当前环境中所有正在运行的mongos进程 例: 1mlaunch kill shard-a secondary 该命令将杀死当前环境中名为“ shard-a”的所有正在运行的secondary节点。 例: 1mlaunch kill config primary 该命令不会杀死任何节点，因为没有节点同时具有config和primary标签。 例: 1mlaunch kill 27017 此命令将杀死在端口27017上运行的节点。 另外，某些标签可以与后续编号组合。 这些标签是：mongos，shard，config，secondary。 例: 1mlaunch kill shard 1 This command kills all members of shard 1 in the current sharded environment. 例: 1mlaunch kill shard 2 primary This command kills the primary of the second shard in the current sharded environment. 例: 1mlaunch kill secondary 1 This command kills the first secondary node of all shards if the environment is sharded. If the environment is a replicaset, it only applies to the first secondary. 如果是shard, kill所有shared的第一个secondary节点. 如果是replicaset, 只kill第一个secondary 例: 1mlaunch kill 该命令将信号SIGTERM（15）发送到当前环境中的所有正在运行的进程。 例: 1mlaunch kill --signal SIGUSR1 此命令将信号SIGUSR1（30）发送到当前环境中的所有正在运行的进程，这在MongoDB中导致日志轮换log rotation。 startstart命令根据指定的标签启动当前环境中当前处于关闭状态的某些或所有节点。如果未指定标签，则mlaunch start将启动所有节点。如果指定了一个或多个标签，则启动将仅启动具有所有给定标签的节点（设置交集）。 Usage1mlaunch start [TAG [TAG ...]] [--dir DIR] [--verbose] Tag ParametersThe following tags are used with mlaunch, although not all tags are present in every environment: all: 当前环境中的所有节点 running: 当前正在运行的节点 down: 所有关闭的节点 mongos: 所有mongos进程 mongod: 所有mongod进程(包括arbiters和config servers). config: 所有config servers shard: 该标签仅用于标识特定的分片号（请参见下文） &lt;shard name&gt;: 对于分片环境，分片的每个成员都将分片名称作为标签，例如“shard-a” arbiter: 所有arbiter &lt;port number&gt;: 每个节点都将其端口号作为标记 Different to the stop command, there tags for primary and secondary are not available for the start command. This is because the replica set state of a running node is undetermined. stopstop命令通过将shutdown命令发送到mongod或mongos实例来停止当前环境中某些或所有正在运行的节点（取决于指定的标签）。 如果未指定标签，则mlaunch stop将停止所有节点。 如果指定了一个或多个标签，则mlaunch stop将仅停止具有所有给定标签的节点（设置交集）。 在经过身份验证的环境中，stop命令要求管理员数据库中的用户具有clusterAdmin角色。 否则，停止命令将不会成功。 在这种情况下，可以改用kill命令。 Changed in version 1.2.3 As of version 1.2.3, the stop command is an alias for the kill command. Usage1mlaunch stop [TAG [TAG ...]] [--dir DIR] [--verbose] Tag ParametersThe tags for the stop command are the same as for kill. restartrestart命令将停止，然后重新启动当前环境中的某些或所有节点，具体取决于指定的标签。 为方便起见，添加了它，其行为类似于连续的停止和启动命令。 如果未指定标签，则mlaunch restart将重新启动所有节点。 如果指定了一个或多个标签，则mlaunch restart将仅重新启动具有所有给定标签的节点（设置交集）。 Usage1mlaunch restart [TAG [TAG ...]] [--dir DIR] [--verbose] Tag ParametersSee start and stop. listlist命令显示当前环境中所有节点的概述，以及它们的状态（运行/关闭）和端口。 使用可选的–verbose标志，list命令还显示每个节点的所有标签。 Usage1mlaunch list [--dir DIR] [--startup] [--tags] For example: 1234567891011121314151617181920mlaunch listPROCESS STATUS PORTmongos running 27017mongos running 27018config server running 27025config server running 27026config server down 27027shard01 primary running 27019 secondary running 27020 arbiter running 27021shard02 mongod down 27022 mongod down 27023 mongod down 27024 此命令显示所有节点的列表，它们的状态和端口号。 在这种情况下，环境由以下命令初始化： 1mlaunch --sharded 2 --replicaset --nodes 2 --arbiter --config 3 --mongos 2 Optional Parameters --tags 此选项还显示一列，其中包含实例可以使用的所有标签。 标签可用于将某些实例作为启动，停止，终止等命令的目标。 For example: 1234567mlaunch list --tagsPROCESS STATUS PORT TAGSprimary running 27017 27017, all, mongod, primary, runningsecondary running 27018 27018, all, mongod, running, secondarymongod down 27019 27019, all, down, mongod 该命令显示所有节点的列表，它们的状态和端口号，以及它们的标签。 在这种情况下，环境由以下命令初始化： 1mlaunch --replicaset --startup 此选项还显示带有启动命令的列，该启动命令用于运行给定实例。 如果需要手动启动实例，这很有用。 For example: 1234567mlaunch list --startupPROCESS PORT STATUS PID STARTUP COMMANDsecondary 27017 running 4264 mongod --replSet replset --dbpath /tmp/data/replset/rs1/db --logpath /tmp/data/replset/rs1/mongod.log --port 27017 --logappend --fork -vvmongod 27018 running 4267 mongod --replSet replset --dbpath /tmp/data/replset/rs2/db --logpath /tmp/data/replset/rs2/mongod.log --port 27018 --logappend --fork -vvmongod 27019 running 4270 mongod --replSet replset --dbpath /tmp/data/replset/rs3/db --logpath /tmp/data/replset/rs3/mongod.log --port 27019 --logappend --fork -vv 此命令显示所有节点的列表，它们的状态和端口号，以及它们的启动命令。","categories":[],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://fuxkdb.com/tags/MongoDB/"},{"name":"mtools","slug":"mtools","permalink":"http://fuxkdb.com/tags/mtools/"}]},{"title":"Canal dynamicTopic问题","slug":"2020-01-12-Canal-dynamicTopic问题","date":"2020-01-12T04:23:00.000Z","updated":"2020-03-07T13:21:57.359Z","comments":true,"path":"2020/01/12/2020-01-12-Canal-dynamicTopic问题/","link":"","permalink":"http://fuxkdb.com/2020/01/12/2020-01-12-Canal-dynamicTopic%E9%97%AE%E9%A2%98/","excerpt":"未来同事跑了几个月的canal突然报下面的错, 使用了dynamicTopic. 其实我没有用过dynamicTopic, 只能搜一搜issue 关于dynamicTopic和partitionHash的说明 canal.mq.dynamicTopic 表达式说明12345678910111213141516&gt;canal 1.1.3版本之后, 支持配置格式：schema 或 schema.table，多个配置之间使用逗号或分号分隔&gt;例子1：test\\\\.test 指定匹配的单表，发送到以test_test为名字的topic上&gt;例子2：.*\\\\..* 匹配所有表，则每个表都会发送到各自表名的topic上&gt;例子3：test 指定匹配对应的库，一个库的所有表都会发送到库名的topic上&gt;例子4：test\\\\.* 指定匹配的表达式，针对匹配的表会发送到各自表名的topic上&gt;例子5：test,test1\\\\.test1，指定多个表达式，会将test库的表都发送到test的topic上，test1\\\\.test1的表发送到对应的test1_test1 topic上，其余的表发送到默认的canal.mq.topic值&gt;为满足更大的灵活性，允许对匹配条件的规则指定发送的topic名字，配置格式：topicName:schema 或 topicName:schema.table&gt;例子1: test:test\\\\.test 指定匹配的单表，发送到以test为名字的topic上&gt;例子2: test:.*\\\\..* 匹配所有表，因为有指定topic，则每个表都会发送到test的topic下&gt;例子3: test:test 指定匹配对应的库，一个库的所有表都会发送到test的topic下&gt;例子4：testA:test\\\\.* 指定匹配的表达式，针对匹配的表会发送到testA的topic下&gt;例子5：test0:test,test1:test1\\\\.test1，指定多个表达式，会将test库的表都发送到test0的topic下，test1\\\\.test1的表发送到对应的test1的topic下，其余的表发送到默认的canal.mq.topic值&gt;大家可以结合自己的业务需求，设置匹配规则，建议MQ开启自动创建topic的能力 canal.mq.partitionHash 表达式说明123456789&gt;canal 1.1.3版本之后, 支持配置格式：schema.table:pk1^pk2，多个配置之间使用逗号分隔&gt;例子1：test\\\\.test:pk1^pk2 指定匹配的单表，对应的hash字段为pk1 + pk2&gt;例子2：.*\\\\..*:id 正则匹配，指定所有正则匹配的表对应的hash字段为id&gt;例子3：.*\\\\..*:$pk$ 正则匹配，指定所有正则匹配的表对应的hash字段为表主键(自动查找)&gt;例子4: 匹配规则啥都不写，则默认发到0这个partition上&gt;例子5：.*\\\\..* ，不指定pk信息的正则匹配，将所有正则匹配的表,对应的hash字段为表名&gt;按表hash: 一张表的所有数据可以发到同一个分区，不同表之间会做散列 (会有热点表分区过大问题)&gt;例子6: test\\\\.test:id,.\\\\..* , 针对test的表按照id散列,其余的表按照table散列 注意：大家可以结合自己的业务需求，设置匹配规则，多条匹配规则之间是按照顺序进行匹配(命中一条规则就返回) 其他详细参数可参考Canal AdminGuide","text":"未来同事跑了几个月的canal突然报下面的错, 使用了dynamicTopic. 其实我没有用过dynamicTopic, 只能搜一搜issue 关于dynamicTopic和partitionHash的说明 canal.mq.dynamicTopic 表达式说明12345678910111213141516&gt;canal 1.1.3版本之后, 支持配置格式：schema 或 schema.table，多个配置之间使用逗号或分号分隔&gt;例子1：test\\\\.test 指定匹配的单表，发送到以test_test为名字的topic上&gt;例子2：.*\\\\..* 匹配所有表，则每个表都会发送到各自表名的topic上&gt;例子3：test 指定匹配对应的库，一个库的所有表都会发送到库名的topic上&gt;例子4：test\\\\.* 指定匹配的表达式，针对匹配的表会发送到各自表名的topic上&gt;例子5：test,test1\\\\.test1，指定多个表达式，会将test库的表都发送到test的topic上，test1\\\\.test1的表发送到对应的test1_test1 topic上，其余的表发送到默认的canal.mq.topic值&gt;为满足更大的灵活性，允许对匹配条件的规则指定发送的topic名字，配置格式：topicName:schema 或 topicName:schema.table&gt;例子1: test:test\\\\.test 指定匹配的单表，发送到以test为名字的topic上&gt;例子2: test:.*\\\\..* 匹配所有表，因为有指定topic，则每个表都会发送到test的topic下&gt;例子3: test:test 指定匹配对应的库，一个库的所有表都会发送到test的topic下&gt;例子4：testA:test\\\\.* 指定匹配的表达式，针对匹配的表会发送到testA的topic下&gt;例子5：test0:test,test1:test1\\\\.test1，指定多个表达式，会将test库的表都发送到test0的topic下，test1\\\\.test1的表发送到对应的test1的topic下，其余的表发送到默认的canal.mq.topic值&gt;大家可以结合自己的业务需求，设置匹配规则，建议MQ开启自动创建topic的能力 canal.mq.partitionHash 表达式说明123456789&gt;canal 1.1.3版本之后, 支持配置格式：schema.table:pk1^pk2，多个配置之间使用逗号分隔&gt;例子1：test\\\\.test:pk1^pk2 指定匹配的单表，对应的hash字段为pk1 + pk2&gt;例子2：.*\\\\..*:id 正则匹配，指定所有正则匹配的表对应的hash字段为id&gt;例子3：.*\\\\..*:$pk$ 正则匹配，指定所有正则匹配的表对应的hash字段为表主键(自动查找)&gt;例子4: 匹配规则啥都不写，则默认发到0这个partition上&gt;例子5：.*\\\\..* ，不指定pk信息的正则匹配，将所有正则匹配的表,对应的hash字段为表名&gt;按表hash: 一张表的所有数据可以发到同一个分区，不同表之间会做散列 (会有热点表分区过大问题)&gt;例子6: test\\\\.test:id,.\\\\..* , 针对test的表按照id散列,其余的表按照table散列 注意：大家可以结合自己的业务需求，设置匹配规则，多条匹配规则之间是按照顺序进行匹配(命中一条规则就返回) 其他详细参数可参考Canal AdminGuide canal.log报INVALID_TOPIC_EXCEPTION 12345672020-01-10 15:58:31.552 [canal-instance-scan-0] INFO com.alibaba.otter.canal.deployer.CanalController - auto notify start example successful.2020-01-10 16:37:26.946 [canal-instance-scan-0] INFO com.alibaba.otter.canal.deployer.CanalController - auto notify start mgr_fanboshi successful.2020-01-10 16:38:03.992 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 10 : &#123;=INVALID_TOPIC_EXCEPTION&#125;2020-01-10 16:38:04.096 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 11 : &#123;=INVALID_TOPIC_EXCEPTION&#125;2020-01-10 16:38:04.200 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 12 : &#123;=INVALID_TOPIC_EXCEPTION&#125;2020-01-10 16:38:04.304 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 13 : &#123;=INVALID_TOPIC_EXCEPTION&#125;2020-01-10 16:38:04.409 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Error while fetching metadata with correlation id 14 : &#123;=INVALID_TOPIC_EXCEPTION&#125; instance的log中会报错 12345678910111213141516171819202122232020-01-10 16:37:27.159 [destination = mgr_fanboshi , address = /172.18.8.200:3399 , EventParser] WARN c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - ---&gt; find start position successfully, EntryPosition[included=false,journalName=0080323399-mysql-bin.000004,position=1065960584,serverId=&lt;null&gt;,gtid=69ce1dcb-1b67-5e4b-9945-7cc64c64a14f:1-566281:1000009-2590159:3005066-3017688,ce947b65-6b70-539b-8e6a-59e9bcde28a0:206083443-206083524:207812793-207825891:207921053-207921085,d58aa5c2-b773-5944-bfe9-0a1142ef87f4:1706527:2447252-2447264,f5d4a39c-7800-52ec-b181-8c751ba2d078:868915-869106:1968590-1968591,timestamp=&lt;null&gt;] cost : 2ms , the next step is binlog dump2020-01-10 16:39:03.987 [pool-3-thread-2] ERROR com.alibaba.otter.canal.kafka.CanalKafkaProducer - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. at com.alibaba.otter.canal.kafka.CanalKafkaProducer.produce(CanalKafkaProducer.java:215) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.kafka.CanalKafkaProducer.send(CanalKafkaProducer.java:179) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.kafka.CanalKafkaProducer.send(CanalKafkaProducer.java:117) ~[canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter.worker(CanalMQStarter.java:183) [canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter.access$500(CanalMQStarter.java:23) [canal.server-1.1.4.jar:na] at com.alibaba.otter.canal.server.CanalMQStarter$CanalMQRunnable.run(CanalMQStarter.java:225) [canal.server-1.1.4.jar:na] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. at org.apache.kafka.clients.producer.KafkaProducer$FutureFailure.&lt;init&gt;(KafkaProducer.java:1150) ~[kafka-clients-1.1.1.jar:na] at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:846) ~[kafka-clients-1.1.1.jar:na] at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:784) ~[kafka-clients-1.1.1.jar:na] at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:671) ~[kafka-clients-1.1.1.jar:na] at com.alibaba.otter.canal.kafka.CanalKafkaProducer.produce(CanalKafkaProducer.java:199) ~[canal.server-1.1.4.jar:na] ... 8 common frames omittedCaused by: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. 搜到一个issue给了我一些启发https://github.com/alibaba/canal/issues/1716 在instance.properties里面配置同步库配置canal.instance.filter.regex=A\\\\..*,B\\\\..*,C\\\\..* 消息队列配置使用正则路由schema和table格式topic:schema.table,topic:schema.table,topic:schema.tablecanal.mq.dynamicTopic=topic_A:A,topic_B:B,topic_C:C最好配置一个canal.mq.topic作为默认的topic，在实践中发现，有一些mysql schema的binlog也会读进来（建表语句，grant语句等），如果没有这个默认的topic，会报找不到分区的错误，从而导致canal停止写入。canal.mq.topic=a_default_topic 分区设置 如果每个topic只有一个分区设置如下canal.mq.partition=0 如果每个topic有多个分区，且希望按照表名做hash进行如下配置# hash partition configcanal.mq.partitionsNum=3canal.mq.partitionHash=.*\\\\..* 是不是canal.mq.topic这个参数随便写一个存在的topic，然后主要设置在这里对吧？canal.mq.dynamicTopic=topic_A:A,topic_B:B,topic_C:C 是的。canal.mq.topic算是一个默认的，没有匹配到的schema和table的日志都会输入到那个默认的里面去。消费的时候只关心你设置的动态分区。 我的理解就是一些不是我们匹配规则所需要的消息因为没有配置默认topic而找不到topic可写, 于是才会报错INVALID_TOPIC_EXCEPTION 虽然未来同事配置了canal.instance.filter.regex=broker\\\\..* 但是有可能grant之类的语句还是会有问题吧 最后附上自己1.1.4的配置吧 canal.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142########################################################## common argument ############################################################### tcp bind ipcanal.ip = 172.18.8.32# register ip to zookeepercanal.register.ip = 172.18.8.32canal.port = 11111canal.metrics.pull.port = 11112# canal instance user/passwd# canal.user = canal# canal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458# canal admin config#canal.admin.manager = 127.0.0.1:8089canal.admin.port = 11110canal.admin.user = admincanal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441canal.zkServers = 172.18.12.212:2181,172.18.12.213:2181# flush data to zkcanal.zookeeper.flush.period = 1000canal.withoutNetty = true# tcp, kafka, RocketMQcanal.serverMode = kafka# flush meta cursor/parse position to filecanal.file.data.dir = $&#123;canal.conf.dir&#125;canal.file.flush.period = 1000## memory store RingBuffer size, should be Math.pow(2,n)canal.instance.memory.buffer.size = 16384## memory store RingBuffer used memory unit size , default 1kbcanal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZEcanal.instance.memory.batch.mode = MEMSIZEcanal.instance.memory.rawEntry = true## detecing configcanal.instance.detecting.enable = false#canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()canal.instance.detecting.sql = select 1canal.instance.detecting.interval.time = 3canal.instance.detecting.retry.threshold = 3canal.instance.detecting.heartbeatHaEnable = false# support maximum transaction size, more than the size of the transaction will be cut into multiple transactions deliverycanal.instance.transaction.size = 1024# mysql fallback connected to new master should fallback timescanal.instance.fallbackIntervalInSeconds = 60# network configcanal.instance.network.receiveBufferSize = 16384canal.instance.network.sendBufferSize = 16384canal.instance.network.soTimeout = 30# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.instance.filter.table.error = falsecanal.instance.filter.rows = falsecanal.instance.filter.transaction.entry = false# binlog format/image checkcanal.instance.binlog.format = ROW,STATEMENT,MIXED canal.instance.binlog.image = FULL,MINIMAL,NOBLOB# binlog ddl isolationcanal.instance.get.ddl.isolation = false# parallel parser configcanal.instance.parser.parallel = true## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()#canal.instance.parser.parallelThreadSize = 16## disruptor ringbuffer size, must be power of 2canal.instance.parser.parallelBufferSize = 256# table meta tsdb infocanal.instance.tsdb.enable = falsecanal.instance.tsdb.dir = $&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url = jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;canal.instance.tsdb.dbUsername = canalcanal.instance.tsdb.dbPassword = canal# dump snapshot interval, default 24 hourcanal.instance.tsdb.snapshot.interval = 24# purge snapshot expire , default 360 hour(15 days)canal.instance.tsdb.snapshot.expire = 360# aliyun ak/sk , support rds/mqcanal.aliyun.accessKey =canal.aliyun.secretKey =########################################################## destinations ##############################################################canal.destinations =# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml#canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xmlcanal.instance.global.mode = springcanal.instance.global.lazy = falsecanal.instance.global.manager.address = $&#123;canal.admin.manager&#125;#canal.instance.global.spring.xml = classpath:spring/memory-instance.xml#canal.instance.global.spring.xml = classpath:spring/file-instance.xmlcanal.instance.global.spring.xml = classpath:spring/default-instance.xml########################################################### MQ ###############################################################canal.mq.servers = 172.18.12.212:9092canal.mq.retries = 0canal.mq.batchSize = 16384canal.mq.maxRequestSize = 33554432canal.mq.lingerMs = 100canal.mq.bufferMemory = 33554432canal.mq.canalBatchSize = 50canal.mq.canalGetTimeout = 100canal.mq.flatMessage = truecanal.mq.compressionType = nonecanal.mq.acks = all#canal.mq.properties. =canal.mq.transaction = falsecanal.mq.extraParams.enable.idempotence=truecanal.mq.extraParams.max.in.flight.requests.per.connection=1#canal.mq.producerGroup = test# Set this value to &quot;cloud&quot;, if you want open message trace feature in aliyun.#canal.mq.accessChannel = local# aliyun mq namespace#canal.mq.namespace =########################################################### Kafka Kerberos Info ################################################################canal.mq.kafka.kerberos.enable = false#canal.mq.kafka.kerberos.krb5FilePath = &quot;../conf/kerberos/krb5.conf&quot;#canal.mq.kafka.kerberos.jaasFilePath = &quot;../conf/kerberos/jaas.conf&quot; instance.properties 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859################################################### mysql serverId , v1.0.26+ will autoGen# canal.instance.mysql.slaveId=0# enable gtid use true/falsecanal.instance.gtidon=true# position infocanal.instance.master.address=172.18.8.200:3372canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=canal.instance.master.gtid=# rds oss binlogcanal.instance.rds.accesskey=canal.instance.rds.secretkey=canal.instance.rds.instanceId=# table meta tsdb infocanal.instance.tsdb.enable=false#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb#canal.instance.tsdb.dbUsername=canal#canal.instance.tsdb.dbPassword=canal#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position =#canal.instance.standby.timestamp =#canal.instance.standby.gtid=# username/passwordcanal.instance.dbUsername=canal_rcanal.instance.dbPassword=canal1234!@#$canal.instance.connectionCharset = UTF-8# enable druid Decrypt database passwordcanal.instance.enableDruid=false#canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ==# table regexcanal.instance.filter.regex=fanboshi\\\\..*# table black regexcanal.instance.filter.black.regex=# table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch# table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch# mq configcanal.mq.topic=#canal.mq.topic=cherry_default# dynamic topic route by schema or table regex#canal.mq.dynamicTopic=mytest1.user,mytest2\\\\..*,.*\\\\..*canal.mq.dynamicTopic= cherry:fanboshi#canal.mq.partition=0# hash partition config#canal.mq.partitionsNum=3#canal.mq.partitionHash=test.table:id^name,.*\\\\..*#################################################","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Canal","slug":"Canal","permalink":"http://fuxkdb.com/tags/Canal/"}]},{"title":"译文 Streaming MySQL Backups with Percona XtraBackup – Another Alternative","slug":"2020-01-09-译文-Streaming-MySQL-Backups-with-Percona-XtraBackup-–-Another-Alternative","date":"2020-01-09T02:01:00.000Z","updated":"2021-01-09T02:01:55.650Z","comments":true,"path":"2020/01/09/2020-01-09-译文-Streaming-MySQL-Backups-with-Percona-XtraBackup-–-Another-Alternative/","link":"","permalink":"http://fuxkdb.com/2020/01/09/2020-01-09-%E8%AF%91%E6%96%87-Streaming-MySQL-Backups-with-Percona-XtraBackup-%E2%80%93-Another-Alternative/","excerpt":"Streaming MySQL Backups with Percona XtraBackup – Another Alternative使用Percona XtraBackup流式传输MySQL备份-另一种选择 今天我将向你介绍另一种使用Percona XtraBackup在服务器间创建并传递备份的方式. 这个方法和网上的其他方法有什么不同吗? 其实并没有很大不同, 但是在性能和可用性方面很有趣. 我们将xbstream实用程序与pigz和socat的功能结合在一起, 以在拥有多个处理器的情况下利用多处理功能, 同时在此组件成为瓶颈的情况下, 减少对网络带宽的使用. 因此, 让我们解释每个组件: socat: 代表SOcket CAT. 它是一个用于在两个地址之间进行数据传输的实用程序. 使socat如此通用的原因是地址可以代表网络套接字, 任何文件描述符, Unix域数据报或流套接字, TCP和UDP（在IPv4和IPv6上）, 在IPv4 / IPv6上的SOCKS 4 / 4a, SCTP , PTY, 数据报和流套接字, 命名管道和未命名管道, 原始IP套接字, OpenSSL, 或者在Linux上甚至任何任意网络设备上 Pigz: 它代表gzip的并行实现, 它是gzip的全功能替代品, 在压缩数据时利用了多个处理器和多个内核. xbstream : （具有并行性）并行处理多个文件. 所需软件包: pigz, socat, 当然还有 Percona XtraBackup","text":"Streaming MySQL Backups with Percona XtraBackup – Another Alternative使用Percona XtraBackup流式传输MySQL备份-另一种选择 今天我将向你介绍另一种使用Percona XtraBackup在服务器间创建并传递备份的方式. 这个方法和网上的其他方法有什么不同吗? 其实并没有很大不同, 但是在性能和可用性方面很有趣. 我们将xbstream实用程序与pigz和socat的功能结合在一起, 以在拥有多个处理器的情况下利用多处理功能, 同时在此组件成为瓶颈的情况下, 减少对网络带宽的使用. 因此, 让我们解释每个组件: socat: 代表SOcket CAT. 它是一个用于在两个地址之间进行数据传输的实用程序. 使socat如此通用的原因是地址可以代表网络套接字, 任何文件描述符, Unix域数据报或流套接字, TCP和UDP（在IPv4和IPv6上）, 在IPv4 / IPv6上的SOCKS 4 / 4a, SCTP , PTY, 数据报和流套接字, 命名管道和未命名管道, 原始IP套接字, OpenSSL, 或者在Linux上甚至任何任意网络设备上 Pigz: 它代表gzip的并行实现, 它是gzip的全功能替代品, 在压缩数据时利用了多个处理器和多个内核. xbstream : （具有并行性）并行处理多个文件. 所需软件包: pigz, socat, 当然还有 Percona XtraBackup 以下示例操作中的环境信息Source: 进行备份的来源数据库(MySQL 5.7 installed on CentOS 7.8) Target: 备份将发送到的目的地(MySQL 5.7 installed on CentOS 7.8) 步骤 在Source和Target安装依赖包 12Source # yum install -y pigz socatTarget # yum install -y pigz socat 如果你尚未在源端和目标端安装Percona XtraBackup, 请参照以下步骤https://www.percona.com/doc/percona-xtrabackup/2.4/index.html#installation 确保您拥有一个具有适当特权的用户, 可以在源数据库上进行备份: 12345+---------------------------------------------------------------------------+| Grants for bkpuser@localhost |+---------------------------------------------------------------------------+| GRANT RELOAD, PROCESS, REPLICATION CLIENT ON *.* TO &#x27;bkpuser&#x27;@&#x27;localhost&#x27; |+---------------------------------------------------------------------------+ 在Target执行: 停止当前的数据库服务(如果有): 1Target # systemctl stop mysqld 删除datadir内容（假设它已使用默认设置安装）, 并确保您已登录到目标服务器！ 1Target # rm -rf /var/lib/mysql/* 最后, 我们将执行命令以从源（Source）接收备份: 1Target # socat -u TCP-LISTEN:4444,reuseaddr stdio | pigz -dc -p 4 - | xbstream —p 4 -x -C /var/lib/mysql 在Source执行命令以将备份发送到目标（Target）. 1Source # xtrabackup --defaults-file=/etc/my.cnf --backup --user=bkpuser --password=Bkpuser123! --stream=xbstream --parallel 4 --no-timestamp --target-dir=/tmp | pigz -k -1 -p4 - | socat -u stdio TCP:Target:4444 您将看到类似以下的输出: 12345678910111213141516171819202122232425xtrabackup: recognized server arguments: --datadir=/var/lib/mysql --server-id=1 --log_bin=/var/lib/mysql/mysql-bin --innodb_log_file_size=200M --innodb_log_files_in_group=2 --open_files_limit=65535 --parallel=4xtrabackup: recognized client arguments: --backup=1 --user=bkpuser --password=* --stream=xbstream --target-dir=/tmp200822 11:10:16 version_check Connecting to MySQL server with DSN &#x27;dbi:mysql:;mysql_read_default_group=xtrabackup&#x27; as &#x27;bkpuser&#x27; (using password: YES).200822 11:10:16 version_check Connected to MySQL server200822 11:10:16 version_check Executing a version check against the server...200822 11:10:16 version_check Done.200822 11:10:16 Connecting to MySQL server host: localhost, user: bkpuser, password: set, port: not set, socket: not setUsing server version 5.7.30-logxtrabackup version 2.4.20 based on MySQL server 5.7.26 Linux (x86_64) (revision id: c8b4056)xtrabackup: uses posix_fadvise().xtrabackup: cd to /var/lib/mysqlxtrabackup: open files limit requested 65535, set to 65535xtrabackup: using the following InnoDB configuration:xtrabackup: innodb_data_home_dir = .xtrabackup: innodb_data_file_path = ibdata1:12M:autoextendxtrabackup: innodb_log_group_home_dir = ./xtrabackup: innodb_log_files_in_group = 2xtrabackup: innodb_log_file_size = 209715200InnoDB: Number of pools: 1200822 11:10:16 &gt;&gt; log scanned up to (6724690490)xtrabackup: Generating a list of tablespacesInnoDB: Allocated tablespace ID 2 for mysql/plugin, old maximum was 0xtrabackup: Starting 4 threads for parallel data files transfer200822 11:10:16 [01] Streaming ./ibdata1...etc 完成第3步后, 您将在Target节点上看到如下输出: 12345678...MySQL binlog position: filename &#x27;mysql-bin.000091&#x27;, position &#x27;102205647&#x27;200822 11:10:21 [00] Streaming &lt;STDOUT&gt;200822 11:10:21 [00] ...done200822 11:10:21 [00] Streaming &lt;STDOUT&gt;200822 11:10:21 [00] ...donextrabackup: Transaction log of lsn (4308505553) to (4308505562) was copied.200822 11:10:21 completed OK! 第2步也将完成, 因此您必须在Target节点中执行以下命令: 1Target # xtrabackup --prepare --use-memory=1G --target-dir=/var/lib/mysql/ 从文档中: 使用xtrabackup –backup选项进行备份后, 首先需要准备它以进行还原. 在准备好数据文件之前, 它们在时间点上是不一致的, 因为它们是在程序运行时在不同的时间复制的, 并且在此过程中它们可能已被更改. 如果您尝试使用这些数据文件启动InnoDB, 它将检测到损坏并自身崩溃, 以防止您在损坏的数据上运行. 该xtrabackup -prepare步骤使得文件在单个时间瞬间完全一致, 这样你就可以在上面运行的InnoDB. 12Target # chown -R mysql:mysql /var/lib/mysql/*Target # systemctl start mysqld 您已经克隆了一个新数据库！ 当然, 您可以将新数据库设置为副本, 并在目标节点中执行以下附加步骤: 查看文件xtrabackup_binlog_info的内容, 它将类似于: 123Target # cat /var/lib/mysql/xtrabackup_binlog_infomysql-bin.000091 102205647 （我们假设在源数据库中创建了以下用户/授权, 如果没有, 请创建它） 123456mysql&gt; show grants for replicator;+----------------------------------------------------+| Grants for replicator@% |+----------------------------------------------------+| GRANT REPLICATION SLAVE ON *.* TO &#x27;replicator&#x27;@&#x27;%&#x27; |+----------------------------------------------------+ 连接到数据库, 然后运行: 1Target # mysql -u root -p 12mysql&gt; change master to master_host=&#x27;Source&#x27;,master_port=3306,master_user=&#x27;replicator&#x27;,master_password=&#x27;R3pl1c4t10n!&#x27;,master_log_file=&#x27;mysql-bin.000091&#x27;,master_log_pos=102205647;Query OK, 0 rows affected (0.00 sec) 12mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec) 1mysql&gt; pager egrep -i &quot;Master_Host|Master_User|Master_Port|file|behind&quot; 12345678910111213mysql&gt; show slave status\\G Master_Host: master Master_User: replicator Master_Port: 3306 Master_Log_File: mysql-bin.000091 Relay_Log_File: relay.000001 Relay_Master_Log_File: mysql-bin.000091 Until_Log_File: Master_SSL_CA_File: Seconds_Behind_Master: 0 Master_Info_File: mysql.slave_master_info1 row in set (0.00 sec) 就这样. 直播愉快！","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"TiDB Syncer不同表名库名同步且支持pt-osc改表","slug":"2019-12-12-TiDB-Syncer不同表名库名同步且支持pt-osc改表","date":"2019-12-12T13:55:00.000Z","updated":"2019-12-10T13:56:31.000Z","comments":true,"path":"2019/12/12/2019-12-12-TiDB-Syncer不同表名库名同步且支持pt-osc改表/","link":"","permalink":"http://fuxkdb.com/2019/12/12/2019-12-12-TiDB-Syncer%E4%B8%8D%E5%90%8C%E8%A1%A8%E5%90%8D%E5%BA%93%E5%90%8D%E5%90%8C%E6%AD%A5%E4%B8%94%E6%94%AF%E6%8C%81pt-osc%E6%94%B9%E8%A1%A8/","excerpt":"TiDB Syncer不同表名库名同步且支持pt-osc改表mysql端库名叫sysbench, 表名sbtest11tidb端库名ptosc_sysbench,表名ptosc_sbtest11 1234567891011121314151617181920[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;ptosc_sbtest11&quot;[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;~.*_sbtest11_new&quot;[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;~.*_sbtest11_old&quot;[[route-rules]]pattern-schema = &quot;sysbench&quot;pattern-table = &quot;sbtest11&quot;target-schema = &quot;ptosc_sysbench&quot;target-table = &quot;ptosc_sbtest11&quot;","text":"TiDB Syncer不同表名库名同步且支持pt-osc改表mysql端库名叫sysbench, 表名sbtest11tidb端库名ptosc_sysbench,表名ptosc_sbtest11 1234567891011121314151617181920[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;ptosc_sbtest11&quot;[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;~.*_sbtest11_new&quot;[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;~.*_sbtest11_old&quot;[[route-rules]]pattern-schema = &quot;sysbench&quot;pattern-table = &quot;sbtest11&quot;target-schema = &quot;ptosc_sysbench&quot;target-table = &quot;ptosc_sbtest11&quot; MySQL执行pt-osc 123456789101112131415161718192021222324252627282930313233343536373839404142#pt-online-schema-change --ask-pass --check-interval=1 --no-check-replication-filters --no-check-alter --no-version-check --chunk-size 3 --recursion-method=none --max-load=&#x27;Threads_running=200&#x27; --critical-load=&#x27;Threads_running=500&#x27; --recurse=0 --no-drop-old-table --alter=&quot;drop index c&quot; --print h=10.133.x.52,P=3308,u=fanboshi,D=sysbench,t=sbtest11,A=utf8 --executeEnter MySQL password: No slaves found. See --recursion-method if host node10-133-1-52 has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `sysbench`.`sbtest11`...Creating new table...CREATE TABLE `sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_binCreated new table sysbench._sbtest11_new OK.Altering new table...ALTER TABLE `sysbench`.`_sbtest11_new` drop index cAltered `sysbench`.`_sbtest11_new` OK.2019-12-04T22:32:44 Creating triggers...2019-12-04T22:32:44 Created triggers OK.2019-12-04T22:32:44 Copying approximately 3 rows...INSERT LOW_PRIORITY IGNORE INTO `sysbench`.`_sbtest11_new` (`id`, `k`, `c`, `pad`) SELECT `id`, `k`, `c`, `pad` FROM `sysbench`.`sbtest11` LOCK IN SHARE MODE /*pt-online-schema-change 11937 copy table*/2019-12-04T22:32:44 Copied rows OK.2019-12-04T22:32:44 Analyzing new table...2019-12-04T22:32:44 Swapping tables...RENAME TABLE `sysbench`.`sbtest11` TO `sysbench`.`__sbtest11_old`, `sysbench`.`_sbtest11_new` TO `sysbench`.`sbtest11`2019-12-04T22:32:44 Swapped original and new tables OK.Not dropping old table because --no-drop-old-table was specified.2019-12-04T22:32:44 Dropping triggers...DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_del`DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_upd`DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_ins`2019-12-04T22:32:44 Dropped triggers OK.Successfully altered `sysbench`.`sbtest11`. syncer日志 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381392019/12/04 22:32:05 syncer.go:754: [info] [query]CREATE TABLE `sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin [current pos](10523308-mysql-bin.000023, 557640598) [next pos](10523308-mysql-bin.000023, 557641108) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755555 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555562019/12/04 22:32:05 syncer.go:795: [info] [ddl][schema]sysbench [start]USE `ptosc_sysbench`; CREATE TABLE `ptosc_sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;2019/12/04 22:32:05 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557641108 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555562019/12/04 22:32:05 syncer.go:803: [info] [ddl][end]USE `ptosc_sysbench`; CREATE TABLE `ptosc_sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;2019/12/04 22:32:05 ddl.go:110: [warning] will split alter table statement: ALTER TABLE `sysbench`.`_sbtest11_new` drop index c2019/12/04 22:32:05 ast.go:521: [info] spec &amp;&#123;node:&#123;text:&#125; IfExists:false IfNotExists:false NoWriteToBinlog:false OnAllPartitions:false Tp:6 Name:c Constraint:&lt;nil&gt; Options:[] OrderByList:[] NewTable:&lt;nil&gt; NewColumns:[] NewConstraints:[] OldColumnName:&lt;nil&gt; NewColumnName:&lt;nil&gt; Position:&lt;nil&gt; LockType: Algorithm:DEFAULT Comment: FromKey: ToKey: Partition:&lt;nil&gt; PartitionNames:[] PartDefinitions:[] WithValidation:false Num:0 Visibility:0&#125;2019/12/04 22:32:05 ddl.go:114: [warning] splitted alter table statement: [ALTER TABLE `sysbench`.`_sbtest11_new` DROP INDEX `c`]2019/12/04 22:32:05 syncer.go:754: [info] [query]ALTER TABLE `sysbench`.`_sbtest11_new` drop index c [current pos](10523308-mysql-bin.000023, 557641108) [next pos](10523308-mysql-bin.000023, 557641303) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755556 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555572019/12/04 22:32:05 syncer.go:795: [info] [ddl][schema]sysbench [start]USE `ptosc_sysbench`; ALTER TABLE `ptosc_sysbench`.`_sbtest11_new` DROP INDEX `c`;2019/12/04 22:32:06 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557641303 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555572019/12/04 22:32:06 syncer.go:803: [info] [ddl][end]USE `ptosc_sysbench`; ALTER TABLE `ptosc_sysbench`.`_sbtest11_new` DROP INDEX `c`;2019/12/04 22:32:06 syncer.go:754: [info] [query]DROP TABLE IF EXISTS `_sbtest11_new` /* generated by server */ [current pos](10523308-mysql-bin.000023, 557641303) [next pos](10523308-mysql-bin.000023, 557641509) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755557 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555582019/12/04 22:32:06 syncer.go:795: [info] [ddl][schema]sysbench [start]USE `ptosc_sysbench`; DROP TABLE IF EXISTS `ptosc_sysbench`.`_sbtest11_new`;2019/12/04 22:32:07 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557641509 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555582019/12/04 22:32:07 syncer.go:803: [info] [ddl][end]USE `ptosc_sysbench`; DROP TABLE IF EXISTS `ptosc_sysbench`.`_sbtest11_new`;2019/12/04 22:32:30 syncer.go:955: [info] [syncer]total events = 4, tps = 0.100000, master-binlog = (10523308-mysql-bin.000023, 557641509), master-binlog-gtid=a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755558, syncer-binlog = (10523308-mysql-bin.000023, 557641509), syncer-binlog-gtid = a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555582019/12/04 22:32:44 syncer.go:754: [info] [query]CREATE TABLE `sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin [current pos](10523308-mysql-bin.000023, 557641509) [next pos](10523308-mysql-bin.000023, 557642019) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755558 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555592019/12/04 22:32:44 syncer.go:795: [info] [ddl][schema]sysbench [start]USE `ptosc_sysbench`; CREATE TABLE `ptosc_sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;2019/12/04 22:32:46 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557642019 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555592019/12/04 22:32:46 syncer.go:803: [info] [ddl][end]USE `ptosc_sysbench`; CREATE TABLE `ptosc_sysbench`.`_sbtest11_new` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) COLLATE utf8mb4_bin NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;2019/12/04 22:32:46 ddl.go:110: [warning] will split alter table statement: ALTER TABLE `sysbench`.`_sbtest11_new` drop index c2019/12/04 22:32:46 ast.go:521: [info] spec &amp;&#123;node:&#123;text:&#125; IfExists:false IfNotExists:false NoWriteToBinlog:false OnAllPartitions:false Tp:6 Name:c Constraint:&lt;nil&gt; Options:[] OrderByList:[] NewTable:&lt;nil&gt; NewColumns:[] NewConstraints:[] OldColumnName:&lt;nil&gt; NewColumnName:&lt;nil&gt; Position:&lt;nil&gt; LockType: Algorithm:DEFAULT Comment: FromKey: ToKey: Partition:&lt;nil&gt; PartitionNames:[] PartDefinitions:[] WithValidation:false Num:0 Visibility:0&#125;2019/12/04 22:32:46 ddl.go:114: [warning] splitted alter table statement: [ALTER TABLE `sysbench`.`_sbtest11_new` DROP INDEX `c`]2019/12/04 22:32:46 syncer.go:754: [info] [query]ALTER TABLE `sysbench`.`_sbtest11_new` drop index c [current pos](10523308-mysql-bin.000023, 557642019) [next pos](10523308-mysql-bin.000023, 557642214) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755559 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555602019/12/04 22:32:46 syncer.go:795: [info] [ddl][schema]sysbench [start]USE `ptosc_sysbench`; ALTER TABLE `ptosc_sysbench`.`_sbtest11_new` DROP INDEX `c`;2019/12/04 22:32:47 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557642214 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555602019/12/04 22:32:47 syncer.go:803: [info] [ddl][end]USE `ptosc_sysbench`; ALTER TABLE `ptosc_sysbench`.`_sbtest11_new` DROP INDEX `c`;2019/12/04 22:32:47 ddl.go:64: [error] encountered incompatible DDL in TiDB: CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_del` AFTER DELETE ON `sysbench`.`sbtest11` FOR EACH ROW DELETE IGNORE FROM `sysbench`.`_sbtest11_new` WHERE `sysbench`.`_sbtest11_new`.`id` &lt;=&gt; OLD.`id`please confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:47 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_del` AFTER DELETE ON `sysbench`.`sbtest11` FOR EACH ROW DELETE IGNORE FROM `sysbench`.`_sbtest11_new` WHERE `sysbench`.`_sbtest11_new`.`id` &lt;=&gt; OLD.`id` [current pos](10523308-mysql-bin.000023, 557642214) [next pos](10523308-mysql-bin.000023, 557642590) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755560 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555612019/12/04 22:32:47 ddl.go:64: [error] encountered incompatible DDL in TiDB: CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_upd` AFTER UPDATE ON `sysbench`.`sbtest11` FOR EACH ROW BEGIN DELETE IGNORE FROM `sysbench`.`_sbtest11_new` WHERE !(OLD.`id` &lt;=&gt; NEW.`id`) AND `sysbench`.`_sbtest11_new`.`id` &lt;=&gt; OLD.`id`;REPLACE INTO `sysbench`.`_sbtest11_new` (`id`, `k`, `c`, `pad`) VALUES (NEW.`id`, NEW.`k`, NEW.`c`, NEW.`pad`);ENDplease confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:47 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_upd` AFTER UPDATE ON `sysbench`.`sbtest11` FOR EACH ROW BEGIN DELETE IGNORE FROM `sysbench`.`_sbtest11_new` WHERE !(OLD.`id` &lt;=&gt; NEW.`id`) AND `sysbench`.`_sbtest11_new`.`id` &lt;=&gt; OLD.`id`;REPLACE INTO `sysbench`.`_sbtest11_new` (`id`, `k`, `c`, `pad`) VALUES (NEW.`id`, NEW.`k`, NEW.`c`, NEW.`pad`);END [current pos](10523308-mysql-bin.000023, 557642214) [next pos](10523308-mysql-bin.000023, 557643116) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755560 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555622019/12/04 22:32:47 ddl.go:64: [error] encountered incompatible DDL in TiDB: CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_ins` AFTER INSERT ON `sysbench`.`sbtest11` FOR EACH ROW REPLACE INTO `sysbench`.`_sbtest11_new` (`id`, `k`, `c`, `pad`) VALUES (NEW.`id`, NEW.`k`, NEW.`c`, NEW.`pad`)please confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:47 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]CREATE DEFINER=`fanboshi`@`%` TRIGGER `pt_osc_sysbench_sbtest11_ins` AFTER INSERT ON `sysbench`.`sbtest11` FOR EACH ROW REPLACE INTO `sysbench`.`_sbtest11_new` (`id`, `k`, `c`, `pad`) VALUES (NEW.`id`, NEW.`k`, NEW.`c`, NEW.`pad`) [current pos](10523308-mysql-bin.000023, 557642214) [next pos](10523308-mysql-bin.000023, 557643506) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755560 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555632019/12/04 22:32:47 syncer.go:754: [info] [query]RENAME TABLE `sysbench`.`sbtest11` TO `sysbench`.`__sbtest11_old`, `sysbench`.`_sbtest11_new` TO `sysbench`.`sbtest11` [current pos](10523308-mysql-bin.000023, 557644557) [next pos](10523308-mysql-bin.000023, 557645022) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755564 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555662019/12/04 22:32:47 syncer.go:795: [info] [ddl][schema]sysbench [start]RENAME TABLE `ptosc_sysbench`.`ptosc_sbtest11` TO `ptosc_sysbench`.`__sbtest11_old`2019/12/04 22:32:48 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557644557 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555642019/12/04 22:32:48 syncer.go:803: [info] [ddl][end]RENAME TABLE `ptosc_sysbench`.`ptosc_sbtest11` TO `ptosc_sysbench`.`__sbtest11_old`2019/12/04 22:32:48 syncer.go:795: [info] [ddl][schema]sysbench [start]RENAME TABLE `ptosc_sysbench`.`_sbtest11_new` TO `ptosc_sysbench`.`ptosc_sbtest11`2019/12/04 22:32:49 meta.go:135: [info] save position to file, binlog-name:10523308-mysql-bin.000023 binlog-pos:557645022 binlog-gtid:a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555662019/12/04 22:32:49 syncer.go:803: [info] [ddl][end]RENAME TABLE `ptosc_sysbench`.`_sbtest11_new` TO `ptosc_sysbench`.`ptosc_sbtest11`2019/12/04 22:32:49 ddl.go:64: [error] encountered incompatible DDL in TiDB: DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_del`please confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:49 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_del` [current pos](10523308-mysql-bin.000023, 557645022) [next pos](10523308-mysql-bin.000023, 557645230) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755566 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555672019/12/04 22:32:49 ddl.go:64: [error] encountered incompatible DDL in TiDB: DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_upd`please confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:49 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_upd` [current pos](10523308-mysql-bin.000023, 557645022) [next pos](10523308-mysql-bin.000023, 557645438) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755566 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555682019/12/04 22:32:49 ddl.go:64: [error] encountered incompatible DDL in TiDB: DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_ins`please confirm your DDL statement is correct and needed.for TiDB compatible DDL, please see the docs: English version: https://github.com/pingcap/docs/blob/master/sql/ddl.md Chinese version: https://github.com/pingcap/docs-cn/blob/master/sql/ddl.mdif the DDL is not needed, you can modify the meta file and restart syncer to skip it.2019/12/04 22:32:49 syncer.go:745: [warning] [skip query event][schema]sysbench [sql]DROP TRIGGER IF EXISTS `sysbench`.`pt_osc_sysbench_sbtest11_ins` [current pos](10523308-mysql-bin.000023, 557645022) [next pos](10523308-mysql-bin.000023, 557645646) [current gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755566 [next gtid set]a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555692019/12/04 22:33:00 syncer.go:955: [info] [syncer]total events = 7, tps = 0.233333, master-binlog = (10523308-mysql-bin.000023, 557645646), master-binlog-gtid=a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755569, syncer-binlog = (10523308-mysql-bin.000023, 557645022), syncer-binlog-gtid = a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555662019/12/04 22:33:30 syncer.go:955: [info] [syncer]total events = 7, tps = 0.000000, master-binlog = (10523308-mysql-bin.000023, 557645646), master-binlog-gtid=a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755569, syncer-binlog = (10523308-mysql-bin.000023, 557645022), syncer-binlog-gtid = a5c0e0de-19bb-5df7-8be2-29248264cc14:1-7555662019/12/04 22:34:00 syncer.go:955: [info] [syncer]total events = 7, tps = 0.000000, master-binlog = (10523308-mysql-bin.000023, 557645646), master-binlog-gtid=a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755569, syncer-binlog = (10523308-mysql-bin.000023, 557645022), syncer-binlog-gtid = a5c0e0de-19bb-5df7-8be2-29248264cc14:1-755566 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869root@10.132.2.164 22:28:34 [ptosc_sysbench]&gt; show tables;+--------------------------+| Tables_in_ptosc_sysbench |+--------------------------+| __sbtest11_old || _sbtest11_old || ptosc_sbtest1 || ptosc_sbtest10 || ptosc_sbtest11 || ptosc_sbtest12 || ptosc_sbtest13 || ptosc_sbtest2 || ptosc_sbtest3 || ptosc_sbtest4 || ptosc_sbtest5 || ptosc_sbtest6 || ptosc_sbtest7 || ptosc_sbtest8 || ptosc_sbtest9 |+--------------------------+15 rows in set (0.01 sec)root@10.132.2.164 22:32:58 [ptosc_sysbench]&gt; show create table ptosc_sbtest11\\G*************************** 1. row *************************** Table: ptosc_sbtest11Create Table: CREATE TABLE `ptosc_sbtest11` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin AUTO_INCREMENT=300041 row in set (0.00 sec)root@10.132.2.164 22:33:07 [ptosc_sysbench]&gt; show create table __sbtest11_old\\G*************************** 1. row *************************** Table: __sbtest11_oldCreate Table: CREATE TABLE `__sbtest11_old` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `c` char(120) NOT NULL DEFAULT &#x27;&#x27;, `pad` char(60) NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY (`id`), KEY `k_1` (`k`), KEY `c` (`c`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin AUTO_INCREMENT=300041 row in set (0.00 sec)root@10.132.2.164 22:33:12 [ptosc_sysbench]&gt; select * from ptosc_sbtest11;+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 1 | 250731 | 68487932199-96439406143-93774651418-41631865787-96406072701-20604855487-25459966574-28203206787-41238978918-19503783441 | 22195207048-70116052123-74140395089-76317954521-98694025897 || 2 | 251240 | 13241531885-45658403807-79170748828-69419634012-13605813761-77983377181-01582588137-21344716829-87370944992-02457486289 | 28733802923-10548894641-11867531929-71265603657-36546888392 || 3 | 249472 | 51185622598-89397522786-28007882305-52050087550-68686337807-48942386476-96555734557-05264042377-33586177817-31986479495 | 00592560354-80393027097-78244247549-39135306455-88936868384 |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+3 rows in set (0.00 sec)root@10.132.2.164 22:33:19 [ptosc_sysbench]&gt; select * from __sbtest11_old;+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 1 | 250731 | 68487932199-96439406143-93774651418-41631865787-96406072701-20604855487-25459966574-28203206787-41238978918-19503783441 | 22195207048-70116052123-74140395089-76317954521-98694025897 || 2 | 251240 | 13241531885-45658403807-79170748828-69419634012-13605813761-77983377181-01582588137-21344716829-87370944992-02457486289 | 28733802923-10548894641-11867531929-71265603657-36546888392 || 3 | 249472 | 51185622598-89397522786-28007882305-52050087550-68686337807-48942386476-96555734557-05264042377-33586177817-31986479495 | 00592560354-80393027097-78244247549-39135306455-88936868384 |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+3 rows in set (0.00 sec) MySQL执行 123456789101112root@localhost 22:31:10 [sysbench]&gt; select * from sbtest11;+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 1 | 250731 | 68487932199-96439406143-93774651418-41631865787-96406072701-20604855487-25459966574-28203206787-41238978918-19503783441 | 22195207048-70116052123-74140395089-76317954521-98694025897 || 2 | 251240 | 13241531885-45658403807-79170748828-69419634012-13605813761-77983377181-01582588137-21344716829-87370944992-02457486289 | 28733802923-10548894641-11867531929-71265603657-36546888392 || 3 | 249472 | 51185622598-89397522786-28007882305-52050087550-68686337807-48942386476-96555734557-05264042377-33586177817-31986479495 | 00592560354-80393027097-78244247549-39135306455-88936868384 |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+3 rows in set (0.00 sec)root@localhost 22:31:14 [sysbench]&gt; delete from sbtest11 where id=2;Query OK, 1 row affected (0.00 sec) TIDB查看 12345678910111213141516171819root@10.132.2.164 22:33:19 [ptosc_sysbench]&gt; select * from __sbtest11_old;+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 1 | 250731 | 68487932199-96439406143-93774651418-41631865787-96406072701-20604855487-25459966574-28203206787-41238978918-19503783441 | 22195207048-70116052123-74140395089-76317954521-98694025897 || 2 | 251240 | 13241531885-45658403807-79170748828-69419634012-13605813761-77983377181-01582588137-21344716829-87370944992-02457486289 | 28733802923-10548894641-11867531929-71265603657-36546888392 || 3 | 249472 | 51185622598-89397522786-28007882305-52050087550-68686337807-48942386476-96555734557-05264042377-33586177817-31986479495 | 00592560354-80393027097-78244247549-39135306455-88936868384 |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+3 rows in set (0.00 sec)root@10.132.2.164 22:33:24 [ptosc_sysbench]&gt; select * from ptosc_sbtest11;+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| id | k | c | pad |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+| 1 | 250731 | 68487932199-96439406143-93774651418-41631865787-96406072701-20604855487-25459966574-28203206787-41238978918-19503783441 | 22195207048-70116052123-74140395089-76317954521-98694025897 || 3 | 249472 | 51185622598-89397522786-28007882305-52050087550-68686337807-48942386476-96555734557-05264042377-33586177817-31986479495 | 00592560354-80393027097-78244247549-39135306455-88936868384 |+----+--------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+2 rows in set (0.00 sec) 如果只是配置 12345678910[[replicate-do-table]]db-name = &quot;ptosc_sysbench&quot;tbl-name = &quot;ptosc_sbtest11&quot;[[route-rules]]pattern-schema = &quot;sysbench&quot;pattern-table = &quot;sbtest11&quot;target-schema = &quot;ptosc_sysbench&quot;target-table = &quot;ptosc_sbtest11&quot; 会报错 12345678910112019/12/03 11:39:13 syncer.go:754: [info] [query]RENAME TABLE `sale_risk_log`.`t_risk_third_request_log` TO `sale_risk_log`.`_t_risk_third_request_log_old`, `sale_risk_log`.`_t_risk_third_request_log_new` TO `sale_risk_log`.`t_risk_third_request_log` [current pos](0022263310-mysql-bin.003273, 481842042) [next pos](0022263310-mysql-bin.003273, 481842397) [current gtid set]2fb23bcc-e5b3-5f84-88ce-15271019b97d:1-2550946489,38f8425e-9182-5934-b32a-7e4317fe4a04:270797037-273174391:5392264317-5392487512,ce9be252-2b71-11e6-b8f4-00212844f856:909572082-909633988:913857848-913889001 [next gtid set]ce9be252-2b71-11e6-b8f4-00212844f856:909572082-909633988:913857848-913889001,2fb23bcc-e5b3-5f84-88ce-15271019b97d:1-2550946490,38f8425e-9182-5934-b32a-7e4317fe4a04:270797037-273174391:5392264317-53924875122019/12/03 11:39:13 syncer.go:795: [info] [ddl][schema]sale_risk_log [start]RENAME TABLE `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` TO `tidb_sale_risk_log`.`_t_risk_third_request_log_old`2019/12/03 11:39:13 db.go:143: [warning] [exec][sql]RENAME TABLE `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` TO `tidb_sale_risk_log`.`_t_risk_third_request_log_old`[args][][error]Error 1050: Table &#x27;tidb_sale_risk_log._t_risk_third_request_log_old&#x27; already exists2019/12/03 11:39:13 db.go:115: [error] [exec][sql][RENAME TABLE `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` TO `tidb_sale_risk_log`.`_t_risk_third_request_log_old`][args][[]][error]Error 1050: Table &#x27;tidb_sale_risk_log._t_risk_third_request_log_old&#x27; already exists2019/12/03 11:39:13 syncer.go:491: [warning] [ignore ddl error][sql]RENAME TABLE `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` TO `tidb_sale_risk_log`.`_t_risk_third_request_log_old`[args][][error]Error 1050: Table &#x27;tidb_sale_risk_log._t_risk_third_request_log_old&#x27; already exists2019/12/03 11:39:13 meta.go:135: [info] save position to file, binlog-name:0022263310-mysql-bin.003273 binlog-pos:481842042 binlog-gtid:ce9be252-2b71-11e6-b8f4-00212844f856:909572082-909633988:913857848-913889001,2fb23bcc-e5b3-5f84-88ce-15271019b97d:1-2550946489,38f8425e-9182-5934-b32a-7e4317fe4a04:270797037-273174391:5392264317-53924875122019/12/03 11:39:13 syncer.go:803: [info] [ddl][end]RENAME TABLE `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` TO `tidb_sale_risk_log`.`_t_risk_third_request_log_old`2019/12/03 11:39:13 syncer.go:795: [info] [ddl][schema]sale_risk_log [start]RENAME TABLE `tidb_sale_risk_log`.`_t_risk_third_request_log_new` TO `tidb_sale_risk_log`.`tidb_t_risk_third_request_log`2019/12/03 11:39:13 db.go:143: [warning] [exec][sql]RENAME TABLE `tidb_sale_risk_log`.`_t_risk_third_request_log_new` TO `tidb_sale_risk_log`.`tidb_t_risk_third_request_log`[args][][error]Error 1017: Can&#x27;t find file: &#x27;./tidb_sale_risk_log/_t_risk_third_request_log_new.frm&#x27;2019/12/03 11:39:13 db.go:115: [error] [exec][sql][RENAME TABLE `tidb_sale_risk_log`.`_t_risk_third_request_log_new` TO `tidb_sale_risk_log`.`tidb_t_risk_third_request_log`][args][[]][error]Error 1017: Can&#x27;t find file: &#x27;./tidb_sale_risk_log/_t_risk_third_request_log_new.frm&#x27;2019/12/03 11:39:13 syncer.go:489: [fatal] [error query event][error type]execution error [sql]RENAME TABLE `tidb_sale_risk_log`.`_t_risk_third_request_log_new` TO `tidb_sale_risk_log`.`tidb_t_risk_third_request_log` [schema]sale_risk_log [table]_t_risk_third_request_log_new [current pos](0022263310-mysql-bin.003273, 481842042) [next pos](0022263310-mysql-bin.003273, 481842397) [current gtid set]2fb23bcc-e5b3-5f84-88ce-15271019b97d:1-2550946489,38f8425e-9182-5934-b32a-7e4317fe4a04:270797037-273174391:5392264317-5392487512,ce9be252-2b71-11e6-b8f4-00212844f856:909572082-909633988:913857848-913889001 [next gtid set]ce9be252-2b71-11e6-b8f4-00212844f856:909572082-909633988:913857848-913889001,2fb23bcc-e5b3-5f84-88ce-15271019b97d:1-2550946490,38f8425e-9182-5934-b32a-7e4317fe4a04:270797037-273174391:5392264317-5392487512 [error message]Error 1017: Can&#x27;t find file: &#x27;./tidb_sale_risk_log/_t_risk_third_request_log_new.frm&#x27;","categories":[],"tags":[{"name":"TiDB","slug":"TiDB","permalink":"http://fuxkdb.com/tags/TiDB/"},{"name":"Syncer","slug":"Syncer","permalink":"http://fuxkdb.com/tags/Syncer/"}]},{"title":"Drainer不支持的DDL-1","slug":"2019-12-10-Drainer不支持的DDL-1","date":"2019-12-10T13:23:00.000Z","updated":"2019-12-10T13:52:12.000Z","comments":true,"path":"2019/12/10/2019-12-10-Drainer不支持的DDL-1/","link":"","permalink":"http://fuxkdb.com/2019/12/10/2019-12-10-Drainer%E4%B8%8D%E6%94%AF%E6%8C%81%E7%9A%84DDL-1/","excerpt":"Drainer不支持的DDL-1版本3.0.5 1.rename table t1 to b_db.b_t1已经做好TiDB-&gt;MySQL的同步, 开启了pump, drainer","text":"Drainer不支持的DDL-1版本3.0.5 1.rename table t1 to b_db.b_t1已经做好TiDB-&gt;MySQL的同步, 开启了pump, drainer 问题复现在TiDB创建fanboshi表 12345678910111213141516171819202122232425262728293031323334353637383940root@10.152.x.150 17:53:27 [sysbench]&gt; show tables;ERROR 2006 (HY000): MySQL server has gone awayNo connection. Trying to reconnect...Connection id: 76Current database: sysbench+--------------------+| Tables_in_sysbench |+--------------------+| sbtest1 || sbtest10 || sbtest2 || sbtest3 || sbtest4 || sbtest5 || sbtest6 || sbtest7 || sbtest8 || sbtest9 |+--------------------+10 rows in set (0.01 sec)root@10.152.x.150 18:31:19 [sysbench]&gt; create table fanboshi like sbtest1;Query OK, 0 rows affected (0.13 sec)root@10.152.x.150 18:31:24 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500456 |+----------+1 row in set (0.31 sec)root@10.152.x.150 18:31:42 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500757 |+----------+1 row in set (0.30 sec) 在下游MySQL查看, 同步没有问题 12345678910111213141516171819202122232425root@localhost 18:59:35 [sysbench]&gt; show tables;+--------------------+| Tables_in_sysbench |+--------------------+| fanboshi || sbtest1 || sbtest10 || sbtest2 || sbtest3 || sbtest4 || sbtest5 || sbtest6 || sbtest7 || sbtest8 || sbtest9 |+--------------------+11 rows in set (0.00 sec)root@localhost 18:59:37 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500757 |+----------+1 row in set (0.13 sec) 在TiDB执行 12345root@10.152.x.150 18:57:59 [sysbench]&gt; create database tidb_sysbench;Query OK, 0 rows affected (0.10 sec)root@10.152.x.150 18:58:09 [sysbench]&gt; rename table fanboshi to tidb_sysbench.tidb_fanboshi;Query OK, 0 rows affected (0.10 sec) Drainer报错退出 123456789101112131415161718192021[2019/12/02 18:59:14.979 +08:00] [INFO] [collector.go:284] [&quot;get ddl job&quot;] [job=&quot;ID:79, Type:rename table, State:synced, SchemaState:public, SchemaID:77, TableID:75, RowCount:0, ArgLen:0, start time: 2019-12-02 18:59:14.012 +0800 CST, Err:&lt;nil&gt;, ErrCount:0, SnapshotVersion:0&quot;][2019/12/02 18:59:14.979 +08:00] [INFO] [syncer.go:400] [&quot;add ddl item to syncer, you can add this commit ts to `ignore-txn-commit-ts` to skip this ddl if needed&quot;] [sql=&quot;rename table fanboshi to tidb_sysbench.tidb_fanboshi&quot;] [&quot;commit ts&quot;=412951341698121738][2019/12/02 18:59:19.993 +08:00] [ERROR] [load.go:594] [&quot;exec failed&quot;] [sql=&quot;rename table fanboshi to tidb_sysbench.tidb_fanboshi&quot;] [error=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)&quot;] [errorVerbose=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)\\ngithub.com/pingcap/errors.AddStack\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/errors.go:174\\ngithub.com/pingcap/errors.Trace\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/juju_adaptor.go:15\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:337\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:592\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).put\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:615\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).Run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:478\\ngithub.com/pingcap/tidb-binlog/drainer/sync.(*MysqlSyncer).run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/sync/mysql.go:121\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1337&quot;][2019/12/02 18:59:19.994 +08:00] [INFO] [load.go:435] [&quot;Run()... in Loader quit&quot;][2019/12/02 18:59:19.994 +08:00] [INFO] [load.go:715] [&quot;txnManager has been closed&quot;][2019/12/02 18:59:19.994 +08:00] [INFO] [load.go:659] [&quot;run()... in txnManager quit&quot;][2019/12/02 18:59:19.994 +08:00] [INFO] [mysql.go:117] [&quot;Successes chan quit&quot;][2019/12/02 18:59:19.994 +08:00] [INFO] [syncer.go:251] [&quot;write save point&quot;] [ts=412951341357334529][2019/12/02 18:59:19.994 +08:00] [ERROR] [syncer.go:416] [&quot;Failed to close syncer&quot;] [error=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)&quot;] [errorVerbose=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)\\ngithub.com/pingcap/errors.AddStack\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/errors.go:174\\ngithub.com/pingcap/errors.Trace\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/juju_adaptor.go:15\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:337\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:592\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).put\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:615\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).Run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:478\\ngithub.com/pingcap/tidb-binlog/drainer/sync.(*MysqlSyncer).run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/sync/mysql.go:121\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1337&quot;][2019/12/02 18:59:19.996 +08:00] [INFO] [syncer.go:243] [&quot;handleSuccess quit&quot;][2019/12/02 18:59:19.996 +08:00] [ERROR] [server.go:270] [&quot;syncer exited abnormal&quot;] [error=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)&quot;] [errorVerbose=&quot;Error 1017: Can&#x27;t find file: &#x27;./tidb_sysbench/fanboshi.frm&#x27; (errno: 2 - No such file or directory)\\ngithub.com/pingcap/errors.AddStack\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/errors.go:174\\ngithub.com/pingcap/errors.Trace\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/juju_adaptor.go:15\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:337\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).execDDL\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:592\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*batchManager).put\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:615\\ngithub.com/pingcap/tidb-binlog/pkg/loader.(*loaderImpl).Run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/loader/load.go:478\\ngithub.com/pingcap/tidb-binlog/drainer/sync.(*MysqlSyncer).run\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/sync/mysql.go:121\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1337&quot;][2019/12/02 18:59:19.997 +08:00] [INFO] [util.go:66] [Exit] [name=syncer][2019/12/02 18:59:19.997 +08:00] [INFO] [server.go:406] [&quot;begin to close drainer server&quot;][2019/12/02 18:59:20.001 +08:00] [INFO] [server.go:371] [&quot;has already update status&quot;] [id=knode10-152-1-166:8249][2019/12/02 18:59:20.001 +08:00] [INFO] [server.go:410] [&quot;commit status done&quot;][2019/12/02 18:59:20.001 +08:00] [INFO] [util.go:66] [Exit] [name=heartbeat][2019/12/02 18:59:20.001 +08:00] [INFO] [collector.go:130] [&quot;publishBinlogs quit&quot;][2019/12/02 18:59:20.001 +08:00] [INFO] [pump.go:72] [&quot;pump is closing&quot;] [id=knode10-152-1-166:8250][2019/12/02 18:59:20.001 +08:00] [INFO] [pump.go:72] [&quot;pump is closing&quot;] [id=knode10-152-1-150:8250][2019/12/02 18:59:20.001 +08:00] [INFO] [util.go:66] [Exit] [name=collect][2019/12/02 18:59:20.001 +08:00] [INFO] [main.go:73] [&quot;drainer exit&quot;] 问题修复参考同步时出现上游数据库支持但是下游数据库执行会出错的 DDL，应该怎么办？) 根据drainer.log中的日志信息我们获取这个ddl的commit ts为412951341698121738 在中控机配置文件或者直接在drainer所属服务器的配置文件中直接修改都可以, 我这里在中控机修改, 因为我只有一个drainer. 如果有多个drainer建议在drainer服务器修改配置文件 1.添加要忽略的commit-ts 12# ignore syncing the txn with specified commit ts to downstreamignore-txn-commit-ts = [412951341698121738] 注意这个应该填写integer, 如果填写为&quot;412951341698121738&quot;drainer会起不来, 而且使用systemctl start drainer-8249.service无任何错误提示/DATA1/deploy/log/drainer_stderr.log日志业务内容 12345678910111213141516171819202122232425[root@knode10-152-1-166 deploy]# cat /etc/systemd/system/drainer-8249.service [Unit]Description=drainer-8249 serviceAfter=syslog.target network.target remote-fs.target nss-lookup.target[Service]LimitNOFILE=1000000#LimitCORE=infinityLimitSTACK=10485760User=tidbExecStart=/DATA1/deploy/scripts/run_drainer.shRestart=on-failureRestartSec=15sSendSIGKILL=no[Install]WantedBy=multi-user.target[root@knode10-152-1-166 deploy]# ./bin/drainer --addr=&quot;10.152.x.166:8249&quot; --pd-urls=&quot;http://10.152.x.152:2379,http://10.152.x.146:2379,http://10.152.x.147:2379&quot; --data-dir=&quot;/DATA1/deploy/data.drainer&quot; --log-file=&quot;/DATA1/deploy/log/drainer.log&quot; --config=conf/drainer.toml --initial-commit-ts=&quot;412950687779913731&quot; 2&gt;&gt; &quot;/DATA1/deploy/log/drainer_stderr.log&quot;[2019/12/02 19:17:35.996 +08:00] [FATAL] [main.go:39] [&quot;verifying flags failed, See &#x27;drainer --help&#x27;.&quot;] [error=&quot;toml: cannot load TOML value of type string into a Go integer&quot;] [errorVerbose=&quot;toml: cannot load TOML value of type string into a Go integer\\ngithub.com/pingcap/errors.AddStack\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/errors.go:174\\ngithub.com/pingcap/errors.Trace\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/juju_adaptor.go:15\\ngithub.com/pingcap/tidb-binlog/pkg/util.StrictDecodeFile\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/pkg/util/util.go:188\\ngithub.com/pingcap/tidb-binlog/drainer.(*Config).configFromFile\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/config.go:226\\ngithub.com/pingcap/tidb-binlog/drainer.(*Config).Parse\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/config.go:169\\nmain.main\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/cmd/drainer/main.go:38\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:200\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1337&quot;] [stack=&quot;github.com/pingcap/log.Fatal\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/log@v0.0.0-20190715063458-479153f07ebd/global.go:59\\nmain.main\\n\\t/home/jenkins/agent/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/cmd/drainer/main.go:39\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:200&quot;]查看日志无内容[root@knode10-152-1-166 deploy]# cat /DATA1/deploy/log/drainer_stderr.log[root@knode10-152-1-166 deploy]# 2.在MySQL手动执行失败的DDL语句 12root@localhost 19:09:27 [sysbench]&gt; rename table fanboshi to tidb_sysbench.tidb_fanboshi;Query OK, 0 rows affected (0.00 sec) 3.启动drainer 1234567891011121314151617181920212223242526272829systemctl start drainer-8249.service [2019/12/02 19:19:26.473 +08:00] [INFO] [version.go:50] [&quot;Welcome to Drainer&quot;] [&quot;Release Version&quot;=v3.0.5] [&quot;Git Commit Hash&quot;=88e58f165b6556ba084761c35b4be6db164a5c06] [&quot;Build TS&quot;=&quot;2019-10-25 03:24:55&quot;] [&quot;Go Version&quot;=go1.12] [&quot;Go OS/Arch&quot;=linux/amd64][2019/12/02 19:19:26.474 +08:00] [INFO] [main.go:46] [&quot;start drainer...&quot;] [config=&quot;&#123;\\&quot;log-level\\&quot;:\\&quot;info\\&quot;,\\&quot;node-id\\&quot;:\\&quot;\\&quot;,\\&quot;addr\\&quot;:\\&quot;http://10.152.x.166:8249\\&quot;,\\&quot;advertise-addr\\&quot;:\\&quot;http://10.152.x.166:8249\\&quot;,\\&quot;data-dir\\&quot;:\\&quot;/DATA1/deploy/data.drainer\\&quot;,\\&quot;detect-interval\\&quot;:10,\\&quot;pd-urls\\&quot;:\\&quot;http://10.152.x.152:2379,http://10.152.x.146:2379,http://10.152.x.147:2379\\&quot;,\\&quot;log-file\\&quot;:\\&quot;/DATA1/deploy/log/drainer.log\\&quot;,\\&quot;initial-commit-ts\\&quot;:412950687779913731,\\&quot;sycner\\&quot;:&#123;\\&quot;sql-mode\\&quot;:null,\\&quot;ignore-txn-commit-ts\\&quot;:[412951341698121738],\\&quot;ignore-schemas\\&quot;:\\&quot;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql\\&quot;,\\&quot;ignore-table\\&quot;:null,\\&quot;txn-batch\\&quot;:20,\\&quot;worker-count\\&quot;:4,\\&quot;to\\&quot;:&#123;\\&quot;host\\&quot;:\\&quot;10.133.1.52\\&quot;,\\&quot;user\\&quot;:\\&quot;drainer\\&quot;,\\&quot;password\\&quot;:\\&quot;drainer123\\&quot;,\\&quot;port\\&quot;:3308,\\&quot;checkpoint\\&quot;:&#123;\\&quot;schema\\&quot;:\\&quot;\\&quot;&#125;,\\&quot;dir\\&quot;:\\&quot;\\&quot;,\\&quot;time-limit\\&quot;:\\&quot;\\&quot;,\\&quot;size-limit\\&quot;:\\&quot;\\&quot;,\\&quot;zookeeper-addrs\\&quot;:\\&quot;\\&quot;,\\&quot;kafka-addrs\\&quot;:\\&quot;\\&quot;,\\&quot;kafka-version\\&quot;:\\&quot;\\&quot;,\\&quot;kafka-max-messages\\&quot;:0,\\&quot;topic-name\\&quot;:\\&quot;\\&quot;&#125;,\\&quot;replicate-do-table\\&quot;:null,\\&quot;replicate-do-db\\&quot;:null,\\&quot;db-type\\&quot;:\\&quot;mysql\\&quot;,\\&quot;disable-dispatch\\&quot;:false,\\&quot;safe-mode\\&quot;:false,\\&quot;disable-detect\\&quot;:false&#125;,\\&quot;security\\&quot;:&#123;\\&quot;ssl-ca\\&quot;:\\&quot;\\&quot;,\\&quot;ssl-cert\\&quot;:\\&quot;\\&quot;,\\&quot;ssl-key\\&quot;:\\&quot;\\&quot;&#125;,\\&quot;synced-check-time\\&quot;:5,\\&quot;compressor\\&quot;:\\&quot;\\&quot;,\\&quot;EtcdTimeout\\&quot;:5000000000,\\&quot;MetricsAddr\\&quot;:\\&quot;\\&quot;,\\&quot;MetricsInterval\\&quot;:15&#125;&quot;][2019/12/02 19:19:26.474 +08:00] [INFO] [client.go:144] [&quot;[pd] create pd client with endpoints&quot;] [pd-address=&quot;[http://10.152.x.146:2379,http://10.152.x.147:2379,http://10.152.x.152:2379]&quot;][2019/12/02 19:19:26.483 +08:00] [INFO] [client.go:252] [&quot;[pd] switch leader&quot;] [new-leader=http://10.152.x.152:2379] [old-leader=][2019/12/02 19:19:26.484 +08:00] [INFO] [client.go:163] [&quot;[pd] init cluster id&quot;] [cluster-id=6751370831202615818][2019/12/02 19:19:26.484 +08:00] [INFO] [server.go:111] [&quot;get cluster id from pd&quot;] [id=6751370831202615818][2019/12/02 19:19:26.491 +08:00] [INFO] [checkpoint.go:65] [&quot;initialize checkpoint&quot;] [name=mysql] [checkpoint=412951341357334529] [cfg=&quot;&#123;\\&quot;Db\\&quot;:&#123;\\&quot;host\\&quot;:\\&quot;10.133.1.52\\&quot;,\\&quot;user\\&quot;:\\&quot;drainer\\&quot;,\\&quot;password\\&quot;:\\&quot;drainer123\\&quot;,\\&quot;port\\&quot;:3308&#125;,\\&quot;Schema\\&quot;:\\&quot;tidb_binlog\\&quot;,\\&quot;Table\\&quot;:\\&quot;checkpoint\\&quot;,\\&quot;ClusterID\\&quot;:6751370831202615818,\\&quot;InitialCommitTS\\&quot;:412950687779913731,\\&quot;dir\\&quot;:\\&quot;/DATA1/deploy/data.drainer/savepoint\\&quot;&#125;&quot;][2019/12/02 19:19:26.491 +08:00] [INFO] [store.go:69] [&quot;new store&quot;] [path=&quot;tikv://10.152.x.146:2379,10.152.x.147:2379,10.152.x.152:2379?disableGC=true&quot;][2019/12/02 19:19:26.491 +08:00] [INFO] [client.go:144] [&quot;[pd] create pd client with endpoints&quot;] [pd-address=&quot;[10.152.x.146:2379,10.152.x.147:2379,10.152.x.152:2379]&quot;][2019/12/02 19:19:26.495 +08:00] [INFO] [client.go:252] [&quot;[pd] switch leader&quot;] [new-leader=http://10.152.x.152:2379] [old-leader=][2019/12/02 19:19:26.496 +08:00] [INFO] [client.go:163] [&quot;[pd] init cluster id&quot;] [cluster-id=6751370831202615818][2019/12/02 19:19:26.497 +08:00] [INFO] [store.go:75] [&quot;new store with retry success&quot;][2019/12/02 19:19:26.517 +08:00] [INFO] [store.go:69] [&quot;new store&quot;] [path=&quot;tikv://10.152.x.146:2379,10.152.x.147:2379,10.152.x.152:2379?disableGC=true&quot;][2019/12/02 19:19:26.518 +08:00] [INFO] [client.go:144] [&quot;[pd] create pd client with endpoints&quot;] [pd-address=&quot;[10.152.x.146:2379,10.152.x.147:2379,10.152.x.152:2379]&quot;][2019/12/02 19:19:26.521 +08:00] [INFO] [client.go:252] [&quot;[pd] switch leader&quot;] [new-leader=http://10.152.x.152:2379] [old-leader=][2019/12/02 19:19:26.521 +08:00] [INFO] [client.go:163] [&quot;[pd] init cluster id&quot;] [cluster-id=6751370831202615818][2019/12/02 19:19:26.522 +08:00] [INFO] [store.go:75] [&quot;new store with retry success&quot;][2019/12/02 19:19:26.533 +08:00] [INFO] [server.go:246] [&quot;register success&quot;] [&quot;drainer node id&quot;=knode10-152-1-166:8249][2019/12/02 19:19:26.533 +08:00] [INFO] [server.go:296] [&quot;start to server request&quot;] [addr=http://10.152.x.166:8249][2019/12/02 19:19:26.537 +08:00] [INFO] [merge.go:222] [&quot;merger add source&quot;] [&quot;source id&quot;=knode10-152-1-150:8250][2019/12/02 19:19:26.537 +08:00] [INFO] [merge.go:222] [&quot;merger add source&quot;] [&quot;source id&quot;=knode10-152-1-166:8250][2019/12/02 19:19:26.537 +08:00] [INFO] [pump.go:133] [&quot;pump create pull binlogs client&quot;] [id=knode10-152-1-166:8250][2019/12/02 19:19:26.537 +08:00] [INFO] [pump.go:133] [&quot;pump create pull binlogs client&quot;] [id=knode10-152-1-150:8250][2019/12/02 19:19:27.545 +08:00] [INFO] [collector.go:284] [&quot;get ddl job&quot;] [job=&quot;ID:79, Type:rename table, State:synced, SchemaState:public, SchemaID:77, TableID:75, RowCount:0, ArgLen:0, start time: 2019-12-02 18:59:14.012 +0800 CST, Err:&lt;nil&gt;, ErrCount:0, SnapshotVersion:0&quot;][2019/12/02 19:19:27.545 +08:00] [WARN] [syncer.go:317] [&quot;skip txn&quot;] [binlog=&quot;tp:Commit start_ts:412951341698121737 commit_ts:412951341698121738 prewrite_key:\\&quot;mDB:43\\\\000\\\\000\\\\000\\\\374\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000H\\&quot; ddl_query:\\&quot;rename table fanboshi to tidb_sysbench.tidb_fanboshi\\&quot; ddl_job_id:79 &quot;][2019/12/02 19:19:30.855 +08:00] [INFO] [syncer.go:251] [&quot;write save point&quot;] [ts=412951660163760129] 5.验证同步 123456789101112131415[root@node10-133-1-59 fanboshi]# sh sysbench_tidb_run.sh sysbench 1.0.18 (using bundled LuaJIT 2.1.0-beta2)Running the test with following options:Number of threads: 4Report intermediate results every 5 second(s)Initializing random number generator from current timeInitializing worker threads...Threads started![ 5s ] thds: 4 tps: 130.27 qps: 2613.67 (r/w/o: 1830.43/133.87/649.37) lat (ms,95%): 38.94 err/s: 0.00 reconn/s: 0.00[ 10s ] thds: 4 tps: 136.41 qps: 2733.37 (r/w/o: 1913.32/141.41/678.64) lat (ms,95%): 37.56 err/s: 0.00 reconn/s: 0.00 TiDB 1234567root@10.152.x.150 19:20:43 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500882 |+----------+1 row in set (0.27 sec) 下游MySQL 123456789101112131415root@localhost 19:20:08 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500757 |+----------+1 row in set (0.12 sec)root@localhost 19:20:13 [sysbench]&gt; select count(*) from sbtest1;+----------+| count(*) |+----------+| 500882 |+----------+1 row in set (0.11 sec) 同步恢复","categories":[],"tags":[{"name":"TiDB","slug":"TiDB","permalink":"http://fuxkdb.com/tags/TiDB/"},{"name":"Drainer","slug":"Drainer","permalink":"http://fuxkdb.com/tags/Drainer/"}]},{"title":"MGR参数之group_replication_ip_whitelist","slug":"2019-12-10-MGR参数之group_replication_ip_whitelist","date":"2019-12-10T09:45:00.000Z","updated":"2019-12-10T09:46:24.000Z","comments":true,"path":"2019/12/10/2019-12-10-MGR参数之group_replication_ip_whitelist/","link":"","permalink":"http://fuxkdb.com/2019/12/10/2019-12-10-MGR%E5%8F%82%E6%95%B0%E4%B9%8Bgroup_replication_ip_whitelist/","excerpt":"尝试将10.133.1.46加入192.168.2.224的集群失败 需要设置参数group_replication_ip_whitelist 此参数虽然是动态参数, 但是要使之生效需要节点重新加入集群 To specify a whitelist manually, use the group_replication_ip_whitelist option. You cannot change the whitelist on a server while it is an active member of a replication group. If the member is active, you must issue a STOP GROUP_REPLICATION statement before changing the whitelist, and a START GROUP_REPLICATION statement afterwards.","text":"尝试将10.133.1.46加入192.168.2.224的集群失败 需要设置参数group_replication_ip_whitelist 此参数虽然是动态参数, 但是要使之生效需要节点重新加入集群 To specify a whitelist manually, use the group_replication_ip_whitelist option. You cannot change the whitelist on a server while it is an active member of a replication group. If the member is active, you must issue a STOP GROUP_REPLICATION statement before changing the whitelist, and a START GROUP_REPLICATION statement afterwards. 1234set global group_replication_ip_whitelist = &#x27;192.168.0.0/16,10.0.0.0/8&#x27;;my.cnf 添加loose_group_replication_ip_whitelist = &#x27;192.168.0.0/16,10.0.0.0/8&#x27; A类IP地址的默认子网掩码为255.0.0.0（由于255相当于二进制的8位1，所以也缩写成“/8”，表示网络号占了8位）;B类的为255.255.0.0（/16）;C类的为255.255.255.0(/24)。/30就是255.255.255.252。32就是255.255.255.255. 注意这里的16, 24不是看ip a里的值 123set global group_replication_ip_whitelist = &#x27;192.168.0.0/16,10.0.0.0/8&#x27;; 10.x.x.x都可以set global group_replication_ip_whitelist = &#x27;192.168.0.0/16,10.133.0.0/16&#x27;; 10.133.x.x都可以set global group_replication_ip_whitelist = &#x27;192.168.0.0/16,10.133.1.0/24&#x27;; 10.133.1.x都可以 相关异常 123456 [ERROR] Plugin group_replication reported: &#x27;[GCS] The member was unable to join the group. Local port: 23307&#x27; 2019-12-09T08:18:18.804055Z 0 [Warning] Plugin group_replication reported: &#x27;[GCS] Connection attempt from IP address 10.133.1.37 refused. Address is not in the IP whitelist.&#x27;2019-12-09T08:18:18.804092Z 0 [ERROR] Plugin group_replication reported: &#x27;[GCS] Error connecting to the local group communication engine instance.&#x27;2019-12-09T08:18:18.922608Z 0 [ERROR] Plugin group_replication reported: &#x27;[GCS] The member was unable to join the group. Local port: 23307&#x27; 注意要加入的节点的group_replication_group_seeds一定要把设置好group_replication_ip_whitelist的节点写在最前面,或者删掉没有设置好group_replication_ip_whitelist的节点, 否则会一直报错 1[ERROR] Plugin group_replication reported: &#x27;[GCS] The member was unable to join the group. Local port: 23307&#x27; 举例子 123192.168.2.224 group_replication_ip_whitelist=&#x27;&#x27;;192.168.2.225 group_replication_ip_whitelist=&#x27;192.168.0.0/16,10.133.0.0/16&#x27;;192.168.2.226 group_replication_ip_whitelist=&#x27;192.168.0.0/16,10.133.0.0/16&#x27;; 要把10.133.1.46加入集群, 而它的group_replication_group_seeds为192.168.2.224:23310,192.168.2.225:23310,192.168.2.226:23310 那么它会一直报错无法加入集群 同时192.168.2.224会报 原因就是224是10.133.1.46的group_replication_group_seeds的第一个…","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"}]},{"title":"TiDB Binlog部署","slug":"2019-12-02-TiDB-Binlog部署","date":"2019-12-02T11:31:00.000Z","updated":"2019-12-02T11:36:03.000Z","comments":true,"path":"2019/12/02/2019-12-02-TiDB-Binlog部署/","link":"","permalink":"http://fuxkdb.com/2019/12/02/2019-12-02-TiDB-Binlog%E9%83%A8%E7%BD%B2/","excerpt":"TiDB Binlog部署环境信息中控机是10.152.x.133 123456789101112131415161718192021222324252627282930313233[tidb_servers]10.152.x.1010.152.x.1110.152.x.12[tikv_servers]10.152.x.1010.152.x.1110.152.x.12[pd_servers]10.152.x.1010.152.x.1110.152.x.12## Monitoring Part# prometheus and pushgateway servers[monitoring_servers]10.152.x.133[grafana_servers]10.152.x.133# node_exporter and blackbox_exporter servers[monitored_servers]10.152.x.1010.152.x.1110.152.x.1210.152.x.133[alertmanager_servers]10.152.x.133","text":"TiDB Binlog部署环境信息中控机是10.152.x.133 123456789101112131415161718192021222324252627282930313233[tidb_servers]10.152.x.1010.152.x.1110.152.x.12[tikv_servers]10.152.x.1010.152.x.1110.152.x.12[pd_servers]10.152.x.1010.152.x.1110.152.x.12## Monitoring Part# prometheus and pushgateway servers[monitoring_servers]10.152.x.133[grafana_servers]10.152.x.133# node_exporter and blackbox_exporter servers[monitored_servers]10.152.x.1010.152.x.1110.152.x.1210.152.x.133[alertmanager_servers]10.152.x.133 先造点数据 12345678yum install sysbench -ymysql -uroot -p -h10.152.x.10 -P4000 -e&quot;create database sysbench&quot;sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=root --mysql-password=supersecret --mysql-port=4000 \\--mysql-host=10.152.x.10 \\--mysql-db=sysbench --tables=10 --table-size=5000 --threads=4 \\--events=5000 --report-interval=5 --db-driver=mysql prepare 部署Pump1.修改 tidb-ansible/inventory.ini 文件 设置 enable_binlog = True，表示 TiDB 集群开启 binlog 12## binlog triggerenable_binlog = True 2.为 pump_servers 主机组添加部署机器 IP 1234[pump_serverspump1 ansible_host=10.152.x.10pump2 ansible_host=10.152.x.11pump3 ansible_host=10.152.x.12 如果想要为pump专门指定目录, 可以使用deploy_dir 123pump1 ansible_host=10.152.x.10 deploy_dir=/data1/pumppump2 ansible_host=10.152.x.11 deploy_dir=/data2/pumppump3 ansible_host=10.152.x.12 deploy_dir=/data3/pump 默认 Pump 保留 7 天数据，如需修改可修改 tidb-ansible/conf/pump.yml（TiDB 3.0.2 及之前版本中为 tidb-ansible/conf/pump-cluster.yml）文件中 gc 变量值，并取消注释。 1234global: # an integer value to control the expiry date of the binlog data, which indicates for how long (in days) the binlog data would be stored # must be bigger than 0 # gc: 7 3.部署 pump_servers 和 node_exporters 1ansible-playbook deploy.yml --tags=pump -l pump1,pump2,pump3 如果没有为pump指定别名则为 ansible-playbook deploy.yml –tags=pump -l 10.152.x.10,10.152.x.11,10.152.x.12 以上命令中，逗号后不要加空格，否则会报错 4.启动 pump_servers 1ansible-playbook start.yml --tags=pump 查看一下 123# ps -ef| grep pumptidb 26199 1 0 21:05 ? 00:00:00 bin/pump --addr=0.0.0.0:8250 --advertise-addr=pump1:8250 --pd-urls=http://10.152.x.10:2379,http://10.152.x.11:2379,http://10.152.x.12:2379 --data-dir=/DATA1/home/tidb/deploy/data.pump --log-file=/DATA1/home/tidb/deploy/log/pump.log --config=conf/pump.toml 5.更新并重启 tidb_servers 为了让enable_binlog = True生效 12ansible-playbook rolling_update.yml --tags=tidb 6.更新监控信息 12ansible-playbook rolling_update_monitor.yml --tags=prometheus 7.查看 Pump 服务状态 使用 binlogctl 查看 Pump 服务状态，pd-urls 参数请替换为集群 PD 地址，结果 State 为 online 表示 Pump 启动成功 12345$resources/bin/binlogctl -pd-urls=http://10.152.x.10:2379 -cmd pumps[2019/12/01 21:11:47.655 +08:00] [INFO] [nodes.go:49] [&quot;query node&quot;] [type=pump] [node=&quot;&#123;NodeID: knode10-152-x-12:8250, Addr: pump3:8250, State: online, MaxCommitTS: 412930776489787394, UpdateTime: 2019-12-01 21:11:45 +0800 CST&#125;&quot;][2019/12/01 21:11:47.655 +08:00] [INFO] [nodes.go:49] [&quot;query node&quot;] [type=pump] [node=&quot;&#123;NodeID: knode10-152-x-10:8250, Addr: pump1:8250, State: online, MaxCommitTS: 412930776463572993, UpdateTime: 2019-12-01 21:11:45 +0800 CST&#125;&quot;][2019/12/01 21:11:47.655 +08:00] [INFO] [nodes.go:49] [&quot;query node&quot;] [type=pump] [node=&quot;&#123;NodeID: knode10-152-x-11:8250, Addr: pump2:8250, State: online, MaxCommitTS: 412930776489787393, UpdateTime: 2019-12-01 21:11:45 +0800 CST&#125;&quot;] 或者登陆TiDB使用show pump status查看状态 12345678910root@10.152.x.10 21:10:32 [(none)]&gt; show pump status;+-----------------------+------------+--------+--------------------+---------------------+| NodeID | Address | State | Max_Commit_Ts | Update_Time |+-----------------------+------------+--------+--------------------+---------------------+| knode10-152-x-10:8250 | pump1:8250 | online | 412930792991752193 | 2019-12-01 21:12:48 || knode10-152-x-11:8250 | pump2:8250 | online | 412930793017966593 | 2019-12-01 21:12:48 || knode10-152-x-12:8250 | pump3:8250 | online | 412930793017966594 | 2019-12-01 21:12:48 |+-----------------------+------------+--------+--------------------+---------------------+3 rows in set (0.01 sec) 部署Drainer部署Drainer将MySQL作为TiDB从库1.下载tidb-enterprise-tools安装包 123wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gztar -zxvf tidb-enterprise-tools-latest-linux-amd64 2.使用TiDB Mydumper备份TiDB数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344$sudo ./tidb-enterprise-tools-latest-linux-amd64/bin/mydumper --ask-password -h 10.152.x.10 -P 4000 -u root --threads=4 --chunk-filesize=64 --skip-tz-utc --regex &#x27;^(?!(mysql\\.|information_schema\\.|performance_schema\\.))&#x27; -o /mfw_rundata/dump/ --verbose=3Enter MySQL Password: ** Message: 21:30:17.767: Server version reported as: 5.7.25-TiDB-v3.0.5** Message: 21:30:17.767: Connected to a TiDB server** Message: 21:30:17.771: Skipping locks because of TiDB** Message: 21:30:17.772: Set to tidb_snapshot &#x27;412931068452667405&#x27;** Message: 21:30:17.782: Started dump at: 2019-12-01 21:30:17** Message: 21:30:17.782: Written master status** Message: 21:30:17.784: Thread 1 connected using MySQL connection ID 20** Message: 21:30:17.794: Thread 1 set to tidb_snapshot &#x27;412931068452667405&#x27;** Message: 21:30:17.796: Thread 2 connected using MySQL connection ID 21** Message: 21:30:17.807: Thread 2 set to tidb_snapshot &#x27;412931068452667405&#x27;** Message: 21:30:17.809: Thread 3 connected using MySQL connection ID 22** Message: 21:30:17.819: Thread 3 set to tidb_snapshot &#x27;412931068452667405&#x27;** Message: 21:30:17.820: Thread 4 connected using MySQL connection ID 23** Message: 21:30:17.832: Thread 4 set to tidb_snapshot &#x27;412931068452667405&#x27;** Message: 21:30:17.843: Thread 2 dumping data for `sysbench`.`sbtest1`** Message: 21:30:17.844: Non-InnoDB dump complete, unlocking tables** Message: 21:30:17.843: Thread 3 dumping data for `sysbench`.`sbtest2`** Message: 21:30:17.843: Thread 1 dumping data for `sysbench`.`sbtest10`** Message: 21:30:17.843: Thread 4 dumping data for `sysbench`.`sbtest3`** Message: 21:30:17.882: Thread 4 dumping data for `sysbench`.`sbtest4`** Message: 21:30:17.883: Thread 2 dumping data for `sysbench`.`sbtest5`** Message: 21:30:17.887: Thread 3 dumping data for `sysbench`.`sbtest6`** Message: 21:30:17.890: Thread 1 dumping data for `sysbench`.`sbtest7`** Message: 21:30:17.911: Thread 4 dumping data for `sysbench`.`sbtest8`** Message: 21:30:17.925: Thread 1 dumping data for `sysbench`.`sbtest9`** Message: 21:30:17.938: Thread 4 dumping schema for `sysbench`.`sbtest1`** Message: 21:30:17.939: Thread 4 dumping schema for `sysbench`.`sbtest10`** Message: 21:30:17.941: Thread 4 dumping schema for `sysbench`.`sbtest2`** Message: 21:30:17.942: Thread 4 dumping schema for `sysbench`.`sbtest3`** Message: 21:30:17.943: Thread 4 dumping schema for `sysbench`.`sbtest4`** Message: 21:30:17.944: Thread 4 dumping schema for `sysbench`.`sbtest5`** Message: 21:30:17.945: Thread 4 dumping schema for `sysbench`.`sbtest6`** Message: 21:30:17.946: Thread 4 dumping schema for `sysbench`.`sbtest7`** Message: 21:30:17.947: Thread 4 dumping schema for `sysbench`.`sbtest8`** Message: 21:30:17.948: Thread 4 dumping schema for `sysbench`.`sbtest9`** Message: 21:30:17.949: Thread 4 shutting down** Message: 21:30:18.079: Thread 2 shutting down** Message: 21:30:18.084: Thread 3 shutting down** Message: 21:30:18.087: Thread 1 shutting down** Message: 21:30:18.087: Finished dump at: 2019-12-01 21:30:18 获取tso值 123456789101112$sudo cat /mfw_rundata/dump/metadataStarted dump at: 2019-12-01 21:30:17SHOW MASTER STATUS: Log: tidb-binlog Pos: 412931068452667405 GTID:Finished dump at: 2019-12-01 21:30:18tso为 412931068452667405 为下游MySQL创建Drainer专用同步账号 1234CREATE USER IF NOT EXISTS &#x27;drainer&#x27;@&#x27;%&#x27;;ALTER USER &#x27;drainer&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;drainer_supersecret&#x27;;GRANT INSERT, UPDATE, DELETE, CREATE, DROP, ALTER, EXECUTE, INDEX, SELECT ON *.* TO &#x27;drainer&#x27;@&#x27;%&#x27;; 导入数据到MySQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$sudo ./tidb-enterprise-tools-latest-linux-amd64/bin/loader -d /mfw_rundata/dump/ -h 10.132.2.143 -u drainer -p drainer_supersecret -P 3308 -t 2 -m &#x27;&#x27; -status-addr &#x27;:8723&#x27;2019/12/01 22:12:55 printer.go:52: [info] Welcome to loader2019/12/01 22:12:55 printer.go:53: [info] Release Version: v1.0.0-76-gad009d92019/12/01 22:12:55 printer.go:54: [info] Git Commit Hash: ad009d917b2cdc2a9cc26bc4e7046884c1ff43e72019/12/01 22:12:55 printer.go:55: [info] Git Branch: master2019/12/01 22:12:55 printer.go:56: [info] UTC Build Time: 2019-10-21 06:22:032019/12/01 22:12:55 printer.go:57: [info] Go Version: go version go1.12 linux/amd642019/12/01 22:12:55 main.go:51: [info] config: &#123;&quot;log-level&quot;:&quot;info&quot;,&quot;log-file&quot;:&quot;&quot;,&quot;status-addr&quot;:&quot;:8723&quot;,&quot;pool-size&quot;:2,&quot;dir&quot;:&quot;/mfw_rundata/dump/&quot;,&quot;db&quot;:&#123;&quot;host&quot;:&quot;10.132.2.143&quot;,&quot;user&quot;:&quot;drainer&quot;,&quot;port&quot;:3308,&quot;sql-mode&quot;:&quot;&quot;,&quot;max-allowed-packet&quot;:67108864&#125;,&quot;checkpoint-schema&quot;:&quot;tidb_loader&quot;,&quot;config-file&quot;:&quot;&quot;,&quot;route-rules&quot;:null,&quot;do-table&quot;:null,&quot;do-db&quot;:null,&quot;ignore-table&quot;:null,&quot;ignore-db&quot;:null,&quot;rm-checkpoint&quot;:false&#125;2019/12/01 22:12:55 loader.go:532: [info] [loader] prepare takes 0.000565 seconds2019/12/01 22:12:55 checkpoint.go:207: [info] calc checkpoint finished. finished tables (map[])2019/12/01 22:12:55 loader.go:715: [info] [loader][run db schema]/mfw_rundata/dump//sysbench-schema-create.sql[start]2019/12/01 22:12:55 loader.go:720: [info] [loader][run db schema]/mfw_rundata/dump//sysbench-schema-create.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest10-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest10-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest3-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest3-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest9-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest9-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest2-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest2-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest4-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest4-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest5-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest5-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest7-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest7-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest6-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest6-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest8-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest8-schema.sql[finished]2019/12/01 22:12:55 loader.go:736: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest1-schema.sql[start]2019/12/01 22:12:55 loader.go:741: [info] [loader][run table schema]/mfw_rundata/dump//sysbench.sbtest1-schema.sql[finished]2019/12/01 22:12:55 loader.go:715: [info] [loader][run db schema]/mfw_rundata/dump//test-schema-create.sql[start]2019/12/01 22:12:55 loader.go:720: [info] [loader][run db schema]/mfw_rundata/dump//test-schema-create.sql[finished]2019/12/01 22:12:55 loader.go:773: [info] [loader] create tables takes 0.334379 seconds2019/12/01 22:12:55 loader.go:788: [info] [loader] all data files have been dispatched, waiting for them finished 2019/12/01 22:12:55 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest3.sql[start]2019/12/01 22:12:55 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest8.sql[start]2019/12/01 22:12:55 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest8.sql scanned finished.2019/12/01 22:12:55 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest3.sql scanned finished.2019/12/01 22:12:56 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest8.sql[finished]2019/12/01 22:12:56 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest9.sql[start]2019/12/01 22:12:56 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest3.sql[finished]2019/12/01 22:12:56 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest4.sql[start]2019/12/01 22:12:56 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest9.sql scanned finished.2019/12/01 22:12:56 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest4.sql scanned finished.2019/12/01 22:12:56 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest4.sql[finished]2019/12/01 22:12:56 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest7.sql[start]2019/12/01 22:12:56 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest9.sql[finished]2019/12/01 22:12:56 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest6.sql[start]2019/12/01 22:12:56 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest7.sql scanned finished.2019/12/01 22:12:56 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest6.sql scanned finished.2019/12/01 22:12:56 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest7.sql[finished]2019/12/01 22:12:56 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest1.sql[start]2019/12/01 22:12:57 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest6.sql[finished]2019/12/01 22:12:57 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest10.sql[start]2019/12/01 22:12:57 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest1.sql scanned finished.2019/12/01 22:12:57 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest10.sql scanned finished.2019/12/01 22:12:57 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest10.sql[finished]2019/12/01 22:12:57 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest2.sql[start]2019/12/01 22:12:57 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest1.sql[finished]2019/12/01 22:12:57 loader.go:158: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest5.sql[start]2019/12/01 22:12:57 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest2.sql scanned finished.2019/12/01 22:12:57 loader.go:216: [info] data file /mfw_rundata/dump/sysbench.sbtest5.sql scanned finished.2019/12/01 22:12:57 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest2.sql[finished]2019/12/01 22:12:57 loader.go:165: [info] [loader][restore table data sql]/mfw_rundata/dump//sysbench.sbtest5.sql[finished]2019/12/01 22:12:57 loader.go:791: [info] [loader] all data files has been finished, takes 2.037124 seconds2019/12/01 22:12:57 main.go:88: [info] loader stopped and exits 3.修改 tidb-ansible/inventory.ini 文件 为 drainer_servers 主机组添加部署机器 IP，initial_commit_ts 请设置为获取的 initial_commit_ts，仅用于 Drainer 第一次启动 123[drainer_servers]drainer_mysql ansible_host=10.152.x.12 initial_commit_ts=&quot;412931068452667405&quot; 修改配置文件 12345678910111213[tidb@knode10-152-x-133 21:33:21 ~/tidb-ansible]$cp conf/drainer.toml conf/drainer_mysql_drainer.tomlvim conf/drainer_mysql_drainer.tomldb-type = &quot;mysql&quot;[syncer.to]host = &quot;10.132.2.143&quot;user = &quot;drainer&quot;password = &quot;drainer_supersecret&quot;port = 3308 4.部署Drainer 12ansible-playbook deploy_drainer.yml 5.启动 Drainer 123ansible-playbook start_drainer.yml 6.查看Drainer状态 12345678910111213root@10.152.x.10 22:06:49 [(none)]&gt; show drainer status;+-----------------------+------------------+--------+--------------------+---------------------+| NodeID | Address | State | Max_Commit_Ts | Update_Time |+-----------------------+------------------+--------+--------------------+---------------------+| knode10-152-x-12:8249 | 10.152.x.12:8249 | online | 412931643727675393 | 2019-12-01 22:06:52 |+-----------------------+------------------+--------+--------------------+---------------------+1 row in set (0.00 sec)$resources/bin/binlogctl -pd-urls=http://10.152.x.10:2379 -cmd drainers[2019/12/01 22:07:27.531 +08:00] [INFO] [nodes.go:49] [&quot;query node&quot;] [type=drainer] [node=&quot;&#123;NodeID: knode10-152-x-12:8249, Addr: 10.152.x.12:8249, State: online, MaxCommitTS: 412931651605102594, UpdateTime: 2019-12-01 22:07:26 +0800 CST&#125;&quot;]","categories":[],"tags":[{"name":"TiDB","slug":"TiDB","permalink":"http://fuxkdb.com/tags/TiDB/"},{"name":"Binlog","slug":"Binlog","permalink":"http://fuxkdb.com/tags/Binlog/"}]},{"title":"ProxySQL平滑下线节点","slug":"2019-11-12-ProxySQL平滑下线节点","date":"2019-11-12T13:57:00.000Z","updated":"2019-11-13T15:19:00.000Z","comments":true,"path":"2019/11/12/2019-11-12-ProxySQL平滑下线节点/","link":"","permalink":"http://fuxkdb.com/2019/11/12/2019-11-12-ProxySQL%E5%B9%B3%E6%BB%91%E4%B8%8B%E7%BA%BF%E8%8A%82%E7%82%B9/","excerpt":"ProxySQL平滑下线节点因为MGR多主DDL和DML不能并发在不同节点执行, 而我们又存在即通过KO又通过ProxySQL访问的数据库的情况, 为了避免在发生故障, 我们决定将ProxySQL中配置的集群真实ip删除, 改为使用KO的vip.本文介绍如何平滑下线ProxySQL节点, 从而做到对业务无感知. KO我司自研php中间件 环境介绍 10.133.x.59 跑sysbench 10.133.x.52:3307 测试集群节点 10.133.x.53:3307 测试集群节点 10.133.x.54:3307 测试集群节点 VIP 访问类型 所在服务器 10.133.x.202 KO 10.133.x.202 10.133.x.203 ProxySQL 10.133.x.203 ProxySQL版本1234567admin@127.0.0.1 23:12:02 [(none)]&gt; select version();+-------------------+| version() |+-------------------+| 2.0.8-67-g877cab1 |+-------------------+1 row in set (0.00 sec)MySQL版本1234567root@localhost 23:17:56 [(none)]&gt; select version();+------------+| version() |+------------+| 5.7.26-log |+------------+1 row in set (0.00 sec)","text":"ProxySQL平滑下线节点因为MGR多主DDL和DML不能并发在不同节点执行, 而我们又存在即通过KO又通过ProxySQL访问的数据库的情况, 为了避免在发生故障, 我们决定将ProxySQL中配置的集群真实ip删除, 改为使用KO的vip.本文介绍如何平滑下线ProxySQL节点, 从而做到对业务无感知. KO我司自研php中间件 环境介绍 10.133.x.59 跑sysbench 10.133.x.52:3307 测试集群节点 10.133.x.53:3307 测试集群节点 10.133.x.54:3307 测试集群节点 VIP 访问类型 所在服务器 10.133.x.202 KO 10.133.x.202 10.133.x.203 ProxySQL 10.133.x.203 ProxySQL版本1234567admin@127.0.0.1 23:12:02 [(none)]&gt; select version();+-------------------+| version() |+-------------------+| 2.0.8-67-g877cab1 |+-------------------+1 row in set (0.00 sec)MySQL版本1234567root@localhost 23:17:56 [(none)]&gt; select version();+------------+| version() |+------------+| 5.7.26-log |+------------+1 row in set (0.00 sec) 错误的下线方式1.59跑压测1234567891011121314# cat sysbench_by_proxysql.sh sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=sysbench_user --mysql-password=superSecret --mysql-port=16034 \\--mysql-host=10.133.x.203 \\--mysql-db=sysbench --tables=10 --table-size=5000000 --threads=$1 \\--events=5000000 --report-interval=5 --db-driver=mysql --time=$2 run# sh sysbench_by_proxysql.sh 16 1800[ 5s ] thds: 16 tps: 1125.70 qps: 22546.06 (r/w/o: 15787.64/4503.82/2254.61) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00[ 10s ] thds: 16 tps: 1125.31 qps: 22516.34 (r/w/o: 15762.30/4503.43/2250.61) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00[ 15s ] thds: 16 tps: 1127.13 qps: 22542.62 (r/w/o: 15778.83/4509.72/2254.06) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00[ 20s ] thds: 16 tps: 1130.82 qps: 22613.98 (r/w/o: 15829.67/4522.48/2261.84) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00... 查看ProxySQL vip所在节点的ProxySQL配置1234567891011121314151617admin@127.0.0.1 17:58:46 [(none)]&gt; select * from runtime_mysql_servers;+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| 10 | 10.133.x.53 | 3307 | 0 | ONLINE | 3 | 0 | 1000 | 0 | 0 | 0 | || 11 | 10.133.x.54 | 3307 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | || 11 | 10.133.x.52 | 3307 | 0 | ONLINE | 2 | 0 | 1000 | 0 | 0 | 0 | |+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+admin@127.0.0.1 18:00:31 [(none)]&gt; select * from mysql_servers;+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| 10 | 10.133.x.53 | 3307 | 0 | ONLINE | 3 | 0 | 1000 | 0 | 0 | 0 | || 11 | 10.133.x.52 | 3307 | 0 | ONLINE | 2 | 0 | 1000 | 0 | 0 | 0 | || 11 | 10.133.x.54 | 3307 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+我们打算添加KO vip到mysql server中, 所以一开始我尝试删除mysql servers所有配置, 然后插入10.133.x.202:33071234567891011121314[2019-11-12 18:01:37]admin@127.0.0.1 18:00:49 [(none)]&gt; delete from mysql_servers;[2019-11-12 18:01:37]Query OK, 3 rows affected (0.00 sec)[2019-11-12 18:01:37][2019-11-12 18:03:16]admin@127.0.0.1 18:01:37 [(none)]&gt; insert into mysql_servers values(10,&#x27;10.133.x.202&#x27;,3307,0,&#x27;ONLINE&#x27;,10,0,1000,0,0,0,&#x27;&#x27;);[2019-11-12 18:03:16]Query OK, 1 row affected (0.00 sec)[2019-11-12 18:03:16][2019-11-12 18:03:19]admin@127.0.0.1 18:03:16 [(none)]&gt; select * from mysql_servers;[2019-11-12 18:03:19]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:19]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 18:03:19]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:19]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 10 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 18:03:19]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:19]1 row in set (0.00 sec)[2019-11-12 18:03:19]加载配置到runtime, 使配置真正生效123456789101112131415161718192021222324252627[2019-11-12 18:03:43]admin@127.0.0.1 18:03:19 [(none)]&gt; load mysql servers to runtime;[2019-11-12 18:03:43]Query OK, 0 rows affected (0.00 sec)[2019-11-12 18:03:43][2019-11-12 18:03:44]admin@127.0.0.1 18:03:43 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 18:03:44]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:44]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 18:03:44]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:44]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 10 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 18:03:44]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_HARD | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 18:03:44]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:44]2 rows in set (0.00 sec)[2019-11-12 18:03:44][2019-11-12 18:03:45]admin@127.0.0.1 18:03:44 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 18:03:45]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:45]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 18:03:45]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:45]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 10 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 18:03:45]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:45]1 row in set (0.00 sec)[2019-11-12 18:03:45][2019-11-12 18:03:46]admin@127.0.0.1 18:03:45 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 18:03:46]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:46]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 18:03:46]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:46]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 10 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 18:03:46]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 18:03:46]1 row in set (0.01 sec) 此时查看sysbench, 发现在配置加载后, 连接立马报错.显然, 如果以这种方式在生产环境下线节点, 业务肯定是不可以接受的12345678910111213141516171819202122232425262728293031[2019-11-12 18:03:40][ 180s ] thds: 16 tps: 1134.87 qps: 22716.96 (r/w/o: 15904.95/4542.07/2269.94) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest7 WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:419: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;COMMIT&#x27;[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT DISTINCT c FROM sbtest3 WHERE id BETWEEN ? AND ? ORDER BY c&#x27;[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest6 WHERE id BETWEEN ? AND ?&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:409: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest8 WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest6 WHERE id=?&#x27;[2019-11-12 18:03:43](last message repeated 2 times)[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;UPDATE sbtest8 SET c=? WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest2 WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:432: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:419: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:432: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:419: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43](last message repeated 3 times)[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:469: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest1 WHERE id BETWEEN ? AND ?&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:432: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;COMMIT&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:409: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT DISTINCT c FROM sbtest8 WHERE id BETWEEN ? AND ? ORDER BY c&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:432: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT c FROM sbtest8 WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:419: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;DELETE FROM sbtest2 WHERE id=?&#x27;[2019-11-12 18:03:43]FATAL: mysql_stmt_execute() returned error 2013 (Lost connection to MySQL server during query) for query &#x27;SELECT DISTINCT c FROM sbtest1 WHERE id BETWEEN ? AND ? ORDER BY c&#x27;[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:432: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:43]FATAL: `thread_run&#x27; function failed: /usr/share/sysbench/oltp_common.lua:487: SQL error, errno = 2013, state = &#x27;HY000&#x27;: Lost connection to MySQL server during query[2019-11-12 18:03:48]Error in my_thread_global_end(): 16 threads didn&#x27;t exit 正确的下线方式1.59跑sysbench123456789101112131415161718192021[2019-11-12 21:33:03][root@node10-133-1-59 fanboshi]# sh sysbench_by_proxysql.sh 4 1800[2019-11-12 21:33:03]sysbench 1.0.18 (using bundled LuaJIT 2.1.0-beta2)[2019-11-12 21:33:03][2019-11-12 21:33:03]Running the test with following options:[2019-11-12 21:33:03]Number of threads: 4[2019-11-12 21:33:03]Report intermediate results every 5 second(s)[2019-11-12 21:33:03]Initializing random number generator from current time[2019-11-12 21:33:03][2019-11-12 21:33:03][2019-11-12 21:33:03]Initializing worker threads...[2019-11-12 21:33:03][2019-11-12 21:33:03]Threads started![2019-11-12 21:33:03][2019-11-12 21:33:08][ 5s ] thds: 4 tps: 329.27 qps: 6592.58 (r/w/o: 4615.37/1317.88/659.34) lat (ms,95%): 13.95 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:13][ 10s ] thds: 4 tps: 339.83 qps: 6799.25 (r/w/o: 4760.65/1358.93/679.66) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:18][ 15s ] thds: 4 tps: 338.36 qps: 6768.12 (r/w/o: 4736.98/1354.42/676.71) lat (ms,95%): 13.22 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:23][ 20s ] thds: 4 tps: 335.65 qps: 6714.51 (r/w/o: 4700.84/1342.38/671.29) lat (ms,95%): 13.22 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:28][ 25s ] thds: 4 tps: 336.79 qps: 6734.32 (r/w/o: 4713.01/1347.94/673.37) lat (ms,95%): 13.46 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:33][ 30s ] thds: 4 tps: 340.61 qps: 6812.86 (r/w/o: 4770.58/1360.85/681.43) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:33:38][ 35s ] thds: 4 tps: 339.20 qps: 6779.83 (r/w/o: 4745.02/1356.41/678.40) lat (ms,95%): 13.46 err/s: 0.00 reconn/s: 0.00... 1.52插入KO VIP, 修改其他节点状态为OFFLINE_SOFT1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[2019-11-12 21:33:58]admin@127.0.0.1 21:33:28 [(none)]&gt; insert into mysql_servers values(10,&#x27;10.133.x.202&#x27;,3307,0,&#x27;ONLINE&#x27;,100,0,1000,0,0,0,&#x27;&#x27;);[2019-11-12 21:33:58]Query OK, 1 row affected (0.00 sec)[2019-11-12 21:33:58][2019-11-12 21:34:03]admin@127.0.0.1 21:33:58 [(none)]&gt; select * from mysql_servers; [2019-11-12 21:34:03]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:03]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:34:03]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:03]| 10 | 10.133.x.53 | 3307 | 0 | ONLINE | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:03]| 11 | 10.133.x.52 | 3307 | 0 | ONLINE | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:03]| 11 | 10.133.x.54 | 3307 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:03]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:03]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:03]4 rows in set (0.00 sec)[2019-11-12 21:34:03][2019-11-12 21:34:08]admin@127.0.0.1 21:34:04 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:34:08]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:08]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:34:08]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:08]| 10 | 10.133.x.53 | 3307 | 0 | ONLINE | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:08]| 11 | 10.133.x.54 | 3307 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:08]| 11 | 10.133.x.52 | 3307 | 0 | ONLINE | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:08]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:08]3 rows in set (0.00 sec)[2019-11-12 21:34:08][2019-11-12 21:34:14]admin@127.0.0.1 21:34:08 [(none)]&gt; update mysql_servers set status=&#x27;OFFLINE_SOFT&#x27; where hostname!=&#x27;10.133.x.202&#x27;;[2019-11-12 21:34:14]Query OK, 3 rows affected (0.00 sec)[2019-11-12 21:34:14][2019-11-12 21:34:18]admin@127.0.0.1 21:34:14 [(none)]&gt; select * from mysql_servers; [2019-11-12 21:34:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:18]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:34:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:18]| 10 | 10.133.x.53 | 3307 | 0 | OFFLINE_SOFT | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:18]| 11 | 10.133.x.52 | 3307 | 0 | OFFLINE_SOFT | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:18]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_SOFT | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:18]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:18]4 rows in set (0.00 sec)[2019-11-12 21:34:18][2019-11-12 21:34:22]admin@127.0.0.1 21:34:18 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:34:22]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:22]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:34:22]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:22]| 10 | 10.133.x.53 | 3307 | 0 | ONLINE | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:22]| 11 | 10.133.x.54 | 3307 | 0 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:22]| 11 | 10.133.x.52 | 3307 | 0 | ONLINE | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:34:22]+--------------+-------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:34:22]3 rows in set (0.00 sec)加载配置到runtime1234567891011121314[2019-11-12 21:34:47]admin@127.0.0.1 21:34:28 [(none)]&gt; load mysql servers to runtime; [2019-11-12 21:34:47]Query OK, 0 rows affected (0.00 sec)[2019-11-12 21:34:47][2019-11-12 21:35:01]admin@127.0.0.1 21:34:47 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:35:01]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:35:01]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:35:01]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:35:01]| 10 | 10.133.x.53 | 3307 | 0 | OFFLINE_SOFT | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:35:01]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_SOFT | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:35:01]| 11 | 10.133.x.52 | 3307 | 0 | OFFLINE_SOFT | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:35:01]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:35:01]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:35:01]4 rows in set (0.00 sec)[2019-11-12 21:35:01] 查看sysbench, 正常运行连接无异常1234[2019-11-12 21:34:23][ 80s ] thds: 4 tps: 332.59 qps: 6646.37 (r/w/o: 4651.64/1329.75/664.98) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:34:28][ 85s ] thds: 4 tps: 339.42 qps: 6798.35 (r/w/o: 4759.64/1359.67/679.03) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:34:33][ 90s ] thds: 4 tps: 338.40 qps: 6763.76 (r/w/o: 4734.57/1352.39/676.80) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:34:38][ 95s ] thds: 4 tps: 337.20 qps: 6742.19 (r/w/o: 4719.79/1348.00/674.40) lat (ms,95%): 13.46 err/s: 0.00 reconn/s: 0.00 查看proxysql转台信息, 发现status为OFFLINE_SOFT, 连接仍然在10.133.x.53上1234567891011121314151617[2019-11-12 21:35:10]admin@127.0.0.1 21:35:01 [(none)]&gt; SELECT hostgroup hg, srv_host, status, ConnUsed, ConnFree, ConnOK, ConnERR FROM stats_mysql_connection_pool WHERE ConnUsed+ConnFree &gt; 0 ORDER BY hg, srv_host;[2019-11-12 21:35:10]+----+-------------+--------------+----------+----------+--------+---------+[2019-11-12 21:35:10]| hg | srv_host | status | ConnUsed | ConnFree | ConnOK | ConnERR |[2019-11-12 21:35:10]+----+-------------+--------------+----------+----------+--------+---------+[2019-11-12 21:35:10]| 10 | 10.133.x.53 | OFFLINE_SOFT | 4 | 0 | 26 | 0 |[2019-11-12 21:35:10]+----+-------------+--------------+----------+----------+--------+---------+[2019-11-12 21:35:10]1 row in set (0.01 sec)[2019-11-12 21:35:10][2019-11-12 21:35:16]admin@127.0.0.1 21:35:10 [(none)]&gt; select * from stats_mysql_processlist;[2019-11-12 21:35:16]+----------+-----------+------------+----------+-------------+----------+-----------+-------------+------------+-------------+----------+---------+---------+---------------------------------------------------------------------+--------------+---------------+[2019-11-12 21:35:16]| ThreadID | SessionID | user | db | cli_host | cli_port | hostgroup | l_srv_host | l_srv_port | srv_host | srv_port | command | time_ms | info | status_flags | extended_info |[2019-11-12 21:35:16]+----------+-----------+------------+----------+-------------+----------+-----------+-------------+------------+-------------+----------+---------+---------+---------------------------------------------------------------------+--------------+---------------+[2019-11-12 21:35:16]| 10 | 15664 | sysbench_user | sysbench | 10.133.x.59 | 60008 | 10 | 10.133.x.52 | 60207 | 10.133.x.53 | 3307 | Execute | 0 | COMMIT | 0 | NULL |[2019-11-12 21:35:16]| 5 | 15665 | sysbench_user | sysbench | 10.133.x.59 | 60012 | 10 | 10.133.x.52 | 28221 | 10.133.x.53 | 3307 | Execute | 0 | SELECT c FROM sbtest8 WHERE id=? | 0 | NULL |[2019-11-12 21:35:16]| 19 | 15666 | sysbench_user | sysbench | 10.133.x.59 | 60014 | 10 | 10.133.x.52 | 25673 | 10.133.x.53 | 3307 | Execute | 0 | SELECT DISTINCT c FROM sbtest10 WHERE id BETWEEN ? AND ? ORDER BY c | 0 | NULL |[2019-11-12 21:35:16]| 4 | 15667 | sysbench_user | sysbench | 10.133.x.59 | 60010 | 10 | 10.133.x.52 | 28223 | 10.133.x.53 | 3307 | Execute | 0 | SELECT c FROM sbtest3 WHERE id BETWEEN ? AND ? | 0 | NULL |[2019-11-12 21:35:16]+----------+-----------+------------+----------+-------------+----------+-----------+-------------+------------+-------------+----------+---------+---------+---------------------------------------------------------------------+--------------+---------------+ 我们手动停止sysbench, 重启sysbench1234567891011121314151617182019-11-12 21:35:37][root@node10-133-1-59 fanboshi]# sh sysbench_by_proxysql.sh 4 1800[2019-11-12 21:35:37]sysbench 1.0.18 (using bundled LuaJIT 2.1.0-beta2)[2019-11-12 21:35:37][2019-11-12 21:35:37]Running the test with following options:[2019-11-12 21:35:37]Number of threads: 4[2019-11-12 21:35:37]Report intermediate results every 5 second(s)[2019-11-12 21:35:37]Initializing random number generator from current time[2019-11-12 21:35:37][2019-11-12 21:35:37][2019-11-12 21:35:37]Initializing worker threads...[2019-11-12 21:35:37][2019-11-12 21:35:37]Threads started![2019-11-12 21:35:37][2019-11-12 21:35:42][ 5s ] thds: 4 tps: 432.41 qps: 8659.34 (r/w/o: 6062.50/1731.23/865.61) lat (ms,95%): 11.24 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:35:47][ 10s ] thds: 4 tps: 433.20 qps: 8661.86 (r/w/o: 6064.24/1731.21/866.41) lat (ms,95%): 11.24 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:35:52][ 15s ] thds: 4 tps: 433.79 qps: 8676.81 (r/w/o: 6074.06/1735.16/867.58) lat (ms,95%): 11.24 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:35:57][ 20s ] thds: 4 tps: 436.80 qps: 8736.88 (r/w/o: 6114.46/1748.82/873.61) lat (ms,95%): 11.24 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:36:02][ 25s ] thds: 4 tps: 434.99 qps: 8701.31 (r/w/o: 6092.20/1739.14/869.97) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00 再次查看连接状况123456789101112131415161718[2019-11-12 21:35:41]admin@127.0.0.1 21:35:25 [(none)]&gt; SELECT hostgroup hg, srv_host, status, ConnUsed, ConnFree, ConnOK, ConnERR FROM stats_mysql_connection_pool WHERE ConnUsed+ConnFree &gt; 0 ORDER BY hg, srv_host;[2019-11-12 21:35:41]+----+--------------+--------+----------+----------+--------+---------+[2019-11-12 21:35:41]| hg | srv_host | status | ConnUsed | ConnFree | ConnOK | ConnERR |[2019-11-12 21:35:41]+----+--------------+--------+----------+----------+--------+---------+[2019-11-12 21:35:41]| 10 | 10.133.x.202 | ONLINE | 4 | 0 | 4 | 0 |[2019-11-12 21:35:41]+----+--------------+--------+----------+----------+--------+---------+[2019-11-12 21:35:41]1 row in set (0.00 sec)[2019-11-12 21:35:41][2019-11-12 21:35:50]admin@127.0.0.1 21:35:42 [(none)]&gt; select * from stats_mysql_processlist;[2019-11-12 21:35:50]+----------+-----------+------------+----------+-------------+----------+-----------+--------------+------------+--------------+----------+---------+---------+--------------------------------------------------------------------+--------------+---------------+[2019-11-12 21:35:50]| ThreadID | SessionID | user | db | cli_host | cli_port | hostgroup | l_srv_host | l_srv_port | srv_host | srv_port | command | time_ms | info | status_flags | extended_info |[2019-11-12 21:35:50]+----------+-----------+------------+----------+-------------+----------+-----------+--------------+------------+--------------+----------+---------+---------+--------------------------------------------------------------------+--------------+---------------+[2019-11-12 21:35:50]| 2 | 15731 | sysbench_user | sysbench | 10.133.x.59 | 60330 | 10 | 10.133.x.202 | 15300 | 10.133.x.202 | 3307 | Sleep | 0 | NULL | 0 | NULL |[2019-11-12 21:35:50]| 4 | 15732 | sysbench_user | sysbench | 10.133.x.59 | 60334 | 10 | 10.133.x.202 | 15304 | 10.133.x.202 | 3307 | Sleep | 0 | NULL | 0 | NULL |[2019-11-12 21:35:50]| 7 | 15733 | sysbench_user | sysbench | 10.133.x.59 | 60336 | 10 | 10.133.x.202 | 15302 | 10.133.x.202 | 3307 | Execute | 0 | COMMIT | 0 | NULL |[2019-11-12 21:35:50]| 6 | 15734 | sysbench_user | sysbench | 10.133.x.59 | 60332 | 10 | 10.133.x.202 | 15306 | 10.133.x.202 | 3307 | Execute | 0 | SELECT DISTINCT c FROM sbtest9 WHERE id BETWEEN ? AND ? ORDER BY c | 0 | NULL |[2019-11-12 21:35:50]+----------+-----------+------------+----------+-------------+----------+-----------+--------------+------------+--------------+----------+---------+---------+--------------------------------------------------------------------+--------------+---------------+[2019-11-12 21:35:50]4 rows in set (0.01 sec)可以看到连接已经转发到了10.133.x.202上 删除无用配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[2019-11-12 21:36:18]admin@127.0.0.1 21:35:50 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:36:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:18]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:36:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:18]| 10 | 10.133.x.53 | 3307 | 0 | OFFLINE_SOFT | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:18]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_SOFT | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:18]| 11 | 10.133.x.52 | 3307 | 0 | OFFLINE_SOFT | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:18]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:18]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:18]4 rows in set (0.01 sec)[2019-11-12 21:36:18][2019-11-12 21:36:23]admin@127.0.0.1 21:36:18 [(none)]&gt; select * from mysql_servers;[2019-11-12 21:36:23]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:23]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:36:23]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:23]| 10 | 10.133.x.53 | 3307 | 0 | OFFLINE_SOFT | 3 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:23]| 11 | 10.133.x.52 | 3307 | 0 | OFFLINE_SOFT | 2 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:23]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_SOFT | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:23]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:23]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:23]4 rows in set (0.00 sec)[2019-11-12 21:36:23][2019-11-12 21:36:29]admin@127.0.0.1 21:36:23 [(none)]&gt; delete from mysql_servers where hostname!=&#x27;10.133.x.202&#x27;;[2019-11-12 21:36:29]Query OK, 3 rows affected (0.00 sec)[2019-11-12 21:36:29][2019-11-12 21:36:31]admin@127.0.0.1 21:36:29 [(none)]&gt; select * from mysql_servers;[2019-11-12 21:36:31]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:31]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:36:31]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:31]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:31]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:31]1 row in set (0.00 sec)[2019-11-12 21:36:31][2019-11-12 21:36:42]admin@127.0.0.1 21:36:31 [(none)]&gt; load mysql servers to runtime;[2019-11-12 21:36:42]Query OK, 0 rows affected (0.00 sec)[2019-11-12 21:36:42][2019-11-12 21:36:49]admin@127.0.0.1 21:36:42 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:36:49]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:49]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:36:49]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:49]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:49]| 11 | 10.133.x.54 | 3307 | 0 | OFFLINE_HARD | 1 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:49]+--------------+--------------+------+-----------+--------------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:49]2 rows in set (0.00 sec)[2019-11-12 21:36:49][2019-11-12 21:36:55]admin@127.0.0.1 21:36:49 [(none)]&gt; select * from runtime_mysql_servers;[2019-11-12 21:36:55]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:55]| hostgroup_id | hostname | port | gtid_port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |[2019-11-12 21:36:55]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:55]| 10 | 10.133.x.202 | 3307 | 0 | ONLINE | 100 | 0 | 1000 | 0 | 0 | 0 | |[2019-11-12 21:36:55]+--------------+--------------+------+-----------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+[2019-11-12 21:36:55]1 row in set (0.00 sec) 连接无异常1234[2019-11-12 21:36:37][ 60s ] thds: 4 tps: 418.79 qps: 8379.26 (r/w/o: 5866.30/1675.37/837.59) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:36:42][ 65s ] thds: 4 tps: 421.60 qps: 8433.50 (r/w/o: 5904.67/1685.62/843.21) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:36:47][ 70s ] thds: 4 tps: 416.80 qps: 8334.11 (r/w/o: 5832.93/1667.58/833.59) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00[2019-11-12 21:36:52][ 75s ] thds: 4 tps: 413.81 qps: 8273.84 (r/w/o: 5790.77/1655.45/827.62) lat (ms,95%): 11.65 err/s: 0.00 reconn/s: 0.00 关于OFFLINE_SOFT详见https://github.com/sysown/proxysql/wiki/Main-(runtime)#mysql_servers status: ONLINE - backend server is fully operational SHUNNED - backend sever is temporarily taken out of use because of either too many connection errors in a time that was too short, or replication lag exceeded the allowed threshold OFFLINE_SOFT - when a server is put into OFFLINE_SOFT mode, new incoming connections aren’t accepted anymore, while the existing connections are kept until they became inactive. In other words, connections are kept in use until the current transaction is completed. This allows to gracefully detach a backend OFFLINE_HARD - when a server is put into OFFLINE_HARD mode, the existing connections are dropped, while new incoming connections aren’t accepted either. This is equivalent to deleting the server from a hostgroup, or temporarily taking it out of the hostgroup for maintenance work","categories":[],"tags":[{"name":"ProxySQL","slug":"ProxySQL","permalink":"http://fuxkdb.com/tags/ProxySQL/"}]},{"title":"如何自己制作机械键盘","slug":"2019-10-21-如何自己制作机械键盘","date":"2019-10-21T13:57:00.000Z","updated":"2019-11-03T11:07:40.000Z","comments":true,"path":"2019/10/21/2019-10-21-如何自己制作机械键盘/","link":"","permalink":"http://fuxkdb.com/2019/10/21/2019-10-21-%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E5%88%B6%E4%BD%9C%E6%9C%BA%E6%A2%B0%E9%94%AE%E7%9B%98/","excerpt":"如何自己制作机械键盘本文记录自己的初次”客制化”经历, 键圈大佬请略过, 错误之处请指正, 装逼犯自动忽视~ 材料准备轴体轴体这里就不做介绍了, 如果你也是萌新的话, 就先了解这四种轴吧: 青轴 茶轴 红轴 黑轴","text":"如何自己制作机械键盘本文记录自己的初次”客制化”经历, 键圈大佬请略过, 错误之处请指正, 装逼犯自动忽视~ 材料准备轴体轴体这里就不做介绍了, 如果你也是萌新的话, 就先了解这四种轴吧: 青轴 茶轴 红轴 黑轴 就我个人来说, 入坑第一把键盘是G80-3000, 后面也买过国产青轴和半价Filco(酷冷至尊), 也买过Filco奶酪绿. 16年双11在KDBFans入了阿米洛Va87m红轴后就只认线性轴了, 阿米洛红轴是我买过用过的量产里认为手感最好的, 某up主也有过类似的言论, 在此之前我还一直以为是自己的错觉… 其他轴体信息请参考下面的文章: 垃圾佬在zF杂谈篇——各种轴体的不准确资料 客制化透明轴体盘点 关于目前市面上的机械键盘轴体的汇总与介绍 List of all keyboard switches 大致来说, 单颗轴体价格在几毛到十几块钱这个范围内吧, 例如: GATERON佳达隆KS-3X1 – 0.98一颗 Cherry红轴 – 2.5左右一颗 GATERON机械键盘轴体Tiffany – 6.8左右一颗 而且目前来说, 貌似也不是Cherry第一了~, 比如Tiffany轴就备受好评. 既然是自己组一把, 那肯定要用好的 外壳想好自己想要64, 68, 84, 87, 104还是其他配列. 量产键盘最多的就是87和104.外壳价格几十至几千不等, 而且很多需要跟团上车才能买到, 如果没上车可能就只能闲鱼了.咱就做这个64配列 PCB板就是电路板了, 要把轴焊接到板子上, 也有热插拔的, 就是不用焊接, 可以随时更换轴体. 想了很久还是决定买热插拔的, 焊的话太麻烦了, 而且自己也没用过热插拔的键盘(并不是不会, 我小学三年级就在少年宫焊收音机了🤣)购买前可以和客服确认一下是否支持自己想要的配列. 灯的话, 就选pcb上有灯的就好, 分清楚底灯和轴灯(底灯就是下面亮, 轴灯就是上面亮..). 有些配列还需要自己刷机. 我买的直接就是我要的配列, 所以这一步也免了 QMK系列刷机网址： https://config.qmk.fm/ QMK系列刷机软件下载地址： https://pan.baidu.com/s/1GMQGlyg3091JWQRYFl7s3A RGB的指导视频 https://www.ixigua.com/i6710847720807989260/ 非RGB的指导视频 https://www.ixigua.com/i6710847016513044492/ 示例 需要焊接的(本例只有底灯):正面背面 热插拔的(本例只有轴灯)正面背面 定位板每个键轴有固定的位置卡在定位板上，键轴再焊接到主板上，可以实现键轴的稳固和排列整齐等。有不同材质和不同配列可供选择。需要注意的是，选用的定位板的配列要是主板能够支持的配列^1 定位板有碳纤维的, 铝的, 铜的, 不同材质可能手感也不同. 卫星轴卫星轴也有很多类型..(懒得查资料写了怎么办😥) 主要用来平衡大键卫星轴貌似分两类吧: 定位板卫星轴 pcb卫星轴常见的是靠插头固定还有靠螺丝固定的 我这次就选GMK螺丝卫星轴了, 更稳固些. 键帽请看一下这篇文章【科普】配列、键帽高度以及常见的键帽倍数识别总的来说键帽也是几十至几千价格不等. 国外的GMK, SP都很贵, 而且都需要团购, 等待期很长, 国内的也有些不错的, 比如MelGeek. 排骨的, 忆光等等建议看一下这个视频【赤瞳】0-9999元键帽推荐，我全都要！！！ 这次我买ARC死灵, 被这个毒到了 其他 偏口钳(卫星轴剪脚) 螺丝刀(建议带磁头, 我买的这个没有..) 润滑脂(杜邦的润轴, 22058润卫星轴钢丝) 镊子 笔刷(刷润滑脂) 数据线 特氟龙胶带(或用创口贴替代) 安装因为是热插拔, 安装很简单, 我感觉都没必要写了.., 参考这个视频吧, 只不过他没用定位板(貌似成了一个链接汇总文章了..😅)【键盘客制化】组装一个GD64左移配列热插拔RGB光污染机械键盘再看一下如何调教大键, 主要就是剪脚和润焕, 看视频即可从此大键不再肉！机械键盘卫星轴调校教程机械键盘垃圾大键的恶心肉感和抓狂钢丝声教你解决它！润滑轴体少年，润滑一下吗？机械键盘轴体润滑指南 经验教训1.我的pcb买的是GK64, GK64没有给左shift开pcb卫星轴的孔(2u shift是三个菊花的), 所以如果买的是这种定位板就没法上卫星轴了.要买这种定位板(用定位板卫星轴) 带卫星轴定位板 xd64 / dz60/64 / 貌似都开了孔, 但不是热插拔GD64 也开了孔, 但没有现货, 需要等上车, 且也不支持阶梯capslock dz64貌似右shift不能分裂 不过插上键帽试了下也还好, 就这样吧 北瓜的定位板(带卫星轴, 卖家说支持lc60, Tofu外壳, 我没试过) 2.GK64只支持普通capslock, 如果想要上面死灵那个阶梯capslock, xd64 / dz60/64 都可以. 学会看这个图 定位板还用这个就行caslock开口可以向左移动的, 主要看pcb支持不支持了便宜的不锈钢非左移定位板(支持xd/dz 64) 3.GK64热插拔长度 29.5厘米 放不进去LC60……后来又买了一块ymdk64(支持阶梯capslock) 长度29.2厘米, 就能放进去了.. 就差3毫米 最后的成品ToFu + 蒂芙尼轴 Typing sound 购物清单 蒂芙尼轴 Tofu 60%外壳-电泳白 忆光GK64RGB机械键盘热插拔PCB主板声控主板 GH60机械键盘DZ60铜定位板分裂2U shift 60%布局64配列 GMK螺丝卫星轴 ARC死灵键帽 偏口钳 世达工具sata pen 杜邦GPL105,205混合半流体润滑脂 Permatex太阳牌22058油脂 镊子 笔刷 数据线 特氟龙胶带 与本次无关的其他 上油板 开轴器 无铅99%焊锡 吸锡器 电烙铁 烙铁架 防烫垫","categories":[],"tags":[{"name":"生活","slug":"生活","permalink":"http://fuxkdb.com/tags/%E7%94%9F%E6%B4%BB/"},{"name":"兴趣爱好","slug":"兴趣爱好","permalink":"http://fuxkdb.com/tags/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/"}]},{"title":"MySQL Error log监控","slug":"2019-07-21-MySQL-Error-log监控","date":"2019-07-21T01:59:00.000Z","updated":"2019-07-23T10:40:47.000Z","comments":true,"path":"2019/07/21/2019-07-21-MySQL-Error-log监控/","link":"","permalink":"http://fuxkdb.com/2019/07/21/2019-07-21-MySQL-Error-log%E7%9B%91%E6%8E%A7/","excerpt":"MySQL Error log监控告警日志监控其实比较简单了, 怎么做都行. 目前我们这里是使用下面的方法FileBeat采集日志 -&gt; Kafka -&gt; 自己写脚本消费出来 -&gt; 企业微信机器人告警效果如下","text":"MySQL Error log监控告警日志监控其实比较简单了, 怎么做都行. 目前我们这里是使用下面的方法FileBeat采集日志 -&gt; Kafka -&gt; 自己写脚本消费出来 -&gt; 企业微信机器人告警效果如下 这里给出FileBeat配置, 很简单12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@node002142 filebeat-7.2.0-dba]# cat filebeat.ymllogging: level: warning json: truefilebeat.config.inputs: enabled: true path: configs/*.yml reload.enabled: true reload.period: 10shttp: enabled: true host: &quot;0.0.0.0&quot; port: 5066processors: - drop_fields: fields: [&quot;beat.name&quot;, &quot;beat.version&quot;, &quot;input_type&quot;, &quot;offset&quot;] - add_host_metadata: netinfo.enabled: trueoutput.kafka: hosts: [&quot;192.168.x.xx:9092&quot;,&quot;192.168.x.xx:9092&quot;,&quot;192.168.x.xx:9092&quot;] topic: &#x27;%&#123;[kafka_topic]&#125;&#x27; partition.round_robin: reachable_only: true required_acks: 1 max_message_bytes: 8388608 compression: gzip bulk_max_size: 2048 worker: 6 keep_alive: 600 channel_buffer_size: 2560 version: 2.0.0 --filebeat7.2才支持最新版本的kafka, 虽然我们的kafka是2.1.2 但是这里也得写2.0.0[root@node002142 configs]# lltotal 8-rw-r--r-- 1 root root 247 Jul 15 18:40 mysql_error_log.yml[root@node002142 filebeat-7.2.0-dba]# cat configs/mysql_error_log.yml- type: log paths: - /data/mysql_*/logs/*.err fields: type: mysql_error_log format: plain kafka_topic: log_mysql_error_log fields_under_root: true max_backoff: 3s写入Kafka的消息如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; &quot;@timestamp&quot;: &quot;2019-07-21T01:43:22.091Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.2.0&quot;, &quot;topic&quot;: &quot;log_mysql_error_log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;kafka_topic&quot;: &quot;log_mysql_error_log&quot;, &quot;type&quot;: &quot;mysql_error_log&quot;, &quot;format&quot;: &quot;plain&quot;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;node00xxx&quot;, &quot;id&quot;: &quot;ea3afe477be14c22abf234dd3cb80f55&quot;, &quot;containerized&quot;: false, &quot;ip&quot;: [&quot;10.1.x.xx, &quot;fe80::xx:xx:xx&quot;, &quot;192.168.x.xx&quot;, &quot;fe80::xx:xx:xx&quot;], &quot;mac&quot;: [&quot;80:18:xx:xx:xx&quot;, &quot;80:18:xx:xx:xx&quot;, &quot;80:18:44:xx:xx:xx&quot;, &quot;80:18:44:xx:xx:xx&quot;], &quot;hostname&quot;: &quot;node002111&quot;, &quot;architecture&quot;: &quot;x86_64&quot;, &quot;os&quot;: &#123; &quot;platform&quot;: &quot;centos&quot;, &quot;version&quot;: &quot;7 (Core)&quot;, &quot;family&quot;: &quot;redhat&quot;, &quot;name&quot;: &quot;CentOS Linux&quot;, &quot;kernel&quot;: &quot;4.15.9-1.el7.elrepo.x86_64&quot;, &quot;codename&quot;: &quot;Core&quot; &#125; &#125;, &quot;agent&quot;: &#123; &quot;ephemeral_id&quot;: &quot;c97d819c-c456-42d0-xxxx-xxxxxxxxx&quot;, &quot;hostname&quot;: &quot;node00xxx&quot;, &quot;id&quot;: &quot;359f6ddb-de27-42c9-9ce1-1624205d6af0&quot;, &quot;version&quot;: &quot;7.2.0&quot;, &quot;type&quot;: &quot;filebeat&quot; &#125;, &quot;log&quot;: &#123; &quot;offset&quot;: 2585741, &quot;file&quot;: &#123; &quot;path&quot;: &quot;/data/mysql_3306/logs/node00xxxx.err&quot; &#125; &#125;, &quot;message&quot;: &quot;2019-07-21T01:43:21.585528Z 4345026 [Note] Access denied for user &#x27;user&#x27;@&#x27;192.168.x.xx&#x27; (using password: YES)&quot;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.0.0&quot; &#125;&#125;Python脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195# -*- coding: utf8 -*-# __author__ = &#x27;Fan()&#x27;# Date: 2019-07-18import timeimport jsonimport pytzimport requestsimport datetimeimport loggingfrom utils.conn_db import Fandbfrom utils.config import *from confluent_kafka import Consumer, KafkaError, TopicPartition, OFFSET_END, OFFSET_BEGINNING, Producerclass MyRequest(): @staticmethod def get(url, params=None, timeout=(2, 5)): response = requests.get(url=url, params=params, timeout=timeout) if response.status_code == requests.codes.ok: return response.json() else: response.raise_for_status() @staticmethod def post(url, data=None, json=None, timeout=(5)): response = requests.post(url=url, data=data, json=json, timeout=timeout) if response.status_code == requests.codes.ok: return response.json() else: response.raise_for_status()def confLog(logfile): logging.basicConfig(level=logging.INFO, format=&#x27;%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s&#x27;, datefmt=&#x27;%Y-%m-%d %H:%M:%S&#x27;, filename=logfile, filemode=&#x27;a&#x27;)def _on_send_response(err, partations): pt = partations[0] if isinstance(err, KafkaError): print(&#x27;Topic &#123;&#125; 偏移量 &#123;&#125; 提交异常. &#123;&#125;&#x27;.format(pt.topic, pt.offset, err)) logging.error(&#x27;Topic &#123;&#125; 偏移量 &#123;&#125; 提交异常. &#123;&#125;&#x27;.format(pt.topic, pt.offset, err)) # raise Exception(err)def getConsumer(topic_name, bootstrap_servers, offset_end=True): config = &#123;&#x27;bootstrap.servers&#x27;: bootstrap_servers, &quot;group.id&quot;: topic_name, &#x27;enable.auto.commit&#x27;: True, &quot;fetch.wait.max.ms&quot;: 3000, &quot;max.poll.interval.ms&quot;: 60000, &#x27;session.timeout.ms&#x27;: 60000, &quot;on_commit&quot;: _on_send_response, &quot;default.topic.config&quot;: &#123;&quot;auto.offset.reset&quot;: &quot;latest&quot;&#125;&#125; consumer = Consumer(config) offset = OFFSET_END if offset_end else OFFSET_BEGINNING pt = TopicPartition(topic_name, 0, offset) # 动态获取 一级kafka的 topic consumer.assign([pt]) # consumer.seek(pt) try: while True: ret = consumer.consume(num_messages=10, timeout=0.1) if ret is None: print(&quot;No message Continue!&quot;) continue for msg in ret: if msg.error() is None: # print(&quot;Received message:&#123;&#125;&quot;.format(msg.value().decode(&quot;utf-8&quot;))) yield msg.value().decode(&quot;utf-8&quot;) elif msg.error(): if msg.error().code() == KafkaError._PARTITION_EOF: continue else: raise Exception(msg.error()) except Exception as e: print(e) consumer.close() except KeyboardInterrupt: consumer.close()def utc_to_local(utc_time_str, utc_format=&#x27;%Y-%m-%dT%H:%M:%S.%fZ&#x27;): local_tz = pytz.timezone(&#x27;Asia/Chongqing&#x27;) local_format = &quot;%Y-%m-%d %H:%M:%S&quot; utc_dt = datetime.datetime.strptime(utc_time_str, utc_format) local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz) time_str = local_dt.strftime(local_format) return time_strdef get_mysql_ip(ips, hostname): last_ip = str(int(hostname[-3:])) for i in ips: x = i.split(&#x27;.&#x27;) if len(x) == 4: # 过滤掉本机vip if x[3] == last_ip and x[2] not in (&#x27;3&#x27;, &#x27;8&#x27;, &#x27;16&#x27;) and x[0] == &#x27;192&#x27;: return idef get_mysql_port(error_log_file): spliter = &#x27;mysql_&#x27; mysql_port = error_log_file.split(spliter)[1].split(&#x27;/&#x27;)[0] if mysql_port == &#x27;&#x27;: mysql_port = 3306 return mysql_portdef sendWechatBot(send_message): web_hoot_address = &#x27;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=你的机器人key&#x27; body = &#123; &quot;msgtype&quot;: &quot;markdown&quot;, &quot;markdown&quot;: &#123; &quot;content&quot;: &#x27;&#x27;&#x27;&lt;font color=\\&quot;warning\\&quot;&gt;捕获告警日志报错信息:&lt;/font&gt;\\n&gt;产品线:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;product_name&#125;&lt;/font&gt;&gt;项目名称:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;project_name&#125;&lt;/font&gt;&gt;高可用组:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;ha_group_name&#125;&lt;/font&gt;&gt;IP:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;ip_app&#125;&lt;/font&gt;&gt;PORT:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;port&#125;&lt;/font&gt;&gt;节点角色:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;role_name&#125;&lt;/font&gt;&gt;告警时间:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;error_timestamp&#125;&lt;/font&gt;&gt;捕获时间:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;catch_timestamp&#125;&lt;/font&gt;&gt;下发时间:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;send_timestamp&#125;&lt;/font&gt;告警内容:&lt;font color=\\&quot;comment\\&quot;&gt;&#123;error_message&#125;&lt;/font&gt;\\n&#x27;&#x27;&#x27;.format(**send_message) &#125; &#125; data = MyRequest.post(web_hoot_address, json=body) return datadef get_mysql_info(mysql_ip, mysql_port): conn = Fandb(cmdb_host, cmdb_port, cmdb_user, cmdb_pass, cmdb_schema, dic=True) sql = &#x27;一个根据ip端口查询数据库实例想关信息的SQL&#x27; res = conn.dql(sql) conn.close() return res[0]if __name__ == &#x27;__main__&#x27;: verbose = 1 logfile = &#x27;/tools/mysql_error_log_watchdog.log&#x27; confLog(logfile) topic = &#x27;log_mysql_error_log&#x27; bootstrap_servers = &quot;192.168.x.xxx:9092,192.168.x.xxx:9092,192.168.x.xxx:9092&quot; consumer = getConsumer(topic, bootstrap_servers) for message in consumer: message_dict = json.loads(message) if verbose &gt;= 3: print(message_dict) catch_timestamp = utc_to_local(message_dict[&#x27;@timestamp&#x27;]) error_log_file = message_dict[&#x27;log&#x27;][&#x27;file&#x27;][&#x27;path&#x27;] hostname = message_dict[&#x27;host&#x27;][&#x27;hostname&#x27;] ips = message_dict[&#x27;host&#x27;][&#x27;ip&#x27;] mysql_ip = get_mysql_ip(ips, hostname) mysql_port = get_mysql_port(error_log_file) error_log_message = message_dict[&#x27;message&#x27;] if &#x27;[ERROR]&#x27; in error_log_message: try: catch_timestamp = utc_to_local(message_dict[&#x27;@timestamp&#x27;]) error_log_file = message_dict[&#x27;log&#x27;][&#x27;file&#x27;][&#x27;path&#x27;] hostname = message_dict[&#x27;host&#x27;][&#x27;hostname&#x27;] ips = message_dict[&#x27;host&#x27;][&#x27;ip&#x27;] mysql_ip = get_mysql_ip(ips, hostname) mysql_port = get_mysql_port(error_log_file) error_timestamp = utc_to_local(error_log_message.split(&#x27; &#x27;)[0]) error_message = &#x27; &#x27;.join(message_dict[&#x27;message&#x27;].split(&#x27; &#x27;)[2:]) send_message_dict = get_mysql_info(mysql_ip, mysql_port) send_message_dict[&#x27;catch_timestamp&#x27;] = catch_timestamp send_message_dict[&#x27;error_timestamp&#x27;] = error_timestamp send_message_dict[&#x27;send_timestamp&#x27;] = datetime.datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) send_message_dict[&#x27;error_message&#x27;] = error_message logging.info(send_message_dict) sendWechatBot(send_message_dict) time.sleep(3) #企业微信机器人限制3秒一条 except Exception as e: logging.exception(e) logging.exception(error_log_message) 如果觉得上面的太麻烦,其实一个shell脚本也可以搞, 就是得在每个机器部署运行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/bin/bash# Oracle警告日志文件监控脚本# 2015/4/14 King.# 发送邮件sendMail()&#123; echo &quot;$1&quot; mailTo=$(echo $mailTo | sed &#x27;s/,/ /g&#x27;) echo &quot;$1&quot; | /usr/bin/mutt -s &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) 警告日志错误&quot; -b $&#123;mailTo&#125; -c $&#123;mailCc&#125;&#125;# 判断错误信息中是否有未扫描的行checkHis()&#123; tag=0 hisLine=$(cat $errLineNumFile) for i in $hisLine do if [ &quot;$1&quot; == &quot;$i&quot; ]; then tag=1 fi done return $tag &#125;#scriptDir=`pwd $0`scriptName=`basename $0`logDir=$scriptDir/logslogfile=$logDir/alert_error.logerrLineNumFile=$logDir/.alert_errLineNum#设置警告日日志文件路径alertFilePath=&quot;/data/mysql_3306/logs/nodexx.err&quot;# 设置邮件接收者，多个用逗号分隔mailTo=&quot;xx@163.com&quot;# 设置邮件抄送者，多个用逗号分隔mailCc=&quot;a@163.com,b@163.com&quot;[ ! -f $alertFilePath ] &amp;&amp; echo &quot;[Error]: $alertFilePath no such file or directory.&quot; &amp;&amp; exit 1[ ! -d $logDir ] &amp;&amp; mkdir -p $logDirtouch $errLineNumFileecho &quot;正在监控 $alertFilePath... &quot;while truedo arrayNum=() isError=false # 取出警告日志中 ”ORA?“关键字所在的行 errNum=$(cat $alertFilePath | grep -n -i &quot;[ERROR]]&quot;) n=0 if [ &quot;x$errNum&quot; != &quot;x&quot; ]; then # 取出错误行号 errLineNum=$(echo &quot;$errNum&quot; | awk -F: &#x27;&#123;print $1&#125;&#x27;) for num in $errLineNum do #判断该行错误信息是否已扫描 if [ &quot;x$errLineNum&quot; != &quot;x&quot; ]; then checkHis &quot;$num&quot; if [ $? -eq 0 ]; then # 如果该行错误未扫描，记录该行信息 isError=true echo $num &gt;&gt; $errLineNumFile arrayNum[$n]=$num let n++ fi else #如果没有错误休眠10s后重新扫描 sleep 10s break fi donefi # 如果发现未扫描的错误信息则根据行号取出该行信息记录日志，并且发送邮件 if [ &quot;$isError&quot; == &quot;true&quot; ] then echo &quot;-------------------------------- $(date +&quot;%Y-%m-%d %H:%M:%S&quot;) ---------------------------------------&quot; &gt;&gt; $logfile i=0 errMsg=$( while [ $i -lt $&#123;#arrayNum[@]&#125; ]; do echo &quot;$errNum&quot; | grep &quot;^$&#123;arrayNum[$i]&#125;:&quot; let i++ done) echo &quot;$errMsg&quot; &gt;&gt; $logfile sendMail &quot;$errMsg&quot; fi #每10s，扫描一次警告日志文件 sleep 10sdone","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL监控","slug":"MySQL监控","permalink":"http://fuxkdb.com/tags/MySQL%E7%9B%91%E6%8E%A7/"}]},{"title":"记一次PMM容器被误删除的恢复","slug":"2019-07-20-记一次PMM容器被误删除的恢复","date":"2019-07-20T06:01:00.000Z","updated":"2019-07-20T06:02:08.000Z","comments":true,"path":"2019/07/20/2019-07-20-记一次PMM容器被误删除的恢复/","link":"","permalink":"http://fuxkdb.com/2019/07/20/2019-07-20-%E8%AE%B0%E4%B8%80%E6%AC%A1PMM%E5%AE%B9%E5%99%A8%E8%A2%AB%E8%AF%AF%E5%88%A0%E9%99%A4%E7%9A%84%E6%81%A2%E5%A4%8D/","excerpt":"记一次PMM容器被误删除的恢复昨天PMM Data Container和PMM Server Container 都不小心被误删了, 还好是使用的docker rm container_id删的, 没有加-v. 本人docker渣渣, 后来找运维同时帮忙终于恢复了首先找到宿主机volumes路径1234567891011121314[root@node1 volumes]# pwd/var/lib/docker/volumes[root@node1 volumes]# lltotal 32drwxr-xr-x 3 root root 26 Aug 3 2018 4430beeb61fbc26802dc1b31d9fe3a02772482d8ae31bd50110957f16256f02edrwxr-xr-x 3 root root 26 Jul 19 18:29 47ce712fd5e32abcf60e9650f349ac1304e0c20d062bd54724123f6cb700a79edrwxr-xr-x 3 root root 26 Aug 3 2018 6ecf1c674f9f8e6f3dda7f7da352b70860125147b4489c6a8fe00fa59892436fdrwxr-xr-x 3 root root 26 Aug 3 2018 83db182943136c354bf95c0b4d07a8272017fd1698557d27b97dae49c82a8e76drwxr-xr-x 3 root root 26 Jul 19 18:29 c9e3a964fd76dcad88ac992933ca97ac920c41239f9807f1ca91f939bd71ea03drwxr-xr-x 3 root root 26 Jul 19 18:29 e92a37881c9b378bc4b00094b3f7e38ace3ae3655c33677a79da1be5652a318adrwxr-xr-x 3 root root 26 Aug 3 2018 fcddf90d2cb64035c8c69165f69c5d7a780ac580c991f02af801bc65fa2ca609-rw------- 1 root root 65536 Jul 19 18:29 metadata.dbdrwxr-xr-x 3 root root 26 Jul 19 18:26 sentry-datadrwxr-xr-x 3 root root 26 Jul 19 18:26 sentry-postgres","text":"记一次PMM容器被误删除的恢复昨天PMM Data Container和PMM Server Container 都不小心被误删了, 还好是使用的docker rm container_id删的, 没有加-v. 本人docker渣渣, 后来找运维同时帮忙终于恢复了首先找到宿主机volumes路径1234567891011121314[root@node1 volumes]# pwd/var/lib/docker/volumes[root@node1 volumes]# lltotal 32drwxr-xr-x 3 root root 26 Aug 3 2018 4430beeb61fbc26802dc1b31d9fe3a02772482d8ae31bd50110957f16256f02edrwxr-xr-x 3 root root 26 Jul 19 18:29 47ce712fd5e32abcf60e9650f349ac1304e0c20d062bd54724123f6cb700a79edrwxr-xr-x 3 root root 26 Aug 3 2018 6ecf1c674f9f8e6f3dda7f7da352b70860125147b4489c6a8fe00fa59892436fdrwxr-xr-x 3 root root 26 Aug 3 2018 83db182943136c354bf95c0b4d07a8272017fd1698557d27b97dae49c82a8e76drwxr-xr-x 3 root root 26 Jul 19 18:29 c9e3a964fd76dcad88ac992933ca97ac920c41239f9807f1ca91f939bd71ea03drwxr-xr-x 3 root root 26 Jul 19 18:29 e92a37881c9b378bc4b00094b3f7e38ace3ae3655c33677a79da1be5652a318adrwxr-xr-x 3 root root 26 Aug 3 2018 fcddf90d2cb64035c8c69165f69c5d7a780ac580c991f02af801bc65fa2ca609-rw------- 1 root root 65536 Jul 19 18:29 metadata.dbdrwxr-xr-x 3 root root 26 Jul 19 18:26 sentry-datadrwxr-xr-x 3 root root 26 Jul 19 18:26 sentry-postgres 找到对应的目录1234567891011121314151617181920prometheus[root@node1 volumes]# ls /var/lib/docker/volumes/6ecf1c674f9f8e6f3dda7f7da352b70860125147b4489c6a8fe00fa59892436f/_data00 06 0c 12 18 1e 24 2a 30 36 3c 42 48 4e 54 5a 60 66 6c 72 78 7e 84 8a 90 96 9c a2 a8 ae b2 b8 be c4 ca d0 d6 dc e1 e7 ed f3 f9 ff01 07 0d 13 19 1f 25 2b 31 37 3d 43 49 4f 55 5b 61 67 6d 73 79 7f 85 8b 91 97 9d a3 a9 af b3 b9 bf c5 cb d1 d7 dd e2 e8 ee f4 fa heads.db02 08 0e 14 1a 20 26 2c 32 38 3e 44 4a 50 56 5c 62 68 6e 74 7a 80 86 8c 92 98 9e a4 aa archived_fingerprint_to_metric b4 ba c0 c6 cc d2 d8 de e3 e9 ef f5 fb labelname_to_labelvalues03 09 0f 15 1b 21 27 2d 33 39 3f 45 4b 51 57 5d 63 69 6f 75 7b 81 87 8d 93 99 9f a5 ab archived_fingerprint_to_timerange b5 bb c1 c7 cd d3 d9 df e4 ea f0 f6 fc labelpair_to_fingerprints04 0a 10 16 1c 22 28 2e 34 3a 40 46 4c 52 58 5e 64 6a 70 76 7c 82 88 8e 94 9a a0 a6 ac b0 b6 bc c2 c8 ce d4 da DIRTY e5 eb f1 f7 fd mappings.db05 0b 11 17 1d 23 29 2f 35 3b 41 47 4d 53 59 5f 65 6b 71 77 7d 83 89 8f 95 9b a1 a7 ad b1 b7 bd c3 c9 cf d5 db e0 e6 ec f2 f8 fe VERSIONconsul[root@node1 volumes]# ls /var/lib/docker/volumes/fcddf90d2cb64035c8c69165f69c5d7a780ac580c991f02af801bc65fa2ca609/_datacheckpoint-signature node-id raft serfmysql[root@node1 volumes]# ls /var/lib/docker/volumes/4430beeb61fbc26802dc1b31d9fe3a02772482d8ae31bd50110957f16256f02e/_dataibdata1 ib_logfile0 ib_logfile1 mysql mysql.sock orchestrator performance_schema pmm pmm@002dmanaged RPM_UPGRADE_HISTORY RPM_UPGRADE_MARKER-LAST testgrafana[root@node1 volumes]# ls /var/lib/docker/volumes/83db182943136c354bf95c0b4d07a8272017fd1698557d27b97dae49c82a8e76/_datagrafana.db PERCONA_DASHBOARDS_VERSION plugins png sessions 重新创建data container123456docker create --name pmm-data2 \\-v /var/lib/docker/volumes/6ecf1c674f9f8e6f3dda7f7da352b70860125147b4489c6a8fe00fa59892436f/_data:/opt/prometheus/data \\-v /var/lib/docker/volumes/fcddf90d2cb64035c8c69165f69c5d7a780ac580c991f02af801bc65fa2ca609/_data:/opt/consul-data \\-v /var/lib/docker/volumes/4430beeb61fbc26802dc1b31d9fe3a02772482d8ae31bd50110957f16256f02e/_data:/var/lib/mysql \\-v /var/lib/docker/volumes/83db182943136c354bf95c0b4d07a8272017fd1698557d27b97dae49c82a8e76/_data:/var/lib/grafana \\percona/pmm-server:latest /bin/ture重新创建server1234567891011docker run -d \\ -p 8080:80 \\ --volumes-from pmm-data2 \\ --name pmm-server \\ -e SERVER_USER=username \\ -e SERVER_PASSWORD=password \\ -e METRICS_RETENTION=720h \\ -e METRICS_RESOLUTION=5s \\ -e DISABLE_TELEMETRY=true \\ --restart always \\ percona/pmm-server:latest 之后PMM页面就可以访问了, 不过如果你做了一些额外配置, 比如我再prometheus.yml中加了canal监控, 那就需要重新添加了.","categories":[],"tags":[{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"MySQL慢查询平台架构方案","slug":"2019-07-20-MySQL慢查询平台架构方案","date":"2019-07-20T05:23:00.000Z","updated":"2019-07-20T05:34:45.000Z","comments":true,"path":"2019/07/20/2019-07-20-MySQL慢查询平台架构方案/","link":"","permalink":"http://fuxkdb.com/2019/07/20/2019-07-20-MySQL%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/","excerpt":"MySQL慢查询平台架构方案方案1 Filebeat读取slow log 到kafka , logstash从kafka消费后解析成出各个列, 然后写入MySQL, 但是这样的问题是查询语句是这样的select * from A where id=1. 而我们需要去除谓词的SQL, 也就是select * from A where id=? 这样的, 这样才好对SQL进行聚合分析于是用Canal拉binlog, 再写到kafka, 然后python消费, 获取id, sql文本, 使用pt-fingerprint去除谓词后再更新回去. 这个架构看起来蛮高大上 想了想还是复杂了写, 环节太些, 容易出问题. 优点: 慢查询准实时获取(但可能是伪需求) 缺点: 架构复制易出错, MySQL版本升级如果slow log格式发生变化, 维护logstash的grok会很麻烦 有人说为什么不用Filebeat ingest直接到ES, 这要图标用Kibana就好了. 我的考虑是:1.还要做谓词去除2.这样做出的图没法和我们业务的服务数关联, 业务人员查询就必须知道数据库的IP端口详细方案可以参考下面的文档 https://www.jianshu.com/p/f3be9cce9b77https://www.jianshu.com/p/3cf0e2a8d23d","text":"MySQL慢查询平台架构方案方案1 Filebeat读取slow log 到kafka , logstash从kafka消费后解析成出各个列, 然后写入MySQL, 但是这样的问题是查询语句是这样的select * from A where id=1. 而我们需要去除谓词的SQL, 也就是select * from A where id=? 这样的, 这样才好对SQL进行聚合分析于是用Canal拉binlog, 再写到kafka, 然后python消费, 获取id, sql文本, 使用pt-fingerprint去除谓词后再更新回去. 这个架构看起来蛮高大上 想了想还是复杂了写, 环节太些, 容易出问题. 优点: 慢查询准实时获取(但可能是伪需求) 缺点: 架构复制易出错, MySQL版本升级如果slow log格式发生变化, 维护logstash的grok会很麻烦 有人说为什么不用Filebeat ingest直接到ES, 这要图标用Kibana就好了. 我的考虑是:1.还要做谓词去除2.这样做出的图没法和我们业务的服务数关联, 业务人员查询就必须知道数据库的IP端口详细方案可以参考下面的文档 https://www.jianshu.com/p/f3be9cce9b77https://www.jianshu.com/p/3cf0e2a8d23d 方案2 这个方案关键点在于将log_output参数设置为&#39;FILE,TABLE&#39;, 那么MySQL会在mysql.slow_log表中记录慢查询信息 而这不是正是方案1中我们费了半天劲用logstash想要达到的结果吗? 而mysql.slow_log又恰好是一个csv引擎的表 根据官方文档描述, csv这个格式可以读,甚至可以写. 那它其实就是个文本啊, 我们用Filebeat读它呗. 这个架构构思巧妙, 我都佩服我自己了, 但是为啥业界没人用呢? 优点: 架构简单易实现, 日志实时获取 缺点: 需要测试,为啥没人用?可能有坑 方案3老路子 很简单的架构, 对技术水平要求很低, 初级DBA就可以实现的方案原理是在每个MySQL服务器部署定时任务, 执行pt-query-digest1/bin/pt-query-digest --user=slow_query_w --password=$FUCK --review h=192.168.2.142,D=db_slow_query,t=t_$&#123;ha_group_name&#125;_query_review --history h=192.168.2.142,D=db_slow_query,t=t_$&#123;ha_group_name&#125;_query_review_history --no-report --limit=0% --filter=&quot; \\$event-&gt;&#123;Bytes&#125; = length(\\$event-&gt;&#123;arg&#125;) and \\$event-&gt;&#123;hostname&#125;=\\&quot;$HOSTNAME\\&quot;&quot; $slow_log-`date +%Y%m%d`pt-query-digest 会分析slow log, 将结果插入数据库这个结果就是就包含了去除谓词的SQL 这个方案其实是比较老也比较简单经典的方案, Anemometer, Query-Digest-UI都是这么做的. 最开始讨论方案时, 其实我个人是由于不想重复老路子才没选这个方案 优点: 架构简单易于实现与维护 缺点: 实时性较差, 逼格低","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL监控","slug":"MySQL监控","permalink":"http://fuxkdb.com/tags/MySQL%E7%9B%91%E6%8E%A7/"}]},{"title":"记录一些有用的pt-query-digest filter","slug":"2019-07-03-记录一些有用的pt-query-digest-filter","date":"2019-07-03T15:03:33.000Z","updated":"2019-07-03T15:10:45.000Z","comments":true,"path":"2019/07/03/2019-07-03-记录一些有用的pt-query-digest-filter/","link":"","permalink":"http://fuxkdb.com/2019/07/03/2019-07-03-%E8%AE%B0%E5%BD%95%E4%B8%80%E4%BA%9B%E6%9C%89%E7%94%A8%E7%9A%84pt-query-digest-filter/","excerpt":"自己写了一些和整理了些filter, 百度是搜不到的哈, 除了最后那俩anemometer的 不输出Database 为 mysql|performance_schema|information_schema|sys|db_monitor 的查询1((\\$event-&gt;&#123;db&#125; || &#x27;&#x27;) =~ m/^(?!(mysql|performance_schema|information_schema|sys|db_monitor))/","text":"自己写了一些和整理了些filter, 百度是搜不到的哈, 除了最后那俩anemometer的 不输出Database 为 mysql|performance_schema|information_schema|sys|db_monitor 的查询1((\\$event-&gt;&#123;db&#125; || &#x27;&#x27;) =~ m/^(?!(mysql|performance_schema|information_schema|sys|db_monitor))/ 不输出 Users 为 pmm|mysqlcheck|dbms_monitor_r 用户的查询1(\\$event-&gt;&#123;user&#125; || &#x27;&#x27;) =~ m/^(?!(pmm|mysqlcheck|dbms_monitor_r|proxysql))/i 不输出cheksum=’64EF0EA126730002088884A136067321’的 也就是throttle: 396 ‘index not used’ warning(s) suppressed.\\G1\\$event-&gt;&#123;fingerprint&#125; &amp;&amp; make_checksum(\\$event-&gt;&#123;fingerprint&#125;) ne &#x27;64EF0EA126730002088884A136067321&#x27; \\$event-&gt;{Bytes} = length(\\$event-&gt;{arg}) 和 \\$event-&gt;{hostname}=\\”$HOSTNAME\\” 增加 Bytes 和 hostname 内容123456789101112131415161718192021222324252627282930313233343536373839# Query 19: 0 QPS, 0x concurrency, ID 0xE3B736208C9A5A96A9D7E7DFF3BEF268 at byte 2298558# This item is included in the report because it matches --limit.# Scores: V/M = 0.00# Time range: all events occurred at 2019-07-03T03:09:30# Attribute pct total min max avg 95% stddev median# ============ === ======= ======= ======= ======= ======= ======= =======# Count 0 1# Exec time 0 448us 448us 448us 448us 448us 0 448us# Lock time 0 132us 132us 132us 132us 132us 0 132us# Rows sent 0 10 10 10 10 10 0 10# Rows examine 0 172 172 172 172 172 0 172# Query size 0 117 117 117 117 117 0 117# Bytes 0 117 117 117 117 117 0 117 --增加的# String:# Databases mydb# Hosts 192.168.x.110# hostname node00xxx --增加的# Users xxx# Query_time distribution...对比没有使用# Query 5: 0.00 QPS, 0.00x concurrency, ID 0x39226EA9FD5344CDD6F89A75B34D97F3 at byte 2168772# Scores: V/M = 0.00# Time range: 2019-07-02T22:54:11 to 2019-07-03T14:58:30# Attribute pct total min max avg 95% stddev median# ============ === ======= ======= ======= ======= ======= ======= =======# Count 0 157# Exec time 1 11s 60ms 94ms 70ms 75ms 5ms 68ms# Lock time 0 15ms 48us 321us 95us 167us 42us 84us# Rows sent 0 157 1 1 1 1 0 1# Rows examine 31 29.50M 192.11k 192.78k 192.42k 192.13k 339 192.13k# Query size 0 12.42k 80 81 80.99 80.10 0.43 80.10# String:# Databases mydb# Hosts 192.168.x.50 (90/57%), 192.168.x.211 (67/42%)# Users xxx# Query_time distribution... 最终1/bin/pt-query-digest --limit=0% --filter=&quot; \\$event-&gt;&#123;Bytes&#125; = length(\\$event-&gt;&#123;arg&#125;) and \\$event-&gt;&#123;hostname&#125;=\\&quot;$HOSTNAME\\&quot; and ((\\$event-&gt;&#123;db&#125; || &#x27;&#x27;) =~ m/^(?!(mysql|performance_schema|information_schema|sys|db_monitor))/ and (\\$event-&gt;&#123;user&#125; || &#x27;&#x27;) =~ m/^(?!(pmm|mysqlcheck|dbms_monitor_r|proxysql))/i and \\$event-&gt;&#123;fingerprint&#125; &amp;&amp; make_checksum(\\$event-&gt;&#123;fingerprint&#125;) ne &#x27;64EF0EA126730002088884A136067321&#x27; )&quot; slow-queries.log不过上面的命令不能直接在终端执行, 只能写到shell脚本里再去执行脚本","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Percona Toolkit","slug":"Percona-Toolkit","permalink":"http://fuxkdb.com/tags/Percona-Toolkit/"}]},{"title":"MySQL大表传输表空间的坑","slug":"2019-06-22-MySQL大表传输表空间的坑","date":"2019-06-22T11:23:00.000Z","updated":"2020-12-08T02:26:13.721Z","comments":true,"path":"2019/06/22/2019-06-22-MySQL大表传输表空间的坑/","link":"","permalink":"http://fuxkdb.com/2019/06/22/2019-06-22-MySQL%E5%A4%A7%E8%A1%A8%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4%E7%9A%84%E5%9D%91/","excerpt":"MySQL大表传输表空间的坑 最近刚帮业务线拆分完数据库, 源环境遗留了一张700G的大表, 虽说现在不用了, 但是业务方还是不希望删掉, 于是打算把这张表迁移到归档库, 这样有需要是还可以查询. 700G的表想了想, 如果是逻辑导出再导入的话, 感觉会很慢. 于是决定使用传输表空间方式恢复到归档库中 源库和归档库均为MGR集群 Multi-Primary Mode. 恢复备份过程简单写, 并非本文重点12345xbstream -vx --parallel=10 &lt; mysql_backup_20190618.xbstream -C /rundata/backup/ &gt; /tmp/xb.log 2&gt;&amp;1 &amp;cd /rundata/backup/datadirrm 除这张700g表A的所有其他表的qp文件xtrabackup --decompress --parallel=10 --remove-original --target-dir=/rundata/backup/innobackupex --apply-log --export /rundata/backup/","text":"MySQL大表传输表空间的坑 最近刚帮业务线拆分完数据库, 源环境遗留了一张700G的大表, 虽说现在不用了, 但是业务方还是不希望删掉, 于是打算把这张表迁移到归档库, 这样有需要是还可以查询. 700G的表想了想, 如果是逻辑导出再导入的话, 感觉会很慢. 于是决定使用传输表空间方式恢复到归档库中 源库和归档库均为MGR集群 Multi-Primary Mode. 恢复备份过程简单写, 并非本文重点12345xbstream -vx --parallel=10 &lt; mysql_backup_20190618.xbstream -C /rundata/backup/ &gt; /tmp/xb.log 2&gt;&amp;1 &amp;cd /rundata/backup/datadirrm 除这张700g表A的所有其他表的qp文件xtrabackup --decompress --parallel=10 --remove-original --target-dir=/rundata/backup/innobackupex --apply-log --export /rundata/backup/ 开始传输表空间1.在归档库创建该表1mysql&gt; create table A3.在归档库写节点执行1mysql&gt; alter table A discard tablespace;3.将上一步产生的 A.ibd A.cfg 传输到归档库MGR集群每一个节点的datadir目录下12chown mysql:mysql A.*mysql&gt; alter table A import tablespace; 拷贝到所有节点的原因详见 主从传输表空间的坑 此时发现import tablespace卡住了State为System lock, 查看processlist1234567891011121314151617+----------+----------------+----------------------+----------+-------------+----------+---------------------------------------------------------------+-------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----------+----------------+----------------------+----------+-------------+----------+---------------------------------------------------------------+-------------------------------------------------------+| 370 | system user | | NULL | Connect | 26094679 | executing | NULL || 373 | system user | | NULL | Connect | 3703683 | Slave has read all relay log; waiting for more updates | NULL || 374 | system user | | NULL | Connect | 26094679 | Waiting for an event from Coordinator | NULL || 375 | system user | | NULL | Connect | 19086256 | Waiting for an event from Coordinator | NULL || 376 | system user | | NULL | Connect | 19086744 | Waiting for an event from Coordinator | NULL || 377 | system user | | NULL | Connect | 19086744 | Waiting for an event from Coordinator | NULL || 378 | system user | | NULL | Connect | 19086744 | Waiting for an event from Coordinator | NULL || 379 | system user | | NULL | Connect | 19086894 | Waiting for an event from Coordinator | NULL || 380 | system user | | NULL | Connect | 19086894 | Waiting for an event from Coordinator | NULL || 381 | system user | | NULL | Connect | 19086903 | Waiting for an event from Coordinator | NULL || 33199176 | dbms_monitor_r | 192.168.X.81:38804 | NULL | Binlog Dump | 7696834 | Master has sent all binlog to slave; waiting for more updates | NULL || 58758525 | root | localhost | archive | Query | 1598 | System lock | ALTER TABLE A IMPORT TABLESPACE || 58764027 | root | localhost | NULL | Query | 0 | starting | show processlist |+----------+----------------+----------------------+----------+-------------+----------+---------------------------------------------------------------+-------------------------------------------------------+查看error log, 发现在阶段1 Update all pages122019-06-22T06:34:14.173468Z 58758525 [Note] InnoDB: Importing tablespace for table &#x27;origin_db/A&#x27; that was exported from host &#x27;Hostname unknown&#x27;2019-06-22T06:34:14.173672Z 58758525 [Note] InnoDB: Phase I - Update all pages又等了一会, 发现监控任务执行的SQL被阻塞了.12345678910111213141516171819+----------+----------------+----------------------+--------------------+-------------+----------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----------+----------------+----------------------+--------------------+-------------+----------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+| 370 | system user | | NULL | Connect | 26094907 | executing | NULL || 373 | system user | | NULL | Connect | 3703911 | Slave has read all relay log; waiting for more updates | NULL || 374 | system user | | NULL | Connect | 26094907 | Waiting for an event from Coordinator | NULL || 375 | system user | | NULL | Connect | 19086484 | Waiting for an event from Coordinator | NULL || 376 | system user | | NULL | Connect | 19086972 | Waiting for an event from Coordinator | NULL || 377 | system user | | NULL | Connect | 19086972 | Waiting for an event from Coordinator | NULL || 378 | system user | | NULL | Connect | 19086972 | Waiting for an event from Coordinator | NULL || 379 | system user | | NULL | Connect | 19087122 | Waiting for an event from Coordinator | NULL || 380 | system user | | NULL | Connect | 19087122 | Waiting for an event from Coordinator | NULL || 381 | system user | | NULL | Connect | 19087131 | Waiting for an event from Coordinator | NULL || 33199176 | dbms_monitor_r | 192.168.X.81:38804 | NULL | Binlog Dump | 7697062 | Master has sent all binlog to slave; waiting for more updates | NULL || 58758525 | root | localhost | archive | Query | 1826 | System lock | ALTER TABLE A IMPORT TABLESPACE || 58764075 | dbms_monitor_r | 192.168.X.142:41144 | information_schema | Query | 214 | Waiting for table metadata lock | select * from tables || 58764750 | dbms_monitor_r | 192.168.X.142:25374 | mysql | Query | 18 | Waiting for table metadata lock | select * from information_schema.tables where table_schema not in (&#x27;mysql&#x27;,&#x27;information_schema&#x27;,&#x27;sys || 58764840 | root | localhost | NULL | Query | 0 | starting | show processlist |+----------+----------------+----------------------+--------------------+-------------+----------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ 查看innodb status12345678910111213141516171819[2019-06-22 14:35:11]root@localhost 14:35:02 [(none)]&gt; show engine innodb status\\G[2019-06-22 14:35:11]------------[2019-06-22 14:35:11]TRANSACTIONS[2019-06-22 14:35:11]------------[2019-06-22 14:35:11]Trx id counter 3747414252[2019-06-22 14:35:11]Purge done for trx&#x27;s n:o &lt; 3747414158 undo n:o &lt; 0 state: running but idle[2019-06-22 14:35:11]History list length 1[2019-06-22 14:35:11]LIST OF TRANSACTIONS FOR EACH SESSION:[2019-06-22 14:35:11]---TRANSACTION 422073426655200, not started...[2019-06-22 14:35:11]0 lock struct(s), heap size 1136, 0 row lock(s)[2019-06-22 14:35:11]MySQL thread id 58758525, OS thread handle 140583989245696, query id 6550373059 localhost root System lock[2019-06-22 14:35:11]ALTER TABLE sales_sku_operation_log IMPORT TABLESPACE[2019-06-22 14:35:11]---TRANSACTION 3747414146, ACTIVE 57 sec importing tablespace[2019-06-22 14:35:11]mysql tables in use 1, locked 1[2019-06-22 14:35:11]1 lock struct(s), heap size 1136, 0 row lock(s)[2019-06-22 14:35:11]MySQL thread id 58758525, OS thread handle 140583989245696, query id 6550373059 localhost root System lock[2019-06-22 14:35:11]ALTER TABLE sales_sku_operation_log IMPORT TABLESPACE虽然innodb status我已经基本不会看了, 但是从TRANSACTIONS这里也可以看到IMPORT TABLESPACE语句并没有被什么锁阻塞 当时想不明白为啥import tablespace会这么慢, 以为被MGR复制的相关线程阻塞了,或者有什么bug之类的, 怕影响业务(虽说是归档库). 于是我kill了这个import tablespace语句,1kill 58758525;在这之后如果想重新执行ALTER TABLE A IMPORT TABLESPACE会报错12root@localhost 15:11:53 [mafengwo]&gt; ALTER TABLE sales_sku_operation_log IMPORT TABLESPACE;ERROR 1815 (HY000): Internal error: Cannot reset LSNs in table `mafengwo`.`sales_sku_operation_log` : Data structure corruption观察集群三个节点的文件状态发现, 执行ALTER TABLE A IMPORT TABLESPACE语句的节点的A.ibd文件修改时间变了(而其他节点没有变化).12345[root@node001189 mafengwo]# ll | grep A-rw-r--r-- 1 mysql mysql 1091 Jun 21 13:20 A.cfg-rw-r----- 1 mysql mysql 16384 Jun 21 13:20 A.exp-rw-r----- 1 mysql mysql 9094 Jun 22 14:11 A.frm-rw-r--r-- 1 mysql mysql 799329484800 Jun 21 13:20 A.ibd执行ALTER后1234[root@node001189 mafengwo]# ll | grep A -rw-r--r-- 1 mysql mysql 1091 Jun 21 13:20 A.cfg-rw-r----- 1 mysql mysql 16384 Jun 21 13:20 A.exp-rw-r--r-- 1 mysql mysql 799329484800 Jun 22 15:12 A.ibd --变了 为什么IMPORT TABLESPACE会卡住?正常的import流程,日志中会有如下输出1234562013-07-18 15:15:01 34960 [Note] InnoDB: Importing tablespace for table &#x27;test/t&#x27; that was exported from host &#x27;ubuntu&#x27;2013-07-18 15:15:01 34960 [Note] InnoDB: Phase I - Update all pages2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk - done!2013-07-18 15:15:01 34960 [Note] InnoDB: Phase III - Flush changes to disk2013-07-18 15:15:01 34960 [Note] InnoDB: Phase IV - Flush complete我觉得问题关键在于InnoDB: Phase I - Update all pages到底做了什么, 因为事实上import过程一直卡在这里, 没有进入下一步google了一圈发现一个类似的问题, 但是并没有答案(现在有了是我回答的)https://dba.stackexchange.com/questions/165147/import-tablespace-is-hanging-in-the-system-lock-state/241182#241182 查看官方文档https://dev.mysql.com/doc/refman/8.0/en/tablespace-copying.htmlWhen ALTER TABLE … IMPORT TABLESPACE is run on the destination instance, the import algorithm performs the following operations for each tablespace being imported: Each tablespace page is checked for corruption. The space ID and log sequence numbers (LSNs) on each page are updated Flags are validated and LSN updated for the header page. Btree pages are updated. The page state is set to dirty so that it is written to disk. 查看workloghttps://dev.mysql.com/worklog/task/?id=552212345678910111213141516171819202122232425262728293031Import algorithm================We scan the blocks in extents and modify individual blocks rather than using logical index structure.foreach page in tablespace &#123; 1. Check each page for corruption. 2. Update the space id and LSN on every page --I think this is what &quot;InnoDB: Phase I - Update all pages&quot; does * For the header page - Validate the flags - Update the LSN 3. On Btree pages * Set the index id * Update the max trx id * In a cluster index, update the system columns * In a cluster index, update the BLOB ptr, set the space id * Purge delete marked records, but only if they can be easily removed from the page * Keep a counter of number of rows, ie. non-delete-marked rows * Keep a counter of number of delete marked rows * Keep a counter of number of purge failure * If a page is stamped with an index id that isn&#x27;t in the .cfg file we assume it is deleted and the page can be ignored. * We can&#x27;t tell free pages from allocated paes (for now). Therefore the assumption is that the free pages are either empty or are logically consistent. TODO: Cache the extent bitmap and check free pages. 4. Set the page state to dirty so that it will be written to disk.&#125; 还有一个文档, 不过感觉不如上面两个说的明白https://bugs.mysql.com/bug.php?id=75706 看到这里我认为InnoDB: Phase I - Update all pages 阶段执行的就是 (按extend扫描整个表, 修改每个page的LSN等)12342. Update the space id and LSN on every page * For the header page - Validate the flags - Update the LSN这一步一定很耗时, 查看监控, 也确实发现执行ALTER TABLE A IMPORT TABLESPACE的节点再这个时间段IO有明显上升 其他节点 所以这会导致什么问题?会导致复制延迟无论主从复制, PXC 还是 MGR ,本质都是应用binlog 示例, 与之前的操作无关123456789101112131415161718/usr/local/mysql-5.7.23-linux-glibc2.12-x86_64/bin/mysqlbinlog -vv --base64-output=decode-rows 0242133310-relay-bin-group_replication_applier.000002|lessSET @@SESSION.GTID_NEXT= &#x27;88f93f74-ed3a-50c0-bc63-beb926cedcb5:1033040&#x27;/*!*/;# at 15134136#190621 10:45:09 server id 242123310 end_log_pos 126 Query thread_id=4948397 exec_time=0 error_code=0SET TIMESTAMP=1561085109/*!*/;alter table t_monitor_sync_delay DISCARD TABLESPACE/*!*/;# at 15134262#190227 16:34:38 server id 242123310 end_log_pos 61 GTID last_committed=196812 sequence_number=196813 rbr_only=noSET @@SESSION.GTID_NEXT= &#x27;88f93f74-ed3a-50c0-bc63-beb926cedcb5:1033041&#x27;/*!*/;# at 15134323#190621 10:46:21 server id 242123310 end_log_pos 125 Query thread_id=4948487 exec_time=0 error_code=0SET TIMESTAMP=1561085181/*!*/;alter table t_monitor_sync_delay import tablespace/*!*/;# at 15134448#190227 16:34:38 server id 242123310 end_log_pos 61 GTID last_committed=198535 sequence_number=198536 rbr_only=no可以看到binlog中其实只记录了alter table t_monitor_sync_delay DISCARD TABLESPACE 和 alter table t_monitor_sync_delay import tablespace两个语句, 并没有对这张表的INSERT语句 我们假设发起import tablespace的节点最终花费1小时更新了所有A表pages的LSN和其他信息, 成功的跑完了import tablespace语句. 当其他节点应用到这个import tablespace语句是就也要花费1小时去做相同的操作, 那面后面的事务是否就需要等待这个import tablespace执行完毕呢? 所以应该如何导入?以三节点MGR为例先建表1create table A然后一个一个节点执行1234567stop group_replicationset read_only=1, 确保没有数据写入set sql_log_bin=0alter table A discard tablespacealter table A import tablespaceset sql_log_bin=1start group_replication","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"传输表空间","slug":"传输表空间","permalink":"http://fuxkdb.com/tags/%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4/"}]},{"title":"限制备份速度防止网卡打满","slug":"限制备份速度防止网卡打满","date":"2019-06-11T20:00:00.000Z","updated":"2019-06-11T19:51:02.000Z","comments":true,"path":"2019/06/12/限制备份速度防止网卡打满/","link":"","permalink":"http://fuxkdb.com/2019/06/12/%E9%99%90%E5%88%B6%E5%A4%87%E4%BB%BD%E9%80%9F%E5%BA%A6%E9%98%B2%E6%AD%A2%E7%BD%91%E5%8D%A1%E6%89%93%E6%BB%A1/","excerpt":"","text":"限制备份速度防止网卡打满最近在做拆库, 于是就做了很多表迁移工作, 需要使用mysqldump远程备份数据, 然后发现备份时很容易就把源库网卡打满了. 想过用tc命令限速, 发现有点复杂. 今天无意间看xtrabackup文档发现一个方法 Throttling the throughput to 10MB/sec. This requires the ‘pv’ tools; you can find them at the official site or install it from the distribution package (“apt-get install pv”)12$ innobackupex --stream=tar ./ | pv -q -L10m \\| ssh user@desthost &quot;cat - &gt; /data/backups/backup.tar&quot; Make a Streaming Backuphttps://www.percona.com/doc/percona-xtrabackup/2.4/howtos/recipes_ibkx_stream.html 查看pv的介绍, 主要是辅助查看一些进度 progressbar ETA什么的: 如何使用 pv 命令监控 linux 命令的执行进度 如何使用“pv”命令监视（复制/备份/压缩）数据的进度 - Howtoing运维教程 pv命令介绍 在google搜索mysqldump pv 基本也是些查看进度的文章 http://landcareweb.com/questions/6489/you-mei-you-ban-fa-rang-mysqldumpjin-du-tiao-xian-shi-yong-hu-de-bei-fen-zhuang-taihttps://www.2cto.com/database/201310/248423.htmlhttps://stackoverflow.com/questions/4852933/does-mysqldump-support-a-progress-bar 我们这里主要目的是限速, 照猫画虎, 可以这样实现导出限速1mysqldump | pv -q -L10m &gt; xx.sql限速导入1cat xx.sql | pv -q -L10m | mysql导入查看进度1pv xx.sql | mysql","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"修改sysbench输出格式为csv或json, 添加自定义指标","slug":"修改sysbench输出格式为csv或json 添加自定义指标","date":"2019-03-12T09:53:00.000Z","updated":"2019-03-12T10:44:02.000Z","comments":true,"path":"2019/03/12/修改sysbench输出格式为csv或json 添加自定义指标/","link":"","permalink":"http://fuxkdb.com/2019/03/12/%E4%BF%AE%E6%94%B9sysbench%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E4%B8%BAcsv%E6%88%96json%20%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87/","excerpt":"修改sysbench输出格式为csv或json, 添加自定义指标在oltp_common.lua添加(二选一) 12345678910111213141516171819-- 输出csv-- function sysbench.hooks.report_intermediate(stat)-- sysbench.report_csv(stat)-- end-- 输出json-- function sysbench.hooks.report_intermediate(stat)-- sysbench.report_json(stat)-- end 自己去掉 -- 注释","text":"修改sysbench输出格式为csv或json, 添加自定义指标在oltp_common.lua添加(二选一) 12345678910111213141516171819-- 输出csv-- function sysbench.hooks.report_intermediate(stat)-- sysbench.report_csv(stat)-- end-- 输出json-- function sysbench.hooks.report_intermediate(stat)-- sysbench.report_json(stat)-- end 自己去掉 -- 注释 加hook 123456789101112131415161718192021sysbench.hooks.report_intermediate = function (stat) if con == nil then con = assert(sysbench.sql.driver():connect()) end sysbench.report_default(stat) create_time, node_219, node_220 = con:query_row([[select create_time,(select delay from test.test_mgr_delay_detail where ip=&#x27;192.168.2.219&#x27; order by create_time desc limit 1) as &#x27;node_219&#x27;,(select delay from test.test_mgr_delay_detail where ip=&#x27;192.168.2.220&#x27; order by create_time desc limit 1) as &#x27;node_220&#x27; from test.test_mgr_delay_detail order by create_time desc limit 1;]]) print(&quot;create_time: &quot;..create_time..&quot; node_219: &quot;..node_219..&quot; node_220: &quot;..node_220)end 这样的效果就是 123456[ 5s ] thds: 30 tps: 73.76 qps: 1538.67 (r/w/o: 1084.35/300.82/153.51) lat (ms,95%): 733.00 err/s: 0.00 reconn/s: 0.00create_time: 2018-01-01 10:10:00 node_219: 10 node_220: 20[ 10s ] thds: 30 tps: 75.80 qps: 1519.27 (r/w/o: 1062.85/304.81/151.61) lat (ms,95%): 590.56 err/s: 0.00 reconn/s: 0.00create_time: 2018-01-01 10:15:00 node_219: 10 node_220: 20[ 15s ] thds: 30 tps: 58.39 qps: 1183.57 (r/w/o: 831.24/235.55/116.78) lat (ms,95%): 926.33 err/s: 0.00 reconn/s: 0.00create_time: 2018-01-01 10:20:00 node_219: 10 node_220: 20 总之就是你可以自定以一些sql查询或者系统命令去增加一些指标. 我这里就是想看不同XX下对从库复制延迟的影响","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Sysbench","slug":"Sysbench","permalink":"http://fuxkdb.com/tags/Sysbench/"}]},{"title":"使confluent_kafka支持SASL_PLAINTEXT","slug":"使confluent_kafka支持SASL_PLAINTEXT","date":"2019-03-08T10:53:00.000Z","updated":"2019-03-10T05:21:48.000Z","comments":true,"path":"2019/03/08/使confluent_kafka支持SASL_PLAINTEXT/","link":"","permalink":"http://fuxkdb.com/2019/03/08/%E4%BD%BFconfluent_kafka%E6%94%AF%E6%8C%81SASL_PLAINTEXT/","excerpt":"","text":"同事之前一直使用kafka-python开发. 上了ACL以后发现kafka-python居然不支持SASL_PLAINTEXT https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.htmlsasl_mechanism (str) – string picking sasl mechanism when security_protocol is SASL_PLAINTEXT or SASL_SSL. Currently only PLAIN is supported. Default: None 看了一下confluent-kafka是支持的但需要重新编译librdkafka否则会报错:1KafkaException: KafkaError&#123;code=_INVALID_ARG,val=-186,str=&quot;Failed to create producer: No provider for SASL mechanism GSSAPI: recompile librdkafka with libsasl2 or openssl support. Current build options: PLAIN SASL_SCRAM&quot;&#125; 安装confluent-kafka1234pip install confluent-kafkaCollecting confluent-kafka Downloading https://files.pythonhosted.org/packages/2a/ba/dccb27376453f91ad8fa57f75a7ba5dc188023700c1789273dec976477b2/confluent_kafka-0.11.6-cp37-cp37m-manylinux1_x86_64.whl (3.9MB) 100% |████████████████████████████████| 3.9MB 984kB/s 安装librdkafka克隆下来123456789[root@node004110 18:21:05 /tmp]git clone https://github.com/edenhill/librdkafka.gitCloning into &#x27;librdkafka&#x27;...remote: Enumerating objects: 213, done.remote: Counting objects: 100% (213/213), done.remote: Compressing objects: 100% (111/111), done.remote: Total 18133 (delta 133), reused 149 (delta 98), pack-reused 17920Receiving objects: 100% (18133/18133), 11.15 MiB | 946.00 KiB/s, done.Resolving deltas: 100% (13758/13758), done.检查依赖1234rpm -qa| grep opensslopenssl-1.0.2k-16.el7.x86_64openssl-libs-1.0.2k-16.el7.x86_64openssl-devel-1.0.2k-16.el7.x86_64 The GNU toolchainGNU makepthreadszlib-dev (optional, for gzip compression support)libssl-dev (optional, for SSL and SASL SCRAM support) –这个对于centos openssl-devellibsasl2-dev (optional, for SASL GSSAPI support)libzstd-dev (optional, for ZStd compression support)安装12./configuremake &amp;&amp; make install注意下make的时候这里是ok就行12checking for libssl (by pkg-config)... okchecking for libssl (by compile)... ok (cached) 替换库文件查找12find / -name &quot;librdkafka*&quot; /root/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs/librdkafka.so.1替换1234567891011121314151617181920212223242526#cd /root/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs/[root@node004110 18:24:03 ~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs]#lltotal 10316-rwxr-xr-x 1 root root 2903144 Mar 8 18:20 libcrypto-4c524931.so.1.0.0-rwxr-xr-x 1 root root 6944424 Mar 8 18:20 librdkafka.so.1-rwxr-xr-x 1 root root 584072 Mar 8 18:20 libssl-01b7eff1.so.1.0.0-rwxr-xr-x 1 root root 87848 Mar 8 18:20 libz-a147dcb0.so.1.2.3-rw-r--r-- 1 root root 35336 Mar 8 18:20 monitoring-interceptor.so[root@node004110 18:24:04 ~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs]#mv librdkafka.so.1 librdkafka.so.1.bak[root@node004110 18:24:11 ~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs]#ln -s /usr/local/lib/librdkafka.so.1 librdkafka.so.1[root@node004110 18:24:23 ~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/confluent_kafka/.libs]#lltotal 10316-rwxr-xr-x 1 root root 2903144 Mar 8 18:20 libcrypto-4c524931.so.1.0.0lrwxrwxrwx 1 root root 30 Mar 8 18:24 librdkafka.so.1 -&gt; /usr/local/lib/librdkafka.so.1-rwxr-xr-x 1 root root 6944424 Mar 8 18:20 librdkafka.so.1.bak-rwxr-xr-x 1 root root 584072 Mar 8 18:20 libssl-01b7eff1.so.1.0.0-rwxr-xr-x 1 root root 87848 Mar 8 18:20 libz-a147dcb0.so.1.2.3-rw-r--r-- 1 root root 35336 Mar 8 18:20 monitoring-interceptor.so 验证12345678910111213141516171819202122#pythonPython 3.7.2 (default, Mar 4 2019, 16:55:21) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from confluent_kafka import Producer &gt;&gt;&gt; p = Producer(&#123;&#x27;bootstrap.servers&#x27;: &#x27;192.168.4.114:9092&#x27;, &#x27;security.protocol&#x27;: &#x27;SASL_PLAINTEXT&#x27;, &#x27;sasl.mechanism&#x27;:&#x27;SCRAM-SHA-256&#x27;,&#x27;sasl.username&#x27;:&#x27;admin&#x27;,&#x27;sasl.password&#x27;:&#x27;your-admin-pass&#x27;&#125;) &gt;&gt;&gt; def delivery_report(err, msg):... if err is not None:... print(&#x27;Message delivery failed: &#123;&#125;&#x27;.format(err))... else:... print(&#x27;Message delivered to &#123;&#125; [&#123;&#125;]&#x27;.format(msg.topic(), msg.partition()))... &gt;&gt;&gt; for data in [&#x27;hello&#x27;,&#x27;word&#x27;]: ... p.produce(&#x27;test_acl&#x27;, data.encode(&#x27;utf-8&#x27;), callback=delivery_report)... &gt;&gt;&gt; p.poll(10) Message delivered to test_acl [0]Message delivered to test_acl [0]1&gt;&gt;&gt; p.flush()0&gt;&gt;&gt; quit() 成功收到消息123[root@node004114 kafka]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.4.114:9092 --topic test_acl --consumer.config config/client-sasl.properties --from-beginninghelloword","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://fuxkdb.com/tags/Kafka/"}]},{"title":"假如Kafka集群中一个broker宕机无法恢复, 应该如何处理?","slug":"2019-02-22-假如Kafka集群中一个broker宕机无法恢复,-应该如何处理","date":"2019-02-22T03:58:00.000Z","updated":"2019-02-22T03:58:58.000Z","comments":true,"path":"2019/02/22/2019-02-22-假如Kafka集群中一个broker宕机无法恢复,-应该如何处理/","link":"","permalink":"http://fuxkdb.com/2019/02/22/2019-02-22-%E5%81%87%E5%A6%82Kafka%E9%9B%86%E7%BE%A4%E4%B8%AD%E4%B8%80%E4%B8%AAbroker%E5%AE%95%E6%9C%BA%E6%97%A0%E6%B3%95%E6%81%A2%E5%A4%8D,-%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86/","excerpt":"假如Kafka集群中一个broker宕机无法恢复, 应该如何处理?坐地铁时想到这个问题, 印象中书中说添加新的broker, 是不会自动同步旧数据的. 笨办法环境介绍三个broker的集群, zk,kafka装在一起12345| broker | IP | broker.id ||---------|---------------|-----------|| broker1 | 172.18.12.211 | 211 || broker2 | 172.18.12.212 | 212 || broker3 | 172.18.12.213 | 213 |","text":"假如Kafka集群中一个broker宕机无法恢复, 应该如何处理?坐地铁时想到这个问题, 印象中书中说添加新的broker, 是不会自动同步旧数据的. 笨办法环境介绍三个broker的集群, zk,kafka装在一起12345| broker | IP | broker.id ||---------|---------------|-----------|| broker1 | 172.18.12.211 | 211 || broker2 | 172.18.12.212 | 212 || broker3 | 172.18.12.213 | 213 | 创建测试topic12#./bin/kafka-topics.sh --zookeeper 172.18.12.212:2181 --create --topic test1 --replication-factor 3 --partitions 1Created topic &quot;test1&quot;. 查看123#./bin/kafka-topics.sh --zookeeper 172.18.12.212:2181 --describe --topic test1Topic:test1 PartitionCount:1 ReplicationFactor:3 Configs: Topic: test1 Partition: 0 Leader: 213 Replicas: 213,212,211 Isr: 213,212,211注意当前Replicas: 213,212,211Isr: 213,212,211 造一些消息1234#./bin/kafka-console-producer.sh --broker-list 172.18.12.212:9092 --topic test1&gt;1&gt;2&gt;3 kill broker2123456[root@node024212 ~]# ps -ef| grep kafkaroot 17633 1 1 Feb17 ? 00:55:18 /usr/local/java/bin/java -server -Xmx2g -Xms2g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=85 -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Xloggc:/usr/local/kafka/bin/../logs/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=9966 -Dkafka.logs.dir=/usr/local/kafka/bin/../logs -Dlog4j.configuration=file:./bin/../config/log4j.properties -cp .:/usr/local/java/lib:/usr/local/java/jre/lib:/usr/local/kafka/bin/../libs/activation-1.1.1.jar:/usr/local/kafka/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/usr/local/kafka/bin/../libs/argparse4j-0.7.0.jar:/usr/local/kafka/bin/../libs/audience-annotations-0.5.0.jar:/usr/local/kafka/bin/../libs/commons-lang3-3.5.jar:/usr/local/kafka/bin/../libs/compileScala.mapping:/usr/local/kafka/bin/../libs/compileScala.mapping.asc:/usr/local/kafka/bin/../libs/connect-api-2.1.0.jar:/usr/local/kafka/bin/../libs/connect-basic-auth-extension-2.1.0.jar:/usr/local/kafka/bin/../libs/connect-file-2.1.0.jar:/usr/local/kafka/bin/../libs/connect-json-2.1.0.jar:/usr/local/kafka/bin/../libs/connect-runtime-2.1.0.jar:/usr/local/kafka/bin/../libs/connect-transforms-2.1.0.jar:/usr/local/kafka/bin/../libs/guava-20.0.jar:/usr/local/kafka/bin/../libs/hk2-api-2.5.0-b42.jar:/usr/local/kafka/bin/../libs/hk2-locator-2.5.0-b42.jar:/usr/local/kafka/bin/../libs/hk2-utils-2.5.0-b42.jar:/usr/local/kafka/bin/../libs/jackson-annotations-2.9.7.jar:/usr/local/kafka/bin/../libs/jackson-core-2.9.7.jar:/usr/local/kafka/bin/../libs/jackson-databind-2.9.7.jar:/usr/local/kafka/bin/../libs/jackson-jaxrs-base-2.9.7.jar:/usr/local/kafka/bin/../libs/jackson-jaxrs-json-provider-2.9.7.jar:/usr/local/kafka/bin/../libs/jackson-module-jaxb-annotations-2.9.7.jar:/usr/local/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/usr/local/kafka/bin/../libs/javax.annotation-api-1.2.jar:/usr/local/kafka/bin/../libs/javax.inject-1.jar:/usr/local/kafka/bin/../libs/javax.inject-2.5.0-b42.jar:/usr/local/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/local/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/usr/local/kafka/bin/../libs/javax.ws.rs-api-2.1.jar:/usr/local/kafka/bin/../libs/jaxb-api-2.3.0.jar:/usr/local/kafka/bin/../libs/jersey-client-2.27.jar:/usr/local/kafka/bin/../libs/jersey-common-2.27.jar:/usr/local/kafka/bin/../libs/jersey-container-servlet-2.27.jar:/usr/local/kafka/bin/../libs/jersey-container-servlet-core-2.27.jar:/usr/local/kafka/bin/../libs/jersey-hk2-2.27.jar:/usr/local/kafka/bin/../libs/jersey-media-jaxb-2.27.jar:/usr/local/kafka/bin/../libs/jersey-server-2.27.jar:/usr/local/kafka/bin/../libs/jetty-client-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-continuation-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-http-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-io-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-security-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-server-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-servlet-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-servlets-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jetty-util-9.4.12.v20180830.jar:/usr/local/kafka/bin/../libs/jopt-simple-5.0.4.jar:/usr/local/kafka/bin/../libs/kafka_2.12-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka_2.12-2.1.0-sources.jar:/usr/local/kafka/bin/../libs/kafka-clients-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-log4j-appender-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-streams-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-streams-examples-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-streams-scala_2.12-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-streams-test-utils-2.1.0.jar:/usr/local/kafka/bin/../libs/kafka-tools-2.1.0.jar:/usr/local/kafka/bin/../libs/log4j-1.2.17.jar:/usr/local/kafka/bin/../libs/lz4-java-1.5.0.jar:/usr/local/kafka/bin/../libs/maven-artifact-3.5.4.jar:/usr/local/kafka/bin/../libs/metrics-core-2.2.0.jar:/usr/local/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/local/kafka/bin/../libs/plexus-utils-3.1.0.jar:/usr/local/kafka/bin/../libs/reflections-0.9.11.jar:/usr/local/kafka/bin/../libs/rocksdbjni-5.14.2.jar:/usr/local/kafka/bin/../libs/scala-library-2.12.7.jar:/usr/local/kafka/bin/../libs/scala-logging_2.12-3.9.0.jar:/usr/local/kafka/bin/../libs/scala-reflect-2.12.7.jar:/usr/local/kafka/bin/../libs/slf4j-api-1.7.25.jar:/usr/local/kafka/bin/../libs/slf4j-log4j12-1.7.25.jar:/usr/local/kafka/bin/../libs/snappy-java-1.1.7.2.jar:/usr/local/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/usr/local/kafka/bin/../libs/zkclient-0.10.jar:/usr/local/kafka/bin/../libs/zookeeper-3.4.13.jar:/usr/local/kafka/bin/../libs/zstd-jni-1.3.5-4.jar kafka.Kafka config/server.propertiesroot 21806 21651 0 11:27 pts/2 00:00:00 grep --color=auto kafka[root@node024212 ~]# kill -9 17633[root@node024212 ~]# ps -ef| grep kafkaroot 21875 21651 0 11:27 pts/2 00:00:00 grep --color=auto kafka 稍等一会, 再次describe test1123#./bin/kafka-topics.sh --zookeeper 172.18.12.212:2181 --describe --topic test1Topic:test1 PartitionCount:1 ReplicationFactor:3 Configs: Topic: test1 Partition: 0 Leader: 213 Replicas: 213,212,211 Isr: 213,211可看到副本仍然是Replicas: 213,212,211ISR已经变为Isr: 213,211 在212启动新broker创建一份新的配置文件, 自动一个新的broker12345# cp server.properties server2.properties # vim server2.properties 只修改这两个参数broker.id=218log.dirs=/DATA21/kafka/kafka-logs,/DATA22/kafka/kafka-logs,/DATA23/kafka/kafka-logs,/DATA24/kafka/kafka-logs创建相应目录1234mkdir -p /DATA21/kafka/kafka-logsmkdir -p /DATA22/kafka/kafka-logsmkdir -p /DATA23/kafka/kafka-logsmkdir -p /DATA24/kafka/kafka-logs启动新broker1./bin/kafka-server-start.sh -daemon config/server2.properties 稍等, 查看 test1 状态 123#./bin/kafka-topics.sh --zookeeper 172.18.12.212:2181 --describe --topic test1Topic:test1 PartitionCount:1 ReplicationFactor:3 Configs: Topic: test2 Partition: 0 Leader: 213 Replicas: 213,212,211 Isr: 213,218,211 可以看到 test1 副本仍然是Replicas: 213,212,211ISR为Isr: 213,218,211. 也就是说缺失的副本不会自动迁移到新broker上. 使用kafka-reassign-partitions.sh重分配分区将212删除,添加21812345678910111213141516[root@node024211 12:04:48 /usr/local/kafka]#echo &#x27;&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[211,213,218]&#125;]&#125;&#x27; &gt; increase-replication-factor.json[root@node024211 12:58:30 /usr/local/kafka]#./bin/kafka-reassign-partitions.sh --zookeeper 172.18.12.211:2181 --reassignment-json-file increase-replication-factor.json --executeCurrent partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;test1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[213,212,211],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions.[root@node024211 12:58:49 /usr/local/kafka]#./bin/kafka-reassign-partitions.sh --zookeeper 172.18.12.211:2181 --reassignment-json-file increase-replication-factor.json --verifyStatus of partition reassignment: Reassignment of partition test1-0 completed successfully查看topic信息1234#./bin/kafka-topics.sh --zookeeper 172.18.12.212:2181 --describe --topic test1Topic:test1 PartitionCount:1 ReplicationFactor:3 Configs: Topic: test1 Partition: 0 Leader: 213 Replicas: 211,213,218 Isr: 213,211,218 验证218是否有全部数据虽然看副本信息中已经有了218, 但是218是否包含旧消息呢?我的办法是, kill 211,213, 然后–from-beginning 消费218数据, 实际测试也是可以的12345678910111213#./bin/kafka-console-consumer.sh --bootstrap-server 172.18.12.212:9092 --topic test1 --from-beginning123456789101111看了下211 218的log文件大小也是一样的123456[2019-02-21 13:29:19]#ls -l /DATA22/kafka/kafka-logs/test1-0/[2019-02-21 13:29:19]total 8[2019-02-21 13:29:19]-rw-r--r--. 1 root root 10485760 Feb 21 12:58 00000000000000000000.index[2019-02-21 13:29:19]-rw-r--r--. 1 root root 381 Feb 21 13:00 00000000000000000000.log[2019-02-21 13:29:19]-rw-r--r--. 1 root root 10485756 Feb 21 12:58 00000000000000000000.timeindex[2019-02-21 13:29:19]-rw-r--r--. 1 root root 16 Feb 21 13:00 leader-epoch-checkpoint 更简单的办法通过阅读文档发现https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Howtoreplaceafailedbroker? How to replace a failed broker?When a broker fails, Kafka doesn’t automatically re-replicate the data on the failed broker to other brokers. This is because in the common case, one brings down a broker to apply code or config changes, and will bring up the broker quickly afterward. Re-replicating the data in this case will be wasteful. In the rarer case that a broker fails completely, one will need to bring up another broker with the same broker id on a new server. The new broker will automatically replicate the missing data. 这上面说的,如果服务器真的坏了, 只需要新启动一个broker, 把broker.id设置为 损坏的那个broker的id, 就会自动复制过去丢失的数据 我实际测试了一下, 确实可以恢复","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://fuxkdb.com/tags/Kafka/"}]},{"title":"Kafka SASL_SCRAM+ACL实现动态创建用户及权限控制","slug":"2019-01-26-Kafka-SASL_SCRAM+ACL实现动态创建用户及权限控制","date":"2019-01-26T14:01:00.000Z","updated":"2019-03-08T07:39:20.000Z","comments":true,"path":"2019/01/26/2019-01-26-Kafka-SASL_SCRAM+ACL实现动态创建用户及权限控制/","link":"","permalink":"http://fuxkdb.com/2019/01/26/2019-01-26-Kafka-SASL_SCRAM+ACL%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E5%8F%8A%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/","excerpt":"SASL_SCRAM+ACL实现动态创建用户及权限控制&nbsp;&nbsp;研究了一段时间Kafka的权限控制. 之前一直看SASL_PLAINTEXT, 结果发现这玩意不能动态创建用户. 今天没死心又查了查,发现这个人也问了这个问题https://stackoverflow.com/questions/54147460/kafka-adding-sasl-users-dynamically-without-cluster-restart于是研究了下SCRAM. 写了这篇完整的文档 本篇文档中使用的是自己部署的zookeeper, zookeeper无需做任何特殊配置","text":"SASL_SCRAM+ACL实现动态创建用户及权限控制&nbsp;&nbsp;研究了一段时间Kafka的权限控制. 之前一直看SASL_PLAINTEXT, 结果发现这玩意不能动态创建用户. 今天没死心又查了查,发现这个人也问了这个问题https://stackoverflow.com/questions/54147460/kafka-adding-sasl-users-dynamically-without-cluster-restart于是研究了下SCRAM. 写了这篇完整的文档 本篇文档中使用的是自己部署的zookeeper, zookeeper无需做任何特殊配置 使用SASL / SCRAM进行身份验证请先在不配置任何身份验证的情况下启动Kafka 1. 创建SCRAM CredentialsKafka中的SCRAM实现使用Zookeeper作为凭证(credential)存储。 可以使用kafka-configs.sh在Zookeeper中创建凭据。 对于启用的每个SCRAM机制，必须通过添加具有机制名称的配置来创建凭证。 必须在启动Kafka broker之前创建代理间通信的凭据。 可以动态创建和更新客户端凭证，并使用更新的凭证来验证新连接。 创建broker建通信用户(或称超级用户)1bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config &#x27;SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]&#x27; --entity-type users --entity-name admin 创建客户端用户fanboshi1bin/kafka-configs.sh --zookeeper 192.168.2.229:2182 --alter --add-config &#x27;SCRAM-SHA-256=[iterations=8192,password=fanboshi],SCRAM-SHA-512=[password=fanboshi]&#x27; --entity-type users --entity-name fanboshi 查看SCRAM证书12[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name fanboshiConfigs for user-principal &#x27;fanboshi&#x27; are SCRAM-SHA-512=salt=MWwwdWJqcjBncmUwdzY1Mzdoa2NwNXppd3A=,stored_key=mGCJy5k3LrE2gs6Dp4ALRhgy37l1WYPUIdoOncCF+B3Ti3wL2sQNmzg8oEz3tUs9DFsclFCygjbysb0S0BU9bA==,server_key=iTyX0U0Jt02dkddUm6QrVwNf3lJk72dBNs9EDHTqe8kLlNGIp9ypzRkcgkc+WVMd1bkAF3cg8vk9Q1LrJ/2i/A==,iterations=4096,SCRAM-SHA-256=salt=ZDg5MHVlYW40dW9jbXJ6MndvZDVlazd3ag==,stored_key=cgX1ldpXnDL1+TlLHJ3IHn7tAQS/7pQ7BVZUtECpQ3A=,server_key=i7Mcnb5sPUqfIFs6qKWWHZ2ortoKiRc7oabHOV5dawI=,iterations=8192 删除SCRAM证书这里只演示,不操作1[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config &#x27;SCRAM-SHA-512&#x27; --entity-type users --entity-name fanboshi 2. 配置Kafka Brokers 在每个Kafka broker的config目录中添加一个类似下面的JAAS文件，我们称之为kafka_server_jaas.conf：123456[root@node002229 config]# cat kafka_server_jaas.confKafkaServer &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;admin&quot; password=&quot;admin-secret&quot;;&#125;; 注意不要少写了分号 将JAAS配置文件位置作为JVM参数传递给每个Kafka broker：修改 /usr/local/kafka/bin/kafka-server-start.sh将exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka &quot;$@&quot; 注释, 增加下面的内容 12#exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka &quot;$@&quot;exec $base_dir/kafka-run-class.sh $EXTRA_ARGS -Djava.security.auth.login.config=$base_dir/../config/kafka_server_jaas.conf kafka.Kafka &quot;$@&quot; 或者不修改kafka-server-start.sh脚本, 而是将下面的内容添加到~/.bashrc 12export KAFKA_PLAIN_PARAMS=&quot;-Djava.security.auth.login.config=/usr/local/kafka/config/kafka_server_jaas.conf&quot;export KAFKA_OPTS=&quot;$KAFKA_PLAIN_PARAMS $KAFKA_OPTS&quot; 如此处所述，在server.properties中配置SASL端口和SASL机制。 例如： 12345678910# 认证配置listeners=SASL_PLAINTEXT://node002229:9092security.inter.broker.protocol=SASL_PLAINTEXTsasl.mechanism.inter.broker.protocol=SCRAM-SHA-256sasl.enabled.mechanisms=SCRAM-SHA-256# ACL配置allow.everyone.if.no.acl.found=falsesuper.users=User:adminauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer 在官方文档中写的是 12listeners=SASL_SSL://host.name:portsecurity.inter.broker.protocol=SASL_SSL 这里其实没必要写成SASL_SSL , 我们可以根据自己的需求选择SSL或PLAINTEXT, 我这里选择PLAINTEXT不加密明文传输, 省事, 性能也相对好一些 重启ZK/Kafka重启ZK / Kafka服务. 所有broker在连接之前都会引用’kafka_server_jaas.conf’.Zookeeper所有节点 12345678[root@node002229 zookeeper]# zkServer.sh stop /usr/local/zookeeper/bin/../conf/zoo.cfg ZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo1.cfgStopping zookeeper ... STOPPED[root@node002229 zookeeper]# zkServer.sh start /usr/local/zookeeper/bin/../conf/zoo.cfg ZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo1.cfg Kafka所有Broker12cd /usr/local/kafka/;bin/kafka-server-stop.shcd /usr/local/kafka/;bin/kafka-server-start.sh -daemon config/server.properties 客户端配置先使用kafka-console-producer 和 kafka-console-consumer 测试一下 kafka-console-producer 创建 config/client-sasl.properties 文件123[root@node002229 kafka]# vim config/client-sasl.propertiessecurity.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256 创建config/kafka_client_jaas_admin.conf文件123456[root@node002229 kafka]# vim config/kafka_client_jaas_admin.conf KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;admin&quot; password=&quot;admin-secret&quot;;&#125;; 修改kafka-console-producer.sh脚本这里我复制一份,再改1234cp bin/kafka-console-producer.sh bin/kafka-console-producer-admin.shvim bin/kafka-console-producer-admin.sh#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleProducer &quot;$@&quot;exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_admin.conf kafka.tools.ConsoleProducer &quot;$@&quot; 创建测试topic1bin/kafka-topics.sh --create --zookeeper localhost:2182 --partitions 1 --replication-factor 1 --topic test 测试生产消息123bin/kafka-console-producer-admin.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties&gt;1&gt; 可以看到admin用户无需配置ACL就可以生成消息 测试fanboshi用户如法炮制, 我们创建一个bin/kafka-console-producer-fanboshi.sh文件, 只是修改其中的kafka_client_jaas_admin.conf 为 kafka_client_jaas_fanboshi.conf 12345678910vim config/kafka_client_jaas_fanboshi.conf KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;fanboshi&quot; password=&quot;fanboshi&quot;;&#125;;cp bin/kafka-console-producer-admin.sh bin/kafka-console-producer-fanboshi.shvi bin/kafka-console-producer-fanboshi.shexec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_fanboshi.conf kafka.tools.ConsoleProducer &quot;$@&quot; 生产消息12345[root@node002229 kafka]# bin/kafka-console-producer-fanboshi.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties&gt;1[2019-01-26 18:07:50,099] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : &#123;test=TOPIC_AUTHORIZATION_FAILED&#125; (org.apache.kafka.clients.NetworkClient)[2019-01-26 18:07:50,100] ERROR Error when sending message to topic test with key: null, value: 1 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [test] 可以看到报错了, 因为fanboshi用户还没有权限 kafka-console-consumer 创建 config/consumer-fanboshi.properties 文件1234[root@node002229 kafka]# vim config/consumer-fanboshi.propertiessecurity.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256group.id=fanboshi-group 创建 bin/kafka-console-consumer-fanboshi.sh 文件1234cp bin/kafka-console-consumer.sh bin/kafka-console-consumer-fanboshi.shvim bin/kafka-console-consumer-fanboshi.sh#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleConsumer &quot;$@&quot;exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_fanboshi.conf kafka.tools.ConsoleConsumer &quot;$@&quot; 测试消费者1bin/kafka-console-consumer-fanboshi.sh --bootstrap-server 192.168.2.229:9092 --topic test --consumer.config config/consumer-fanboshi.properties --from-beginning 其实也会报错的, 报错内容就不贴了 ACL配置授予fanboshi用户对test topic 写权限, 只允许 192.168.2.* 网段1bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Write --topic test --allow-host 192.168.2.* 授予fanboshi用户对test topic 读权限, 只允许 192.168.2.* 网段1bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Read --topic test --allow-host 192.168.2.* 授予fanboshi用户, fanboshi-group 消费者组 对test topic 读权限, 只允许 192.168.2.* 网段1bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:fanboshi --operation Read --group fanboshi-group --allow-host 192.168.2.* 查看acl配置1234567[root@node002229 kafka]# bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --listCurrent ACLs for resource `Group:LITERAL:fanboshi-group`: User:fanboshi has Allow permission for operations: Read from hosts: * Current ACLs for resource `Topic:LITERAL:test`: User:fanboshi has Allow permission for operations: Write from hosts: * User:fanboshi has Allow permission for operations: Read from hosts: * 删除配置1bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --remove --allow-principal User:bob --operation Read --topic example10 --allow-host 192.168.1.158 再次测试生产者123456[root@node002229 kafka]# bin/kafka-console-producer-fanboshi.sh --broker-list 192.168.2.229:9092 --topic test --producer.config config/client-sasl.properties&gt;1[2019-01-26 18:07:50,099] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : &#123;test=TOPIC_AUTHORIZATION_FAILED&#125; (org.apache.kafka.clients.NetworkClient)[2019-01-26 18:07:50,100] ERROR Error when sending message to topic test with key: null, value: 1 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [test]&gt;1消费者123[root@node002229 kafka]# bin/kafka-console-consumer-fanboshi.sh --bootstrap-server 192.168.2.229:9092 --topic test --consumer.config config/consumer-fanboshi.properties --from-beginning11都没问题了 如何查看我们创建了哪些”用户”好像只能去zookeeper看?123zkCli.sh -server node002229:2182ls /config/users[admin, alice, fanboshi]尝试删除alice12345678910[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name aliceConfigs for user-principal &#x27;alice&#x27; are SCRAM-SHA-512=salt=MWt1OHRhZnd3cWZvZ2I4bXcwdTM0czIyaTQ=,stored_key=JYeud1Cx5Z2+FaJgJsZGbMcIi63B9XtA9Wyc+KEm2gXK8+2IxxAVvi1CfSjlkqeupfeIMFJ7/EUkOw+zqvYz6w==,server_key=O4NIgjleroia7puK01/ZZoagFeoxh+zHzckGXXooBsWTdx/7Shb0pMHniMu4IY2jb5orWB2t9K8MZkxCliJDsg==,iterations=4096,SCRAM-SHA-256=salt=MTJ3bXRod3EyN3FtZWdsNHk0NXoyeWdlNjE=,stored_key=chQX35reoBYtfg/U5HBtkzvBAk+gSCgskNzUiScOrUE=,server_key=rRTbUzAehwVMUDTMuoOMumGEuvc7wDecKcqK6yYlbWY=,iterations=8192[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config &#x27;SCRAM-SHA-512&#x27; --entity-type users --entity-name aliceCompleted Updating config for entity: user-principal &#x27;alice&#x27;.[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice Configs for user-principal &#x27;alice&#x27; are SCRAM-SHA-256=salt=MTJ3bXRod3EyN3FtZWdsNHk0NXoyeWdlNjE=,stored_key=chQX35reoBYtfg/U5HBtkzvBAk+gSCgskNzUiScOrUE=,server_key=rRTbUzAehwVMUDTMuoOMumGEuvc7wDecKcqK6yYlbWY=,iterations=8192[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config &#x27;SCRAM-SHA-256&#x27; --entity-type users --entity-name alice Completed Updating config for entity: user-principal &#x27;alice&#x27;.[root@node002229 kafka]# bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice Configs for user-principal &#x27;alice&#x27; are 去ZK查看12[zk: node002229:2182(CONNECTED) 0] ls /config/users[admin, alice, fanboshi]还是有, 难道只能在ZK手动删除? 参考文献http://kafka.apache.org/documentation/#security_sasl_scramhttps://sharebigdata.wordpress.com/category/kafka/kafka-sasl-scram-with-w-o-ssl/https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/https://developer.ibm.com/tutorials/kafka-authn-authz/Kafka并不难学! 入门、进阶、商业实战","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://fuxkdb.com/tags/Kafka/"}]},{"title":"MGR vs PXC Benchmark","slug":"MGR vs PXC Benchmark","date":"2018-09-11T14:23:00.000Z","updated":"2020-03-17T07:42:49.117Z","comments":true,"path":"2018/09/11/MGR vs PXC Benchmark/","link":"","permalink":"http://fuxkdb.com/2018/09/11/MGR%20vs%20PXC%20Benchmark/","excerpt":"MGR vs PXC Benchmark本次测试将MGR与PXC进行对比, 意在比较MGR自身不同流控阈值下性能表现以及观察MGR与PXC相同负载下的性能表现. 测试工具使用sysbench 1.1.0-431660d 基础环将信息如下表所示, 在三个测试服务器部署了3306, 3307两套集群 Host IP MGR PXC data-191 192.168.8.191 3306 3307 data-219 192.168.8.219 3306 3307 data-220 192.168.8.220 3306 3307","text":"MGR vs PXC Benchmark本次测试将MGR与PXC进行对比, 意在比较MGR自身不同流控阈值下性能表现以及观察MGR与PXC相同负载下的性能表现. 测试工具使用sysbench 1.1.0-431660d 基础环将信息如下表所示, 在三个测试服务器部署了3306, 3307两套集群 Host IP MGR PXC data-191 192.168.8.191 3306 3307 data-219 192.168.8.219 3306 3307 data-220 192.168.8.220 3306 3307 这里我们不关心buffer size和iops 本次测试一个不严谨的地方是,pxc使用percona server5.7.20, MGR使用mysql server5.7.22 下文将展示测试结果和说明测试方法, 尽量客观展示测试事实(语文不太好, 见谅) 下文中: 25000表示MGR默认流控阈值下的测试情况 50000表示MGR默认流控阈值一倍下的测试情况 one_node_threshold_50000表示阈值5W关闭一个节点的流控下的测试情况 threshold_disable表示所有节点关闭流控下的测试情况 OLTP_WRITE_ONLY模式不同流控阈值下MGR表现 OLTP_WRITE_ONLY模式下每个事务包含 1个 index_updates UPDATE sbtest%u SET k=k+1 WHERE id=? 1个 non_index_updates UPDATE sbtest%u SET c=? WHERE id=? 1个 deletes DELETE FROM sbtest%u WHERE id=? 1个 inserts INSERT INTO sbtest%u (id, k, c, pad) VALUES (?, ?, ?, ?) 16线程压力下, 四种流控阈值场景下qps相差不多, 响应时间在阈值25000时较高, 这也符合预期 32 qps接近6W 64线程. 可以看到关闭流控和只关闭一个节点的流控的情况下qps差不多, 对于25000和5000可以看到qps一直在上下起伏震荡 128线程. 受内存或iops的限制可以看到对比64线程one_node_threshold_50000和threshold_disabled qps并没有再增高,响应时间稍有增高. 观察25000和5000可以发现在压测开始时,两者qps都能达到10W, 由于流控, qps迅速下降,然后就一直在5W左右震荡 256线程. one_node_threshold_50000和threshold_disabled qps并没有再增高,观察25000和5000可以发现在压测开始时,两者qps都能达到10W(比前两者的还高)之后又由于流控, qps迅速下降,然后就一直在5W左右震荡 对于响应时间one_node_threshold_disabled和threshold_disabled在20ms左右而25000和5000几个波峰达到了800ms+ 这里还有一个有趣的现象, 16-256的各个负载下qps波谷有重合, 这个目前还不清楚是什么原因导致的 OLTP_READ_WRITE 模式MGR与PXC性能对比 仅从qps上看, MGR被完虐 仅从响应时间上看, MGR被完虐 OLTP_WRITE_ONLY模式MGR PXC性能对比 qps还是pxc稳一些 响应时间上MGR挽回了一些颜面, 线很稳, 但原因是因为MGR关了流控 集群中有短板节点场景下的测试这里使用sysbench在220节点跑io压测, 制造短板节点, 在进行一轮测试. OLTP_WRITE_ONLY模式不同流控阈值下MGR表现 可以看到,与没有短板时不同的是关闭流控时性能明显优于其他三种流控阈值场景 但随着并发数增加, 关闭流控时波谷越来越大, 其他三种流控阈值场景的qps也越来越低 响应时间方面 ,16线程时, 关闭流控响应时间却比其他三种流控阈值场景都高, 之后随着并发增大, 可以看到关闭流控后响应时间非常稳定, 而其他三种流控阈值场景响应时间基本在1s左右 对于复制延迟, 随着流控阈值越来越大, 延迟涨幅也越来越大 128线程时qps最高, 接近10W. 有一点值得注意的是即便关了流控, 还是周期性会有一些明显的波谷, 原因还不确定可能需要针对性进一步测试 OLTP_READ_WRITE 模式MGR与PXC性能对比 可看到,有短板的场景下, PXC的qps略低于MGR, 但是抖动较小, 当然最平稳的还是关闭流控的MGR PXC响应时间略高于关闭了流控的MGR 复制延迟方面, 不是PXC性能好, 是PXC流控限制了qps, 所以没延迟 OLTP_WRITE_ONLY模式MGR PXC性能对比基本和read write差不多了 结论? 可能是伪结论 只有在有明显短板时, 关闭了流控的MGR性能优于PXC. 但是在这种情况下关闭流控, 短板节点就不能作为读节点了, 复制延迟越来越大, 优点是不会因为短板而影响整个集群. 而PXC如果遇到短板节点影响了整个集群的节点怎么办呢?只能关闭这个拖后腿的节点了. 当问题解决想要再次重新加入集群时就可能需要做SST 性能方面是上面的情况, 那么对于运维方面呢? 优秀的数据库因当时稳定的, 可控的, 可诊断的. 从这三点来看: 稳定: PXC有年头了, 有很多案例了. MGR还需要时间验证 可控: 对流控的处理, MGR显然优于PXC, 可以关闭流控 增加节点, DBA只需拿一个my.cnf启动新节点指定doner即可, 剩下的事交给PXC自己去做SST. 而对于MGR, 需要DBA自己去拿备份恢复(好像也不能明确指定一个doner?) 可诊断: MGR(5.7)的相关视图,status的还太少, 比如触发流控时,是没有status或日志信息的 总之, 还需要再多测测. 本次测试的数据原始数据和图表自取: 链接:https://pan.baidu.com/s/1WwXjG34uxN-eZG42bwRWXQ 密码:nu0m","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"性能测试","slug":"性能测试","permalink":"http://fuxkdb.com/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"}]},{"title":"MySQL Group Replication- understanding Flow Control[译文]","slug":"MySQL Group Replication- understanding Flow Control[译文]","date":"2018-08-26T15:05:00.000Z","updated":"2018-08-26T15:06:46.000Z","comments":true,"path":"2018/08/26/MySQL Group Replication- understanding Flow Control[译文]/","link":"","permalink":"http://fuxkdb.com/2018/08/26/MySQL%20Group%20Replication-%20understanding%20Flow%20Control[%E8%AF%91%E6%96%87]/","excerpt":"","text":"MySQL Group Replication: understanding Flow Control使用MySQL Group Replication时，某些成员可能落后于该组。 由于负载，硬件限制等原因…这种滞后可能会导致不能保持良好的性能认证行为和不能尽可能降低认证失败次数.应用队列(applying queue)越大, 与尚未应用的事务发生冲突的风险就越大（这在Multi-Primary Groups上是有问题的）。 Galera用户已经熟悉这个概念(译注: 指流控的概念). MySQL Group Replication的实现与Galera有两个主要不同点: the Group is never totally stalled(译注: Group不会彻底停止接收写请求) the node having issues doesn’t send flow control messages to the rest of the group asking for slowing down(译注: 短板节点不会向Group的其他节点发送flow control消息来要求集群减速) 实际上，该组的每个成员都会向其他成员发送有关其队列（applier queue and certification queue）的一些统计信息。 然后，如果每个节点意识到一个节点达到了其中一个队列的阈值，则决定是否减速： 12345group_replication_flow_control_applier_threshold (default is 25000)group_replication_flow_control_certifier_threshold (default is 25000) 因此，如果在节点上将group_replication_flow_control_mode设置为QUOTA，并且看到该集群的其他成员之一落后（达到阈值），则会将写入操作限制为最小配额。 此配额是根据最后一秒中应用的transactions数计算的，然后通过减去上一个周期的“over the quota”消息来减少。 and then it is reduced below that by subtracting the “over the quota” messages from the last period. 这意味着与Galera相反，Galera的(flow control)阈值是有短板节点决定的，对于我们在MySQL组复制中，编写事务的节点(译注: 写节点)检查其阈值流量控制值并将它们与来自其他节点的统计数据进行比较以决定是否进行flow control。 您可以在Vitor的文章Zooming-in on Group Replication Performance一文中找到有关组复制流控制的更多信息","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"}]},{"title":"如何确定Single-Primary模式下的MGR主节点","slug":"如何确定Single-Primary模式下的MGR主节点","date":"2018-08-25T15:00:00.000Z","updated":"2018-08-25T15:05:16.000Z","comments":true,"path":"2018/08/25/如何确定Single-Primary模式下的MGR主节点/","link":"","permalink":"http://fuxkdb.com/2018/08/25/%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9ASingle-Primary%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84MGR%E4%B8%BB%E8%8A%82%E7%82%B9/","excerpt":"","text":"如何确定Single-Primary模式下的MGR主节点(文档 ID 2214438.1)MySQL 5.7可以通过global status group_replication_primary_member确定123456789mysql&gt; SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = &#x27;group_replication_primary_member&#x27;;+--------------------------------------+| VARIABLE_VALUE |+--------------------------------------+| 9d7f8c28-c02c-11e6-9829-08002715584a |+--------------------------------------+1 row in set (0.00 sec) 如果是Multi-Primary默认则结果为空 可以结合performance_schema.replication_group_members表 获取主机名和端口信息:1234567891011121314151617SELECT MEMBER_HOST, MEMBER_PORTFROM performance_schema.replication_group_membersWHERE MEMBER_ID = (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = &#x27;group_replication_primary_member&#x27;);+-------------+-------------+| MEMBER_HOST | MEMBER_PORT |+-------------+-------------+| ol7 | 3306 |+-------------+-------------+1 row in set (0.00 sec)或者获取全部成员信息:123456789101112131415161718192021SELECT MEMBER_ID, MEMBER_HOST, MEMBER_PORT, MEMBER_STATE, IF(global_status.VARIABLE_NAME IS NOT NULL, &#x27;PRIMARY&#x27;, &#x27;SECONDARY&#x27;) AS MEMBER_ROLEFROM performance_schema.replication_group_members LEFT JOIN performance_schema.global_status ON global_status.VARIABLE_NAME = &#x27;group_replication_primary_member&#x27; AND global_status.VARIABLE_VALUE = replication_group_members.MEMBER_ID;+--------------------------------------+-------------+-------------+--------------+-------------+| MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE | MEMBER_ROLE |+--------------------------------------+-------------+-------------+--------------+-------------+| 9d7f8c28-c02c-11e6-9829-08002715584a | ol7 | 3306 | ONLINE | PRIMARY || f2bbb11d-c0c4-11e6-98ec-08002715584a | ol7 | 3308 | ONLINE | SECONDARY || f5bb7d78-c02c-11e6-9c56-08002715584a | ol7 | 3307 | ONLINE | SECONDARY |+--------------------------------------+-------------+-------------+--------------+-------------+3 rows in set (0.00 sec) MySQL 8.0.2 and LaterMySQL 8.0.2开始, Performance Schema被扩展123456789mysql&gt; SELECT MEMBER_HOST, MEMBER_PORT FROM performance_schema.replication_group_members WHERE MEMBER_ROLE = &#x27;PRIMARY&#x27;;+-------------+-------------+| MEMBER_HOST | MEMBER_PORT |+-------------+-------------+| ol7 | 3306 |+-------------+-------------+1 row in set (0.00 sec)或者获取全部成员信息:12345678910mysql&gt; SELECT MEMBER_ID, MEMBER_HOST, MEMBER_PORT, MEMBER_STATE, MEMBER_ROLE, MEMBER_VERSION FROM performance_schema.replication_group_members;+--------------------------------------+-------------+-------------+--------------+-------------+----------------+| MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE | MEMBER_ROLE | MEMBER_VERSION |+--------------------------------------+-------------+-------------+--------------+-------------+----------------+| 33565eab-731f-11e7-8e94-08002715584a | ol7 | 3307 | ONLINE | SECONDARY | 8.0.2 || 86034b0a-731f-11e7-9f33-08002715584a | ol7 | 3308 | ONLINE | SECONDARY | 8.0.2 || a45d3804-731e-11e7-9003-08002715584a | ol7 | 3306 | ONLINE | PRIMARY | 8.0.2 |+--------------------------------------+-------------+-------------+--------------+-------------+----------------+3 rows in set (0.00 sec) Referenceshttps://dev.mysql.com/doc/refman/en/group-replication-find-primary.htmlhttps://dev.mysql.com/doc/refman/en/group-replication-options.html#sysvar_group_replication_single_primary_modehttps://dev.mysql.com/doc/refman/en/replication-group-members-table.htmlhttp://mysqlhighavailability.com/group-replication-extending-group-replication-performance_schema-tables/http://lefred.be/content/mysql-group-replication-who-is-the-primary-master-updated/","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MGR","slug":"MGR","permalink":"http://fuxkdb.com/tags/MGR/"}]},{"title":"执行sql文件限制频率避免流控","slug":"执行sql文件限制频率避免流控","date":"2018-08-20T15:04:00.000Z","updated":"2018-08-20T15:05:06.000Z","comments":true,"path":"2018/08/20/执行sql文件限制频率避免流控/","link":"","permalink":"http://fuxkdb.com/2018/08/20/%E6%89%A7%E8%A1%8Csql%E6%96%87%E4%BB%B6%E9%99%90%E5%88%B6%E9%A2%91%E7%8E%87%E9%81%BF%E5%85%8D%E6%B5%81%E6%8E%A7/","excerpt":"","text":"避免流控对于pxc, 为了避免流控, 可以在导入.sql文件时, 先对文件做处理1awk &#x27;1;NR%1000==0&#123;print &quot;select sleep(1);&quot;&#125;&#x27; xxx.sql &gt; xxx_dba.sql上面的命令每1000行增加一行select sleep(1);, 这样执行频率是1k/s 对于mysqldump产生的sql文件, 则需要在导出是指定1mysqldump --skip-extended-insert每行一个insert语句, 之后再使用awk处理 使用pt-fifo-split 12345678910111213141516171819FLAT_FILE=&quot;/tmp/big_file.txt&quot;FIFO_PATH=&quot;$&#123;FLAT_FILE&#125;.fifo&quot;LOAD_FILE=&quot;$&#123;FLAT_FILE&#125;.load&quot;CHUNK_SIZE=1000 # Split the filept-fifo-split --force --lines $&#123;CHUNK_SIZE&#125; $&#123;FLAT_FILE&#125; --fifo $&#123;FIFO_PATH&#125; &amp;# Sleep 10 seconds to assure $&#123;FIFO_PATH&#125; exists before entering loopsleep 10while [ -e $&#123;FIFO_PATH&#125; ]do # Write chunk to disk cat $&#123;FIFO_PATH&#125; &gt; $&#123;LOAD_FILE&#125; # Load chunk into table mysql --database=test \\ --show-warnings \\ -vv &lt; $&#123;LOAD_FILE&#125; sleep 1done","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"Write-set Cache(GCache)","slug":"Write-set Cache(GCache)","date":"2018-06-02T05:23:00.000Z","updated":"2018-06-15T09:26:09.000Z","comments":true,"path":"2018/06/02/Write-set Cache(GCache)/","link":"","permalink":"http://fuxkdb.com/2018/06/02/Write-set%20Cache(GCache)/","excerpt":"","text":"Write-set Cache(GCache)Galear Cluster将write-sets存储在一个称为Write-set Cache(或称为GCache)的特殊的cache中. GCache cache is a memory allocator for write-sets.主要目的是为了最大限度地减少RAM上的write-setfootprint. Galera集群通过将卸载写入集存储到磁盘来改善此问题. GCache采用三种类型的存储： Permanent In-Memory Store Here write-sets allocate using the default memory allocator for the operating system. This is useful in systems that have spare RAM. The store has a hard size limit. By default it is disabled. 这个就是说可以用操作系统的内存空间, 对于有空闲内存的系统比较合适. 这个空间大小有一个硬性的限制, 貌似是这个参数gcache.mem_size=0[^1] 默认不使用内存. Permanent Ring-Buffer File Here write-sets pre-allocate to disk during cache initialization. This is intended as the main write-set store. 这一个块循环使用的区域, 是写到磁盘上的 , 貌似是这个参数gcache.size = 128M^2 On-Demand Page Store 这里写集根据需要在运行时分配给内存映射的页面文件。 默认情况下，其大小为128Mb(gcache.page_size[^3])，但如果需要存储更大的写入集，则可能会更大。页面存储的大小受可用磁盘空间的限制。默认情况下，Galera Cluster会在不使用时删除页面文件，但您可以对要保留的页面文件的总大小设置限制。 When all other stores are disabled, at least one page file remains present on disk. Galera集群使用分配算法，尝试按上述顺序存储写入集。也就是说，它首先尝试使用永久性内存存储。如果写入集没有足够的空间，它将尝试存储到永久环形缓冲区文件。页面存储总是成功，除非写入集大于可用磁盘空间。 Galera Cluster uses an allocation algorithm that attempts to store write-sets in the above order. That is, first it attempts to use permanent in-memory store. If there is not enough space for the write-set, it attempts to store to the permanent ring-buffer file. The page store always succeeds, unless the write-set is larger than the available disk space. 默认情况下，写集缓存分配进程工作目录中的文件。您可以使用gcache.dir参数指定写入集缓存的专用位置. [^1]: Deprecated in 5.6.22-25.8 . This variable has been deprecated and shouldn’t be used as it could cause a node to crash. 当节点接收状态传输时，它们不能处理传入的写集，直到它们完成状态更新。在某些方法下，发送状态转移的节点也被阻塞。为防止数据库进一步落后，GCache将内存映射文件中的传入写入集保存到磁盘。 此参数定义要为当前环形缓冲区存储分配的磁盘空间量。节点在启动数据库服务器时分配此空间。 [^3]: This variable can be used to specify the size of the page files in the page storage.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"}]},{"title":"Want IST Not SST for Node Rejoins? We Have a Solution!","slug":"gcache.freeze_purge_at_seqno","date":"2018-06-02T04:23:00.000Z","updated":"2018-06-15T09:26:20.000Z","comments":true,"path":"2018/06/02/gcache.freeze_purge_at_seqno/","link":"","permalink":"http://fuxkdb.com/2018/06/02/gcache.freeze_purge_at_seqno/","excerpt":"","text":"Want IST Not SST for Node Rejoins? We Have a Solution!Krunal Bauskar | February 13, 2018 | Posted In: High-availability, MySQL, Percona XtraDB Cluster 如果我们告诉你，有一种确定的方法可以让节点rejoin使用IST而不是SST？您可以保证新节点使用IST重新加入. 听起来很有趣？请继续阅读. 通常当一个节点脱离集群一段时间(处于维护目的或就是shutdown了), 集群上其他节点的gcache将用来在前者重新加入集群时提供前者在脱离期间缺失的write-set(s). 如果您配置了较大的gcache, 或downtime足够段, 则此方法可行. 对于生产环境来说, 无论是设置较大的gcache或者缩短停机窗口都不够好. 在停机之前在潜在的DONOR节点上重新配置gcache需要关闭节点.(gcache不能动态调整大小), Restoring it back to original size needs another shutdown. So “three shutdowns” for a single downtime. *No way …… not acceptable with busy production clusters and the possibility of more errors.* Introducing “gcache.freeze_purge_at_seqno”基于以上痛点, 我们在Percona XtraDB Cluster 5.7.20引入了gcache.freeze_purge_at_seqno.这将控制清除gcache, 从而在节点重新加入时保留更多的数据以促进IST. Galera集群世界中的所有事务都被分配了唯一的全局序列号（seqno）.跟踪事情发生使用此seqno（如wsrep_last_applied，wsrep_last_committed，wsrep_replicated，wsrep_local_cached_downto等^1）.wsrep_local_cached_downto表示gcache已被清除的序列号。假设wsrep_local_cached_downto = N，那么gcache具有来自[N，wsrep_replicated]的数据, 并清除了[1，N)数据。 gcache.freeze_purge_at_seqno takes three values: 1. -1(默认值): no freeze, the purge operates as normal. 2. **x (should be valid seqno in gcache):** freeze purge of write-sets &gt;= x. The best way to select x is to use the wsrep_last_applied value as an indicator from the node that you plan to shut down. (wsrep_applied * 0.09. Retain this extra 10% to trick the [safety gap heuristic algorithm of IST](https://www.percona.com/blog/2017/11/15/understanding-ist-donor-selected/).) 3. **now:** freeze purge of write-sets &amp;gt;= smallest seqno currently in gcache. Instant freeze of gcache-purge. (If tracing x (above) is difficult, simply use “now” and you are good). 在集群的现有节点上进行设置（这将继续作为集群的一部分，并可以充当潜在的捐助者）。该节点继续保留写集，从而允许重启节点使用IST重新加入。 （您可以在重启需要rejoin的节点时通过指定—wsrep_sst_donor将该节点作为首选DONOR进行提供） Set this on an existing node of the cluster (that will continue to be part of the cluster and can act as potential DONOR). This node continues to retain the write-sets, thereby allowing the restarting node to rejoin using IST. (You can feed the said node as a preferred DONOR through wsrep_sst_donor while restarting the said rejoining node.) 请记住，一旦节点重新加入，请将其设回-1。这可以避免超出上述时间表的DONOR上的占用空间。在下一个清除周期中，所有旧的保留写入集也会被释放（回收空间回到原始状态）. Note: 12345To find out existing value of gcache.freeze_purge_at_seqno query wsrep_provider_options.select @@wsrep_provider_options;To set gcache.freeze_purge_at_seqnoset global wsrep_provider_options=&quot;gcache.freeze_purge_at_seqno = now&quot;; Why should you use it? gcache动态增长（使用现有的页面存储机制），并在用户将其设置回-1时收缩。这意味着您只在需要时使用(更多的)磁盘空间. 不需要重新启动. 用户只需专注于需要维护的节点. No complex math or understanding of seqno involved (simply use “now”). Less prone to error, as SST is one of the major error-prone areas with the cluster. So why wait? Give it a try! It is part of Percona XtraDB Cluster 5.7.20 onwards, and helps you get IST not SST for node rejoins Note: If you need more information about gcache, check here and here","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"}]},{"title":"[译]PXC7中故障场景及恢复方法","slug":"PXC7种故障场景及恢复方法","date":"2018-05-29T04:23:00.000Z","updated":"2018-06-08T13:35:54.000Z","comments":true,"path":"2018/05/29/PXC7种故障场景及恢复方法/","link":"","permalink":"http://fuxkdb.com/2018/05/29/PXC7%E7%A7%8D%E6%95%85%E9%9A%9C%E5%9C%BA%E6%99%AF%E5%8F%8A%E6%81%A2%E5%A4%8D%E6%96%B9%E6%B3%95/","excerpt":"与标准的MySQL复制不同，PXC群集就像一个逻辑实体，它负责关注每个节点的状态和一致性以及群集状态。这样可以保持更好的数据完整性，然后您可以从传统的异步复制中获益，同时允许在同一时间在多个节点上进行安全写入. 假设我们有一个PXC集群包含3个节点 情景1 节点A正常停止, 例如需要停库做一些维护, 配置变更等操作. 在这种情况下，其他节点从该节点接收”good bye”消息, 因此集群大小将会缩小(在这个例子中缩小为2), 并且某些属性如quorum caculation和auto increment(我理解为auto_increment_increment和auto_increment_offset会由于集群扩缩动态调整). 一旦我们重新开始的一个节点，它会根据它的my.cnf中wsrep_cluster_address设置加入群集. 这个过程与普通的复制有很大的不同 - 在A节点再次与集群完全同步之前，A节点不会接受供任何请求，因此仅仅是与集群建立连接是不够的，而是必须要先完成state transfer. 如果B或C节点的writeset cache (gcache.size), 仍然有恢复A节点所需的所有事务, 那么将使用IST 否则使用 SST . 因此，如本文所示，确定最佳donor很重要. 如果由于donor的gcache中缺少交易而导致IST不可用，则由donor作出回退决定，而SST自动启动。","text":"与标准的MySQL复制不同，PXC群集就像一个逻辑实体，它负责关注每个节点的状态和一致性以及群集状态。这样可以保持更好的数据完整性，然后您可以从传统的异步复制中获益，同时允许在同一时间在多个节点上进行安全写入. 假设我们有一个PXC集群包含3个节点 情景1 节点A正常停止, 例如需要停库做一些维护, 配置变更等操作. 在这种情况下，其他节点从该节点接收”good bye”消息, 因此集群大小将会缩小(在这个例子中缩小为2), 并且某些属性如quorum caculation和auto increment(我理解为auto_increment_increment和auto_increment_offset会由于集群扩缩动态调整). 一旦我们重新开始的一个节点，它会根据它的my.cnf中wsrep_cluster_address设置加入群集. 这个过程与普通的复制有很大的不同 - 在A节点再次与集群完全同步之前，A节点不会接受供任何请求，因此仅仅是与集群建立连接是不够的，而是必须要先完成state transfer. 如果B或C节点的writeset cache (gcache.size), 仍然有恢复A节点所需的所有事务, 那么将使用IST 否则使用 SST . 因此，如本文所示，确定最佳donor很重要. 如果由于donor的gcache中缺少交易而导致IST不可用，则由donor作出回退决定，而SST自动启动。 情景2 节点A和B正常停止. 与之前的情况类似, 集群size缩小至1, 因此即使单个剩余的节点C也是主要组件并且正在服务于客户端请求. 为了让节点回到集群中，你只需要启动它们. 然而，节点C将被切换到“Donor/Desynced”状态，因为它将不得不向至少第一加入节点提供state transfer。在这个过程中，它仍然可以读/写，但它可能会慢得多，这取决于它需要发送多大的state transfer. Also some load balancers may consider the donor node as not operational and remove it from the pool. So it is best to avoid situation when only one node is up. 但请注意，如果您按顺序重新启动A然后B，则可能需要确保B不会使用A作为状态转移捐助者，因为A可能没有在它的gcache中包含所有需要的写入集。因此，只需以这种方式将C节点指定为捐助者即可（“nodeC”名称是您使用wsrep_node_name变量指定的名称）： 1service mysql start --wsrep_sst_donor=nodeC 情景3 三个节点都正常停止. 整个集群已经不可用了. 现在的问题是, 如何重新initialize集群. 有一个关键点需要知道的是, 在clean shutdown的过程中, PXC节点会将自己最后执行的位置记录到grastate.dat文件中. 通过对比grastate.dat中的seqno, 可以帮助我们找到the most advanced one(最有可能的是最后一个停止的节点). 集群必须 bootstrapped using this node, 换言之, seqno最大的节点需要执行full SST给其他joiner. To bootstrap the first node, invoke the startup script like this: 1/etc/init.d/mysql bootstrap-pxc 或 1service mysql bootstrap-pxc 或 1service mysql start --wsrep_new_cluster 或 1service mysql start --wsrep-cluster-address=&quot;gcomm://&quot; or in packages using systemd service manager (Centos7 at the moment): 1systemctl start mysql@bootstrap.service In older PXC versions, to bootstrap cluster, you had to edit my.cnf and replace previous wsrep_cluster_address line with empty value like this: wsrep_cluster_address=gcomm:// and start mysql normally. More details to be found here. Please note that even if you bootstrap from the most advanced node, so the other nodes have lower sequence number, they will have to still join via full-SST because the Galera Cache is not retained on restart. For that reason, it is recommended to stop writes to the cluster before it’s full shutdown, so that all nodes stop in the same position. Edit: This changes since Galera 3.19 thanks to gcache-recover option 情景 4 节点A从集群中消失(断电, 硬件故障, kernel panic, mysqld crash, kill -9 on mysqld pid, OOMkiller). B/C节点意识到与A节点连接中断, 会尝试重连. 重连超时后, both agree that node A is really down and remove it “officially” from the cluster. Quorum is saved ( 2 out of 3 nodes are up), so no service disruption happens. After restarting, A will join automatically the same way as in scenario 1. 情景5 A,B节点均从集群中消失. The node C is not able to form the quorum alone, 集群切换到non-primary模式, 拒绝所有应用请求. 在这种情景下, C节点的mysqld进程仍在运行, 你可以连接到C节点的mysql, 但是执行任何语句都将失败 12mysql&gt; select * from test.t1;ERROR 1047 (08S01): Unknown command 事实上, A,B节点消失初时, C几点仍然可也以完成查询一些请求, 但当C几点意识到无法连接A,B节点后, 就无法完成查询请求了. 而写入请求在certification based replication保障下则会在A,B消失时就立即被拒绝. This is what we are going to see in the remaining node’s log: 123456140814 0:42:13 [Note] WSREP: commit failed for reason: 3140814 0:42:13 [Note] WSREP: conflict state: 0140814 0:42:13 [Note] WSREP: cluster conflict due to certification failure for threads:140814 0:42:13 [Note] WSREP: Victim thread:THD: 7, mode: local, state: executing, conflict: cert failure, seqno: -1SQL: insert into t values (1) 然后单个节点C等待它的对等体再次出现，并且在某些情况下，如果发生这种情况，例如当网络中断和这些节点一直都在运行时，群集将自动再次形成. 同样，如果节点B和C刚刚从第一个节点断开网络，但它们仍然可以到达对方，那么它们将继续运行，因为它们仍然构成法定人数。如果A和B由于停电而崩溃（由于数据不一致，错误等）或关闭，您需要执行手动操作才能在C节点上启用primary component ，before you can bring A and B back。这样，我们告诉C节点“嘿，你现在可以单独形成一个新的群集，忘记A和B！”。执行此操作的命令是^1： 1SET GLOBAL wsrep_provider_options=&#x27;pc.bootstrap=true&#x27;; However, you should double check in order to be very sure the other nodes are really down before doing that! Otherwise, you will most likely end up with two clusters having different data. 情景6 所有节点意外宕机(All nodes went down without proper shutdown procedure) 这种情况发生在例如数据中心挂了, 或者遇上了MySQL或Galera bug导致所有节点crash. 但也由于数据一致性的影响，集群检测到每个节点都有不同的数据导致所有节点挂了. In each of those cases, the grastate.dat file is not updated and does not contain valid sequence number (seqno). It may look like this: 123456cat /var/lib/mysql/grastate.dat# GALERA saved stateversion: 2.1uuid: 220dcdcb-1629-11e4-add3-aec059ad3734seqno: -1cert_index: 在这种情况下，我们不确定所有节点是否彼此一致，因此找到最先进(most advanced one )的节点以便使用它来boostrap群集至关重要。在任何节点上启动mysql守护进程之前，您必须通过检查它的事务状态来提取最后一个序列号。你可以这样做： 123456[root@percona3 ~]# mysqld_safe --wsrep-recover140821 15:57:15 mysqld_safe Logging to &#x27;/var/lib/mysql/percona3_error.log&#x27;.140821 15:57:15 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql140821 15:57:15 mysqld_safe WSREP: Running position recovery with --log_error=&#x27;/var/lib/mysql/wsrep_recovery.6bUIqM&#x27; --pid-file=&#x27;/var/lib/mysql/percona3-recover.pid&#x27;140821 15:57:17 mysqld_safe WSREP: Recovered position 4b83bbe6-28bb-11e4-a885-4fc539d5eb6a:2140821 15:57:19 mysqld_safe mysqld from pid file /var/lib/mysql/percona3.pid ended 因此，此节点上最后提交的事务序列号为2.现在，您只需首先从最新节点引导，然后启动其他节点。 So the last committed transaction sequence number on this node was 2. Now you just need to bootstrap from the latest node first and then start the others. However, the above procedure won’t be needed in the recent Galera versions (3.6+?), available since PXC 5.6.19. There is a new option – pc.recovery (enabled by default), which saves the cluster state into a file named gvwstate.dat on each member node. As the variable name says (pc – primary component), it saves only a cluster being in PRIMARY state. An example content of that file may look like this: 123456789cat /var/lib/mysql/gvwstate.datmy_uuid: 76de8ad9-2aac-11e4-8089-d27fd06893b9#vwbegview_id: 3 6c821ecc-2aac-11e4-85a5-56fe513c651f 3bootstrap: 0member: 6c821ecc-2aac-11e4-85a5-56fe513c651f 0member: 6d80ec1b-2aac-11e4-8d1e-b2b2f6caf018 0member: 76de8ad9-2aac-11e4-8089-d27fd06893b9 0#vwend We can see three node cluster above with all members being up. Thanks to this new feature, in the case of power outage in our datacenter, after power is back, the nodes will read the last state on startup and will try to restore primary component once all the members again start to see each other. This makes the PXC cluster to automatically recover from being powered down without any manual intervention! In the logs we will see: 1234567891011121314151617181920212223242526272829303132333435363738140823 15:28:55 [Note] WSREP: restore pc from disk successfully(...)140823 15:29:59 [Note] WSREP: declaring 6c821ecc at tcp://192.168.90.3:4567 stable140823 15:29:59 [Note] WSREP: declaring 6d80ec1b at tcp://192.168.90.4:4567 stable140823 15:29:59 [Warning] WSREP: no nodes coming from prim view, prim not possible140823 15:29:59 [Note] WSREP: New COMPONENT: primary = no, bootstrap = no, my_idx = 2, memb_num = 3140823 15:29:59 [Note] WSREP: Flow-control interval: [28, 28]140823 15:29:59 [Note] WSREP: Received NON-PRIMARY.140823 15:29:59 [Note] WSREP: New cluster view: global state: 4b83bbe6-28bb-11e4-a885-4fc539d5eb6a:11, view# -1: non-Primary, number of nodes: 3, my index: 2, protocol version -1140823 15:29:59 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.140823 15:29:59 [Note] WSREP: promote to primary component140823 15:29:59 [Note] WSREP: save pc into disk140823 15:29:59 [Note] WSREP: New COMPONENT: primary = yes, bootstrap = yes, my_idx = 2, memb_num = 3140823 15:29:59 [Note] WSREP: STATE EXCHANGE: Waiting for state UUID.140823 15:29:59 [Note] WSREP: clear restored view(...)140823 15:29:59 [Note] WSREP: Bootstrapped primary 00000000-0000-0000-0000-000000000000 found: 3.140823 15:29:59 [Note] WSREP: Quorum results:version = 3,component = PRIMARY,conf_id = -1,members = 3/3 (joined/total),act_id = 11,last_appl. = -1,protocols = 0/6/2 (gcs/repl/appl),group UUID = 4b83bbe6-28bb-11e4-a885-4fc539d5eb6a140823 15:29:59 [Note] WSREP: Flow-control interval: [28, 28]140823 15:29:59 [Note] WSREP: Restored state OPEN -&gt; JOINED (11)140823 15:29:59 [Note] WSREP: New cluster view: global state: 4b83bbe6-28bb-11e4-a885-4fc539d5eb6a:11, view# 0: Primary, number of nodes: 3, my index: 2, protocol version 2140823 15:29:59 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.140823 15:29:59 [Note] WSREP: REPL Protocols: 6 (3, 2)140823 15:29:59 [Note] WSREP: Service thread queue flushed.140823 15:29:59 [Note] WSREP: Assign initial position for certification: 11, protocol version: 3140823 15:29:59 [Note] WSREP: Service thread queue flushed.140823 15:29:59 [Note] WSREP: Member 1.0 (percona3) synced with group.140823 15:29:59 [Note] WSREP: Member 2.0 (percona1) synced with group.140823 15:29:59 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 11)140823 15:29:59 [Note] WSREP: Synchronized with group, ready for connections 这个是说PXC 5.6.19以后, 每个节点都有一个gvwstate.dat 文件记录自己挂之前的执行进度, 我的实验kill一个节点后,这个节点 12345678910[root@namenode-2 data]# cat gvwstate.dat my_uuid: 4db0c1c2-5fe0-11e8-9709-0be9bc3897c1#vwbegview_id: 3 3561356d-5f11-11e8-bba0-82ddca1badfc 16bootstrap: 0member: 3561356d-5f11-11e8-bba0-82ddca1badfc 0member: 4db0c1c2-5fe0-11e8-9709-0be9bc3897c1 0member: 57b4144b-5f11-11e8-b442-47001a216bae 0member: 6af280fd-5f0f-11e8-9deb-9fe0e283dd49 0#vwend 其他正常节点 123456789[root@datanode-1 data]# cat gvwstate.dat my_uuid: 6af280fd-5f0f-11e8-9deb-9fe0e283dd49#vwbegview_id: 3 3561356d-5f11-11e8-bba0-82ddca1badfc 17bootstrap: 0member: 3561356d-5f11-11e8-bba0-82ddca1badfc 0member: 57b4144b-5f11-11e8-b442-47001a216bae 0member: 6af280fd-5f0f-11e8-9deb-9fe0e283dd49 0#vwend 情景7 Cluster lost it’s primary state due to split brain situation.为了举例, 我们假设由偶数个节点形成- 6个, ABC在一个机房, DEF在另一个机房, 两个机房之间的网络中断了. 当然最好的方案是避免这种拓扑结构, 如果没机器了实在不行还可以用arbitrator (garbd) node或者调整pc.weight参数.但是，当分裂大脑以任何方式发生时，所有分离的组都不能维持法定人数 - 所有节点都必须停止提供请求，并且这两个部分只是不断尝试重新连接。如果要在恢复网络链接之前恢复服务，则可以使用与方案5中相同的命令使其中一个组再次成为主服务器： 1SET GLOBAL wsrep_provider_options=&#x27;pc.bootstrap=true&#x27;; After that, you are able to work on the manually restored part of the cluster, and the second half should be able to automatically re-join using incremental state transfer (IST) once the network link is restored. But beware: if you set the bootstrap option on both the separated parts, you will end up with two living cluster instances, with data likely diverging away from each other. Restoring network link in that case won’t make them to re-join until nodes are restarted and try to re-connect to members specified in configuration file. Then, as Galera replication model truly cares about data consistency – once the inconsistency will be detected, nodes that cannot execute row change statement due to different data – will perform emergency shutdown and the only way to bring them back to the cluster will be via full SST. I hope I covered most of the possible failure scenarios of Galera-based clusters, and made the recovery procedures bit more clear. https://www.percona.com/blog/2014/09/01/galera-replication-how-to-recover-a-pxc-cluster/","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PXC","slug":"PXC","permalink":"http://fuxkdb.com/tags/PXC/"}]},{"title":"如何做好MySQL的备份","slug":"如何做好MySQL的备份","date":"2018-04-27T16:23:00.000Z","updated":"2018-05-03T08:37:41.000Z","comments":true,"path":"2018/04/28/如何做好MySQL的备份/","link":"","permalink":"http://fuxkdb.com/2018/04/28/%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BDMySQL%E7%9A%84%E5%A4%87%E4%BB%BD/","excerpt":"如何做好MySQL的备份物理备份还是逻辑备份?其实物理备份和逻辑备份并没有好坏之分, 关键是要适合你的场景. 两种备份方式的备份工具有: 逻辑备份工具: [mysqldump, mysqlpump, mydumper, select into out file] 物理备份工具: [xtrabackup, TokuBackup, Tokudb-xtrabackup] 两种备份方式的对比如下 备份速度 物理备份比逻辑备份快吗? 不要想当然, 至少我的测试结果并不是这样 恢复速度 物理备份恢复实际就是mv操作(使用xtrabackup,在备份机做prepare), 而逻辑备份则是漫长的导入. 同机器 1.5T库, 逻辑备份大小151G, 做恢复需要27小时左右, 而物理备份恢复则完全取决于磁盘iops, 不用测也知道要比逻辑备份快很多 备份集大小 实际测试1T的库(大部分为InnoDB表), 逻辑备份集大小为46G, 而物理备份为255G 根据以上三点, 就可以选择备份方式了吗? 我认为不能. 还有一点是数据库服务器和备份服务器之间的网络情况, 和你期望的恢复时间是多久","text":"如何做好MySQL的备份物理备份还是逻辑备份?其实物理备份和逻辑备份并没有好坏之分, 关键是要适合你的场景. 两种备份方式的备份工具有: 逻辑备份工具: [mysqldump, mysqlpump, mydumper, select into out file] 物理备份工具: [xtrabackup, TokuBackup, Tokudb-xtrabackup] 两种备份方式的对比如下 备份速度 物理备份比逻辑备份快吗? 不要想当然, 至少我的测试结果并不是这样 恢复速度 物理备份恢复实际就是mv操作(使用xtrabackup,在备份机做prepare), 而逻辑备份则是漫长的导入. 同机器 1.5T库, 逻辑备份大小151G, 做恢复需要27小时左右, 而物理备份恢复则完全取决于磁盘iops, 不用测也知道要比逻辑备份快很多 备份集大小 实际测试1T的库(大部分为InnoDB表), 逻辑备份集大小为46G, 而物理备份为255G 根据以上三点, 就可以选择备份方式了吗? 我认为不能. 还有一点是数据库服务器和备份服务器之间的网络情况, 和你期望的恢复时间是多久 选择什么样的备份取决于你期望的MTTR值和你的数据库体积以目前我管理的数据库为例, 有三台备份服务器用于存储备份, 我们的库都是建在ECS上, 各种云都有.如果数据库服务器和备服务器可以通过内网来传输备份, 那么大概100多G的备份要传2小时(大概100M/s), 如果你的库就是这么大, 那么无论你选择哪种方式备份, 都做不到快速恢复. 更别提很多数据库服务器和备份服务器还只能用外网传输备份了 如果可以严格控制数据库大小, 同时据库服务器和备服务器间使用万兆网卡传输备份, 那么无疑, 物理备份是最好的, 因为恢复快 如果无法控制数据库大小(不讨论为什么无法控制), 同时据库服务器和备服务器间网络垃圾, 那么任何一种备份方式都无法做到快速恢复, 此时优先考虑使用备份集较小的方式进行备份. 无论那种备份方式, 都不是用来做故障恢复的, 出故障了请做切换, 提前做好高可用, 这些备份是用来搭建从库, 恢复部分数据 和 做数据的最后一道保障用的. 选择好了备份方式, 如和用好/避免踩坑?开源的东西, 没有什么是100%靠谱的, 就在刚才, Mydumper作者还处理了我提的一个issue 现在网上大部分的mydumper文档都是错的, 因为官方README就是错的, 并且他们眼高手低没有实际测试 ​ 无论使用哪种备份方式请先搞懂备份原理, 自己开General Log观察分析, 基本上这样已经能研究个八九十了, 然后可以看大神的文章:mysqldump与innobackupex备份过程知多少（一）mysqldump与innobackupex备份过程知多少（二）mysqldump与innobackupex备份过程知多少（三）mysqldump与innobackupex备份过程知多少（完结篇） 逻辑备份深坑! mysqldump 不锁非事务表如果你们数据库有很多MyISAM表, 那真的是不应该啊, 已经是被时代抛弃的引擎了mysqldump在备份MyISAM表时, 是不锁DML的 ,也就是说如果备份期间对MyISAM表有DML操作的话, 那么整个备份集是不能保证一致性的, 详见mysqldump与mysqldump与innobackupex备份过程知多少（三）中的坑一 备份期间当心DDL这个坑使用官方发型版本的MySQL数据库时, 无论使用xtrabackup还是mysqldump还是mydumper都有可能发生具体就是使用START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */;语句显式开启一个事务之后，该事务执行select之前，该表被其他会话执行了DDL之后无法查询数据 详见mysqldump与mysqldump与innobackupex备份过程知多少（三）中的坑二 A会话 123456789101112root@localhost 10:52: [test1]&gt; show create table fan;+-------+------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------+| fan | CREATE TABLE `fan` ( `id` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+-------+------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@localhost 10:52: [test1]&gt; START TRANSACTION /*!40100 WITH CONSISTENT SNAPSHOT */;Query OK, 0 rows affected (0.00 sec) B会话 执行DDL语句增加一列 123root@localhost 10:53: [test1]&gt; alter table fan add col1 varchar(10);Query OK, 0 rows affected (0.34 sec)Records: 0 Duplicates: 0 Warnings: 0 A会话再去查询数据, 报错 12root@localhost 10:52: [test1]&gt; select * from fan;ERROR 1412 (HY000): Table definition has changed, please retry transaction Xtrabackup会抛出异常日志 1[FATAL] InnoDB: An optimized(without redo logging) DDLoperation has been performed. All modified pages may not have been flushed to the disk yet. 而mysqldump和mydumper貌似不会? 忘了. 总之, 这会导致备份不完整. Percona版本通过LOCK TABLES FOR BACKUP解决了这个问题 LOCK TABLES FOR BACKUP使用新的MDL锁类型来阻止更新非事务表和所有表的和DDL语句. 更具体地说，如果有一个活动的LOCK TABLES FOR BACKUP锁，所有DDL语句以及对MyISAM，CSV，MEMORY和ARCHIVE表的所有更新将被阻止,并在PERFORMANCE_SCHEMA或PROCESSLIST以Waiting for backup lock的状态显示. 针对所有表的SELECT查询和针对InnoDB，Blackhole和Federated表的INSERT / REPLACE / UPDATE / DELETE不受LOCK TABLES FOR BACKUP的影响。Blackhole表显然与备份无关，并且Federated表被逻辑和物理备份工具忽略 使用Percona版本时 mysqldump增加了--lock-for-backup选项, 而mydumper会自动查看have_backup_locks参数来判断是否可以使用LOCK TABLES FOR BACKUP锁 1234567891011121314151617181920if (!no_locks) &#123; // Percona Backup Locks if(!no_backup_locks)&#123; mysql_query(conn,&quot;SELECT @@have_backup_locks&quot;); MYSQL_RES *rest = mysql_store_result(conn); if(rest != NULL &amp;&amp; mysql_num_rows(rest))&#123; mysql_free_result(rest); g_message(&quot;Using Percona Backup Locks&quot;); have_backup_locks=1; &#125; &#125; if(have_backup_locks)&#123; if(mysql_query(conn, &quot;LOCK TABLES FOR BACKUP&quot;)) &#123; g_critical(&quot;Couldn&#x27;t acquire LOCK TABLES FOR BACKUP, snapshots will not be consistent: %s&quot;,mysql_error(conn)); errors++; &#125; if(mysql_query(conn, &quot;LOCK BINLOG FOR BACKUP&quot;)) &#123; g_critical(&quot;Couldn&#x27;t acquire LOCK BINLOG FOR BACKUP, snapshots will not be consistent: %s&quot;,mysql_error(conn)); errors++; &#125; 当心timestamp类型字段具体见mysqldump导出注意timestamp类型 ​ SELECT INTO OUT FILE这货有什么用?一个DELETE没有写where条件的酸爽你可曾体会?你说, 我有binlog2sql大法我说, 嘻嘻, 我们还在用Statement格式悲剧的我, 还有一根稻草, 那就是SELECT INTO OUT FILE大法假设你要执行下面这更新 1update important_db.important_table set col1=&#x27;abc&#x27; where id&lt;1000 那可得先 1select id,col1 from important_db.important_table where id&lt;1000 into outfile &#x27;/tmp/fan.sql&#x27;; 真的出了问题, 可以这样恢复 12cat fan.sql | awk &#x27;&#123;print &quot;update important_db.important_table set col1=&quot;$2,&quot; where id=&quot;$1&quot;;&quot;&#125;&#x27; &gt; roll_back.sqlmysql important_db important_tableoll_back.sql TokuDB引擎表咋备份?逻辑备份mydumper,mysqldump都可以, 后者无法保证非事务表一致性物理备份, Xtrabackup不支持, TokuBackup没试过, 可以使用Tokudb-xtrabackupTokudb-xtrabackup安装不过使用Tokudb-xtrabackup备份, 基本是你的Tokudb表多大,备份就是多大了, 你有1T的 tokudb表, 物理备出来基本就是1T了…. 所以还是逻辑备份吧, 起码备份集小 重中之重, binlog备份众所周知,Binlog中记录了数据库的所有变化, 通过备份 + 重演binlog, 可以将备份恢复到任意时间点还在用rsync同步binlog吗? 你Out了在5.7版本, mysqlbinlog命令增加一个新功能, 可以伪装成从库持续同步主库的binlog文件详见Using mysqlbinlog to Back Up Binary Log Files 目前线上使用了我写的脚本做了一次封装Binlog_Server 废话这么多, 如何管理好你的备份?数据库众多, 我接手前, 备份通过crontab调用shell脚本实现, 备份完成后再发送到备份机.这样做的几点不足: 备份失败了没告警, 再写一个监控脚本监控日志输出? 太LOW 人脑真的记不住这么多备份与备份服务器的mapping关系, 每次都登录服务器去查吗? 效率太低 你能说出每个库备份多大吗? 备份用了多久? rsync用了多久? 这些指标异常能反应出什么问题 备份需要定期校验可用性, 手工校验费时费力 基于以上原因, 我做了改进, 编写了一个备份工具pybackup pybackup源自于对线上备份脚本的改进和对备份情况的监控需求.原本生产库的备份是通过shell脚本调用mydumper,之后再将备份通过rsync传输到备份机.想要获取备份状态,时间,rsync传输时间等信息只能通过解析日志.pybackup由python编写,调用mydumper和rsync,将备份信息存入数据库中,后期可以通过grafana图形化展示和监控备份pybackup 提供了validate-backup命令 ,目前我再备份服务器上都安装了MySQL, 通过定时任务, pybackup会从catalog元数据中获取需要进行校验的备份自动校验备份 备份信息一览无遗 123456789101112131415161718192021222324252627root@localhost 23:49: [catalogdb]&gt; select * from user_backup where tag=&#x27;BI_Dota_2&#x27; order by id desc limit 1\\G*************************** 1. row *************************** id: 2151 bk_id: b591ccc4-4a16-11e8-953f-005056b106c0 bk_server: xx.xx.xx.xx start_time: 2018-04-27 20:30:03 end_time: 2018-04-27 22:54:50 elapsed_time: 8687 backuped_db: BI,mysql,sys,test,test1 is_complete: Y bk_size: 116G bk_dir: /data3/backup_db/2018-04-27/b591ccc4-4a16-11e8-953f-005056b106c0/ transfer_start: 2018-04-27 22:54:51 transfer_end: 2018-04-27 23:21:27transfer_elapsed: 1596transfer_complete: Y remote_dest: platform@xx.xx.xx.xx/db_backup4/xx.xx.xx.xx/ master_status: mysql-bin.000016,63744177, slave_status: mysql-bin.002559,915074196, tool_version: mydumper 0.9.2, built against MySQL 5.5.53 server_version: 5.7.21-20-logpybackup_version: pybackup 0.10.11.0 bk_command: mydumper --password=supersecrect --user=root --socket=/data/mysqldata/3306/mysql.sock --outputdir=/data3/backup_db/2018-04-27/b591ccc4-4a16-11e8-953f-005056b106c0/ --verbose=3 --compress --threads=6 --triggers --events --routines --use-savepoints --regex=&quot;^(BI\\.|mysql\\.|sys\\.|test\\.|test1\\.)&quot; tag: BI_Dota_2 is_deleted: Nvalidate_status: N/A1 row in set (0.00 sec) 备份在哪一查就知道 123456789root@localhost 23:51: [catalogdb]&gt; select * from user_backup_path;+----+-----------------+----------------+--------------------------------------------+--------------------------------+------------+| id | bk_server | remote_server | real_path | tag | is_offline |+----+-----------------+----------------+--------------------------------------------+--------------------------------+------------+| 1 | 120.27.136.24 | 116.3.130.8 | /data1/backup/db_backup/120.27.16.247/ | 国内平台从1 | N || 2 | 101.37.164.13 | 116.3.130.8 | /data2/backup/db_backup/101.37.14.13/ | 国内平台主2 | N || 3 | 120.27.139.12 | 116.3.130.8 | /data2/backup/db_backup/120.27.19.126/ | 国内日志主1 | N |23 rows in set (0.00 sec) 恢复用时, 心里有数 12345678910111213root@localhost 23:52: [catalogdb]&gt; select * from user_recover_info limit 1\\G*************************** 1. row *************************** id: 36 bk_id: 073836be-e8e5-11e7-b163-00163e0007f1 tag: 国内sdk从1 backup_path: /data2/backup/db_backup/120.55.74.93/2017-12-25/073836be-e8e5-11e7-b163-00163e0007f1/ db: dadian start_time: 2017-12-26 13:59:13 end_time: 2017-12-26 14:20:12elapsed_time: 1258recover_status: sucessvalidate_time: 2017-12-26 16:04:041 row in set (0.00 sec) ​ pybackup还需要完善, 比如对于备份校验的改进, 并且目前也没有做对xtrabackup的支持 2018.04.27 想了下可能应该恢复完成后, 首先去做源库的从库, 同步之后看有没有报错, 再完善点, 则是要做pt-table-checksum校验, 最近没时间, 后面再做吧 我们如何备份?大拿如何备份?说了这么多, 那么我们现在是如何备份的呢? 目前线上用pybackup调用mydumper, 相比xtrabackup备份集小, 相比mysqldump一致性更有保证, 速度更快(mydumper可以并发,mysqldump只能单线程) 参加过一次技术大会, 印象中Facebook 是使用mysqldump备份的, 为什么? 这里要说道一下Facebook有五个足球场大的机房?(可能记错了) , 总之数据库众多, 他们更在意备份集得大小(高可用玩的6, 存储要钱啊)Facebook 的DBA严格限制单个数据库的大小为200G, 这样mysqldump备份时间快, 同时易于管理. Facebook DBA改进了备份方案, 他们大致是周一一个mysqldump全备, 之后每天备份binlog, 并分析binlog对其进行合并压缩举个例子: 周一建了一个表t1, 周二删掉了, 或者周一insert t2的数据A, 周二又delete掉了 那么对于恢复到周二的需求, 就可以将binlog日志就合并, 省略掉t1表相关的所有语句, 和对t2 insert delete A记录的日志, 通过处理的日志, 在备份上应用, 直接通过程序构造一份周二的备份我只能说, 牛批!🐂🐂🐂 如果对此文章有何疑问或改进建议, 欢迎交流","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"Load data奇怪的问题","slug":"Load-data奇怪的问题","date":"2018-03-03T07:26:00.000Z","updated":"2018-05-03T08:38:16.000Z","comments":true,"path":"2018/03/03/Load-data奇怪的问题/","link":"","permalink":"http://fuxkdb.com/2018/03/03/Load-data%E5%A5%87%E6%80%AA%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"Load data奇怪的问题公司一套所谓的BI库(5.5 innodb引擎),由于太大了,考虑转用TokuDB引擎,最近在测试,直接在一个新环境安装Percona5.7.21,然后恢复备份,转换为TokuDB表, 再做从库和原5.5库同步(5.5 是statement格式)准备同步上,然后这套环境部署一下应用,再全面的测一下.结果发现同步报错了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960root@localhost 10:42: [test]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: xx.xx.xx.xx Master_User: slave Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.010346 Read_Master_Log_Pos: 715054772 Relay_Log_File: mysql-relay.000010 Relay_Log_Pos: 376792000 Relay_Master_Log_File: mysql-bin.010288 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.% Last_Errno: 1300 Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.010288, end_log_pos 378517485. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Skip_Counter: 0 Exec_Master_Log_Pos: 376791811 Relay_Log_Space: 74984691638 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1300 Last_SQL_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.010288, end_log_pos 378517485. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Replicate_Ignore_Server_Ids: Master_Server_Id: 12 Master_UUID: Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 180402 00:07:09 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: a3f4b929-31a0-11e8-9714-f8bc123346cc:1-864 Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)","text":"Load data奇怪的问题公司一套所谓的BI库(5.5 innodb引擎),由于太大了,考虑转用TokuDB引擎,最近在测试,直接在一个新环境安装Percona5.7.21,然后恢复备份,转换为TokuDB表, 再做从库和原5.5库同步(5.5 是statement格式)准备同步上,然后这套环境部署一下应用,再全面的测一下.结果发现同步报错了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960root@localhost 10:42: [test]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: xx.xx.xx.xx Master_User: slave Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.010346 Read_Master_Log_Pos: 715054772 Relay_Log_File: mysql-relay.000010 Relay_Log_Pos: 376792000 Relay_Master_Log_File: mysql-bin.010288 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.% Last_Errno: 1300 Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.010288, end_log_pos 378517485. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Skip_Counter: 0 Exec_Master_Log_Pos: 376791811 Relay_Log_Space: 74984691638 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1300 Last_SQL_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.010288, end_log_pos 378517485. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Replicate_Ignore_Server_Ids: Master_Server_Id: 12 Master_UUID: Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 180402 00:07:09 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: a3f4b929-31a0-11e8-9714-f8bc123346cc:1-864 Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)查看告警日志12342018-04-02T00:06:53.815565+08:00 5 [Note] Multi-threaded slave statistics for channel &#x27;&#x27;: seconds elapsed = 151; events assigned = 7924737; worker queues filled over overrun level = 0; waited due a Worker queue full = 0; waited due the total size = 375; waited at clock conflicts = 0 waited (count) when Workers occupied = 0 waited when Workers occupied = 02018-04-02T00:07:09.876177+08:00 6 [ERROR] Slave SQL for channel &#x27;&#x27;: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.010288, end_log_pos 378517485; Error &#x27;Invalid utf8 character string: &#x27;&#x27;&#x27; on query. Default database: &#x27;BI&#x27;. Query: &#x27;LOAD DATA INFILE &#x27;/data/mysqldata/3306/tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data&#x27; IGNORE INTO TABLE `520054_all_role` CHARACTER SET utf8 FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; ESCAPED BY &#x27;&quot;&#x27; LINES TERMINATED BY &#x27;\\n&#x27; (`openid`, `clientid`, `roleid`, `rolename`, `school`, `IP`, `snid`, `rolelogin_time`, `amount`, `val`, `consume_sum`, `own_after`, `first_payment_time`, `last_payment_time`, `last_login_time`, `first_pay_level`, `level`, `vip_level`, `first_pay_amount`, `mission_id`)&#x27;, Error_code: 13002018-04-02T00:07:09.876335+08:00 5 [Warning] Slave SQL for channel &#x27;&#x27;: ... The slave coordinator and worker threads are stopped, possibly leaving data in inconsistent state. A restart should restore consistency automatically, although using non-transactional storage for data or info tables or DDL queries could lead to problems. In such cases you have to examine your data (see documentation for details). Error_code: 17562018-04-02T00:07:09.876385+08:00 5 [Note] Slave SQL thread for channel &#x27;&#x27; exiting, replication stopped in log &#x27;mysql-bin.010288&#x27; at position 376791811总之就是Load data 这个文件/data/mysqldata/3306/tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data出问题了呗 于是我手动执行(这里拷贝了一下文件到/tmp)123#mysql --default-character-set=utf8 --socket=/data/mysqldata/3306/mysql.sock -uroot -p&quot;&quot; --show-warnings test -e &quot;LOAD DATA local INFILE &#x27;/tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data&#x27; IGNORE INTO TABLE 520054_all_role CHARACTER SET utf8 FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;\\&quot;&#x27; ESCAPED BY &#x27;\\&quot;&#x27; LINES TERMINATED BY &#x27;\\n&#x27; (openid, clientid, roleid, rolename, school, IP, snid, rolelogin_time, amount, val, consume_sum, own_after, first_payment_time, last_payment_time, last_login_time, first_pay_level, level, vip_level, first_pay_amount, mission_id)&quot;mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1300 (HY000) at line 1: Invalid utf8 character string: &#x27;&#x27;报错看文件也没看出啥, 我就好奇了, 怎么在5.5主库没问题,在这就有问题呢, 于是我把文件scp到主库, 在test库建同一个表,继续手动Load data, 没报错 导入成功了.. 一开始我怀疑是SQL_MODE ,查了下两边都是SQL_MODE=&#39;&#39; (我就不贴出来证明了)我又怀疑字符集问题, 查了下两边也都一样.. 这下尴尬了, 我比较low,没想到啥好方法, 只能人肉折半查找法, 把这个文件/data/mysqldata/3306/tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data 先删一半,LOAD看报错不, 报错说明有问题的数据就在这半部分,没报错说明在删除的那半部分, 如此往复,最后终于找到了,请看!12#less /tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data.final 2001_74148261,1000032,3JM30C2EN,&lt;U+1F602&gt;&lt;U+1F602&gt;&lt;U+1F602&gt;,,124.152.204.12,2001.0,1510191299.0,,,29216.0,58.0,,,1520993335.0,,0.0,0.0,,就这一行,5.5能导进去,5.7不行你们猜&lt;U+1F602&gt;是啥? 是这个表情😂😂😂,卧了个槽 看下建标语句123456789101112131415161718192021222324252627282930root@localhost 10:42: [test]&gt; show create table 520054_all_role\\G*************************** 1. row *************************** Table: 520054_all_roleCreate Table: CREATE TABLE `520054_all_role` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `openid` varchar(100) NOT NULL COMMENT &#x27;用户平台账号&#x27;, `clientid` int(11) NOT NULL COMMENT &#x27;区服ID&#x27;, `roleid` varchar(64) NOT NULL COMMENT &#x27;用户区服唯一账号&#x27;, `rolename` varchar(100) DEFAULT NULL, `school` varchar(64) DEFAULT NULL COMMENT &#x27;角色门派&#x27;, `IP` varchar(64) DEFAULT NULL, `snid` int(11) NOT NULL COMMENT &#x27;平台ID&#x27;, `rolelogin_time` int(11) NOT NULL COMMENT &#x27;建角时间&#x27;, `amount` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;金额&#x27;, `val` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;购买的游戏币(元宝 、钻石)数量&#x27;, `consume_sum` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;消耗钻石数&#x27;, `own_after` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;消耗后剩余钻石数&#x27;, `first_payment_time` int(11) DEFAULT NULL COMMENT &#x27;最初支付时间戳&#x27;, `last_payment_time` int(11) DEFAULT NULL COMMENT &#x27;最后支付时间戳&#x27;, `last_login_time` int(11) DEFAULT NULL COMMENT &#x27;最后活跃时间戳&#x27;, `first_pay_level` smallint(6) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;首次付费等级&#x27;, `level` smallint(6) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;当前等级&#x27;, `vip_level` smallint(6) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;当前vip等级&#x27;, `first_pay_amount` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;最初付费金额&#x27;, `mission_id` varchar(50) DEFAULT NULL COMMENT &#x27;最高关卡ID&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `openid_clientid` (`openid`,`clientid`), KEY `amount_snid` (`amount`,`snid`)) ENGINE=InnoDB AUTO_INCREMENT=32227 DEFAULT CHARSET=utf8 COMMENT=&#x27;所有角色表&#x27;1 row in set (0.00 sec)DEFAULT CHARSET=utf8 5.5能把这个emoji表情导进去(数据有没有问题另说),5.7不行 最后我再5.7把这个表建成utf8mb4,然后重新导入12#mysql --default-character-set=utf8mb4 --socket=/data/mysqldata/3306/mysql.sock -uroot -p&quot;&quot; --show-warnings fan -e &quot;LOAD DATA local INFILE &#x27;/tmp/SQL_LOAD-a3f4b929-31a0-11e8-9714-f8bc123346cc-11-12025.data&#x27; IGNORE INTO TABLE 520054_all_role CHARACTER SET utf8mb4 FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;\\&quot;&#x27; ESCAPED BY &#x27;\\&quot;&#x27; LINES TERMINATED BY &#x27;\\n&#x27; (openid, clientid, roleid, rolename, school, IP, snid, rolelogin_time, amount, val, consume_sum, own_after, first_payment_time, last_payment_time, last_login_time, first_pay_level, level, vip_level, first_pay_amount, mission_id)&quot;mysql: [Warning] Using a password on the command line interface can be insecure.终于不报错了","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"5.5.x升级5.7.21步骤","slug":"5.5.x升级5.7.21步骤","date":"2018-02-11T03:22:00.000Z","updated":"2018-02-11T08:36:59.000Z","comments":true,"path":"2018/02/11/5.5.x升级5.7.21步骤/","link":"","permalink":"http://fuxkdb.com/2018/02/11/5.5.x%E5%8D%87%E7%BA%A75.7.21%E6%AD%A5%E9%AA%A4/","excerpt":"5.5.x升级5.7.21步骤环境准备上传软件包根据官方文档说明 Upgrading to the latest release is recommended before upgrading to the next version 当前版本5.5.46,需要上传三个包 123mysql-5.5.59-linux-glibc2.12-x86_64.tar.gzmysql-5.6.39-linux-glibc2.12-x86_64.tar.gzmysql-5.7.21-linux-glibc2.12-x86_64.tar.gz 创建目录 1mkdir /usr/local/&#123;mysql-5.5.59,mysql-5.6.39,mysql-5.7.21&#125; 解压 123tar -zxvf mysql-5.5.59-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.5.59/tar -zxvf mysql-5.6.39-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.6.39/tar -zxvf mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.7.21/ 设置环境变量 123456789101112zst_ps1()&#123; Date=$(date +%F) Time=$(date +%H:%M:%S) PS1=&quot;\\\\n\\[\\e[1;37m[\\e[m\\]\\[\\e[1;33m\\u\\e[m\\]\\[\\e[1;33m@\\h\\e[m\\]\\[\\e[1;35m $Time \\e[m\\]\\e[1;36m\\w\\e[m\\e[1;37m]\\e[m\\n\\\\$&quot;&#125;PROMPT_COMMAND=zst_ps1export PATH=/usr/local/mysql/bin:$PATHexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mysql/lib","text":"5.5.x升级5.7.21步骤环境准备上传软件包根据官方文档说明 Upgrading to the latest release is recommended before upgrading to the next version 当前版本5.5.46,需要上传三个包 123mysql-5.5.59-linux-glibc2.12-x86_64.tar.gzmysql-5.6.39-linux-glibc2.12-x86_64.tar.gzmysql-5.7.21-linux-glibc2.12-x86_64.tar.gz 创建目录 1mkdir /usr/local/&#123;mysql-5.5.59,mysql-5.6.39,mysql-5.7.21&#125; 解压 123tar -zxvf mysql-5.5.59-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.5.59/tar -zxvf mysql-5.6.39-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.6.39/tar -zxvf mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz -C /usr/local/mysql-5.7.21/ 设置环境变量 123456789101112zst_ps1()&#123; Date=$(date +%F) Time=$(date +%H:%M:%S) PS1=&quot;\\\\n\\[\\e[1;37m[\\e[m\\]\\[\\e[1;33m\\u\\e[m\\]\\[\\e[1;33m@\\h\\e[m\\]\\[\\e[1;35m $Time \\e[m\\]\\e[1;36m\\w\\e[m\\e[1;37m]\\e[m\\n\\\\$&quot;&#125;PROMPT_COMMAND=zst_ps1export PATH=/usr/local/mysql/bin:$PATHexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mysql/lib 停复制,记录位置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@iZ23pn0u8g5Z tmp]# /data/bin/login_db.sh Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 9500936Server version: 5.5.46-log MySQL Community Server (GPL) by RemiCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.root@localhost 14:53: [dbe8je6i4c3gjd50]&gt; stop slave;Query OK, 0 rows affected (0.03 sec)root@localhost 14:53: [dbe8je6i4c3gjd50]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Master_Host: 10.31.124.23 Master_User: slave Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.007611 Read_Master_Log_Pos: 669126117 Relay_Log_File: mysqld-relay-bin.021406 Relay_Log_Pos: 669125856 Relay_Master_Log_File: mysql-bin.007611 Slave_IO_Running: No Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.% Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 669125710 Relay_Log_Space: 669124929 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 321 row in set (0.00 sec) 关库12[root@iZ23pn0u8g5Z tmp]# /usr/bin/mysql --default-character-set=utf8 --socket=/data/mysql/mysql.sock -uroot -p&quot;password&quot; -e&quot;set global innodb_fast_shutdown=0&quot;[root@iZ23pn0u8g5Z tmp]# /data/bin/stop_db.sh 创建软连接123ln -s /usr/local/mysql-5.5.59/mysql-5.5.59-linux-glibc2.12-x86_64/ /usr/local/mysqlchown mysql:mysql -R /usr/local/mysql-5.5.59/chown mysql:mysql -R /usr/local/mysql my.cnf添加skip_slave_start,sql_mode=””否则启动数据库后会自动开始同步 sql_mode=””是为了避免下一步出现 123456789101112131415161718192021222324252627dbe8je6i4c3gjd50.adminerror : Table rebuild required. Please do &quot;ALTER TABLE `admin` FORCE&quot; or dump/reload to fix it! CREATE TABLE `admin` ( `userid` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `username` varchar(200) NOT NULL COMMENT &#x27;管理员用户名&#x27;, `password` varchar(32) NOT NULL COMMENT &#x27;密码&#x27;, `realname` varchar(200) NOT NULL COMMENT &#x27;管理员姓名&#x27;, `createtime` datetime NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27; COMMENT &#x27;创建时间&#x27;, `status` tinyint(1) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;状态 0 封禁 1正常&#x27;, `myprivate` text NOT NULL COMMENT &#x27;权限&#x27;, `mygame` text NOT NULL COMMENT &#x27;游戏权限&#x27;, `myunion` text NOT NULL COMMENT &#x27;推广查看权限&#x27;, `department_id` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;部门id&#x27;, `job_number` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;呼叫中心的工号&#x27;, `yesorno` varchar(200) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否有xxx权限&#x27;, `myrows` text NOT NULL COMMENT &#x27;列权限&#x27;, `email` varchar(200) NOT NULL COMMENT &#x27;邮箱&#x27;, `bkemail` varchar(200) NOT NULL COMMENT &#x27;备用邮箱&#x27;, `leaderid` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;上级ID&#x27;, `myaccount` text NOT NULL COMMENT &#x27;账号权限&#x27;, `img` varchar(1000) DEFAULT NULL COMMENT &#x27;头像&#x27;, `nickname` varchar(200) DEFAULT NULL COMMENT &#x27;昵称&#x27;, KEY `username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;管理员表&#x27;这是由于createtime列default值为&#x27;0000-00-00 00:00:00&#x27; 违反了默认的sql_mode 启动数据库,执行mysql_upgrade123456789101112131415161718192021222324252627282930313233343536373839404142434445464748source ~/.bash_profile/data/bin/start_db.sh mysql_upgrade --socket=/data/mysql/mysql.sock -uroot -p&quot;password&quot;mysql_upgrade: [Warning] Using a password on the command line interface can be insecure.Checking if update is needed.Checking server version.Running queries to upgrade MySQL server.Checking system database.mysql.columns_priv OKmysql.db OKmysql.engine_cost OKmysql.event OKmysql.func OKmysql.general_log OKmysql.gtid_executed OKmysql.help_category OKmysql.help_keyword OKmysql.help_relation OKmysql.help_topic OKmysql.host OKmysql.innodb_index_stats OKmysql.innodb_table_stats OKmysql.ndb_binlog_index OKmysql.plugin OKmysql.proc OKmysql.procs_priv OKmysql.proxies_priv OKmysql.server_cost OKmysql.servers OKmysql.slave_master_info OKmysql.slave_relay_log_info OKmysql.slave_worker_info OKmysql.slow_log OKmysql.tables_priv OKmysql.time_zone OKmysql.time_zone_leap_second OKmysql.time_zone_name OKmysql.time_zone_transition OKmysql.time_zone_transition_type OKmysql.user OKUpgrading the sys schema.Checking databases.dbe8je6i4c3gjd50.adlist OKdbe8je6i4c3gjd50.adlist_bk OKdbe8je6i4c3gjd50.adminerror : Table rebuild required. Please do &quot;ALTER TABLE `admin` FORCE&quot; or dump/reload to fix it!升级时留意有没有error 关闭数据库12345mysql --default-character-set=utf8 --socket=/data/mysql/mysql.sock -uroot -p&quot;password&quot; -e&quot;set global innodb_fast_shutdown=0&quot;/data/bin/stop_db.sh 删除软连接rm /usr/local/mysql 重复之前的步骤继续升级5.6.39升级只5.7.21修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175[client]user=rootpassword=socket=/data/mysql/mysql.sock[mysqld]########basic settings########server-id = 853306port = 3306autocommit = 1transaction_isolation = REPEATABLE-READcharacter_set_server=utf8skip_name_resolve = 1secure_file_priv=/var/lib/mysql-files/#连接max_connections = 800max_connect_errors = 1000back_log=512socket=/data/mysql/mysql.sockpid-file=/data/run/mysqld.pidbasedir=/usr/local/mysqldatadir = /data/mysqltmpdir = /data/tmpdirexplicit_defaults_for_timestamp = 1#memthread_stack = 512Kthread_cache_size = 1024table_open_cache = 1024table_definition_cache = 1024join_buffer_size = 134217728tmp_table_size = 67108864read_buffer_size = 16777216read_rnd_buffer_size = 33554432sort_buffer_size = 33554432tmp_table_size = 32Mmax_heap_table_size = 32M#bulk_insert_buffer_size = 64M 默认8M#myisam_sort_buffer_size = 128M 默认8Mmax_prepared_stmt_count=50000query_cache_size = 0query_cache_type = 0query_cache_limit =0max_allowed_packet = 128M#sql_mode = &quot;STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER&quot;#默认就是ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTIONsql_mode=&quot;&quot;interactive_timeout = 1800wait_timeout = 1800#optimizer_switch=&quot;index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,engine_condition_pushdown=on&quot;auto_increment_offset = 1auto_increment_increment = 2########log settings########log_output=FILElog_error = /data/mysql/error.logslow_query_log = 1slow_query_log_file = /data/mysql/slow-queries.loglog_queries_not_using_indexes = 1log_slow_admin_statements = 1log_slow_slave_statements = 1log_throttle_queries_not_using_indexes = 10long_query_time = 0.8min_examined_row_limit = 100general_log=0general_log_file=/data/mysql/general_query.logexpire_logs_days=7########replication settings########master_info_repository = TABLErelay_log_info_repository = TABLElog_bin = mysql-binsync_binlog = 10innodb_flush_log_at_trx_commit = 2#gtid_mode = on#enforce_gtid_consistency = 1log_slave_updates=1binlog_format = row relay_log = mysqld-relay-binrelay_log_recovery = 1 #启用这个参数需要repository=TABLE,官方文档没有明确要求,老吴这么说应该是为了一致性吧#Enabling the --relay-log-recovery option when relay-log-purge is disabled risks reading the relay log from files that were not purged, leading to data inconsistency.relay_log_purge=1 binlog_gtid_simple_recovery = 1slave_skip_errors = ddl_exist_errorsslave_parallel_type=LOGICAL_CLOCKslave_parallel_workers=8slave_preserve_commit_order = 1skip_slave_start=1#ZST#1M - 2Mbinlog_cache_size = 2M #max_binlog_size #默认1G, 建议生成间隔2分钟以上 推荐128M 或 256M 这样用mysqlbinlog解析更快些log_bin_trust_function_creators=1 #开启binlog时,是否允许创建存储过程(除非有super权限,且指定deterministic, reads sql data,no sql)#max_binlog_cache_size #binlog最大的cache size,有大SQL写入时需要用到,或者大数据LOAD DATA时. 默认1G吧,基本不用改#binlog_stmt_cache_size #大量prepare statement时.加大#binglog_direct_non_transactional_update#relay_log_purge=1 #relay log使用完就删掉replicate-wild-ignore-table=mysql.%replicate-wild-ignore-table=information_schema.%replicate-wild-ignore-table=performance_schema.%replicate-wild-ignore-table=union_log%.%replicate-wild-ignore-table=test.%########innodb settings#########innodb_page_size = 8192innodb_buffer_pool_size = 45Ginnodb_buffer_pool_instances = 8innodb_buffer_pool_load_at_startup = 1innodb_buffer_pool_dump_at_shutdown = 1#此值= innodb_io_capacity/innodb_buffer_pool_instancesinnodb_lru_scan_depth = 1000innodb_lock_wait_timeout = 5# 根据您的服务器IOPS能力适当调整# 一般配普通SSD盘的话，可以调整到 10000 - 20000# 配置高端PCIe SSD卡的话，则可以调整的更高，比如 50000 - 80000innodb_io_capacity = 1000innodb_io_capacity_max = 2000innodb_flush_method = O_DIRECTinnodb_file_format = Barracudainnodb_file_format_max = Barracudainnodb_log_files_in_group = 3innodb_log_group_home_dir = /data/mysql/innodb_undo_directory = /data/mysql/innodb_undo_logs = 128innodb_undo_tablespaces = 3innodb_flush_neighbors = 1innodb_log_file_size = 1Ginnodb_log_buffer_size = 16777216innodb_purge_threads = 4innodb_large_prefix = 1innodb_thread_concurrency = 128innodb_print_all_deadlocks = 1innodb_strict_mode = 1innodb_sort_buffer_size = 67108864 innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_data_file_path=ibdata1:1024M:autoextend########semi sync replication settings#########plugin_dir=/usr/local/mysql/lib/plugin#plugin_load = &quot;rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so&quot;#loose_rpl_semi_sync_master_enabled = 1#loose_rpl_semi_sync_slave_enabled = 1#loose_rpl_semi_sync_master_timeout = 5000#######performance schema#######performance_schema = 1performance_schema_instrument=&#x27;memory/%=COUNTED&#x27;performance_schema_digests_size = 40000performance_schema_max_table_instances = 40000performance_schema_max_sql_text_length = 4096performance_schema_max_digest_length = 4096#innodb monitorinnodb_monitor_enable=module_innodb,module_dml,module_ddl,module_trx,module_os,module_purge,module_log,module_lock,module_buffer,module_index,module_ibuf_system,module_buffer_page,module_adaptive_hash[mysqld-5.7]innodb_buffer_pool_dump_pct = 40innodb_page_cleaners = 8#innodb_undo_log_truncate = 1#innodb_max_undo_log_size = 2Ginnodb_purge_rseg_truncate_frequency = 128binlog_gtid_simple_recovery=1log_timestamps=system#transaction_write_set_extraction=MURMUR32show_compatibility_56=on[mysql]no-auto-rehashprompt=&quot;\\u@\\h \\R:\\m:\\s [\\d]&gt; &quot; 创建文件夹 12mkdir -p /var/lib/mysql-files/chown mysql:mysql /var/lib/mysql-files 创建软连接 123ln -s /usr/local/mysql-5.7.21/mysql-5.7.21-linux-glibc2.12-x86_64/ /usr/local/mysqlchown mysql:mysql -R /usr/local/mysql-5.7.21/chown mysql:mysql -R /usr/local/mysql mysql_upgrade 1mysql_upgrade -uroot -pmysql -S /data1/mysqldata/3306/mysql.sock 5.7.21由于一些数据类型存储范式有改变比如 TIME3 bytes3 bytes + fractional seconds storage DATETIME8 bytes5 bytes + fractional seconds storage TIMESTAMP4 bytes4 bytes + fractional seconds storage 需要重新建标,所以可能会很慢 可以使用--upgrade-system-tables只升级数据字典,根据pecona的文章也不影响DML和复制 https://www.percona.com/blog/2016/04/27/upgrading-to-mysql-5-7-focusing-on-temporal-types/ 需要注意的参数1234567891011121314151617下面三个参数没有了table_cachethread_concurrencykey_bufferinnodb_additional_mem_pool_size独立undo必须是MySQL5.6.3引入的,需要初始化数据库时就启用,由于这里是5.5升级至5.7所以也没法用了innodb_undo_tablespacesinnodb_undo_directoryinnodb_undo_log_truncateinnodb_max_undo_log_size这个是由于线上主库还是基于position的,如果开了就没法建立同步了gtid_mode = offenforce_gtid_consistency = 0","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL升级","slug":"MySQL升级","permalink":"http://fuxkdb.com/tags/MySQL%E5%8D%87%E7%BA%A7/"}]},{"title":"MySQL 5.5.X升级至5.7.21遇到的坑(一)","slug":"MySQL-5.5.X升级至5.7.21遇到的坑(一)","date":"2018-02-10T12:23:00.000Z","updated":"2018-02-11T01:24:30.000Z","comments":true,"path":"2018/02/10/MySQL-5.5.X升级至5.7.21遇到的坑(一)/","link":"","permalink":"http://fuxkdb.com/2018/02/10/MySQL-5.5.X%E5%8D%87%E7%BA%A7%E8%87%B35.7.21%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91(%E4%B8%80)/","excerpt":"MySQL 5.5.X升级至5.7.21遇到的坑(一)发现问题将一个测试环境的5.5.x升级到5.7.21后,打算将5.7.21作为从库,开始同步主库数据(binlog_format=statement),结果刚一start slave就报错1Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.000004, end_log_pos 812. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any.查看error log12345678910111213142018-02-10T19:52:52.347979+08:00 3 [Warning] Slave I/O for channel &#x27;&#x27;: Notifying master by SET @master_binlog_checksum= @@global.binlog_checksum failed with error: Unknown system variable &#x27;binlog_checksum&#x27;, Error_code: 11932018-02-10T19:52:52.348080+08:00 3 [Warning] Slave I/O for channel &#x27;&#x27;: Unknown system variable &#x27;SERVER_UUID&#x27; on master. A probable cause is that the variable is not supported on the master (version: 5.5.59-log), even though it is on the slave (version: 5.7.21-log), Error_code: 11932018-02-10T19:52:52.445947+08:00 5 [ERROR] Slave SQL for channel &#x27;&#x27;: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log , end_log_pos 2651; Error &#x27;You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#x27;GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;&#x27; at line 1&#x27; on query. Default database: &#x27;fandb&#x27;. Query: &#x27;INSERT INTO postlog ( USERNAME, TIME, IP, GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; )&#x27;, Error_code: 10642018-02-10T19:52:52.446217+08:00 4 [Warning] Slave SQL for channel &#x27;&#x27;: ... The slave coordinator and worker threads are stopped, possibly leaving data in inconsistent state. A restart should restore consistency automatically, although using non-transactional storage for data or info tables or DDL queries could lead to problems. In such cases you have to examine your data (see documentation for details). Error_code: 17562018-02-10T19:52:52.446235+08:00 4 [Note] Slave SQL thread for channel &#x27;&#x27; exiting, replication stopped in log &#x27;mysql-bin.000003&#x27; at position 23132018-02-10T19:53:15.708058+08:00 7 [Note] Slave SQL thread for channel &#x27;&#x27; initialized, starting replication in log &#x27;mysql-bin.000003&#x27; at position 2313, relay log &#x27;./mysql-relay.000009&#x27; position: 304 找出原因头两个Warning是由于主库没有binlog_checksum参数，也没有SERVER_UUID参数(看来从库开始同步时要先去主库查询这两个参数)接着的ERROR报的错误竟然是error in your SQL syntax语法错误. 解析binlog后查到SQL语句为1INSERT INTO postlog ( USERNAME, TIME, IP, GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; );我依次在5.5和5.7的环境执行这个SQL发现该SQL在5.5可以正常执行，而在5.7执行就会报错语法错误于是一点一点分析这个SQL哪有问题，说实话肉眼真没看出来只好在5.7一点一点执行1INSERT INTO postlog ( USERNAME, TIME, IP) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414);没问题可以执行1INSERT INTO postlog ( USERNAME, TIME, IP, GET) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;);报错了！,那显然就是GET有问题。 怀疑是保留字？","text":"MySQL 5.5.X升级至5.7.21遇到的坑(一)发现问题将一个测试环境的5.5.x升级到5.7.21后,打算将5.7.21作为从库,开始同步主库数据(binlog_format=statement),结果刚一start slave就报错1Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.000004, end_log_pos 812. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any.查看error log12345678910111213142018-02-10T19:52:52.347979+08:00 3 [Warning] Slave I/O for channel &#x27;&#x27;: Notifying master by SET @master_binlog_checksum= @@global.binlog_checksum failed with error: Unknown system variable &#x27;binlog_checksum&#x27;, Error_code: 11932018-02-10T19:52:52.348080+08:00 3 [Warning] Slave I/O for channel &#x27;&#x27;: Unknown system variable &#x27;SERVER_UUID&#x27; on master. A probable cause is that the variable is not supported on the master (version: 5.5.59-log), even though it is on the slave (version: 5.7.21-log), Error_code: 11932018-02-10T19:52:52.445947+08:00 5 [ERROR] Slave SQL for channel &#x27;&#x27;: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log , end_log_pos 2651; Error &#x27;You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#x27;GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;&#x27; at line 1&#x27; on query. Default database: &#x27;fandb&#x27;. Query: &#x27;INSERT INTO postlog ( USERNAME, TIME, IP, GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; )&#x27;, Error_code: 10642018-02-10T19:52:52.446217+08:00 4 [Warning] Slave SQL for channel &#x27;&#x27;: ... The slave coordinator and worker threads are stopped, possibly leaving data in inconsistent state. A restart should restore consistency automatically, although using non-transactional storage for data or info tables or DDL queries could lead to problems. In such cases you have to examine your data (see documentation for details). Error_code: 17562018-02-10T19:52:52.446235+08:00 4 [Note] Slave SQL thread for channel &#x27;&#x27; exiting, replication stopped in log &#x27;mysql-bin.000003&#x27; at position 23132018-02-10T19:53:15.708058+08:00 7 [Note] Slave SQL thread for channel &#x27;&#x27; initialized, starting replication in log &#x27;mysql-bin.000003&#x27; at position 2313, relay log &#x27;./mysql-relay.000009&#x27; position: 304 找出原因头两个Warning是由于主库没有binlog_checksum参数，也没有SERVER_UUID参数(看来从库开始同步时要先去主库查询这两个参数)接着的ERROR报的错误竟然是error in your SQL syntax语法错误. 解析binlog后查到SQL语句为1INSERT INTO postlog ( USERNAME, TIME, IP, GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; );我依次在5.5和5.7的环境执行这个SQL发现该SQL在5.5可以正常执行，而在5.7执行就会报错语法错误于是一点一点分析这个SQL哪有问题，说实话肉眼真没看出来只好在5.7一点一点执行1INSERT INTO postlog ( USERNAME, TIME, IP) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414);没问题可以执行1INSERT INTO postlog ( USERNAME, TIME, IP, GET) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;);报错了！,那显然就是GET有问题。 怀疑是保留字？ 查看官方文档,发现还真是https://dev.mysql.com/doc/refman/5.7/en/keywords.html 新的问题这个保留字的问题还真蛋疼，说明开发不规范呗，只能让开发改了.目前由于是测试，想要先跳过这个表，于是使用5.7的新特性动态更改replication filter1change replication filter REPLICATE_WILD_IGNORE_TABLE=(&#x27;mysql.%&#x27;,&#x27;information_schema.%&#x27;,&#x27;performance_schema.%&#x27;,&#x27;union_log%.%&#x27;,&#x27;test.%&#x27;,&#x27;fandb.postlog&#x27;);然后跳过这个错误结果还是报错奇怪了，我一度怀疑自己的复制过滤规则有问题。然后又搭建了一套5.7和一套5.5测试同样的复制过滤规则都管用于是怀疑 5.5做主5.7做从本身有问题，复制过滤规则不起作用 复制过滤规则起作用，但是有别的问题 5.5主库123456789101112131415161718192021222324252627root@localhost 19:54: [fandb]&gt; show create table postlog\\GERROR 2006 (HY000): MySQL server has gone awayNo connection. Trying to reconnect...Connection id: 5Current database: fandb*************************** 1. row *************************** Table: postlogCreate Table: CREATE TABLE `postlog` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;自增ID&#x27;, `username` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;管理员账号&#x27;, `time` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;时间&#x27;, `ip` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;IP&#x27;, `get` text NOT NULL COMMENT &#x27;GET&#x27;, `post` text NOT NULL COMMENT &#x27;POST&#x27;, `file` text NOT NULL COMMENT &#x27;FILE&#x27;, `class` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;CLASS&#x27;, `method` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;METHOD&#x27;, PRIMARY KEY (`id`), KEY `username` (`username`), KEY `c_m` (`class`,`method`), KEY `time` (`time`)) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8 COMMENT=&#x27;提交日志表&#x27;1 row in set (0.00 sec)root@localhost 19:59: [fandb]&gt; select * from postlog;Empty set (0.00 sec)5.7.21从库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263root@localhost 19:58: [fandb]&gt; select * from postlog;Empty set (0.00 sec)root@localhost 19:59: [fandb]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.105.118.10 Master_User: repl Master_Port: 3308 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 1028 Relay_Log_File: mysql-relay.000010 Relay_Log_Pos: 1217 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.%,fandb.postlog Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1028 Relay_Log_Space: 2296 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 323308 Master_UUID: Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: 3b6f4462-0190-11e8-ac03-525400a44d53:1-37445291 Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 5.5主库以兼容的方式插入12345678910root@localhost 19:59: [fandb]&gt; INSERT INTO postlog ( USERNAME, TIME, IP, `GET`, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; );Query OK, 1 row affected, 1 warning (0.00 sec)root@localhost 20:00: [fandb]&gt; select * from postlog;+----+-----------+------------+------------+-----------+------+------+-------+----------------+| id | username | time | ip | get | post | file | class | method |+----+-----------+------------+------------+-----------+------+------+-------+----------------+| 11 | maokaixin | 1518159802 | 2147483647 | gameid:0; | | | api | ajaxGetServers |+----+-----------+------------+------------+-----------+------+------+-------+----------------+1 row in set (0.00 sec) 5.7.21从库查看没有插入数据,复制规律规则生效12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364root@localhost 19:59: [fandb]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.105.118.10 Master_User: repl Master_Port: 3308 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 1395 Relay_Log_File: mysql-relay.000010 Relay_Log_Pos: 1584 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.%,fandb.postlog Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1395 Relay_Log_Space: 2663 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 323308 Master_UUID: Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: 3b6f4462-0190-11e8-ac03-525400a44d53:1-37445291 Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)root@localhost 20:00: [fandb]&gt; select * from postlog;Empty set (0.00 sec) 5.5主库以非兼容方式插入1234567891011root@localhost 20:01: [fandb]&gt; INSERT INTO postlog ( USERNAME, TIME, IP, GET, POST, FILE, CLASS, METHOD ) VALUES ( &#x27;maokaixin&#x27;, 1518159802, 3395959414, &#x27;gameid:0;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;api&#x27;, &#x27;ajaxGetServers&#x27; );Query OK, 1 row affected, 1 warning (0.00 sec)root@localhost 20:02: [fandb]&gt; select * from postlog;+----+-----------+------------+------------+-----------+------+------+-------+----------------+| id | username | time | ip | get | post | file | class | method |+----+-----------+------------+------------+-----------+------+------+-------+----------------+| 11 | maokaixin | 1518159802 | 2147483647 | gameid:0; | | | api | ajaxGetServers || 13 | maokaixin | 1518159802 | 2147483647 | gameid:0; | | | api | ajaxGetServers |+----+-----------+------------+------------+-----------+------+------+-------+----------------+2 rows in set (0.00 sec)5.7.21从库复制sql thread报错123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960root@localhost 20:02: [fandb]&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.105.118.10 Master_User: repl Master_Port: 3308 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 1760 Relay_Log_File: mysql-relay.000010 Relay_Log_Pos: 1584 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: mysql.%,information_schema.%,performance_schema.%,union_log%.%,test.%,fandb.postlog Last_Errno: 1064 Last_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.000004, end_log_pos 1733. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Skip_Counter: 0 Exec_Master_Log_Pos: 1395 Relay_Log_Space: 3028 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULLMaster_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1064 Last_SQL_Error: Coordinator stopped because there were error(s) in the worker(s). The most recent failure being: Worker 1 failed executing transaction &#x27;ANONYMOUS&#x27; at master log mysql-bin.000004, end_log_pos 1733. See error log and/or performance_schema.replication_applier_status_by_worker table for more details about this failure or others, if any. Replicate_Ignore_Server_Ids: Master_Server_Id: 323308 Master_UUID: Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: 180210 20:02:04 Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: 3b6f4462-0190-11e8-ac03-525400a44d53:1-37445291 Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)由此推断,复制过滤规则没有问题,是生效的但是判断时需要先解析sql,当解析时发现sql语法错误(实际为get为MySQL5.7保留字),导致sql thread报错","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL升级","slug":"MySQL升级","permalink":"http://fuxkdb.com/tags/MySQL%E5%8D%87%E7%BA%A7/"}]},{"title":"","slug":"python2 yum module is needed for this  module","date":"2018-02-09T06:00:17.000Z","updated":"2018-02-09T06:01:01.000Z","comments":true,"path":"2018/02/09/python2 yum module is needed for this  module/","link":"","permalink":"http://fuxkdb.com/2018/02/09/python2%20yum%20module%20is%20needed%20for%20this%20%20module/","excerpt":"","text":"pyenv装的2.7.14 无法用ansible yum.(其实不是pyenv的原因,自己编译安装的2.7.14也不行) 123456789101112131415161718192021222324#ansible OA* -m yum -a&quot;name=mutt state=present&quot; OA_P | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;python2 yum module is needed for this module&quot;&#125;OA_S | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;python2 yum module is needed for this module&quot;&#125;#ansible OA* -m shell -a &quot;which python pip&quot;OA_P | SUCCESS | rc=0 &gt;&gt;/root/.pyenv/versions/2.7.14/bin/python/root/.pyenv/versions/2.7.14/bin/pipOA_S | SUCCESS | rc=0 &gt;&gt;/root/.pyenv/versions/2.7.14/bin/python/root/.pyenv/versions/2.7.14/bin/pip#vi /etc/ansible/hosts/oa [OA]OA_P ansible_connection=localOA_S ansible_ssh_host=172.16.200.209 [OA:vars]ansible_python_interpreter=/root/.pyenv/shims/python 使用系统默认的python作为解释器可以使用yum模块123456789101112131415161718192021222324252627[root@OA_P 13:04:15 /etc/ansible/roles]#ansible OA* -m yum -a&quot;name=mutt state=present&quot; OA_P | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;python2 bindings for rpm are needed for this module. python2 yum module is needed for this module&quot;&#125;OA_S | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;python2 bindings for rpm are needed for this module. python2 yum module is needed for this module&quot;&#125;^C[root@OA_P 13:04:28 /etc/ansible/roles]#ansible OA* -m yum -a&quot;name=mutt state=present&quot; -e &quot;ansible_python_interpreter=/usr/bin/python&quot;OA_S | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;5:mutt-1.5.20-8.20091214hg736b6a.el6.x86_64 providing mutt is already installed&quot; ]&#125;OA_P | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;The following packages have pending transactions: mutt-x86_64&quot;, &quot;rc&quot;: 125, &quot;results&quot;: []&#125;这是因为2.6可以import yum而2.7不行1234567891011121314#pythonPython 2.7.14 (default, Dec 15 2017, 23:08:56) [GCC 4.4.7 20120313 (Red Hat 4.4.7-18)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import yumTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ImportError: No module named yum#python2.6Python 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import yum 目前我的解决方法是, 在yum之前先set facts, 之后在set回来123456789101112131415- set_fact: ansible_python_default_interpreter: &quot;&#123;&#123; ansible_python_interpreter &#125;&#125;&quot; ansible_python_interpreter: &quot;/usr/bin/python&quot;- name: 安装依赖包 yum: name: &quot;&#123;&#123; item.line &#125;&#125;&quot; state: present with_items: - &#123; line: &#x27;openssl&#x27; &#125; - &#123; line: &#x27;openssl-devel&#x27; &#125; - &#123; line: &#x27;mutt&#x27; &#125;- set_fact: ansible_python_interpreter: &quot;&#123;&#123; ansible_python_default_interpreter &#125;&#125;&quot; 参考 https://github.com/openshift/openshift-ansible/issues/855https://stackoverflow.com/questions/29711514/no-module-named-yum-error-with-python2-7-when-using-ansible-on-fedora/36138921#36138921","categories":[],"tags":[]},{"title":"","slug":"CONFIG REWRITE命令","date":"2018-01-19T03:09:11.000Z","updated":"2018-01-19T03:34:30.000Z","comments":true,"path":"2018/01/19/CONFIG REWRITE命令/","link":"","permalink":"http://fuxkdb.com/2018/01/19/CONFIG%20REWRITE%E5%91%BD%E4%BB%A4/","excerpt":"","text":"CONFIG REWRITE Available since 2.8.0. CONFIG REWRITE命令用于重写redis server启动时使用的配置文件,以最小的变动反映当前server的实际配置,当前的实际配置可能会由于CONFIG SET的使用而与启动时的配置文件不同. 类似 scope=spfile 重写以非常保守的方式执行: 尽可能保留原始redis.conf的注释和整体结构. 如果一个选项已经存在于旧的redis.conf文件中,它将被原地更新. 如果某个选项尚不存在,但设置为其默认值,则不会由重写过程添加. 如果某个选项尚不存在,但它被设置为非默认值,则会将其附加到文件末尾. Non used lines are blanked. For instance if you used to have multiple save directives, but the current configuration has fewer or none as you disabled RDB persistence, all the lines will be blanked. 未使用的行将被删除.例如,如果您曾经设置多个save,但是后来却通过config set只设置了更少的或者config set save ‘’,那么所有的未使用save将被删除 123456789101112131415161718192021222324252627282930313233343536[root@cn_mu_binlog_backup ~]# redis-cli -p 6379 config get save1) &quot;save&quot;2) &quot;&quot;[root@cn_mu_binlog_backup ~]# cat /usr/local/redis/etc/6379_redis.conf|grep save# save &lt;seconds&gt; &lt;changes&gt;# Will save the DB if both the given number of seconds and the given# In the example below the behaviour will be to save:# Note: you can disable saving completely by commenting out all &quot;save&quot; lines.# It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# save &quot;&quot;save 900 1save 300 10save 60 10000...重写[root@cn_mu_binlog_backup ~]# redis-cli -p 6379 config rewriteOK[root@cn_mu_binlog_backup ~]# cat /usr/local/redis/etc/6379_redis.conf|grep save# save &lt;seconds&gt; &lt;changes&gt;# Will save the DB if both the given number of seconds and the given# In the example below the behaviour will be to save:# Note: you can disable saving completely by commenting out all &quot;save&quot; lines.# It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# save &quot;&quot;# (at least one save point) and the latest background save failed.stop-writes-on-bgsave-error yes# If you want to save some CPU in the saving child set it to &#x27;no&#x27; but# algorithms (in order to save memory), so you can tune it for speed or# the configured save points).# saving process (a background save or AOF log background rewriting) is# Lists are also encoded in a special way to save a lot of space.# order to save a lot of space. This encoding is only used when the length and 如果由于某种原因原来的配置文件不再存在,CONFIG REWRITE也可以重写配置文件. 但是,如果服务器启动时没有配置文件,CONFIG REWRITE将只返回一个错误. 原子重写过程In order to make sure the redis.conf file is always consistent, that is, on errors or crashes you always end with the old file, or the new one, the rewrite is performed with a single write(2) call that has enough content to be at least as big as the old file. Sometimes additional padding in the form of comments is added in order to make sure the resulting file is big enough, and later the file gets truncated to remove the padding at the end. 为了确保redis.conf文件始终保持一致,即在出现错误或崩溃时总是以旧文件或新文件结束,重写将使用具有足够内容的单次写入(2)调用来执行 至少与旧文件一样大. 有时会添加注释形式的其他填充以确保生成的文件足够大,然后文件被截断以删除最后的填充. 返回值Simple string reply: OK 当配置被正确重写. 否则,返回错误","categories":[],"tags":[]},{"title":"redis-cli,Redis命令行界面","slug":"redis-cli","date":"2018-01-17T16:00:00.000Z","updated":"2018-01-18T12:04:24.000Z","comments":true,"path":"2018/01/18/redis-cli/","link":"","permalink":"http://fuxkdb.com/2018/01/18/redis-cli/","excerpt":"redis-cli,Redis命令行界面redis-cli是Redis命令行界面，一个简单的程序，允许向Redis发送命令，并直接从终端读取服务器发送的回复。 它有两种主要的模式：一种是交互模式，在这种模式下，用户输入命令并获得回复的REPL（Read Eval Print Loop）另一种模式是将命令作为redis-cli的参数发送，执行并打印在标准输出上。 在交互模式下，redis-cli具有基本的行编辑功能，可以提供良好的打字体验。 但是，redis-cli不只是这一点。有些选项可以用来启动程序，以便将其放入特殊模式，以便redis-cli可以完成更复杂的任务，例如模拟slave并打印从master接收到的复制流，查看复制延迟，并显示统计数据，甚至一个ASCII艺术频谱图的延迟样本和频率，以及许多其他事情。 本指南将涵盖redis-cli的不同方面，由简入繁。 如果您要广泛使用Redis，或者您已经这么做了，那么很可能会碰巧使用redis-cli。花一些时间来熟悉它可能是一个非常好的主意，一旦你知道了命令行界面的所有技巧，你就会看到你将更有效地使用Redis。 命令行用法如下可以直接非交互模式执行命令并在标准输出获取返回结果12redis-cli incr mycounter(integer) 7 ‘()’内为返回结果的类型.当我们需要获取返回结果作为下一个命令的输入,或者希望将结果重定向到文件中时,我们可能不需要显示类型信息.实际上,redis-cli会自动检测,当它检测到标准输出是一个tty(a terminal basically)时,它会显示类型信息来提升可读性,否则它会启用原始输出模式,如下所示:123$ redis-cli incr mycounter &gt; /tmp/output.txt$ cat /tmp/output.txt8这一次当CLI检测到输出不在写入terminal后,(integer)被删除了不再显示. 你可以使用--raw选项来强制输出到终端的内容也不显示类型信息12$ redis-cli --raw incr mycounter9 同样的,你也可以使用--no-raw强制非tty输出也显示类型信息12345678910[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 incr no(integer) 1[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 --raw incr no2[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 incr no &gt; res.txt[root@cn_mu_binlog_backup ~]# cat res.txt 3[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 --no-raw incr no &gt; res.txt[root@cn_mu_binlog_backup ~]# cat res.txt (integer) 4","text":"redis-cli,Redis命令行界面redis-cli是Redis命令行界面，一个简单的程序，允许向Redis发送命令，并直接从终端读取服务器发送的回复。 它有两种主要的模式：一种是交互模式，在这种模式下，用户输入命令并获得回复的REPL（Read Eval Print Loop）另一种模式是将命令作为redis-cli的参数发送，执行并打印在标准输出上。 在交互模式下，redis-cli具有基本的行编辑功能，可以提供良好的打字体验。 但是，redis-cli不只是这一点。有些选项可以用来启动程序，以便将其放入特殊模式，以便redis-cli可以完成更复杂的任务，例如模拟slave并打印从master接收到的复制流，查看复制延迟，并显示统计数据，甚至一个ASCII艺术频谱图的延迟样本和频率，以及许多其他事情。 本指南将涵盖redis-cli的不同方面，由简入繁。 如果您要广泛使用Redis，或者您已经这么做了，那么很可能会碰巧使用redis-cli。花一些时间来熟悉它可能是一个非常好的主意，一旦你知道了命令行界面的所有技巧，你就会看到你将更有效地使用Redis。 命令行用法如下可以直接非交互模式执行命令并在标准输出获取返回结果12redis-cli incr mycounter(integer) 7 ‘()’内为返回结果的类型.当我们需要获取返回结果作为下一个命令的输入,或者希望将结果重定向到文件中时,我们可能不需要显示类型信息.实际上,redis-cli会自动检测,当它检测到标准输出是一个tty(a terminal basically)时,它会显示类型信息来提升可读性,否则它会启用原始输出模式,如下所示:123$ redis-cli incr mycounter &gt; /tmp/output.txt$ cat /tmp/output.txt8这一次当CLI检测到输出不在写入terminal后,(integer)被删除了不再显示. 你可以使用--raw选项来强制输出到终端的内容也不显示类型信息12$ redis-cli --raw incr mycounter9 同样的,你也可以使用--no-raw强制非tty输出也显示类型信息12345678910[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 incr no(integer) 1[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 --raw incr no2[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 incr no &gt; res.txt[root@cn_mu_binlog_backup ~]# cat res.txt 3[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 --no-raw incr no &gt; res.txt[root@cn_mu_binlog_backup ~]# cat res.txt (integer) 4 Host, port, password and databaseredis-cli默认会连接127.0.0.1,6379端口.可以使用-h 和 -p选项更改host和port12$ redis-cli -h redis15.localnet.org -p 6390 pingPONG如果你的instance设置了密码,可以使用-a选项12$ redis-cli -a myUnguessablePazzzzzword123 pingPONG 可以使用-n选项修改database12345678$ redis-cli flushallOK$ redis-cli -n 1 incr a(integer) 1$ redis-cli -n 1 incr a(integer) 2$ redis-cli -n 2 incr a(integer) 1 Some or all of this information can also be provided by using the -u option and a valid URI:12$ redis-cli -u redis://p%40ssw0rd@redis-16379.hosted.com:16379/0 pingPONG 3.2.9没这个 Getting input from other programs有两种方法可以使用redis-cli来获得来自其他命令的输入.一种是从标准输入stdin读取最后一个argument.例如,set 一个key的值为/etc/services12345678910111213141516171819202122232425262728[root@cn_mu_binlog_backup ~]# less /etc/services# /etc/services:# $Id: services,v 1.48 2009/11/11 14:32:31 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2009-11-10## Note that it is presently the policy of IANA to assign a single well-known# port number for both TCP and UDP; hence, most entries here have two entries# even if the protocol doesn&#x27;t support UDP operations.# Updated from RFC 1700, ``Assigned Numbers&#x27;&#x27; (October 1994). Not all ports# are included, only the more common ones.## The latest IANA port assignments can be gotten from# http://www.iana.org/assignments/port-numbers# The Well Known Ports are those from 0 through 1023.# The Registered Ports are those from 1024 through 49151# The Dynamic and/or Private Ports are those from 49152 through 65535## Each line describes one service, and is of the form:## service-name port/protocol [aliases ...] [# comment]tcpmux 1/tcp # TCP port service multiplexertcpmux 1/udp # TCP port service multiplexerrje 5/tcp # Remote Job Entryrje 5/udp # Remote Job Entryecho 7/tcp可以使用-x选项1234[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 -x set foo &lt; /etc/services OK[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 getrange foo 0 50&quot;# /etc/services:\\n# $Id: services,v 1.48 2009/11/11 &quot; 正如你可以在上面的会话的第一行看到的那样,没有指定SET命令的最后一个参数.而是指定-x选项,并将文件重定向到CLI的标准输入. 所以输入被读取,并被用作命令的最后一个参数. 这对编写脚本很有用. 一个不同的方法是向redis-cli提供一个写在文本文件中的命令列表:12345678910111213141516171819202122$ cat /tmp/commands.txtset foo 100incr fooappend foo xxxget foo$ cat /tmp/commands.txt | redis-cliOK(integer) 101(integer) 6&quot;101xxx&quot;$ cat /tmp/commands.txtset foo 100incr fooappend foo xxxget foo$ cat /tmp/commands.txt | redis-cliOK(integer) 101(integer) 6&quot;101xxx&quot; 中间有错误会继续执行 持续运行相同的命令-r Execute specified command N times. -i When -r is used, waits seconds per command.It is possible to specify sub-second times like -i 0.1. 123456[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 -r 5 -i 1 pingPONGPONGPONGPONG... 使用redis-cli批量插入数据使用redis-cli进行批量插入会被分离出来，因为它本身就是一个有价值的话题。 请参阅我们的批量插入指南. CSV格式输出--csv Output in CSV format. 1234$ redis-cli lpush mylist a b c d(integer) 4$ redis-cli --csv lrange mylist 0 -1&quot;d&quot;,&quot;c&quot;,&quot;b&quot;,&quot;a&quot; 执行lua脚本redis-cli对使用Lua脚本的新Lua调试工具提供了广泛的支持，从Redis 3.2开始。 有关此功能，请参阅Redis Lua调试器文档。 但是，即使不使用调试器，与以交互方式将脚本键入到shell或作为参数相比，您可以使用redis-cli以更舒适的方式从文件运行脚本：123456789$ cat /tmp/script.luareturn redis.call(&#x27;set&#x27;,KEYS[1],ARGV[1])$ redis-cli --eval /tmp/script.lua foo , barOK[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 --eval ./script.lua foo , bar # foo空格,空格barOK[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 get foo&quot;bar&quot; Interactive mode 交互模式到目前为止，我们探讨了如何使用Redis CLI作为命令行程序。 这对于脚本和某些类型的测试非常有用，但是大多数人将大部分时间都花在使用交互模式的redis-cli上。 在交互模式下，用户在提示符下键入Redis命令。 该命令被发送到服务器，进行处理，回复被解析并呈现为更简单的形式来读取。 在交互模式下运行CLI并不需要什么特别的东西 - 只要在没有任何争论的情况下进行午餐就可以了： 123$ redis-cli127.0.0.1:6379&gt; pingPONG 127.0.0.1:6379&gt;是提示符,提示你连接server和database. 当改变server或选择其他非0database时提示符会改变 12345678127.0.0.1:6379&gt; select 2OK127.0.0.1:6379[2]&gt; dbsize(integer) 1127.0.0.1:6379[2]&gt; select 0OK127.0.0.1:6379&gt; dbsize(integer) 503 处理连接和重新连接connect命令,指定hostname和port即可连接另一个instance123[root@cn_mu_binlog_backup ~]# redis-cli -p 6381127.0.0.1:6381&gt; connect 127.0.0.1 6380127.0.0.1:6380&gt; 正如你所看到的提示相应地改变。 如果用户尝试连接到不可访问的实例，则redis-cli将进入断开连接模式，并尝试在执行每个新命令时重新连接：1234567127.0.0.1:6380&gt; connect 127.0.0.1 9999Could not connect to Redis at 127.0.0.1:9999: Connection refusednot connected&gt; pingCould not connect to Redis at 127.0.0.1:9999: Connection refusednot connected&gt; pingCould not connect to Redis at 127.0.0.1:9999: Connection refusednot connected&gt; 通常在检测到断开连接后，CLI始终尝试重新连接：如果尝试失败，则显示错误并进入断开连接状态。 以下是断开和重新连接的示例：12345127.0.0.1:6379&gt; debug restartCould not connect to Redis at 127.0.0.1:6379: Connection refusednot connected&gt; pingPONG127.0.0.1:6379&gt; (now we are connected again) 重新连接时，redis-cli会自动重新选择最后一个选择的数据库编号。 然而，关于连接的所有其他状态都会丢失，例如，如果我们处于中间状态，12345678910$ redis-cli127.0.0.1:6379&gt; multi --标记一个事物开始OK127.0.0.1:6379&gt; pingQUEUED( here the server is manually restarted )127.0.0.1:6379&gt; exec(error) ERR EXEC without MULTI 在交互模式下使用CLI进行测试时，这通常不是问题，但您应该知道这个限制。 Editing, history and completion由于redis-cli使用linenoise行编辑库，因此它总是具有行编辑功能，而不依赖libreadline或其他可选库。 您可以访问已执行的命令的历史记录，以便通过按方向键（向上和向下）来避免重复输入它们。 在通过HOME环境变量指定的用户主目录内的.rediscli_history文件中，CLI的重新启动之间保存历史记录。 可以通过设置REDISCLI_HISTFILE环境变量来使用不同的历史文件名，并通过将其设置为/dev/null来禁用它。 CLI也可以通过按TAB键执行命令补全，如下例所示：123127.0.0.1:6379&gt; Z&lt;TAB&gt;127.0.0.1:6379&gt; ZADD&lt;TAB&gt;127.0.0.1:6379&gt; ZCARD&lt;TAB&gt; Running the same command N times可以通过在命令名前添加一个数字来多次运行相同的命令：123456127.0.0.1:6379&gt; 5 incr mycounter(integer) 1(integer) 2(integer) 3(integer) 4(integer) 5 Showing help about Redis commandsRedis has a number of commands and sometimes, as you test things, you may not remember the exact order of arguments. redis-cli provides online help for most Redis commands, using the help command. The command can be used in two forms: help @ shows all the commands about a given category. The categories are: @generic, @list, @set, @sorted_set, @hash, @pubsub, @transactions, @connection, @server, @scripting, @hyperloglog.help shows specific help for the command given as argument.For example in order to show help for the PFADD command, use: 127.0.0.1:6379&gt; help PFADD PFADD key element [element …] summary: Adds the specified elements to the specified HyperLogLog. since: 2.8.9 Note that help supports TAB completion as well. 123456789101112131415161718127.0.0.1:6381&gt; help @set SADD key member [member ...] summary: Add one or more members to a set since: 1.0.0 SCARD key summary: Get the number of members in a set since: 1.0.0 SDIFF key [key ...] summary: Subtract multiple sets since: 1.0.0 SDIFFSTORE destination key [key ...] summary: Subtract multiple sets and store the resulting set in a key since: 1.0.0... Clearing the terminal screen 清屏1clear 特殊的操作模式至此我们见识了redis-cli的两种主要模式 Redis命令的命令行执行。 交互式的“类似REPL”的用法。 However the CLI performs other auxiliary tasks related to Redis that are explained in the next sections: Monitoring tool to show continuous stats about a Redis server. 监控工具 Scanning a Redis database for very large keys. 扫描database找出very large key Key space scanner with pattern matching. 模式匹配扫描 Acting as a Pub/Sub client to subscribe to channels. 作为Pub/Sub client订阅频道 Monitoring the commands executed into a Redis instance. 监视执行到Redis实例中的命令 Checking the latency of a Redis server in different ways. 以不同的方式检查Redis服务器的延迟 Checking the scheduler latency of the local computer. 检查本地计算机的调度程序延迟 Transferring RDB backups from a remote Redis server locally. 从远程Redis服务器传输RDB备份到本地 Acting as a Redis slave for showing what a slave receives. 扮演Redis的slave,展示slave所接受的东西 Simulating LRU workloads for showing stats about keys hits. 模拟LRU工作负载显示有关keys命中的统计信息 A client for the Lua debugger. Continuous stats mode这可能是redis-cli的一个鲜为人知的特性之一，并且为了实时监控Redis实例非常有用。 要启用此模式，使用--stat选项。 在这种模式下，CLI的行为输出内容显而易见：1234567891011$ redis-cli --stat------- data ------ --------------------- load -------------------- - child -keys mem clients blocked requests connections506 1015.00K 1 0 24 (+0) 7506 1015.00K 1 0 25 (+1) 7506 3.40M 51 0 60461 (+60436) 57506 3.40M 51 0 146425 (+85964) 107507 3.40M 51 0 233844 (+87419) 157507 3.40M 51 0 321715 (+87871) 207508 3.40M 51 0 408642 (+86927) 257508 3.40M 51 0 497038 (+88396) 257 在这种模式下，每秒钟都会打印一行新的信息，并显示旧数据点之间的差异。 您可以轻松了解内存使用情况，连接的客户端等情况。 在这种情况下，-i &lt;间隔&gt;选项作为修改interval。 默认是一秒钟。 Scanning for big keys在这个特殊的模式下，redis-cli作为一个关键的空间分析器。 它扫描大键的数据集，但也提供有关数据集组成的数据类型的信息。 使用--bigkeys选项启用此模式，并生成相当详细的输出：12345678910111213141516171819202122232425$ redis-cli --bigkeys# Scanning the entire keyspace to find biggest keys as well as# average sizes per key type. You can use -i 0.1 to sleep 0.1 sec# per 100 SCAN commands (not usually needed).[00.00%] Biggest string found so far &#x27;key-419&#x27; with 3 bytes[05.14%] Biggest list found so far &#x27;mylist&#x27; with 100004 items[35.77%] Biggest string found so far &#x27;counter:__rand_int__&#x27; with 6 bytes[73.91%] Biggest hash found so far &#x27;myobject&#x27; with 3 fields-------- summary -------Sampled 506 keys in the keyspace!Total key length in bytes is 3452 (avg len 6.82)Biggest string found &#x27;counter:__rand_int__&#x27; has 6 bytesBiggest list found &#x27;mylist&#x27; has 100004 itemsBiggest hash found &#x27;myobject&#x27; has 3 fields504 strings with 1403 bytes (99.60% of keys, avg size 2.78)1 lists with 100004 items (00.20% of keys, avg size 100004.00)0 sets with 0 members (00.00% of keys, avg size 0.00)1 hashs with 3 fields (00.20% of keys, avg size 3.00)0 zsets with 0 members (00.00% of keys, avg size 0.00)在输出的第一部分中，报告每个大于前一个较大键（相同类型）的新键。 摘要部分提供有关Redis实例内数据的一般统计信息。 该程序使用SCAN命令，因此可以在繁忙的服务器上执行，而不会影响操作，但是可以使用-i选项来限制所请求的每个100个键的指定秒数的扫描进程。 例如，-i 0.1会减慢程序的执行速度，但也会将服务器上的负载减少到很小的数量。 请注意，摘要还会以更清晰的形式报告每次发现的最大密钥。 如果对一个非常大的数据集运行，最初的输出只是提供一些有趣的信息。 Getting a list of keys也可以扫描key空间，再次以不阻塞Redis服务器的方式（当您使用诸如KEYS *之类的命令时发生这种情况），打印所有的键名，或者对特定的模式进行过滤。 与-bigkeys选项一样，此模式使用SCAN命令，因此，如果数据集正在更改，可能会多次报告键，但是如果从迭代开始以来存在该键，则不会丢失键。 由于它使用这个选项的命令叫做--scan。 1234567891011$ redis-cli --scan | head -10key-419key-71key-236key-50key-38key-458key-453key-499key-446key-371 --pattern模式匹配123456789101112$ redis-cli --scan --pattern &#x27;*-11*&#x27;key-114key-117key-118key-113key-115key-112key-119key-11key-111key-110key-116 Piping the output through the wc command can be used to count specific kind of objects, by key name:12$ redis-cli --scan --pattern &#x27;user:*&#x27; | wc -l3829433 Pub/sub modeThe CLI is able to publish messages in Redis Pub/Sub channels just using the PUBLISH command. This is expected since the PUBLISH command is very similar to any other command. Subscribing to channels in order to receive messages is different - in this case we need to block and wait for messages, so this is implemented as a special mode in redis-cli. Unlike other special modes this mode is not enabled by using a special option, but simply by using the SUBSCRIBE or PSUBSCRIBE command, both in interactive or non interactive mode: 123456789101112131415161718192021222324252627282930[root@cn_mu_binlog_backup ~]# redis-cli -p 6381 psubscribe &#x27;*&#x27;Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;*&quot;3) (integer) 11) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26381,3ad32fcf6646f1e65f75f1309fe858a66237a590,3,mymaster,127.0.0.1,6381,3&quot;1) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26379,6db624d52ea9a46b76c996532de0d3b874011aef,3,mymaster,127.0.0.1,6381,3&quot;1) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26380,55b2bbd5f4ca70d5bbb8fb019b645eaf39fb5e66,3,mymaster,127.0.0.1,6381,3&quot;1) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26381,3ad32fcf6646f1e65f75f1309fe858a66237a590,3,mymaster,127.0.0.1,6381,3&quot;1) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26379,6db624d52ea9a46b76c996532de0d3b874011aef,3,mymaster,127.0.0.1,6381,3&quot;1) &quot;pmessage&quot;2) &quot;*&quot;3) &quot;__sentinel__:hello&quot;4) &quot;127.0.0.1,26380,55b2bbd5f4ca70d5bbb8fb019b645eaf39fb5e66,3,mymaster,127.0.0.1,6381,3&quot;^C This is very useful for debugging Pub/Sub issues. To exit the Pub/Sub mode just process CTRL-C. Monitoring commands executed in RedisSimilarly to the Pub/Sub mode, the monitoring mode is entered automatically once you use the MONITOR mode. It will print all the commands received by a Redis instance:1234$ redis-cli monitorOK1460100081.165665 [0 127.0.0.1:51706] &quot;set&quot; &quot;foo&quot; &quot;bar&quot;1460100083.053365 [0 127.0.0.1:51707] &quot;get&quot; &quot;foo&quot; Monitoring the latency of Redis instancesRedis经常用在延迟非常关键的环境中。 延迟涉及应用程序中的多个移动部分，从客户端库到网络堆栈，直到Redis实例本身。 CLI具有多个工具来研究Redis实例的延迟，并了解延迟的最大值，平均值和分布。 基本的延迟检查工具是–latency选项。 使用此选项，CLI运行一个循环，将PING命令发送到Redis实例，并测量获得答复的时间。 这种情况每秒发生100次，统计信息在控制台中实时更新：12$ redis-cli --latencymin: 0, max: 1, avg: 0.19 (427 samples) 统计数据以毫秒提供。 通常，由于运行redis-cli本身的系统的内核调度程序导致延迟，所以一个非常快的实例的平均延迟往往被高估了一点，所以0.19以上的平均延迟可能很容易为0.01或更小。 然而，这通常不是一个大问题，因为我们对几毫秒甚至更长时间的事件感兴趣。 有时研究最大和平均潜伏期如何演变是有用的。 --latency-history选项用于此目的：它的工作方式与–latency完全相同，但是每15秒（默认情况下）将从头开始一个新的采样会话：1234$ redis-cli --latency-historymin: 0, max: 1, avg: 0.14 (1314 samples) -- 15.01 seconds rangemin: 0, max: 1, avg: 0.18 (1299 samples) -- 15.00 seconds rangemin: 0, max: 1, avg: 0.20 (113 samples)^C 您可以使用-i &lt;间隔&gt;选项更改采样会话的长度。 最先进的等待时间研究工具，对于没有经验的用户来说也有点难解释，是使用彩色终端显示一系列等待时间的能力。 您会看到一个彩色输出，指示不同百分比的样本，以及指示不同延迟数字的不同ASCII字符。 该模式使用–latency-dist选项启用： 在redis-cli中还有一个非常不寻常的延迟工具。 它不会检查Redis实例的延迟，而是检查运行redis-cli的计算机的延迟。 你可能会问什么延迟？ 内核调度程序固有的延迟，虚拟化实例情况下的管理程序等等。 我们称之为内部延迟，因为大多数程序员对它是不透明的。 如果您的Redis实例的延迟时间很长，而不考虑所有可能的原因，那么通过在运行Redis服务器的系统中直接运行此特殊模式下的redis-cli，可以检查系统的最佳性能。 通过测量内在的延迟，你知道这是基准，Redis不能超越你的系统。 为了在此模式下运行CLI，请使用--intrinsic-latency &lt;test-time&gt;。 测试的时间以秒为单位，并指定redis-cli应该检查当前正在运行的系统的延迟时间。1234567891011121314$ redis-cli --intrinsic-latency 5Max latency so far: 1 microseconds.Max latency so far: 7 microseconds.Max latency so far: 9 microseconds.Max latency so far: 11 microseconds.Max latency so far: 13 microseconds.Max latency so far: 15 microseconds.Max latency so far: 34 microseconds.Max latency so far: 82 microseconds.Max latency so far: 586 microseconds.Max latency so far: 739 microseconds.65433042 total runs (avg latency: 0.0764 microseconds / 764.14 nanoseconds per run).Worst run took 9671x longer than the average latency. 重要信息：必须在要运行Redis服务器的计算机上执行此命令，而不是在不同的主机上运行。 它甚至不连接到Redis实例，只在本地执行测试。 在上述情况下，我的系统不能比739微秒的最坏情况延迟更好，所以我预计某些查询可能会在不到1毫秒的时间内运行。 Remote backups of RDB files在Redis复制的第一次同步期间，主设备和从设备以RDB文件的形式交换整个数据集。 redis-cli利用此功能来提供远程备份功能，允许从任何Redis实例将RDB文件传输到运行redis-cli的本地计算机。 要使用此模式，请使用--rdb &lt;dest-filename&gt;选项调用CLI：123$ redis-cli --rdb /tmp/dump.rdbSYNC sent to master, writing 13256 bytes to &#x27;/tmp/dump.rdb&#x27;Transfer finished with success.这是确保您拥有Redis实例的灾难恢复RDB备份的简单而有效的方法。 但是，在脚本或cron作业中使用此选项时，请确保检查命令的返回值。 如果不为零，则发生如下例所示的错误：1234$ redis-cli --rdb /tmp/dump.rdbSYNC with master failed: -ERR Can&#x27;t SYNC while not connected with my master$ echo $?1 Slave modeCLI的slave模式是用于Redis开发人员和调试操作的高级功能 123456789这边跑着,同时主库执行set foo bar$ redis-cli --slaveSYNC with master, discarding 13256 bytes of bulk transfer...SYNC done. Logging commands from master.&quot;PING&quot;&quot;SELECT&quot;,&quot;0&quot;&quot;set&quot;,&quot;foo&quot;,&quot;bar&quot;&quot;PING&quot;&quot;incr&quot;,&quot;mycounter&quot; 该命令首先丢弃第一个同步的RDB文件，然后以CSV格式记录每个收到的命令。 如果您认为某些命令在您的从站中没有被正确复制，那么这是检查发生了什么的好方法，也是有用的信息，以便改进错误报告。 Performing an LRU simulationRedis通常用作LRU驱逐的缓存。根据keys的数量和为缓存分配的内存量（通过maxmemory指令指定），缓存命中和未命中的数量将会改变。有时，模拟命中率对正确设置缓存非常有用。 CLI有一个特殊的模式，它执行GET和SET操作的模拟，在请求模式中使用80-20％的幂律分布。这意味着20％的keys将被请求80％的时间，这是缓存场景中的普遍分布。 从理论上讲，考虑到请求分布和Redis内存开销，应该可以用数学公式分析计算命中率。但是，Redis可以配置不同的LRU设置（样本数量），LRU的实现（在Redis中近似）在不同版本之间会发生很大的变化。类似的，每个内存的数量可能会在不同的版本之间改变。这就是为什么这个工具是建立起来的：它的主要动机是测试Redis的LRU实现的质量，但是现在也可以用来测试一个给定的版本如何与你想要部署的想法一致。 为了使用这种模式，你需要在测试中指定keys的数量。您还需要配置一个有意义的maxmemory设置作为第一次尝试。 重要注意事项：在Redis配置中配置maxmemory设置是至关重要的：如果最大内存使用量没有上限，则最终命中率将为100％，因为所有的keys都可以存储在内存中。或者如果你指定了太多的keys并且没有最大的内存，最终所有的计算机RAM都将被使用。还需要配置一个适当的maxmemory策略，大多数时候你想要的是allkeys-lru。 在下面的例子中，我配置了一个100MB的内存限制和一个1000万keys的LRU模拟。 警告：测试使用流水线并将压力服务器，不要与生产实例一起使用。123456789$ ./redis-cli --lru-test 10000000156000 Gets/sec | Hits: 4552 (2.92%) | Misses: 151448 (97.08%)153750 Gets/sec | Hits: 12906 (8.39%) | Misses: 140844 (91.61%)159250 Gets/sec | Hits: 21811 (13.70%) | Misses: 137439 (86.30%)151000 Gets/sec | Hits: 27615 (18.29%) | Misses: 123385 (81.71%)145000 Gets/sec | Hits: 32791 (22.61%) | Misses: 112209 (77.39%)157750 Gets/sec | Hits: 42178 (26.74%) | Misses: 115572 (73.26%)154500 Gets/sec | Hits: 47418 (30.69%) | Misses: 107082 (69.31%)151250 Gets/sec | Hits: 51636 (34.14%) | Misses: 99614 (65.86%)该程序每秒显示统计信息。 如您所见，在第一秒钟内缓存开始被填充。 后来的失误率稳定在我们可以预期的实际数字上：1234120750 Gets/sec | Hits: 48774 (40.39%) | Misses: 71976 (59.61%)122500 Gets/sec | Hits: 49052 (40.04%) | Misses: 73448 (59.96%)127000 Gets/sec | Hits: 50870 (40.06%) | Misses: 76130 (59.94%)124250 Gets/sec | Hits: 50147 (40.36%) | Misses: 74103 (59.64%)对于我们的用例，59％的未命中率可能是不可接受的。 所以我们知道100MB的内存是不够的。 让我们试试500M。 几分钟后，我们将看到输出稳定到以下数字：1234140000 Gets/sec | Hits: 135376 (96.70%) | Misses: 4624 (3.30%)141250 Gets/sec | Hits: 136523 (96.65%) | Misses: 4727 (3.35%)140250 Gets/sec | Hits: 135457 (96.58%) | Misses: 4793 (3.42%)140500 Gets/sec | Hits: 135947 (96.76%) | Misses: 4553 (3.24%)所以我们知道，在500MB的情况下，我们的密钥数量（1000万）和分布（80-20样式）已经足够好了。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://fuxkdb.com/tags/Redis/"},{"name":"redis-cli","slug":"redis-cli","permalink":"http://fuxkdb.com/tags/redis-cli/"}]},{"title":"自定义函数改表关联优化一例","slug":"自定义函数改表关联优化一例","date":"2017-12-28T10:05:00.000Z","updated":"2017-12-28T10:13:35.000Z","comments":true,"path":"2017/12/28/自定义函数改表关联优化一例/","link":"","permalink":"http://fuxkdb.com/2017/12/28/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E6%94%B9%E8%A1%A8%E5%85%B3%E8%81%94%E4%BC%98%E5%8C%96%E4%B8%80%E4%BE%8B/","excerpt":"今天朋友丢来一个SQL,叫我帮忙优化一下.受过落总真传,我瞄了几眼就知道咋回事了 123456789101112131415161718192021222324252627282930313233343536SELECT ESS.PK_NO, HE.EMPID, HE.LOCAL_NAME, ESS.ITEM_NO ITEM_NO_NO, ESS.PERSON_ID, GET_DEPT_NAME(HE.DEPTNO, &#x27;zh&#x27;) DEPT_NAME, GET_GLOBAL_NAME(ESS.ITEM_NO, &#x27;zh&#x27;) ITEM_NAME, ESS.AR_DATE_STR, TO_CHAR(ESS.FROM_TIME, &#x27;HH24:MI&#x27;) FROM_TIME, TO_CHAR(ESS.TO_TIME, &#x27;HH24:MI&#x27;) TO_TIME, ESS.QUANTITY, ESS.REMARK, HE.EMPID, ESS.AR_DATE_STR, GET_GLOBAL_NAME(ESS.STATUS_CODE, &#x27;zh&#x27;) STATUS_CODE, GET_GLOBAL_NAME(ESS.ITEM_NO, &#x27;zh&#x27;) ITEM_NO, ESS.REMARK, ESS.LOCK_YN FROM AR_DETAIL_HYOSUNG_JX ESS, HR_EMPLOYEE HE WHERE ESS.PERSON_ID = HE.PERSON_ID AND ESS.PERSON_ID NOT LIKE &#x27;111111%&#x27; AND ESS.ITEM_NO IN (&#x27;141454&#x27;, &#x27;14015951&#x27;, &#x27;141445&#x27;, &#x27;141443&#x27;, &#x27;190000514&#x27;) AND EXISTS (SELECT B1.DEPTID FROM HR_DEPARTMENT B1 WHERE B1.DEPTNO = HE.DEPTNO START WITH B1.DEPTNO in (SELECT HRD.DEPTID FROM HR_DEPARTMENT HRD WHERE HRD.MANAGER_EMP_ID = &#x27;11111117&#x27;) CONNECT BY PRIOR B1.DEPTNO = B1.PARENT_DEPT_NO UNION SELECT AR_SUPERVISOR_INFO.DEPTNO FROM AR_SUPERVISOR_INFO WHERE AR_SUPERVISOR_INFO.DEPTNO = HE.DEPTNO AND AR_SUPERVISOR_INFO.PERSON_ID = &#x27;11111117&#x27;) ORDER BY ESS.AR_DATE_STR ASC, ESS.CREATE_DATE DESC, HE.DEPTNO, HE.EMPID 我让他先执行一下SQL 看看几秒,他说6秒OK,再把select 里面那俩自定义函数GET_DEPT_NAME,GET_GLOBAL_NAME 注释掉查一下, 他说1.5秒那这个SQL就是慢在这俩自定义函数上呗, 这个查询每返回一行,这函数就要执行一次那么函数可以改成 标量 , 标量可以改成 letf join.","text":"今天朋友丢来一个SQL,叫我帮忙优化一下.受过落总真传,我瞄了几眼就知道咋回事了 123456789101112131415161718192021222324252627282930313233343536SELECT ESS.PK_NO, HE.EMPID, HE.LOCAL_NAME, ESS.ITEM_NO ITEM_NO_NO, ESS.PERSON_ID, GET_DEPT_NAME(HE.DEPTNO, &#x27;zh&#x27;) DEPT_NAME, GET_GLOBAL_NAME(ESS.ITEM_NO, &#x27;zh&#x27;) ITEM_NAME, ESS.AR_DATE_STR, TO_CHAR(ESS.FROM_TIME, &#x27;HH24:MI&#x27;) FROM_TIME, TO_CHAR(ESS.TO_TIME, &#x27;HH24:MI&#x27;) TO_TIME, ESS.QUANTITY, ESS.REMARK, HE.EMPID, ESS.AR_DATE_STR, GET_GLOBAL_NAME(ESS.STATUS_CODE, &#x27;zh&#x27;) STATUS_CODE, GET_GLOBAL_NAME(ESS.ITEM_NO, &#x27;zh&#x27;) ITEM_NO, ESS.REMARK, ESS.LOCK_YN FROM AR_DETAIL_HYOSUNG_JX ESS, HR_EMPLOYEE HE WHERE ESS.PERSON_ID = HE.PERSON_ID AND ESS.PERSON_ID NOT LIKE &#x27;111111%&#x27; AND ESS.ITEM_NO IN (&#x27;141454&#x27;, &#x27;14015951&#x27;, &#x27;141445&#x27;, &#x27;141443&#x27;, &#x27;190000514&#x27;) AND EXISTS (SELECT B1.DEPTID FROM HR_DEPARTMENT B1 WHERE B1.DEPTNO = HE.DEPTNO START WITH B1.DEPTNO in (SELECT HRD.DEPTID FROM HR_DEPARTMENT HRD WHERE HRD.MANAGER_EMP_ID = &#x27;11111117&#x27;) CONNECT BY PRIOR B1.DEPTNO = B1.PARENT_DEPT_NO UNION SELECT AR_SUPERVISOR_INFO.DEPTNO FROM AR_SUPERVISOR_INFO WHERE AR_SUPERVISOR_INFO.DEPTNO = HE.DEPTNO AND AR_SUPERVISOR_INFO.PERSON_ID = &#x27;11111117&#x27;) ORDER BY ESS.AR_DATE_STR ASC, ESS.CREATE_DATE DESC, HE.DEPTNO, HE.EMPID 我让他先执行一下SQL 看看几秒,他说6秒OK,再把select 里面那俩自定义函数GET_DEPT_NAME,GET_GLOBAL_NAME 注释掉查一下, 他说1.5秒那这个SQL就是慢在这俩自定义函数上呗, 这个查询每返回一行,这函数就要执行一次那么函数可以改成 标量 , 标量可以改成 letf join. 附上那俩函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109CREATE OR REPLACE FUNCTION &quot;GET_GLOBAL_NAME&quot; (IN_code_NO IN VARCHAR2,in_language VARCHAR2) RETURN VARCHAR2IS v_name VARCHAR2 (100);/****************************************************************************** NAME : -- GET_GLOBAL_NAME PURPOSE : -- 依据传入项目O和语言参数查找国际化名称 IMPUT : -- IN_code_NO code_no, in_language 语言 OUTPUT : -- none Author : -- hj CreateDate : -- 2012-3-2******************************************************************************/BEGIN BEGIN SELECT a.CONTENT INTO v_name FROM sy_global_name a WHERE a.no = IN_code_NO AND a.LANGUAGE = in_language ; EXCEPTION WHEN NO_DATA_FOUND THEN v_name := &#x27;&#x27;; WHEN OTHERS THEN RAISE; END; RETURN v_name;END GET_GLOBAL_NAME;CREATE OR REPLACE FUNCTION &quot;GET_DEPT_NAME&quot;(in_deptno IN VARCHAR2, in_language VARCHAR2) RETURN VARCHAR2 IS out_department VARCHAR2(200); /****************************************************************************** NAME : -- GET_DEPT_NAME PURPOSE : -- 依据法人参数得到此部门编号的部门名称 IMPUT : -- in_deptno 部门名称 IN_CPNY_ID 公司D OUTPUT : -- none Author : -- system CreateDate : -- 2011-12-29 14:57:33 UpdateDate : -- 函数更改信息（包括作者、时间、更改内容等） ******************************************************************************/BEGIN BEGIN IF in_language = &#x27;zh&#x27; then SELECT a.org_name_local INTO out_department FROM hr_department a WHERE a.deptno = in_deptno and rownum = 1; ELSIF in_language = &#x27;en&#x27; then SELECT a.org_name_eng INTO out_department FROM hr_department a WHERE a.deptno = in_deptno and rownum = 1; else SELECT a.org_name_ko INTO out_department FROM hr_department a WHERE a.deptno = in_deptno and rownum = 1; end if; EXCEPTION WHEN NO_DATA_FOUND THEN out_department := &#x27;0&#x27;; END; IF out_department = &#x27;0&#x27; THEN BEGIN IF in_language = &#x27;zh&#x27; then SELECT a.org_name_local INTO out_department FROM org_info a WHERE a.deptno = in_deptno and rownum = 1; ELSIF in_language = &#x27;en&#x27; then SELECT a.org_name_eng INTO out_department FROM org_info a WHERE a.deptno = in_deptno and rownum = 1; ELSE SELECT a.org_name_ko INTO out_department FROM org_info a WHERE a.deptno = in_deptno and rownum = 1; END IF; EXCEPTION WHEN NO_DATA_FOUND THEN out_department := &#x27;&#x27;; END; END IF; RETURN out_department;END get_dept_name; SELECT a.org_name_local INTO out_department FROM org_info a WHERE a.deptno = in_deptno and rownum = 1; 最后改完,0.4秒,交差了1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859with t1 as (select deptno from (SELECT B1.DEPTNO DEPTNO FROM HR_DEPARTMENT B1 START WITH B1.DEPTNO in (SELECT HRD.DEPTID FROM HR_DEPARTMENT HRD WHERE HRD.MANAGER_EMP_ID = &#x27;11111117&#x27;) CONNECT BY PRIOR B1.DEPTNO = B1.PARENT_DEPT_NO UNION SELECT AR_SUPERVISOR_INFO.DEPTNO DEPTNO FROM AR_SUPERVISOR_INFO where AR_SUPERVISOR_INFO.PERSON_ID = &#x27;11111117&#x27;)),global as (select a.CONTENT, a.no from sy_global_name a where a.language = &#x27;zh&#x27;),department as (select max(org_name_local) org_name_local, deptno from hr_department group by deptno),info as (select max(org_name_local) org_name_local, deptno from org_info group by deptno)SELECT ESS.PK_NO, HE.EMPID, HE.LOCAL_NAME, ESS.ITEM_NO ITEM_NO_NO, ESS.PERSON_ID, coalesce(department.org_name_local, info.org_name_local, &#x27;&#x27;) DEPT_NAME, g1.content ITEM_NAME, ESS.AR_DATE_STR, TO_CHAR(ESS.FROM_TIME, &#x27;HH24:MI&#x27;) FROM_TIME, TO_CHAR(ESS.TO_TIME, &#x27;HH24:MI&#x27;) TO_TIME, ESS.QUANTITY, ESS.REMARK, HE.EMPID, ESS.AR_DATE_STR, g2.content STATUS_CODE, g3.content ITEM_NO, ESS.REMARK, ESS.LOCK_YN FROM AR_DETAIL_HYOSUNG_JX ESS inner join HR_EMPLOYEE HE on ESS.PERSON_ID = HE.PERSON_ID left join global g1 on ess.ITEM_NO = g1.no left join global g2 on ESS.STATUS_CODE = g2.no left join global g3 on ESS.ITEM_NO = g3.no left join department on HE.DEPTNO = department.deptno left join info on HE.DEPTNO = info.deptno where ESS.PERSON_ID NOT LIKE &#x27;111111%&#x27; AND ESS.ITEM_NO IN (&#x27;141454&#x27;, &#x27;14015951&#x27;, &#x27;141445&#x27;, &#x27;141443&#x27;, &#x27;190000514&#x27;) AND HE.DEPTNO in (select deptno from t1) ORDER BY ESS.AR_DATE_STR ASC, ESS.CREATE_DATE DESC, HE.DEPTNO, HE.EMPID","categories":[],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://fuxkdb.com/tags/Oracle/"},{"name":"SQL Tuning","slug":"SQL-Tuning","permalink":"http://fuxkdb.com/tags/SQL-Tuning/"}]},{"title":"PMM使用Grafana告警","slug":"PMM使用Grafana告警","date":"2017-12-15T05:26:00.000Z","updated":"2017-12-15T05:59:47.000Z","comments":true,"path":"2017/12/15/PMM使用Grafana告警/","link":"","permalink":"http://fuxkdb.com/2017/12/15/PMM%E4%BD%BF%E7%94%A8Grafana%E5%91%8A%E8%AD%A6/","excerpt":"PMM如何告警?从Grafana v4.0开始增加了Alterting功能 ( PMM 1.0.7 版本时Grafana版本为4.0). 这篇文章将手把手教你如何配置你的告警 开始创建Alert在PMM部署完成后,你可以看到如下界面 此时你可能需要对Threads_connected / Threads_running 指标进行监控 点击对应的Graph标题,点击Edit 按下图方式依次点击ALert -&gt; Create Alert 创建告警 不幸的是,当你尝试对A指标创建如下告警时,Grafana提示一个错误“Template variables are not supported in alert queries.” 首先A代表什么可以从Metrics菜单中看到,从图中可以看到对于Threads_connected值的获取表达式中包含了变量$host, 而$host是箭头所指的Host下拉菜单传递的对于使用变量的Mertrics,无法创建Alert","text":"PMM如何告警?从Grafana v4.0开始增加了Alterting功能 ( PMM 1.0.7 版本时Grafana版本为4.0). 这篇文章将手把手教你如何配置你的告警 开始创建Alert在PMM部署完成后,你可以看到如下界面 此时你可能需要对Threads_connected / Threads_running 指标进行监控 点击对应的Graph标题,点击Edit 按下图方式依次点击ALert -&gt; Create Alert 创建告警 不幸的是,当你尝试对A指标创建如下告警时,Grafana提示一个错误“Template variables are not supported in alert queries.” 首先A代表什么可以从Metrics菜单中看到,从图中可以看到对于Threads_connected值的获取表达式中包含了变量$host, 而$host是箭头所指的Host下拉菜单传递的对于使用变量的Mertrics,无法创建Alert 咋办呢?点击Graph标题 -&gt; Panel Json复制json代码新建Dashboards创建一个Graph点击Graph标题 -&gt; Panel Json粘贴刚才复制的Json,替换掉现有的此时Graph是不可用的,注意红色叹号将$host $interval 替换, duang~ 出图了现在就可以创建Alert了这里我们监控Metrics B也就是Threads_running点击Notification,添加接收告警组 和 注释信息这个DBA组是哪来的呢?其实需要提前建好的. 看图就好,不解释了 到这还没完可以看到上面我们是通过邮件告警(当然Grafana还支持很多方式) . 那么我们还需要配置Grafana让它能发邮件进入容器,编辑grafana.ini1234567891011[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESec8fd0553984 percona/pmm-server:1.5.2 &quot;/opt/entrypoint.sh&quot; 38 hours ago Up 38 hours 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server[root@localhost ~]# docker exec -it ec8fd0553984 /bin/bash[root@ec8fd0553984 opt]# vi /etc/grafana/grafana.ini 在[smtp]区块下添加,以我这里为例enabled = Truehost = &quot;smtp.exmail.qq.com:465&quot;user = &quot;papapa@xxoo.com&quot;password = &quot;durex&quot;from_address = &quot;papapa@xxoo.com&quot;保存退出,重启容器 高大上的告警邮件","categories":[],"tags":[{"name":"MySQL监控","slug":"MySQL监控","permalink":"http://fuxkdb.com/tags/MySQL%E7%9B%91%E6%8E%A7/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"Pt-table-checksum原理浅析","slug":"Pt-table-checksum原理浅析","date":"2017-11-24T03:01:00.000Z","updated":"2017-11-24T03:01:53.000Z","comments":true,"path":"2017/11/24/Pt-table-checksum原理浅析/","link":"","permalink":"http://fuxkdb.com/2017/11/24/Pt-table-checksum%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/","excerpt":"Pt-table-checksum原理浅析主库建一个表12345node1&gt; create table fan(id int) engine=innodb;Query OK, 0 rows affected (0.12 sec)node1&gt; insert into fan values(1);Query OK, 1 row affected (0.07 sec)从库制造不一致123456789node2&gt; select * from fan;+------+| id |+------+| 1 |+------+1 row in set (0.00 sec)node2&gt; update fan set id=2;两边打开general log,然后123[root@node1 ~]# pt-table-checksum --nocheck-replication-filters --no-check-binlog-format h=172.16.83.17,u=root,p=mysql,P=3307 --replicate=percona.checksums --recursion-method=dsn=h=172.16.83.21,D=percona,t=dsns --databases=sysbench --tables=fan TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE11-06T05:12:58 0 1 1 1 0 0.344 sysbench.fan 先看两边的checksums表1234567891011121314node1&gt; select * from `percona`.`checksums` where tbl=&#x27;fan&#x27;;+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| db | tbl | chunk | chunk_time | chunk_index | lower_boundary | upper_boundary | this_crc | this_cnt | master_crc | master_cnt | ts |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| sysbench | fan | 1 | 0.019798 | NULL | NULL | NULL | 42981178 | 1 | 42981178 | 1 | 2017-11-06 05:12:58 |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+node2&gt; select * from `percona`.`checksums` where tbl=&#x27;fan&#x27;;+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| db | tbl | chunk | chunk_time | chunk_index | lower_boundary | upper_boundary | this_crc | this_cnt | master_crc | master_cnt | ts |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| sysbench | fan | 1 | 0.019798 | NULL | NULL | NULL | 40deaf21 | 1 | 42981178 | 1 | 2017-11-06 05:12:58 |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+1 row in set (0.04 sec)","text":"Pt-table-checksum原理浅析主库建一个表12345node1&gt; create table fan(id int) engine=innodb;Query OK, 0 rows affected (0.12 sec)node1&gt; insert into fan values(1);Query OK, 1 row affected (0.07 sec)从库制造不一致123456789node2&gt; select * from fan;+------+| id |+------+| 1 |+------+1 row in set (0.00 sec)node2&gt; update fan set id=2;两边打开general log,然后123[root@node1 ~]# pt-table-checksum --nocheck-replication-filters --no-check-binlog-format h=172.16.83.17,u=root,p=mysql,P=3307 --replicate=percona.checksums --recursion-method=dsn=h=172.16.83.21,D=percona,t=dsns --databases=sysbench --tables=fan TS ERRORS DIFFS ROWS CHUNKS SKIPPED TIME TABLE11-06T05:12:58 0 1 1 1 0 0.344 sysbench.fan 先看两边的checksums表1234567891011121314node1&gt; select * from `percona`.`checksums` where tbl=&#x27;fan&#x27;;+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| db | tbl | chunk | chunk_time | chunk_index | lower_boundary | upper_boundary | this_crc | this_cnt | master_crc | master_cnt | ts |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| sysbench | fan | 1 | 0.019798 | NULL | NULL | NULL | 42981178 | 1 | 42981178 | 1 | 2017-11-06 05:12:58 |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+node2&gt; select * from `percona`.`checksums` where tbl=&#x27;fan&#x27;;+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| db | tbl | chunk | chunk_time | chunk_index | lower_boundary | upper_boundary | this_crc | this_cnt | master_crc | master_cnt | ts |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+| sysbench | fan | 1 | 0.019798 | NULL | NULL | NULL | 40deaf21 | 1 | 42981178 | 1 | 2017-11-06 05:12:58 |+----------+-----+-------+------------+-------------+----------------+----------------+----------+----------+------------+------------+---------------------+1 row in set (0.04 sec)node1的checksums表的this_crc,this_cnt和master_crc,master_cnt肯定是一样的因为.这边是replace into select 先计算this_crc,this_cnt .12REPLACE INTO `percona`.`checksums` (db, tbl, chunk, chunk_index, lower_boundary, upper_boundary, this_cnt, this_crc) SELECT &#x27;sysbench&#x27;, &#x27;fan&#x27;, &#x27;1&#x27;, NULL, NULL, NULL, COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS(&#x27;#&#x27;, `id`, CONCAT(ISNULL(`id`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `sysbench`.`fan` /*checksum table*/SHOW WARNINGS然后再获取this_crc, this_cnt的值1SELECT this_crc, this_cnt FROM `percona`.`checksums` WHERE db = &#x27;sysbench&#x27; AND tbl = &#x27;fan&#x27; AND chunk = &#x27;1&#x27;最后把master_crc,master_cnt更新成这个值1UPDATE `percona`.`checksums` SET chunk_time = &#x27;0.019798&#x27;, master_crc = &#x27;42981178&#x27;, master_cnt = &#x27;1&#x27; WHERE db = &#x27;sysbench&#x27; AND tbl = &#x27;fan&#x27; AND chunk = &#x27;1&#x27;而到了从库这里.由于主库设置了binlog_format=statement 所以replace into select复制过去到从库还是语句,而不是值.此时从库执行这个语句,在自己的checksums表中根据自己的实际值算出了this_crc,this_cnt12REPLACE INTO `percona`.`checksums` (db, tbl, chunk, chunk_index, lower_boundary, upper_boundary, this_cnt, this_crc) SELECT &#x27;sysbench&#x27;, &#x27;fan&#x27;, &#x27;1&#x27;, NULL, NULL, NULL, COUNT(*) AS cnt, COALESCE(LOWER(CONV(BIT_XOR(CAST(CRC32(CONCAT_WS(&#x27;#&#x27;, `id`, CONCAT(ISNULL(`id`)))) AS UNSIGNED)), 10, 16)), 0) AS crc FROM `sysbench`.`fan` /*checksum table*/COMMIT /* implicit, from Xid_log_event */然后拿主库传过来的主库的this_crc, this_cnt的值更新自己的master_crc,master_cnt123BEGINUPDATE `percona`.`checksums` SET chunk_time = &#x27;0.019798&#x27;, master_crc = &#x27;42981178&#x27;, master_cnt = &#x27;1&#x27; WHERE db = &#x27;sysbench&#x27; AND tbl = &#x27;fan&#x27; AND chunk = &#x27;1&#x27;COMMIT /* implicit, from Xid_log_event */这样,从库的checksums表中就存储了自己和主库的hash值. 最后查一把,把结果返回输出到屏幕123456789SELECT MAX(chunk) FROM `percona`.`checksums` WHERE db=&#x27;sysbench&#x27; AND tbl=&#x27;fan&#x27; AND master_crc IS NOT NULLSELECT CONCAT(db, &#x27;.&#x27;, tbl) AS `table`, chunk, chunk_index, lower_boundary, upper_boundary, COALESCE(this_cnt-master_cnt, 0) AS cnt_diff, COALESCE(this_crc &lt;&gt; master_crc OR ISNULL(master_crc) &lt;&gt; ISNULL(this_crc), 0) AS crc_diff, this_cnt, master_cnt, this_crc, master_crc FROM `percona`.`checksums` WHERE (master_cnt &lt;&gt; this_cnt OR master_crc &lt;&gt; this_crc OR ISNULL(master_crc) &lt;&gt; ISNULL(this_crc)) AND (db=&#x27;sysbench&#x27; AND tbl=&#x27;fan&#x27;)这个结果为手动查询的. pt-table-checksum根据这个结果自己整理好格式输出到屏幕+--------------+-------+-------------+----------------+----------------+----------+----------+----------+------------+----------+------------+| table | chunk | chunk_index | lower_boundary | upper_boundary | cnt_diff | crc_diff | this_cnt | master_cnt | this_crc | master_crc |+--------------+-------+-------------+----------------+----------------+----------+----------+----------+------------+----------+------------+| sysbench.fan | 1 | NULL | NULL | NULL | 0 | 1 | 1 | 1 | 40deaf21 | 42981178 |+--------------+-------+-------------+----------------+----------------+----------+----------+----------+------------+----------+------------+ 关于两边查询一致性的保证pt-table-checksum 会设置1SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ这样避免幻读,通过gap lock阻塞其他insert.保证肯定是replace into select先commit;这样肯定是replace into select先复制到从库(binlog是谁先提交记录谁)12345678910111213141516171819202122232425262728293031323334353637383940414243node1&gt; select * from fan;+----+------+| id | name |+----+------+| 1 | fan || 5 | fan || 7 | fan || 9 | fan || 10 | fan || 11 | fan || 12 | fan || 13 | fan || 14 | fan || 15 | fan |+----+------+10 rows in set (0.00 sec)node1&gt; select * from fan2;+----+------+| id | name |+----+------+| 1 | duzi || 5 | duzi || 7 | duzi || 9 | duzi || 10 | duzi || 11 | duzi || 12 | duzi || 13 | duzi || 14 | duzi || 15 | duzi |+----+------+10 rows in set (0.00 sec)node1&gt; SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;Query OK, 0 rows affected (0.00 sec)node1&gt; begin;Query OK, 0 rows affected (0.00 sec)node1&gt; replace into fan2 select * from fan where id&lt;7;Query OK, 4 rows affected, 1 warning (0.01 sec)Records: 2 Duplicates: 2 Warnings: 1node2被锁12345678node2&gt; SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;Query OK, 0 rows affected (0.00 sec)node2&gt; begin;Query OK, 0 rows affected (0.00 sec)node2&gt; insert into fan values(4,&#x27;ceshi&#x27;);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionshow engine innodb status12345678910111213141516171819202122------------TRANSACTIONS------------Trx id counter 163F4Purge done for trx&#x27;s n:o &lt; 163F0 undo n:o &lt; 0History list length 662LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 0, not startedMySQL thread id 46, OS thread handle 0x7f42f81bf700, query id 62676 localhost rootshow engine innodb status---TRANSACTION 163F3, ACTIVE 5 sec insertingmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 376, 1 row lock(s)MySQL thread id 45, OS thread handle 0x7f42b44cd700, query id 62675 localhost root updateinsert into fan values(4,&#x27;ceshi&#x27;)------- TRX HAS BEEN WAITING 5 SEC FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 49 page no 3 n bits 80 index `PRIMARY` of table `sysbench`.`fan` trx id 163F3 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 3 PHYSICAL RECORD: n_fields 4; compact format; info bits 00: len 4; hex 80000005; asc ;;1: len 6; hex 0000000163de; asc c ;;2: len 7; hex ac00000cc70110; asc ;;3: len 3; hex 66616e; asc fan;;1234567mysql&gt; select * from INNODB_TRX ;+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-----------------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+---------------------------+---------------------------+| trx_id | trx_state | trx_started | trx_requested_lock_id | trx_wait_started | trx_weight | trx_mysql_thread_id | trx_query | trx_operation_state | trx_tables_in_use | trx_tables_locked | trx_lock_structs | trx_lock_memory_bytes | trx_rows_locked | trx_rows_modified | trx_concurrency_tickets | trx_isolation_level | trx_unique_checks | trx_foreign_key_checks | trx_last_foreign_key_error | trx_adaptive_hash_latched | trx_adaptive_hash_timeout |+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-----------------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+---------------------------+---------------------------+| 163F3 | LOCK WAIT | 2017-11-06 05:56:40 | 163F3:49:3:3 | 2017-11-06 05:56:40 | 2 | 45 | insert into fan values(4,&#x27;ceshi&#x27;) | inserting | 1 | 1 | 2 | 376 | 1 | 0 | 0 | REPEATABLE READ | 1 | 1 | NULL | 0 | 10000 || 163F2 | RUNNING | 2017-11-06 05:56:29 | NULL | NULL | 6 | 47 | NULL | NULL | 0 | 0 | 4 | 1248 | 5 | 2 | 0 | REPEATABLE READ | 1 | 1 | NULL | 0 | 10000 |+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-----------------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+---------------------------+---------------------------+","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Percona Toolkit","slug":"Percona-Toolkit","permalink":"http://fuxkdb.com/tags/Percona-Toolkit/"}]},{"title":"pybackup使用文档","slug":"pybackup使用文档","date":"2017-11-12T14:04:00.000Z","updated":"2018-05-03T08:36:27.000Z","comments":true,"path":"2017/11/12/pybackup使用文档/","link":"","permalink":"http://fuxkdb.com/2017/11/12/pybackup%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/","excerpt":"pybackup使用文档https://github.com/Fanduzi/pybackuppybackup源自于对线上备份脚本的改进和对备份情况的监控需求.原本生产库的备份是通过shell脚本调用mydumper,之后再将备份通过rsync传输到备份机.想要获取备份状态,时间,rsync传输时间等信息只能通过解析日志.pybackup由python编写,调用mydumper和rsync,将备份信息存入数据库中,后期可以通过grafana图形化展示和监控备份目前不支持2.6,仅在2.7.14做过测试 参数说明帮助信息1234567891011121314151617181920[root@iZ23t8cwo3iZ backup_db]# python pybackup.py -hUsage: pybackup.py mydumper ARG_WITH_NO_--... (([--no-rsync] [--no-history]) | [--only-backup]) pybackup.py only-rsync [--backup-dir=&lt;DIR&gt;] [--bk-id=&lt;id&gt;] [--log-file=&lt;log&gt;] pybackup.py -h | --help pybackup.py --versionOptions: -h --help Show help information. --version Show version. --no-rsync Do not use rsync. --no-history Do not record backup history information. --only-backup Equal to use both --no-rsync and --no-history. --only-rsync When you backup complete, but rsync failed, use this option to rsync your backup. --backup-dir=&lt;DIR&gt; The directory where the backup files need to be rsync are located. [default: ./] --bk-id=&lt;id&gt; bk-id in table user_backup. --log-file=&lt;log&gt; log file [default: ./rsync.log]more help information in:https://github.com/Fanduzi 1pybackup.py mydumper ARG_WITH_NO_--... (([--no-rsync] [--no-history]) | [--only-backup]) 除了最后三个参数,使用的所有参数和mydumper -h中列出的参数相同. 只不过目前只支持长选项,并且不带’–’","text":"pybackup使用文档https://github.com/Fanduzi/pybackuppybackup源自于对线上备份脚本的改进和对备份情况的监控需求.原本生产库的备份是通过shell脚本调用mydumper,之后再将备份通过rsync传输到备份机.想要获取备份状态,时间,rsync传输时间等信息只能通过解析日志.pybackup由python编写,调用mydumper和rsync,将备份信息存入数据库中,后期可以通过grafana图形化展示和监控备份目前不支持2.6,仅在2.7.14做过测试 参数说明帮助信息1234567891011121314151617181920[root@iZ23t8cwo3iZ backup_db]# python pybackup.py -hUsage: pybackup.py mydumper ARG_WITH_NO_--... (([--no-rsync] [--no-history]) | [--only-backup]) pybackup.py only-rsync [--backup-dir=&lt;DIR&gt;] [--bk-id=&lt;id&gt;] [--log-file=&lt;log&gt;] pybackup.py -h | --help pybackup.py --versionOptions: -h --help Show help information. --version Show version. --no-rsync Do not use rsync. --no-history Do not record backup history information. --only-backup Equal to use both --no-rsync and --no-history. --only-rsync When you backup complete, but rsync failed, use this option to rsync your backup. --backup-dir=&lt;DIR&gt; The directory where the backup files need to be rsync are located. [default: ./] --bk-id=&lt;id&gt; bk-id in table user_backup. --log-file=&lt;log&gt; log file [default: ./rsync.log]more help information in:https://github.com/Fanduzi 1pybackup.py mydumper ARG_WITH_NO_--... (([--no-rsync] [--no-history]) | [--only-backup]) 除了最后三个参数,使用的所有参数和mydumper -h中列出的参数相同. 只不过目前只支持长选项,并且不带’–’ 例:1./pybackup.py mydumper password=fanboshi database=fandb outputdir=/data4/recover/pybackup/2017-11-12 logfile=/data4/recover/pybackup/bak.log verbose=3可以使用./pybackup.py mydumper help查看mydumper帮助信息 –no-rsync 不使用rsync传输 –no-history 不记录备份信息到数据库 –only-backup 等价于同时使用–no-rsync和–no-history . 不能与–no-rsync或–no-history同时使用 1pybackup.py only-rsync [--backup-dir=&lt;DIR&gt;] [--bk-id=&lt;id&gt;] [--log-file=&lt;log&gt;] 当备份成功rsync失败时可以使用only-rsync来同步备份成功的文件 –backup-dir 需要使用rsync同步的备份文件路径,如果不指定,则默认为./ –bk-id user_backup表中记录的备份bk_id,如果指定,则会在rsync同步完成后更新指定bk_id行的,传输起始时间,耗时,是否成功等信息.如果不指定则不更新user_backup表 –log-file 本次rsync指定的日志,如果不指定,则默认为当前目录rsync.log文件 配置文件说明配置文件为pbackup.conf123456789101112131415161718192021[root@localhost pybackup]# less pybackup.conf [CATALOG] --存储备份信息的数据库配置db_host=localhostdb_port=3306db_user=rootdb_passwd=fanboshidb_use=catalogdb[TDB] --需要备份的数据库配置db_host=localhostdb_port=3306db_user=rootdb_passwd=fanboshidb_use=information_schemadb_consistency=True --0.7.0新增option,可以不写,不写则为False,后面会对这个option进行说明db_list=test,fandb,union_log_ad_% --指定需要备份的数据库,可以使用mysql支持的通配符. 如果想要备份所有数据库则填写%[rsync]password_file=/data4/recover/pybackup/rsync.sec --等同于--password-filedest=platform@xx.xx.xx.xx/db_backup/xx.xx.xx.xx --传输到哪个目录address= --使用的网卡.可以为空不指定注意12345[TDB]db_list=fan,bo,shi 代表备份fan,bo,shi三个数据库db_list=!fan,!bo,!shi 代表不备份fan,bo,shi三个数据库db_list=% 代表备份所有数据库db_list=!fan,bo 不支持还有一点需要注意,即便在配置文件中定义了db_list参数,也可以在命令行强制指定database=xx / regex / tables-list,例如1pybackup.py mydumper password=&quot;xx&quot; user=root socket=/data/mysql/mysql.sock outputdir=/data/backup_db/ verbose=3 compress threads=8 triggers events routines use-savepoints logfile=/data/backup_db/pybackup.log database=yourdb此时会只备份yourdb而忽略配置文件中的定义 备份信息示例123456789101112131415161718192021*************************** 4. row *************************** id: 4 bk_id: bcd36dc6-c9e7-11e7-9e30-005056b15d9c bk_server: xx.xx.xx.xx start_time: 2017-11-15 17:31:20 end_time: 2017-11-15 17:32:07 elapsed_time: 47 backuped_db: fandb,test,union_log_ad_201710_db,union_log_ad_201711_db is_complete: Y,Y,Y,Y bk_size: 480M bk_dir: /data4/recover/pybackup/2017-11-15 transfer_start: 2017-11-15 17:32:07 transfer_end: 2017-11-15 17:33:36 transfer_elapsed: 89transfer_complete: Y remote_dest: platform@xx.xx.xx.xx/db_backup/xx.xx.xx.xx/ master_status: mysql-bin.000036,61286, slave_status: Not a slave tool_version: mydumper 0.9.2, built against MySQL 5.5.53 server_version: 5.7.18-log bk_command: mydumper --password=supersecrect --outputdir=/data4/recover/pybackup/2017-11-15 --verbose=3 --compress --triggers --events --routines --use-savepoints database=fandb,test,union_log_ad_201710_db,union_log_ad_201711_db 建库建表语句12345678910111213141516171819202122232425create database catalogdb;CREATE TABLE user_backup ( id INT AUTO_INCREMENT NOT NULL PRIMARY KEY, bk_id CHAR(36) NOT NULL UNIQUE KEY, bk_server VARCHAR(15) NOT NULL, start_time DATETIME NOT NULL, end_time DATETIME NOT NULL, elapsed_time INT NOT NULL, backuped_db VARCHAR(200) NOT NULL, is_complete VARCHAR(30) NOT NULL, bk_size VARCHAR(10) NOT NULL, bk_dir VARCHAR(200) NOT NULL, transfer_start DATETIME, transfer_end DATETIME, transfer_elapsed INT, transfer_complete VARCHAR(20) NOT NULL, remote_dest VARCHAR(200) NOT NULL, master_status VARCHAR(200) NOT NULL, slave_status VARCHAR(200) NOT NULL, tool_version VARCHAR(200) NOT NULL, server_version VARCHAR(200) NOT NULL, bk_command VARCHAR(400) NOT NULL, tag varchar(200) NOT NULL DEFAULT &#x27;N/A&#x27;, is_deleted char(1) NOT NULL DEFAULT &#x27;N&#x27;) ENGINE=INNODB CHARACTER SET UTF8 COLLATE UTF8_GENERAL_CI; 关于db_consistency1./pybackup.py mydumper password=fanboshi database=fandb outputdir=/data4/recover/pybackup/2017-11-12 logfile=/data4/recover/pybackup/bak.log verbose=3 以上面命令为例,默认脚本逻辑对于db_list指定的库通过for循环逐一使用mydumper –database=xx 备份如果知道db_consistency=True则会替换为使用 –regex备份db_list中指定的所有数据库, 保证了数据库之间的一致性 备份脚本示例12345678910#!/bin/shDSERVENDAY=`date +%Y-%m-%d --date=&#x27;2 day ago&#x27;`DTODAY=`date +%Y-%m-%d`cd /data/backup_db/rm -rf $DTODAYrm -rf $DSERVENDAYmkdir $DTODAYsource ~/.bash_profilepython /data/backup_db/pybackup.py mydumper password=&quot;papapa&quot; user=root socket=/data/mysql/mysql.sock outputdir=/data/backup_db/$DTODAY verbose=3 compress threads=8 triggers events routines use-savepoints logfile=/data/backup_db/pybackup.log crontab10 4 * * * /data/backup_db/pybackup.sh&gt;&gt; /data/backup_db/pybackup_sh.log 2&gt;&amp;1 logroatate脚本1234567891011121314151617/data/backup_db/pybackup.log &#123; daily rotate 7 missingok compress delaycompress copytruncate&#125;/data/backup_db/pybackup_sh.log &#123; daily rotate 7 missingok compress delaycompress copytruncate&#125;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"为什么双主只建议单节点写入?","slug":"为什么双主建议单节点写入","date":"2017-10-31T09:23:00.000Z","updated":"2017-11-01T01:51:56.000Z","comments":true,"path":"2017/10/31/为什么双主建议单节点写入/","link":"","permalink":"http://fuxkdb.com/2017/10/31/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8C%E4%B8%BB%E5%BB%BA%E8%AE%AE%E5%8D%95%E8%8A%82%E7%82%B9%E5%86%99%E5%85%A5/","excerpt":"为什么双主只建议单节点写入通过下面的案例,你应该可以明白为啥了 问题描述线上一套双主环境123456CentOS release 6.8 (Final)Server version: 5.5.56binlog_format : STATEMENTtx_isolation : REPEATABLE-READ主1 server_id : 32主2 server_id : 33有一个表,每分钟load data. 由于一天会插入近1亿行数据,导致磁盘使用率增长很快,所以现在用计划任务每四天切换一次表12#mobile_ad_50表切换0 3 2,6,10,14,19,23,27 * * source /etc/profile;source /root/.bash_profile;sh /data/scripts/bin/mobile_ad_50.sh &gt;&gt;/data/scripts/log/mobile_ad_50.log切换逻辑是,先rename源表,再重建表12345$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;drop table mobile_ad_50_20170531&quot;echo &quot;drop ok&quot;$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;rename table mobile_ad_50 to mobile_ad_50_20170531&quot;echo &quot;rename ok&quot;$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;CREATE TABLE mobile_ad_50 (表结构1234567891011121314151617181920212223242526272829303132333435*************************** 1. row *************************** Table: mobile_ad_50Create Table: CREATE TABLE `mobile_ad_50` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;主键&#x27;, `dtime` datetime NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27; COMMENT &#x27;时间段 年月日时&#x27;, `union_id` int(11) unsigned NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;媒体ID&#x27;, `ad_id` varchar(100) DEFAULT NULL COMMENT &#x27;广告位ID&#x27;, `ifa` varchar(50) NOT NULL COMMENT &#x27;ifa&#x27;, `mac` varchar(50) NOT NULL COMMENT &#x27;mac&#x27;, `cb_url` varchar(1000) NOT NULL COMMENT &#x27;回调地址&#x27;, `state` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否激活&#x27;, `domain` varchar(30) NOT NULL COMMENT &#x27;游戏域名&#x27;, `game_code` varchar(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;游戏编码&#x27;, `union_app_id` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;渠道商的appid&#x27;, `openudid` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;开源广告标示符&#x27;, `is_send` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;同步次数a&#x27;, `ip` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;点击ip&#x27;, `actip` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;激活ip&#x27;, `opentime` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;打开时间&#x27;, `acttime` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;激活时间&#x27;, `is_open` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否打开&#x27;, PRIMARY KEY (`id`), KEY `ifa` (`ifa`), KEY `d_u_s` (`domain`,`union_id`,`state`), KEY `union_id` (`union_id`), KEY `mac` (`mac`), KEY `dtime` (`dtime`), KEY `ip` (`ip`), KEY `actip` (`actip`), KEY `union_app_id` (`union_app_id`), KEY `openudid` (`openudid`), KEY `state` (`state`), KEY `acttime` (`acttime`)) ENGINE=InnoDB AUTO_INCREMENT=6154739813 DEFAULT CHARSET=utf8 COMMENT=&#x27;手机广告&#x27;1 row in set (0.00 sec)","text":"为什么双主只建议单节点写入通过下面的案例,你应该可以明白为啥了 问题描述线上一套双主环境123456CentOS release 6.8 (Final)Server version: 5.5.56binlog_format : STATEMENTtx_isolation : REPEATABLE-READ主1 server_id : 32主2 server_id : 33有一个表,每分钟load data. 由于一天会插入近1亿行数据,导致磁盘使用率增长很快,所以现在用计划任务每四天切换一次表12#mobile_ad_50表切换0 3 2,6,10,14,19,23,27 * * source /etc/profile;source /root/.bash_profile;sh /data/scripts/bin/mobile_ad_50.sh &gt;&gt;/data/scripts/log/mobile_ad_50.log切换逻辑是,先rename源表,再重建表12345$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;drop table mobile_ad_50_20170531&quot;echo &quot;drop ok&quot;$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;rename table mobile_ad_50 to mobile_ad_50_20170531&quot;echo &quot;rename ok&quot;$&#123;DB_COMMAND&#125; dbe8je6i4c3gjd50 -ss -e &quot;CREATE TABLE mobile_ad_50 (表结构1234567891011121314151617181920212223242526272829303132333435*************************** 1. row *************************** Table: mobile_ad_50Create Table: CREATE TABLE `mobile_ad_50` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#x27;主键&#x27;, `dtime` datetime NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27; COMMENT &#x27;时间段 年月日时&#x27;, `union_id` int(11) unsigned NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;媒体ID&#x27;, `ad_id` varchar(100) DEFAULT NULL COMMENT &#x27;广告位ID&#x27;, `ifa` varchar(50) NOT NULL COMMENT &#x27;ifa&#x27;, `mac` varchar(50) NOT NULL COMMENT &#x27;mac&#x27;, `cb_url` varchar(1000) NOT NULL COMMENT &#x27;回调地址&#x27;, `state` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否激活&#x27;, `domain` varchar(30) NOT NULL COMMENT &#x27;游戏域名&#x27;, `game_code` varchar(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;游戏编码&#x27;, `union_app_id` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;渠道商的appid&#x27;, `openudid` char(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;开源广告标示符&#x27;, `is_send` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;同步次数a&#x27;, `ip` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;点击ip&#x27;, `actip` bigint(20) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;激活ip&#x27;, `opentime` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;打开时间&#x27;, `acttime` int(10) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;激活时间&#x27;, `is_open` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否打开&#x27;, PRIMARY KEY (`id`), KEY `ifa` (`ifa`), KEY `d_u_s` (`domain`,`union_id`,`state`), KEY `union_id` (`union_id`), KEY `mac` (`mac`), KEY `dtime` (`dtime`), KEY `ip` (`ip`), KEY `actip` (`actip`), KEY `union_app_id` (`union_app_id`), KEY `openudid` (`openudid`), KEY `state` (`state`), KEY `acttime` (`acttime`)) ENGINE=InnoDB AUTO_INCREMENT=6154739813 DEFAULT CHARSET=utf8 COMMENT=&#x27;手机广告&#x27;1 row in set (0.00 sec)开发说load只在主1执行,并且这个表数据都是通过load进来的,然后有些update,就再没有其他insert语句了现在发现问题就是发现auto_increment异常增大,表中有两亿数据时,auto_increment列有51亿.1234567891011121314151617181920212223242526mysql&gt; select id,dtime from mobile_ad_50 order by id limit 0,20;+------------+---------------------+| id | dtime |+------------+---------------------+| 2 | 2017-10-29 03:00:56 || 4 | 2017-10-29 03:00:56 || 6 | 2017-10-29 03:00:56 || 8 | 2017-10-29 03:00:57 || 10 | 2017-10-29 03:00:57 || 12 | 2017-10-29 03:00:57 || 14 | 2017-10-29 03:00:57 || 16 | 2017-10-29 03:00:57 || 18 | 2017-10-29 03:00:57 || 20 | 2017-10-29 03:00:57 || 22 | 2017-10-29 03:00:57 || 43 | 0000-00-00 00:00:00 || 5135418110 | 2017-10-29 03:00:10 || 5135418111 | 2017-10-29 03:00:00 || 5135418113 | 2017-10-29 03:00:00 || 5135418115 | 2017-10-29 03:00:00 || 5135418117 | 2017-10-29 03:00:00 || 5135418119 | 2017-10-29 03:00:00 || 5135418121 | 2017-10-29 03:00:00 || 5135418123 | 2017-10-29 03:00:00 |+------------+---------------------+20 rows in set (0.00 sec)看上面的查询是按照主键排序的,id=43一下就涨到51亿我怀疑是取到了rename之前的表的自增值,查看了一下,还真是1234567mysql&gt; select max(id) from mobile_ad_50_20170531;+------------+| max(id) |+------------+| 5135418109 |+------------+1 row in set (0.00 sec)自己分析原因一种是插入大量数据后rollback导致自增丢失,但是实际一次load data文件也就几千行,不可能丢失这么多.第二个可能的原因是innodb_autoinc_lock_mode=1导致的bulk insert时自增丢失,但是我模拟了load 两亿数据,自增id也就2亿1千多万,也不可能丢失到51亿我观察binlog,找2017-10-29 03:00:10左右的,搜索5135418110,找到12345678910111213BEGIN/*!*/;# at 119255971#171029 3:00:10 server id 33 end_log_pos 119255999 IntvarSET INSERT_ID=5135418110/*!*/;# at 119255999#171029 3:00:10 server id 33 end_log_pos 119256303 Query thread_id=227034786 exec_time=6 error_code=0SET TIMESTAMP=1509217210/*!*/;INSERT INTO mobile_ad_50 ( DTIME, UNION_ID, AD_ID, IFA, MAC, CB_URL, STATE, DOMAIN, GAME_CODE, OPENTIME, IS_OPEN ) VALUES ( &#x27;2017-10-29 03:00:10&#x27;, 14800, &#x27;&#x27;, &#x27;865166021645612&#x27;, &#x27;&#x27;, &#x27;&#x27;, 0, &#x27;520050&#x27;, &#x27;android&#x27;, 1509217210, 1 )/*!*/;# at 119256303#171029 3:00:10 server id 33 end_log_pos 119256330 Xid = 99754915507COMMIT/*!*/;这条之前的一个SET INSERT_ID是9000多万1234567891011121314151617BEGIN/*!*/;# at 119256931#171029 3:00:11 server id 33 end_log_pos 119257377 Query thread_id=227034792 exec_time=5 error_code=0SET TIMESTAMP=1509217211/*!*/;UPDATE wechat SET ACCESS_TOKEN=&#x27;oEBoeHxXah3WEPAm_pJbQ-E2dVR2WVTkXn0mQ7YfY20grgt3k29-e518F1OELmHepZumWfxjNuDO7agVyNZZnkfG_xao-yWbfRv90x1ZoN_uQ1ogvsyJazUIVygldMcBBGWdAHAIND&#x27;, ATOKEN_EXPIRES=&#x27;2017-10-29 04:58:11&#x27;, JSAPI_TICKET=&#x27;kgt8ON7yVITDhtdwci0qeXl3u2D35Jw6KZsyUHYlRNK5VfCPXbMWbtYLkPOWe2hDlrlrly_FyrO3yjhXqhSezg&#x27;, JSTICKET_EXPIRES=&#x27;2017-10-29 04:58:11&#x27; WHERE id = 20/*!*/;SET INSERT_ID=90238492/*!*/;# at 119254523#171029 3:00:08 server id 33 end_log_pos 119254827 Query thread_id=227034786 exec_time=8 error_code=0SET TIMESTAMP=1509217208/*!*/;INSERT INTO mobile_ad_55 ( DTIME, UNION_ID, AD_ID, IFA, MAC, CB_URL, STATE, DOMAIN, GAME_CODE, OPENTIME, IS_OPEN ) VALUES ( &#x27;2017-10-29 03:00:08&#x27;, 10332, &#x27;&#x27;, &#x27;863777021706899&#x27;, &#x27;&#x27;, &#x27;&#x27;, 0, &#x27;520055&#x27;, &#x27;android&#x27;, 1509217208, 1 )/*!*/;# at 119254827#171029 3:00:08 server id 33 end_log_pos 119254854 Xid = 99754915501COMMIT/*!*/;开发不是说没有其他的insert语句了吗???怎么这里有一条,看一下server id 33还是主2同步过来的在分析一下两边binlog1234567891011121314151617181920212223242526主1#171029 3:00:15 server id 32 end_log_pos 118195605 Query thread_id=2636976237 exec_time=0 error_code=0SET TIMESTAMP=1509217215/*!*/;rename table mobile_ad_50 to mobile_ad_50_20170531#171029 3:00:10 server id 33 end_log_pos 119255999 IntvarSET INSERT_ID=5135418110/*!*/;# at 119255999#171029 3:00:10 server id 33 end_log_pos 119256303 Query thread_id=227034786 exec_time=6 error_code=0SET TIMESTAMP=1509217210/*!*/;INSERT INTO mobile_ad_50 ( DTIME, UNION_ID, AD_ID, IFA, MAC, CB_URL, STATE, DOMAIN, GAME_CODE, OPENTIME, IS_OPE主2SET TIMESTAMP=1509217215/*!*/;SET @@session.auto_increment_increment=2, @@session.auto_increment_offset=1/*!*/;rename table mobile_ad_50 to mobile_ad_50_20170531/*!*/;#171029 3:00:10 server id 33 end_log_pos 268758734 IntvarSET INSERT_ID=5135418110/*!*/;# at 268758734#171029 3:00:10 server id 33 end_log_pos 268759038 Query thread_id=227034786 exec_time=0 error_code=0SET TIMESTAMP=1509217210/*!*/; 问题原因现在分析一下3:00:10 主2 SET INSERT_ID=5135418110 insert mobile_ad_503:00:15 主1 rename table1234567891011121314151617181920213.00.10 主1 主2 SET INSERT_ID=5135418110 insert mobile_ad_50源 &lt;---通过binlog同步给主1--- 由于复制延迟,主1这一条insert还没执行3.00.15 主1 主2 rename ---通过binlog同步给主2---&gt; rename这时主1才执行SET INSERT_ID=5135418110 insert mobile_ad_50,主1新创建的mobile_ad_50表&quot;继承了&quot;原来的mobile_ad_50也就是mobile_ad_50_20170531表的ID,导致了ID暴增证据是5135418110在主1的 mobile_ad_50中,而主2的mobile_ad_50中没有,实际上5135418110在主2的mobile_ad_50_20170531中mysql&gt; select id,dtime from mobile_ad_50 where id=5135418110;Empty set (0.00 sec)mysql&gt; select id,dtime from mobile_ad_50_20170531 where id=5135418110;+------------+---------------------+| id | dtime |+------------+---------------------+| 5135418110 | 2017-10-29 03:00:10 |+------------+---------------------+","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"Binlog Server python脚本","slug":"Binlog-Server-python脚本","date":"2017-10-18T09:23:00.000Z","updated":"2018-05-03T08:42:23.000Z","comments":true,"path":"2017/10/18/Binlog-Server-python脚本/","link":"","permalink":"http://fuxkdb.com/2017/10/18/Binlog-Server-python%E8%84%9A%E6%9C%AC/","excerpt":"","text":"有问题就是如果nohup python binlog_server.py &amp; ,然后kill 这个脚本,能kill掉,但是ps -ef | grep mysqlbinlog 还是在执行h还有一个小问题就是如果直接以命令行方式运行明文指定密码,那么通过ps -ef | grep mysqlbinlog会直接看到密码…密码写到配置文件的话也不太安全,总之,还需要完善 指定不同的dbname,从不同的数据库拉binlog, dbname就是配置文件里的section 创建用户1GRANT REPLICATION SLAVE ON *.* TO &#x27;binlog_backup&#x27;@&#x27;106.3.130.255&#x27; IDENTIFIED BY &#x27;xxx&#x27; fw.sh添加12#备份binlog$IPTABLES -A INPUT -p tcp -s 106.3.130.255 --dport 3306 -j ACCEPT 脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&quot;&quot;&quot;Usage: binlog_server.py --user=&lt;username&gt; --password=&lt;password&gt; --host=&lt;remote_host&gt; --port=&lt;remote_port&gt; --backup-dir=&lt;backup_dir&gt; --log=&lt;log&gt; [--last-file=&lt;last-file&gt;] binlog_server.py -h | --help binlog_server.py --version binlog_server.py --config=&lt;config_file&gt; --dbname=&lt;database_name&gt; [--last-file=&lt;last-file&gt;]Options: -h --help Show help information. --version Show version. --user=&lt;username&gt; The user name used to connect to the remote server. --password=&lt;password&gt; The password used to connect to the remote server. --host=&lt;remote_host&gt; The remote host IP address. --port=&lt;remote_port&gt; The remote MySQL server port. --backup-dir=&lt;backup_dir&gt; The dest to store binlog. --log=&lt;log&gt; The log. --last-file=&lt;last-file&gt; Specify the starting binlog. --config=&lt;config_file&gt; Config file. --dbname=&lt;database_name&gt; Section name in config file.&quot;&quot;&quot;from docopt import docoptimport subprocessimport loggingimport timeimport ConfigParserimport osarguments = docopt(__doc__, version=&#x27;Binlog server 1.0&#x27;)if arguments[&#x27;--config&#x27;]: cf=ConfigParser.ConfigParser() cf.read(arguments[&#x27;--config&#x27;]) section_name = arguments[&#x27;--dbname&#x27;] db_host = cf.get(section_name, &quot;db_host&quot;) db_port = cf.get(section_name, &quot;db_port&quot;) db_user = cf.get(section_name, &quot;db_user&quot;) db_passwd = cf.get(section_name, &quot;db_passwd&quot;) backup_dir = cf.get(section_name, &quot;backup_dir&quot;) log = cf.get(section_name, &quot;log&quot;) logging.basicConfig(level=logging.DEBUG, format=&#x27;%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s&#x27;, datefmt=&#x27;%a, %d %b %Y %H:%M:%S&#x27;, filename=log, filemode=&#x27;a&#x27;)logging.basicConfig(level=logging.DEBUG, format=&#x27;%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s&#x27;, datefmt=&#x27;%a, %d %b %Y %H:%M:%S&#x27;, filename=arguments[&#x27;--log&#x27;], filemode=&#x27;a&#x27;)def dumpBinlog(user,password,host,port,backup_dir,log,last_file=&#x27;&#x27;): LOCAL_BACKUP_DIR=backup_dir if backup_dir[-1]!= &#x27;/&#x27;: os.exit() #BACKUP_LOG=&#x27;/data4/binlog_backup/120.27.136.247/BB.log&#x27; BACKUP_LOG=log[log.rfind(&#x27;/&#x27;)+1:] while True: if not last_file: cmd=&quot;ls -A &#123;LOCAL_BACKUP_DIR&#125; | grep -v &#123;BACKUP_LOG&#125; | grep -v nohup.out |wc -l&quot;.format(LOCAL_BACKUP_DIR=LOCAL_BACKUP_DIR,BACKUP_LOG=BACKUP_LOG) child=subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE) child.wait() wc_l=int(child.communicate()[0].strip()) if wc_l != 0: cmd=&quot;ls -l %s | grep -v %s | grep -v nohup.out |tail -n 1 |awk &#x27;&#123;print $9&#125;&#x27;&quot; % (LOCAL_BACKUP_DIR,BACKUP_LOG) child=subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE) child.wait() LAST_FILE=child.communicate()[0].strip() else: LAST_FILE=last_file logging.info(&#x27;Last File is %s&#x27; % (LAST_FILE)) mysqlbinlog=&#x27;mysqlbinlog --raw --read-from-remote-server --stop-never --host=&#123;REMOTE_HOST&#125; --port=&#123;REMOTE_PORT&#125; --user=&#123;REMOTE_USER&#125; --password=&#123;REMOTE_PASS&#125; --result-file=&#123;RESULT_FILE&#125; &#123;LAST_FILE&#125;&#x27;.format(REMOTE_HOST=host,REMOTE_PORT=port,REMOTE_USER=user,REMOTE_PASS=password,RESULT_FILE=LOCAL_BACKUP_DIR,LAST_FILE=LAST_FILE) subprocess.call(mysqlbinlog,shell=True) logging.info(&#x27;Binlog server stop!!!,reconnect after 10 seconds&#x27;) time.sleep(10)if __name__ == &#x27;__main__&#x27;: if arguments[&#x27;--config&#x27;]: lock_file=db_host+&quot;_binlog_server.lock&quot; else: lock_file=arguments[&#x27;--host&#x27;]+&quot;_binlog_server.lock&quot; child=subprocess.Popen(&#x27;ls /tmp|grep %s&#x27; % (lock_file),shell=True,stdout=subprocess.PIPE) child.wait() lock=child.communicate()[0].strip() if not lock: subprocess.call(&#x27;touch /tmp/%s&#x27; % (lock_file),shell=True) logging.info(&#x27;Get lock,Binlog server start!!!&#x27;) if not arguments[&#x27;--config&#x27;]: dumpBinlog(arguments[&#x27;--user&#x27;],arguments[&#x27;--password&#x27;],arguments[&#x27;--host&#x27;],arguments[&#x27;--port&#x27;],arguments[&#x27;--backup-dir&#x27;],arguments[&#x27;--log&#x27;],arguments[&#x27;--last-file&#x27;]) else: dumpBinlog(db_user,db_passwd,db_host,db_port,backup_dir,log,arguments[&#x27;--last-file&#x27;]) else: logging.info(&#x27;Binlog server already running!!!&#x27;) print(&#x27;Binlog server already running!!!,please check or reomove the lock file&#x27;) 监控脚本123456789101112#!/bin/bashnum_py=`ps -ef | grep binlog_server.py | grep -v grep | grep GN_PT_SLAVE1 | wc -l`num_mysqlbinlog=`ps -ef | grep mysqlbinlog | grep -v grep | grep 120.27.136.247 | wc -l`TO_MAIL=xoxoxo@papapa.comif [ $num_py -eq 0 ] &amp;&amp; [ $num_mysqlbinlog -eq 0 ];then #发邮件,GN_PT_SLAVE1 binlog server宕了 #重启 nohup python /scripts/binlog_server.py --config=/tmp/binlog_server.cnf --dbname=GN_PT_SLAVE1 &amp; echo &quot;GN_PT_SLAVE1 binlog server宕了&quot; |/usr/bin/mutt -s &quot;Binlog server监控告警&quot; $TO_MAILelif [ $num_py -eq 0 ] &amp;&amp; [ $num_mysqlbinlog -eq 1 ];then #发邮件,GN_PT_SLAVE1 python脚本挂了,但是mysqlbinlog还在跑 echo &quot;GN_PT_SLAVE1 python脚本挂了,但是mysqlbinlog还在跑&quot; |/usr/bin/mutt -s &quot;Binlog server监控告警&quot; $TO_MAILfi 配置文件12345678910111213141516[root@localhost 120.27.143.36]# less /scripts/binlog_server.cnf [GN_PT_SLAVE1]db_host=120.27.136.257db_port=3306db_user=binlog_backupdb_passwd=xxxbackup_dir=/data1/backup/db_backup/120.27.136.247/ --注意一定要以/结尾log=/data1/backup/db_backup/120.27.136.247/BB.log[GN_LOG_MASTER2]db_host=120.27.143.256db_port=3306db_user=binlog_backupdb_passwd=xxxbackup_dir=/data2/backup/db_backup/120.27.143.36/ --注意一定要以/结尾log=/data2/backup/db_backup/120.27.143.36/BB.log 使用方法两种方式1.通过配置文件1python /scripts/binlog_server.py --config=/tmp/binlog_server.cnf --dbname=GN_PT_SLAVE12.命令行指定注意backup-dir一定要以’/‘结尾1python binlog_server.py --user=binlog_backup --password=xxxx --host=xxxx --port=3306 --backup-dir=/data4/binlog_backup/ --log=/data4/binlog_backup/BB.log在脚本中 创建了/tmp/IP_binlog_server.lock 文件,为了防止重复运行.如果有需要停止,需要手动kill binlog_server.py 和 mysqlbinlog, 并且删除/tmp/IP_binlog_server.lock 文件,不然下次起不来","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"PMM出问题排查","slug":"PMM出问题排查","date":"2017-09-29T01:52:00.000Z","updated":"2017-12-15T05:31:18.000Z","comments":true,"path":"2017/09/29/PMM出问题排查/","link":"","permalink":"http://fuxkdb.com/2017/09/29/PMM%E5%87%BA%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","excerpt":"","text":"看各种日志monitoring service12345[root@node4 ~]# ll /var/log/pmm-*-rw-r--r--. 1 root root 1880 Sep 27 18:02 /var/log/pmm-linux-metrics-42000.log-rw-r--r--. 1 root root 783 Sep 27 18:02 /var/log/pmm-mysql-metrics-42002.log-rw-r--r--. 1 root root 7143 Sep 27 18:08 /var/log/pmm-mysql-queries-0.logdocker里的日志12345678910111213141516[root@node4 log]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe4916410b314 percona/pmm-server:latest &quot;/opt/entrypoint.sh&quot; 2 hours ago Up 31 minutes 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server948a9aeb047e percona/pmm-server:latest &quot;/bin/true&quot; 2 hours ago Created pmm-data[root@node4 log]# docker logs e4916410b3142017-09-27 08:39:47,175 CRIT Supervisor running as root (no user in config file)2017-09-27 08:39:47,175 WARN Included extra file &quot;/etc/supervisord.d/pmm.ini&quot; during parsingUnlinking stale socket /var/run/supervisor/supervisor.sock2017-09-27 08:39:47,527 INFO RPC interface &#x27;supervisor&#x27; initialized2017-09-27 08:39:47,528 INFO supervisord started with pid 12017-09-27 08:39:48,536 INFO spawned: &#x27;mysql&#x27; with pid 152017-09-27 08:39:48,543 INFO spawned: &#x27;consul&#x27; with pid 162017-09-27 08:39:48,552 INFO spawned: &#x27;grafana&#x27; with pid 172017-09-27 08:39:48,563 INFO spawned: &#x27;nginx&#x27; with pid 182017-09-27 08:39:48,610 INFO spawned: &#x27;cron&#x27; with pid 192017-09-27 08:39:48,612 INFO spawned: &#x27;qan-api&#x27; with pid 20进去容器看1234docker exec -it e4916410b314 /bin/bash/var/log 下面各种日志/var/log/grafana/grafana.log/var/log/prometheus.log pmm-admin list是yes不代表没问题,check-network看看123456789101112131415161718192021222324252627282930313233[root@node4 ~]# pmm-admin check-networkPMM Network StatusServer Address | 172.16.83.103Client Address | 172.16.83.103 * System TimeNTP Server (0.pool.ntp.org) | 2017-09-27 17:13:58 +0800 CSTPMM Server | 2017-09-27 09:13:58 +0000 GMTPMM Client | 2017-09-27 17:13:58 +0800 CSTPMM Server Time Drift | OKPMM Client Time Drift | OKPMM Client to PMM Server Time Drift | OK* Connection: Client --&gt; Server-------------------- ------- SERVER SERVICE STATUS -------------------- ------- Consul API OKPrometheus API OKQuery Analytics API OKConnection duration | 551.133µsRequest duration | 2.467879msFull round trip | 3.019012ms* Connection: Client &lt;-- Server-------------- ------ -------------------- ------- ---------- ---------SERVICE TYPE NAME REMOTE ENDPOINT STATUS HTTPS/TLS PASSWORD -------------- ------ -------------------- ------- ---------- ---------linux:metrics node4 172.16.83.103:42000 OK YES - mysql:metrics node4 172.16.83.103:42002 OK YES -","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL监控","slug":"MySQL监控","permalink":"http://fuxkdb.com/tags/MySQL%E7%9B%91%E6%8E%A7/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"PMM设置grafana登录用户","slug":"PMM设置grafana登录用户","date":"2017-09-29T01:52:00.000Z","updated":"2018-05-03T08:33:05.000Z","comments":true,"path":"2017/09/29/PMM设置grafana登录用户/","link":"","permalink":"http://fuxkdb.com/2017/09/29/PMM%E8%AE%BE%E7%BD%AEgrafana%E7%99%BB%E5%BD%95%E7%94%A8%E6%88%B7/","excerpt":"前言PMM使用grafana进行展示,默认是允许匿名登陆的,也就是说无需填写用户名密码就可以查看,修改仪表盘但是领导说了,没用户密码就能连上来咋行呢. 进入容器123456[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc74f5be8ed88 percona/pmm-server:latest &quot;/opt/entrypoint.sh&quot; 5 hours ago Up 32 minutes 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server28c991142e6d percona/pmm-server:latest &quot;/bin/true&quot; 5 hours ago Created pmm-data[root@localhost ~]# docker exec -it c74f5be8ed88 /bin/bash[root@c74f5be8ed88 opt]# 查看grafana.ini12345678[root@c74f5be8ed88 opt]# vi /etc/grafana/grafana.ini 找到这里#################################### Anonymous Auth ##########################[auth.anonymous]# enable anonymous access#enabled = True把enabled = Ture注释掉,这样既禁止匿名用户登陆了现在如果重启容器,再打开页面,你会发现自己进不去了..咋办呢 修改数据库进入容器1234567891011登录数据库sqlite3 /var/lib/grafana/grafana.db 修改user表,把admin密码改成adminupdate user set password = &#x27;59acf18b94d7eb0694c61e60ce44c110c7a683ac6a8f09580d626f90f4a242000746579358d77dd9e570e83fa24faa88a8a6&#x27;, salt = &#x27;F3FAxVm33R&#x27; where login = &#x27;admin&#x27;安全起见,也可以把admin密码改成TdPXP4sgupdate user set password=&#x27;11cf3a1ee21b046b939b5f0cdc9d92ab70ba66e4e53f301fb2456ee7b6a665d8abf0d5b387ae0ec53f5f5fc8e477bfbe073e&#x27;,salt=&#x27;AHxOW2Fn34&#x27;,name=&#x27;admin&#x27;,is_admin=1 where login=&#x27;admin&#x27;;创建monitor用户密码mj8toYLBINSERT INTO &quot;user&quot; VALUES(3,0,&#x27;monitor&#x27;,&#x27;monitor@papapa.com&#x27;,&#x27;monitor&#x27;,&#x27;98c8e341360759e957ac43e2543fab4eef420a3521450d03ad79d5a1dd76dee233a9ec11870264c2e4dd7266d1a1f68681c2&#x27;,&#x27;erShkEJCWn&#x27;,&#x27;Y9TF6hFebE&#x27;,&#x27;&#x27;,1,0,0,&#x27;&#x27;,&#x27;2017-09-28 10:21:10&#x27;,&#x27;2017-09-28 10:21:10&#x27;,0);","text":"前言PMM使用grafana进行展示,默认是允许匿名登陆的,也就是说无需填写用户名密码就可以查看,修改仪表盘但是领导说了,没用户密码就能连上来咋行呢. 进入容器123456[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc74f5be8ed88 percona/pmm-server:latest &quot;/opt/entrypoint.sh&quot; 5 hours ago Up 32 minutes 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server28c991142e6d percona/pmm-server:latest &quot;/bin/true&quot; 5 hours ago Created pmm-data[root@localhost ~]# docker exec -it c74f5be8ed88 /bin/bash[root@c74f5be8ed88 opt]# 查看grafana.ini12345678[root@c74f5be8ed88 opt]# vi /etc/grafana/grafana.ini 找到这里#################################### Anonymous Auth ##########################[auth.anonymous]# enable anonymous access#enabled = True把enabled = Ture注释掉,这样既禁止匿名用户登陆了现在如果重启容器,再打开页面,你会发现自己进不去了..咋办呢 修改数据库进入容器1234567891011登录数据库sqlite3 /var/lib/grafana/grafana.db 修改user表,把admin密码改成adminupdate user set password = &#x27;59acf18b94d7eb0694c61e60ce44c110c7a683ac6a8f09580d626f90f4a242000746579358d77dd9e570e83fa24faa88a8a6&#x27;, salt = &#x27;F3FAxVm33R&#x27; where login = &#x27;admin&#x27;安全起见,也可以把admin密码改成TdPXP4sgupdate user set password=&#x27;11cf3a1ee21b046b939b5f0cdc9d92ab70ba66e4e53f301fb2456ee7b6a665d8abf0d5b387ae0ec53f5f5fc8e477bfbe073e&#x27;,salt=&#x27;AHxOW2Fn34&#x27;,name=&#x27;admin&#x27;,is_admin=1 where login=&#x27;admin&#x27;;创建monitor用户密码mj8toYLBINSERT INTO &quot;user&quot; VALUES(3,0,&#x27;monitor&#x27;,&#x27;monitor@papapa.com&#x27;,&#x27;monitor&#x27;,&#x27;98c8e341360759e957ac43e2543fab4eef420a3521450d03ad79d5a1dd76dee233a9ec11870264c2e4dd7266d1a1f68681c2&#x27;,&#x27;erShkEJCWn&#x27;,&#x27;Y9TF6hFebE&#x27;,&#x27;&#x27;,1,0,0,&#x27;&#x27;,&#x27;2017-09-28 10:21:10&#x27;,&#x27;2017-09-28 10:21:10&#x27;,0);这些密码是经过函数运算出来的,我也不知道是啥函数,但是我们可以通过开启用户注册,自己创建用户,然后再查看user表的数据来自己定义密码(不要忘记salt列也要更新)开启用户注册12345#################################### Users ####################################[users]# disable user signup / registrationallow_sign_up = true取消allow_sign_up = true注释 sqlite312345查看user表结构select * from sqlite_master where type=&quot;table&quot; and name=&quot;user&quot;导出成sql文件.output user.sql.dump user 或者修改好admin密码就可以登录了,然后可以修改其他用户的密码把监控用户加入组,不然没有仪表盘","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL监控","slug":"MySQL监控","permalink":"http://fuxkdb.com/tags/MySQL%E7%9B%91%E6%8E%A7/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"MySQL忘记密码处理方法,无需重启","slug":"MySQL忘记密码处理方法,无需重启","date":"2017-09-12T07:18:00.000Z","updated":"2017-09-12T07:18:57.000Z","comments":true,"path":"2017/09/12/MySQL忘记密码处理方法,无需重启/","link":"","permalink":"http://fuxkdb.com/2017/09/12/MySQL%E5%BF%98%E8%AE%B0%E5%AF%86%E7%A0%81%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95,%E6%97%A0%E9%9C%80%E9%87%8D%E5%90%AF/","excerpt":"源库这边跑一个sysbench,为了测试之后kill -HUP是不是会有影响1234567891011sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=30 \\--events=5000000 --report-interval=5 --db-driver=mysql preparesysbench \\/usr/share/sysbench/oltp_read_write.lua \\--mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 \\--threads=30 --report-interval=5 --time=7000 --db-driver=mysql run &gt; binlog_off.txt 把忘记密码的库的mysql.user表传到一个知道密码的测试库12345[root@uz22199 mysql]# scp -p user.* 10.4.3.100:/data/mysqldata/3306/data/fandb/root@10.4.3.100&#x27;s password: user.frm 100% 11KB 10.6KB/s 00:00 user.MYD 100% 736 0.7KB/s 00:00 user.MYI 100% 4096 4.0KB/s 00:00 生成一下insert语句,后面用1234[root@test43100 ~]# mysqldump --user=root --password=&#x27;mysql&#x27; fandb user --where=&quot;host=&#x27;localhost&#x27; and user=&#x27;root&#x27;&quot; |grep INSERTmysqldump: [Warning] Using a password on the command line interface can be insecure.Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don&#x27;t want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events. INSERT INTO `user` VALUES (&#x27;localhost&#x27;,&#x27;root&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,0,0,0,0,&#x27;mysql_native_password&#x27;,&#x27;*81F5E21E35407D884A6CD4A731AEBFB6AF209E1B&#x27;,&#x27;N&#x27;,&#x27;2017-08-04 08:12:53&#x27;,NULL,&#x27;N&#x27;);","text":"源库这边跑一个sysbench,为了测试之后kill -HUP是不是会有影响1234567891011sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=30 \\--events=5000000 --report-interval=5 --db-driver=mysql preparesysbench \\/usr/share/sysbench/oltp_read_write.lua \\--mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 \\--threads=30 --report-interval=5 --time=7000 --db-driver=mysql run &gt; binlog_off.txt 把忘记密码的库的mysql.user表传到一个知道密码的测试库12345[root@uz22199 mysql]# scp -p user.* 10.4.3.100:/data/mysqldata/3306/data/fandb/root@10.4.3.100&#x27;s password: user.frm 100% 11KB 10.6KB/s 00:00 user.MYD 100% 736 0.7KB/s 00:00 user.MYI 100% 4096 4.0KB/s 00:00 生成一下insert语句,后面用1234[root@test43100 ~]# mysqldump --user=root --password=&#x27;mysql&#x27; fandb user --where=&quot;host=&#x27;localhost&#x27; and user=&#x27;root&#x27;&quot; |grep INSERTmysqldump: [Warning] Using a password on the command line interface can be insecure.Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don&#x27;t want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events. INSERT INTO `user` VALUES (&#x27;localhost&#x27;,&#x27;root&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,0,0,0,0,&#x27;mysql_native_password&#x27;,&#x27;*81F5E21E35407D884A6CD4A731AEBFB6AF209E1B&#x27;,&#x27;N&#x27;,&#x27;2017-08-04 08:12:53&#x27;,NULL,&#x27;N&#x27;);新用户密码mysql123456root@mysqldb 15:04: [(none)]&gt; select password(&#x27;mysql&#x27;);+-------------------------------------------+| password(&#x27;mysql&#x27;) |+-------------------------------------------+| *E74858DB86EBA20BC33D0AECAE8A8108C56B17FA |+-------------------------------------------+ 新插入一个用户,把上面的insert改一下1INSERT INTO `user` VALUES (&#x27;localhost&#x27;,&#x27;fanboshi&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;Y&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,0,0,0,0,&#x27;mysql_native_password&#x27;,&#x27;*E74858DB86EBA20BC33D0AECAE8A8108C56B17FA&#x27;,&#x27;N&#x27;,&#x27;2017-08-04 08:12:53&#x27;,NULL,&#x27;N&#x27;);关闭表1root@mysqldb 15:05: [(none)]&gt; flush tables; 传回源库12345[root@test43100 fandb]# scp -p user.* 10.4.1.45:/data/mysqldata/3306/data/mysql/root@10.4.1.45&#x27;s password: user.frm 100% 11KB 10.6KB/s 00:00 user.MYD 100% 864 0.8KB/s 00:00 user.MYI 无法登陆123[root@uz22199 ~]# mysql -ufanboshi -pmysqlmysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1045 (28000): Access denied for user &#x27;fanboshi&#x27;@&#x27;localhost&#x27; (using password: YES) flush privileges123456789101112131415161718root@mysqldb 14:53: [mysql]&gt; flush privileges;Query OK, 0 rows affected (0.82 sec)[root@uz22199 ~]# mysql -ufanboshi -pmysqlmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 44Server version: 5.7.18-log MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.fanboshi@mysqldb 14:54: [(none)]&gt; quitflush privileges完了就可以登陆了,不知道只这样做可以不可以,还是按吴总来吧 看一下现在mysqld进程号12345[root@uz22199 ~]# ps -ef| grep mysqldroot 14676 1 0 Sep11 ? 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --relay-log-recovery=0mysql 16188 14676 2 Sep11 ? 00:33:24 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/data/mysqldata/3306/data --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --relay-log-recovery=0 --log-error=/data/mysqldata/3306/data/../error.log --open-files-limit=65535 --pid-file=/data/mysqldata/3306/mysql.pid --socket=/data/mysqldata/3306/mysql.sock --port=3306root 23497 21466 1 13:49 pts/4 00:00:55 sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 --mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost --mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=2 --report-interval=5 --time=7000 --db-driver=mysql runroot 23775 21421 0 14:54 pts/0 00:00:00 grep mysqld kill -HUP 重新加载配置文件1[root@uz22199 ~]# kill -HUP `pidof mysqld` 看一下mysqld pid没变12345[root@uz22199 ~]# ps -ef| grep mysqldroot 14676 1 0 Sep11 ? 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --relay-log-recovery=0mysql 16188 14676 2 Sep11 ? 00:33:26 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/data/mysqldata/3306/data --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --relay-log-recovery=0 --log-error=/data/mysqldata/3306/data/../error.log --open-files-limit=65535 --pid-file=/data/mysqldata/3306/mysql.pid --socket=/data/mysqldata/3306/mysql.sock --port=3306root 23497 21466 1 13:49 pts/4 00:00:55 sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 --mysql-socket=/data/mysqldata/3306/mysql.sock --mysql-host=localhost --mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=2 --report-interval=5 --time=7000 --db-driver=mysql runroot 23782 21421 0 14:55 pts/0 00:00:00 grep mysqldsysbench没有报错","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"MySQL不完全恢复","slug":"MySQL不完全恢复","date":"2017-09-11T10:16:00.000Z","updated":"2017-09-11T10:22:29.000Z","comments":true,"path":"2017/09/11/MySQL不完全恢复/","link":"","permalink":"http://fuxkdb.com/2017/09/11/MySQL%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%81%A2%E5%A4%8D/","excerpt":"不完全恢复通过做Slave恢复找到问题语句的位置,这里是131512345678910111213141516#170911 14:42:19 server id 330641 end_log_pos 2508 CRC32 0xf697b7bb Xid = 215COMMIT/*!*/;# at 2508#170911 14:50:56 server id 330641 end_log_pos 2573 CRC32 0x8309dea0 GTID last_committed=9 sequence_number=10SET @@SESSION.GTID_NEXT= &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1315&#x27;/*!*/;# at 2573#170911 14:50:56 server id 330641 end_log_pos 2693 CRC32 0xb06b9074 Query thread_id=25 exec_time=1 error_code=0use `fandb`/*!*/;SET TIMESTAMP=1505112656/*!*/;DROP TABLE `users` /* generated by server *//*!*/;SET @@SESSION.GTID_NEXT= &#x27;AUTOMATIC&#x27; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 那一份备份,恢复出来,启动,reset slave all;然后change master12345678910change master tomaster_host=&#x27;10.4.3.100&#x27;,master_user=&#x27;repl&#x27;,master_password=&#x27;repl&#x27;,master_port=3306,master_auto_position=1;重点start slave sql_thread until SQL_BEFORE_GTIDS=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1315&#x27;;start slave io_thread;","text":"不完全恢复通过做Slave恢复找到问题语句的位置,这里是131512345678910111213141516#170911 14:42:19 server id 330641 end_log_pos 2508 CRC32 0xf697b7bb Xid = 215COMMIT/*!*/;# at 2508#170911 14:50:56 server id 330641 end_log_pos 2573 CRC32 0x8309dea0 GTID last_committed=9 sequence_number=10SET @@SESSION.GTID_NEXT= &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1315&#x27;/*!*/;# at 2573#170911 14:50:56 server id 330641 end_log_pos 2693 CRC32 0xb06b9074 Query thread_id=25 exec_time=1 error_code=0use `fandb`/*!*/;SET TIMESTAMP=1505112656/*!*/;DROP TABLE `users` /* generated by server *//*!*/;SET @@SESSION.GTID_NEXT= &#x27;AUTOMATIC&#x27; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 那一份备份,恢复出来,启动,reset slave all;然后change master12345678910change master tomaster_host=&#x27;10.4.3.100&#x27;,master_user=&#x27;repl&#x27;,master_password=&#x27;repl&#x27;,master_port=3306,master_auto_position=1;重点start slave sql_thread until SQL_BEFORE_GTIDS=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1315&#x27;;start slave io_thread; 伪Master恢复找一个备份,恢复,启动1234567891011121314mysql-bin.000005 194 5c351518-78ec-11e7-8e7a-005056a610c3:1-1298innobackupex --copy-back .[root@uz22199 full]# mysqld_safe &amp;root@mysqldb 17:41: [(none)]&gt; reset slave all;Query OK, 0 rows affected (0.03 sec)root@mysqldb 17:43: [(none)]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000001 | 154 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1298 |+------------------+----------+--------------+------------------+---------------------------------------------+1 row in set (0.00 sec) 随便change master一下,目的让它知道自己是从库(要不没有relay log info)123456change master tomaster_host=&#x27;10.4.3.200&#x27;,master_user=&#x27;repl&#x27;,master_password=&#x27;repl&#x27;,master_port=3306;不要加master_auto_position关库1shutdown 主库binlog发过来,改名字为relay1234567891011121314151617181920212223242526272829[root@test43100 bak]# scp relay.0000* 10.4.1.45:/data/mysqldata/3306/data/root@10.4.1.45&#x27;s password: relay.000005 100% 495 0.5KB/s 00:00 relay.000006 100% 476 0.5KB/s 00:00 relay.000007 100% 241 0.2KB/s 00:00 relay.000008 100% 1003 1.0KB/s 00:00 relay.000009 100% 241 0.2KB/s 00:00 relay.000010 100% 495 0.5KB/s 00:00 relay.000011 100% 495 0.5KB/s 00:00 relay.000012 100% 2693 2.6KB/s 00:00 You have mail in /var/spool/mail/rootfor i in `ls relay.0*` ; do echo &quot;./&quot;$i&gt;&gt;relay.index; done[root@uz22199 data]# &gt;relay.index [root@uz22199 data]# for i in `ls relay.0*` ; do echo &quot;./&quot;$i&gt;&gt;relay.index; done[root@uz22199 data]# more relay.index ./relay.000001./relay.000005./relay.000006./relay.000007./relay.000008./relay.000009./relay.000010./relay.000011./relay.000012[root@uz22199 data]# chown mysql:mysql relay.* 启动,注意–relay-log-recovery=0 否则报错Error during –relay-log-recovery: Could not locate rotate event from the master. 1[root@uz22199 full]# mysqld_safe --relay-log-recovery=0 &amp; 再change master一次,目的告诉他relay文件和位置1change master to relay_log_file=&#x27;relay.000001&#x27; , relay_log_pos=4; 启动sql_thread12345678root@mysqldb 17:52: [(none)]&gt; start slave sql_thread until SQL_BEFORE_GTIDS=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1315&#x27;;root@mysqldb 17:52: [(none)]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000003 | 4121 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1314 |+------------------+----------+--------------+------------------+---------------------------------------------+","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"备份恢复","slug":"备份恢复","permalink":"http://fuxkdb.com/tags/%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"通过mysqlbinlog --skip-gtids恢复后再备份可能造成的坑","slug":"通过mysqlbinlog---skip-gtids恢复后再备份可能造成的坑","date":"2017-09-11T02:06:00.000Z","updated":"2017-09-29T02:05:33.000Z","comments":true,"path":"2017/09/11/通过mysqlbinlog---skip-gtids恢复后再备份可能造成的坑/","link":"","permalink":"http://fuxkdb.com/2017/09/11/%E9%80%9A%E8%BF%87mysqlbinlog---skip-gtids%E6%81%A2%E5%A4%8D%E5%90%8E%E5%86%8D%E5%A4%87%E4%BB%BD%E5%8F%AF%E8%83%BD%E9%80%A0%E6%88%90%E7%9A%84%E5%9D%91/","excerpt":"通过mysqlbinlog –skip-gtids恢复后再备份可能造成的坑版本12345678[root@uz22199 backup]# innobackupex --versioninnobackupex version 2.4.8 Linux (x86_64) (revision id: 97330f7)[root@uz22199 backup]# mysql -e&quot;select @@version&quot;+------------+| @@version |+------------+| 5.7.18-log |+------------+ 源库123456789101112131415161718192021222324252627282930313233表结构与数据root@mysqldb 21:51: [fandb]&gt; show create table users\\G*************************** 1. row *************************** Table: usersCreate Table: CREATE TABLE `users` ( `email` varchar(10) DEFAULT NULL, UNIQUE KEY `email` (`email`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)root@mysqldb 18:43: [fandb]&gt; select* from users;+-------+| email |+-------+| 1 || 10 || 20 || 30 || 5 |+-------+插入一条数据insert into users values(50); --GTID=1297再删掉delete from users where email=50; ----GTID=1298当前Executed_Gtid_Setroot@mysqldb 18:35: [fandb]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000005 | 495 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1298 |+------------------+----------+--------------+------------------+---------------------------------------------+1 row in set (0.00 sec) 源库再次应用一下已经执行过得binlog, 再次应用insert into users values(50); 这一条 这里先不考虑有没有可能这样子去恢复数据,只做实验","text":"通过mysqlbinlog –skip-gtids恢复后再备份可能造成的坑版本12345678[root@uz22199 backup]# innobackupex --versioninnobackupex version 2.4.8 Linux (x86_64) (revision id: 97330f7)[root@uz22199 backup]# mysql -e&quot;select @@version&quot;+------------+| @@version |+------------+| 5.7.18-log |+------------+ 源库123456789101112131415161718192021222324252627282930313233表结构与数据root@mysqldb 21:51: [fandb]&gt; show create table users\\G*************************** 1. row *************************** Table: usersCreate Table: CREATE TABLE `users` ( `email` varchar(10) DEFAULT NULL, UNIQUE KEY `email` (`email`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)root@mysqldb 18:43: [fandb]&gt; select* from users;+-------+| email |+-------+| 1 || 10 || 20 || 30 || 5 |+-------+插入一条数据insert into users values(50); --GTID=1297再删掉delete from users where email=50; ----GTID=1298当前Executed_Gtid_Setroot@mysqldb 18:35: [fandb]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000005 | 495 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1298 |+------------------+----------+--------------+------------------+---------------------------------------------+1 row in set (0.00 sec) 源库再次应用一下已经执行过得binlog, 再次应用insert into users values(50); 这一条 这里先不考虑有没有可能这样子去恢复数据,只做实验1234567891011121314151617181920[root@test43100 backup]# mysqlbinlog --skip-gtids --include-gtids=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1297&#x27; mysql-bin.000005 |mysqlroot@mysqldb 18:43: [fandb]&gt; select* from users;+-------+| email |+-------+| 1 || 10 || 20 || 30 || 5 || 50 |+-------+root@mysqldb 18:43: [fandb]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000005 | 617 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1299 |+------------------+----------+--------------+------------------+---------------------------------------------+源库Executed_Gtid_Set 已经到1299了 备份1234567891011innobackupex --user=backup --password=&#x27;backup&#x27; --stream=tar /tmp | gzip -&gt; full.tar.gz170907 18:45:15 Backup created in directory &#x27;/tmp/&#x27;MySQL binlog position: filename &#x27;mysql-bin.000005&#x27;, position &#x27;617&#x27;, GTID of the last change &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1299&#x27;170907 18:45:15 [00] Streaming &lt;STDOUT&gt;170907 18:45:15 [00] ...done170907 18:45:15 [00] Streaming &lt;STDOUT&gt;170907 18:45:15 [00] ...donextrabackup: Transaction log of lsn (3112759) to (3112768) was copied.170907 18:45:16 completed OK!从备份输出信息和xtrabackup_binlog_info都可以看到,这个全备备份了1-129912[root@uz22199 full2]# more xtrabackup_binlog_info mysql-bin.000005 617 5c351518-78ec-11e7-8e7a-005056a610c3:1-1299 把备份随便搞到一个地方恢复出来(恢复过程省略)查看恢复出来的库的Executed_Gtid_Set123456789101112131415161718192021root@mysqldb 18:48: [(none)]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000001 | 154 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1298 |+------------------+----------+--------------+------------------+---------------------------------------------+1 row in set (0.00 sec)虽然看起来只执行到1298,但是50这条数据却有了root@mysqldb 18:43: [fandb]&gt; select* from users;+-------+| email |+-------+| 1 || 10 || 20 || 30 || 5 || 50 |+-------+如果此时我们直接将该库作为从库,change master到源库,那么start slave会报错,1299会再执行一边insert 50,会报1062错误. 而如果我们flush binary logs一次,再做全备12345678910111213141516171819202122root@mysqldb 21:51: [fandb]&gt; flush binary logs;Query OK, 0 rows affected (0.19 sec)root@mysqldb 21:59: [fandb]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000006 | 194 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1299 |+------------------+----------+--------------+------------------+---------------------------------------------+1 row in set (0.00 sec)170907 22:00:58 Backup created in directory &#x27;/tmp/&#x27;MySQL binlog position: filename &#x27;mysql-bin.000006&#x27;, position &#x27;194&#x27;, GTID of the last change &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1299&#x27;170907 22:00:58 [00] Streaming &lt;STDOUT&gt;170907 22:00:58 [00] ...done170907 22:00:58 [00] Streaming &lt;STDOUT&gt;170907 22:00:58 [00] ...donextrabackup: Transaction log of lsn (3115326) to (3115335) was copied.170907 22:00:58 completed OK![root@uz22199 full3]# more xtrabackup_binlog_info mysql-bin.000006 194 5c351518-78ec-11e7-8e7a-005056a610c3:1-1299 Executed_Gtid_Set依旧是1-1299 再次将备份恢复出来,查看新恢复出来的库123456root@mysqldb 22:02: [(none)]&gt; show master status;+------------------+----------+--------------+------------------+---------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+---------------------------------------------+| mysql-bin.000001 | 154 | | | 5c351518-78ec-11e7-8e7a-005056a610c3:1-1299 |+------------------+----------+--------------+------------------+---------------------------------------------+此时恢复出来的库Executed_Gtid_Set为1-1299了 总结那么要么以后通过mysqlbinlog –skip-gtids 恢复数据之后flush 一下binary logs;要么恢复出来的库都手动根据xtrabackup_binlog_info去set global gtid_purged","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"备份恢复","slug":"备份恢复","permalink":"http://fuxkdb.com/tags/%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"在datadir外创建InnoDB表","slug":"在datadir外创建InnoDB表","date":"2017-09-06T03:20:00.000Z","updated":"2017-09-06T03:20:39.000Z","comments":true,"path":"2017/09/06/在datadir外创建InnoDB表/","link":"","permalink":"http://fuxkdb.com/2017/09/06/%E5%9C%A8datadir%E5%A4%96%E5%88%9B%E5%BB%BAInnoDB%E8%A1%A8/","excerpt":"","text":"在datadir外创建表要在MySQL datadir外的特定位置创建新的InnoDB file-per-table tablespace,请在create table时指定DATA DIRECTORY = absolute_path_to_directory子句 提前规划好位置,因为无法使用alter语句修改一个表的DATA DIRECTORY属性. MySQL会在目标目录中创建一个对应于数据库名称的子目录,并在改目录中创建表的.ibd文件用于存储表数据.在MySQL datadir目录下的数据库目录中,MySQL创建一个包含表的路径名称的table_name.isl文件. .isl文件由MySQL处理,像一个符号链接(不过InnoDB表不支持实际的符号链接) 示例:123456789101112131415161718192021222324252627282930313233版本[root@test43100 ~]# cat /etc/redhat-release CentOS release 6.4 (Final)[root@test43100 ~]# mysql -e &quot;\\s&quot;--------------mysql Ver 14.14 Distrib 5.7.18, for linux-glibc2.5 (x86_64) using EditLine wrapperroot@mysqldb 11:00: [(none)]&gt; use fandbDatabase changedroot@mysqldb 11:00: [fandb]&gt; SHOW VARIABLES LIKE &#x27;innodb_file_per_table&#x27;;+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| innodb_file_per_table | ON |+-----------------------+-------+1 row in set (0.00 sec)root@mysqldb 11:00: [fandb]&gt; create table t_out(id int auto_increment primary key) data directory=&#x27;/data/outdir&#x27;;Query OK, 0 rows affected (0.37 sec)查看目标目录[mysql@test43100 data]$ tree outdir/outdir/└── fandb └── t_out.ibd查看datadir[root@test43100 fandb]# ls -lttotal 412-rw-r----- 1 mysql mysql 28 Sep 6 11:03 t_out.isl-rw-r----- 1 mysql mysql 8556 Sep 6 11:03 t_out.frm包含.isl和.frm文件 您还可以将CREATE TABLE … TABLESPACE与DATA DIRECTORY子句结合使用，以便在MySQL数据目录之外创建一个file-per-table tablespace。 为此，您必须指定innodb_file_per_table作为表空间名称。123456789101112131415root@mysqldb 11:03: [fandb]&gt; create table t_out2(id int auto_increment primary key) TABLESPACE = innodb_file_per_table data directory=&#x27;/data/outdir&#x27;;Query OK, 0 rows affected (0.42 sec)查看目标目录[mysql@test43100 data]$ tree outdir/outdir/└── fandb ├── t_out2.ibd └── t_out.ibd查看datadir[root@test43100 fandb]# ls -lttotal 428-rw-r----- 1 mysql mysql 29 Sep 6 11:08 t_out2.isl-rw-r----- 1 mysql mysql 8556 Sep 6 11:08 t_out2.frm使用第二章方法无需启用innodb_file_per_table 使用说明 MySQL最初保持.ibd文件打开，阻止您卸载设备，但如果服务器正忙，最终可能会关闭该表。 当MySQL运行时，请注意不要意外卸载外部设备，或者在设备断开连接时启动MySQL。 当相关的.ibd文件丢失时尝试访问表会导致严重的错误，需要重新启动服务器。如果.ibd文件仍然不在预期路径，服务器重新启动可能会失败。 在这种情况下，请手动删除数据库目录中的table_name.isl文件，并在重新启动后执行DROP TABLE以删除.frm文件，并从数据字典中删除有关该表的信息。 Before tables on an NFS-mounted volume, review potential issues outlined in Using NFS with MySQL. If you use an LVM snapshot, file copy, or other file-based mechanism to back up the .ibd file, always use the FLUSH TABLES … FOR EXPORT statement first to make sure all changes that were buffered in memory are flushed to disk before the backup occurs. DATA DIRECTORY子句是使用符号链接的一个支持的替代方法.InnoDB直接使用符号链接是不支持的","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"innobackupex遇到的坑","slug":"innobackupex遇到的坑","date":"2017-08-31T10:19:00.000Z","updated":"2017-08-31T10:21:25.000Z","comments":true,"path":"2017/08/31/innobackupex遇到的坑/","link":"","permalink":"http://fuxkdb.com/2017/08/31/innobackupex%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"stream选择使用tar方式压缩到时slave_info信息不完整版本123456789101112131415161718192021222324252627282930313233343536[root@test1 backup]# innobackupex --versioninnobackupex version 2.4.8 Linux (x86_64) (revision id: 97330f7)[root@test1 backup]# cat /etc/redhat-release CentOS release 6.4 (Final)[root@test1 backup]# gzip --versiongzip 1.3.12Copyright (C) 2007 Free Software Foundation, Inc.Copyright (C) 1993 Jean-loup Gailly.This is free software. You may redistribute copies of it under the terms ofthe GNU General Public License &lt;http://www.gnu.org/licenses/gpl.html&gt;.There is NO WARRANTY, to the extent permitted by law.Written by Jean-loup Gailly.[root@test1 backup]# mysql -e&quot;\\s&quot;--------------mysql Ver 14.14 Distrib 5.7.18, for linux-glibc2.5 (x86_64) using EditLine wrapperConnection id: 11Current database:Current user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: &#x27;&#x27;Using delimiter: ;Server version: 5.7.18-log MySQL Community Server (GPL)Protocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8mb4Db characterset: utf8mb4Client characterset: utf8mb4Conn. characterset: utf8mb4UNIX socket: /data/mysqldata/3306/mysql.sockUptime: 15 min 52 secThreads: 3 Questions: 61 Slow queries: 0 Opens: 116 Flush tables: 5 Open tables: 0 Queries per second avg: 0.064--------------备份命令1innobackupex --user=backup --password=&#x27;backup&#x27; --slave-info --stream=tar /tmp | gzip -&gt; /data/mysqldata/backup/xtra_full.tar.gz期望得到的完整信息123[root@test1 slave_info]# more xtrabackup_slave_info SET GLOBAL gtid_purged=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1164&#x27;;CHANGE MASTER TO MASTER_AUTO_POSITION=1;","text":"stream选择使用tar方式压缩到时slave_info信息不完整版本123456789101112131415161718192021222324252627282930313233343536[root@test1 backup]# innobackupex --versioninnobackupex version 2.4.8 Linux (x86_64) (revision id: 97330f7)[root@test1 backup]# cat /etc/redhat-release CentOS release 6.4 (Final)[root@test1 backup]# gzip --versiongzip 1.3.12Copyright (C) 2007 Free Software Foundation, Inc.Copyright (C) 1993 Jean-loup Gailly.This is free software. You may redistribute copies of it under the terms ofthe GNU General Public License &lt;http://www.gnu.org/licenses/gpl.html&gt;.There is NO WARRANTY, to the extent permitted by law.Written by Jean-loup Gailly.[root@test1 backup]# mysql -e&quot;\\s&quot;--------------mysql Ver 14.14 Distrib 5.7.18, for linux-glibc2.5 (x86_64) using EditLine wrapperConnection id: 11Current database:Current user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: &#x27;&#x27;Using delimiter: ;Server version: 5.7.18-log MySQL Community Server (GPL)Protocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8mb4Db characterset: utf8mb4Client characterset: utf8mb4Conn. characterset: utf8mb4UNIX socket: /data/mysqldata/3306/mysql.sockUptime: 15 min 52 secThreads: 3 Questions: 61 Slow queries: 0 Opens: 116 Flush tables: 5 Open tables: 0 Queries per second avg: 0.064--------------备份命令1innobackupex --user=backup --password=&#x27;backup&#x27; --slave-info --stream=tar /tmp | gzip -&gt; /data/mysqldata/backup/xtra_full.tar.gz期望得到的完整信息123[root@test1 slave_info]# more xtrabackup_slave_info SET GLOBAL gtid_purged=&#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1164&#x27;;CHANGE MASTER TO MASTER_AUTO_POSITION=1;实际得到的12[root@test1 backup]# more xtrabackup_slave_info SET GLOBAL gtid_purged=&#x27;5c3如果使用stream=xbstream同时指定了–slave-info,则根本不会按照期望的得到full.xbstream压缩文件,而是会直接将备份输出到timestamp文件夹,并产生一个empty的full.xbstream. 看下面例子innobackupex –user=backup –password=’backup’ \\–stream=xbstream ./ &gt; /data/mysqldata/backup/full.xbstream1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253170831 17:17:10 innobackupex: Starting the backup operationIMPORTANT: Please check that the backup run completes successfully. At the end of a successful backup run innobackupex prints &quot;completed OK!&quot;.170831 17:17:10 version_check Connecting to MySQL server with DSN &#x27;dbi:mysql:;mysql_read_default_group=xtrabackup;mysql_socket=/data/mysqldata/3306/mysql.sock&#x27; as &#x27;backup&#x27; (using password: YES).170831 17:17:10 version_check Connected to MySQL server170831 17:17:10 version_check Executing a version check against the server...170831 17:17:10 version_check Done.170831 17:17:10 Connecting to MySQL server host: localhost, user: backup, password: set, port: not set, socket: /data/mysqldata/3306/mysql.sockUsing server version 5.7.18-loginnobackupex version 2.4.8 based on MySQL server 5.7.13 Linux (x86_64) (revision id: 97330f7)xtrabackup: uses posix_fadvise().xtrabackup: cd to /data/mysqldata/3306/dataxtrabackup: open files limit requested 65535, set to 65535xtrabackup: using the following InnoDB configuration:xtrabackup: innodb_data_home_dir = .xtrabackup: innodb_data_file_path = ibdata1:1024M:autoextendxtrabackup: innodb_log_group_home_dir = /data/mysqldata/3306/redologxtrabackup: innodb_log_files_in_group = 2xtrabackup: innodb_log_file_size = 134217728xtrabackup: using O_DIRECTInnoDB: Number of pools: 1170831 17:17:10 &gt;&gt; log scanned up to (2937521)InnoDB: Opened 3 undo tablespacesInnoDB: 0 undo tablespaces made activextrabackup: Generating a list of tablespacesInnoDB: Allocated tablespace ID 27 for sms/SMS_DETAIL_INFO, old maximum was 3170831 17:17:11 [01] Streaming ./ibdata1 170831 17:17:11 &gt;&gt; log scanned up to (2937521)170831 17:17:12 &gt;&gt; log scanned up to (2937521)...170831 17:17:40 [00] ...done170831 17:17:40 Executing FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS...xtrabackup: The latest check point (for incremental): &#x27;2937512&#x27;xtrabackup: Stopping log copying thread..170831 17:17:40 &gt;&gt; log scanned up to (2937521)170831 17:17:41 Executing UNLOCK TABLES170831 17:17:41 All tables unlocked170831 17:17:41 [00] Streaming ib_buffer_pool to &lt;STDOUT&gt;170831 17:17:41 [00] ...done170831 17:17:41 Backup created in directory &#x27;/data/mysqldata/backup/&#x27; --注意一会对比这里MySQL binlog position: filename &#x27;mysql-bin.000002&#x27;, position &#x27;154&#x27;, GTID of the last change &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1164&#x27;170831 17:17:41 [00] Streaming &lt;STDOUT&gt;170831 17:17:41 [00] ...done170831 17:17:41 [00] Streaming &lt;STDOUT&gt;170831 17:17:41 [00] ...donextrabackup: Transaction log of lsn (2937512) to (2937521) was copied.170831 17:17:41 completed OK!正常的备份,生成了.xbstream文件123456[root@test1 backup]# lsfull.xbstream[root@test1 backup]# file full.xbstream full.xbstream: data[root@test1 backup]# rm full.xbstream rm: remove regular file `full.xbstream&#x27;? y加上–slave-info的innobackupex –user=backup –password=’backup’ –slave-info\\ –stream=xbstream ./ &gt; /data/mysqldata/backup/full.xbstream12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152170831 17:18:43 innobackupex: Starting the backup operationIMPORTANT: Please check that the backup run completes successfully. At the end of a successful backup run innobackupex prints &quot;completed OK!&quot;.170831 17:18:43 version_check Connecting to MySQL server with DSN &#x27;dbi:mysql:;mysql_read_default_group=xtrabackup;mysql_socket=/data/mysqldata/3306/mysql.sock&#x27; as &#x27;backup&#x27; (using password: YES).170831 17:18:43 version_check Connected to MySQL server170831 17:18:43 version_check Executing a version check against the server...170831 17:18:43 version_check Done.170831 17:18:43 Connecting to MySQL server host: localhost, user: backup, password: set, port: not set, socket: /data/mysqldata/3306/mysql.sockUsing server version 5.7.18-loginnobackupex version 2.4.8 based on MySQL server 5.7.13 Linux (x86_64) (revision id: 97330f7)xtrabackup: uses posix_fadvise().xtrabackup: cd to /data/mysqldata/3306/dataxtrabackup: open files limit requested 65535, set to 65535xtrabackup: using the following InnoDB configuration:xtrabackup: innodb_data_home_dir = .xtrabackup: innodb_data_file_path = ibdata1:1024M:autoextendxtrabackup: innodb_log_group_home_dir = /data/mysqldata/3306/redologxtrabackup: innodb_log_files_in_group = 2xtrabackup: innodb_log_file_size = 134217728xtrabackup: using O_DIRECTInnoDB: Number of pools: 1170831 17:18:43 &gt;&gt; log scanned up to (2937521)InnoDB: Opened 3 undo tablespacesInnoDB: 0 undo tablespaces made activextrabackup: Generating a list of tablespacesInnoDB: Allocated tablespace ID 27 for sms/SMS_DETAIL_INFO, old maximum was 3170831 17:18:44 [01] Copying ./ibdata1 to /data/mysqldata/backup/2017-08-31_17-18-43/ibdata1 --注意...170831 17:19:05 Finished backing up non-InnoDB tables and files170831 17:19:05 [00] Writing /data/mysqldata/backup/2017-08-31_17-18-43/xtrabackup_binlog_info --注意170831 17:19:05 [00] ...done170831 17:19:06 Executing FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS...xtrabackup: The latest check point (for incremental): &#x27;2937512&#x27;xtrabackup: Stopping log copying thread..170831 17:19:06 &gt;&gt; log scanned up to (2937521)170831 17:19:06 Executing UNLOCK TABLES170831 17:19:06 All tables unlocked170831 17:19:06 [00] Copying ib_buffer_pool to /data/mysqldata/backup/2017-08-31_17-18-43/ib_buffer_pool170831 17:19:06 [00] ...done170831 17:19:06 Backup created in directory &#x27;/data/mysqldata/backup/2017-08-31_17-18-43/&#x27;MySQL binlog position: filename &#x27;mysql-bin.000002&#x27;, position &#x27;154&#x27;, GTID of the last change &#x27;5c351518-78ec-11e7-8e7a-005056a610c3:1-1164&#x27;170831 17:19:06 [00] Writing /data/mysqldata/backup/2017-08-31_17-18-43/backup-my.cnf170831 17:19:06 [00] ...done170831 17:19:06 [00] Writing /data/mysqldata/backup/2017-08-31_17-18-43/xtrabackup_info170831 17:19:06 [00] ...donextrabackup: Transaction log of lsn (2937512) to (2937521) was copied.170831 17:19:06 completed OK!生成一个时间戳目录,和full.xbstream,full.xbstream是空的1234[root@test1 backup]# ls2017-08-31_17-18-43 full.xbstream[root@test1 backup]# file full.xbstreamfull.xbstream: empty并且2017-08-31_17-18-43目录下也没有xtrabackup_slave_info123[root@test1 backup]# cd 2017-08-31_17-18-43[root@test1 2017-08-31_17-18-43]# lsbackup-my.cnf fandb ib_buffer_pool ibdata1 mysql performance_schema sms sys undo001 undo002 undo003 xtrabackup_binlog_info xtrabackup_checkpoints xtrabackup_info xtrabackup_logfile yydb","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"主从传输表空间的坑","slug":"主从传输表空间的坑","date":"2017-08-22T04:25:00.000Z","updated":"2017-08-22T04:26:34.000Z","comments":true,"path":"2017/08/22/主从传输表空间的坑/","link":"","permalink":"http://fuxkdb.com/2017/08/22/%E4%B8%BB%E4%BB%8E%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4%E7%9A%84%E5%9D%91/","excerpt":"主库import tablespace只会在binlog中记录alter table xxx import tablespace语句,而不会记录表中的数据的插入语句1234567891011121314151617181920212223242526[mysql@master2 ~]$ mysqlbinlog -vv --base64-output=decode-rows /data/mysqldata/3306/binlog/mysql-bin.000013 --start-position=694/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 694#170713 7:46:11 server id 23306 end_log_pos 759 CRC32 0xfa604449 GTID last_committed=2 sequence_number=3SET @@SESSION.GTID_NEXT= &#x27;5691c701-382a-11e5-bbc4-000c293d13e1:19&#x27;/*!*/;# at 759#170713 7:46:11 server id 23306 end_log_pos 869 CRC32 0x35860c26 Query thread_id=8 exec_time=0 error_code=0use `fandb`/*!*/;SET TIMESTAMP=1499903171/*!*/;SET @@session.pseudo_thread_id=8/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1075838976/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\\C utf8mb4 *//*!*/;SET @@session.character_set_client=45,@@session.collation_connection=45,@@session.collation_server=45/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;alter table dept import tablespace/*!*/;SET @@SESSION.GTID_NEXT= &#x27;AUTOMATIC&#x27; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;假如在源端flush table xxx for export后,只将xxx.{ibd,cfg}拷贝到主库,那么当主库alter table xxx discard tablespace时,从库也会执行discard tablespace而当主库执行alter table xxx import tablespace时,由于主库有拷贝过来的xxx.{ibd,cfg},所以可以执行成功,而从库没有,会失败1232017-07-10T21:48:13.649264Z 25 [Warning] Slave: InnoDB: ALTER TABLE `fandb`.`dept4` IMPORT TABLESPACE failed with error 44 : &#x27;Tablespace not found&#x27; Error_code: 18162017-07-10T21:48:13.649293Z 25 [Warning] Slave: Tablespace is missing for table `fandb`.`dept4`. Error_code: 18122017-07-10T21:48:13.649321Z 25 [ERROR] Error running query, slave SQL thread aborted. Fix the problem, and restart the slave SQL thread with &quot;SLAVE START&quot;. We stopped at log &#x27;mysql-bin.000001&#x27; position 1015slave会停止","text":"主库import tablespace只会在binlog中记录alter table xxx import tablespace语句,而不会记录表中的数据的插入语句1234567891011121314151617181920212223242526[mysql@master2 ~]$ mysqlbinlog -vv --base64-output=decode-rows /data/mysqldata/3306/binlog/mysql-bin.000013 --start-position=694/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 694#170713 7:46:11 server id 23306 end_log_pos 759 CRC32 0xfa604449 GTID last_committed=2 sequence_number=3SET @@SESSION.GTID_NEXT= &#x27;5691c701-382a-11e5-bbc4-000c293d13e1:19&#x27;/*!*/;# at 759#170713 7:46:11 server id 23306 end_log_pos 869 CRC32 0x35860c26 Query thread_id=8 exec_time=0 error_code=0use `fandb`/*!*/;SET TIMESTAMP=1499903171/*!*/;SET @@session.pseudo_thread_id=8/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1075838976/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\\C utf8mb4 *//*!*/;SET @@session.character_set_client=45,@@session.collation_connection=45,@@session.collation_server=45/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;alter table dept import tablespace/*!*/;SET @@SESSION.GTID_NEXT= &#x27;AUTOMATIC&#x27; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;假如在源端flush table xxx for export后,只将xxx.{ibd,cfg}拷贝到主库,那么当主库alter table xxx discard tablespace时,从库也会执行discard tablespace而当主库执行alter table xxx import tablespace时,由于主库有拷贝过来的xxx.{ibd,cfg},所以可以执行成功,而从库没有,会失败1232017-07-10T21:48:13.649264Z 25 [Warning] Slave: InnoDB: ALTER TABLE `fandb`.`dept4` IMPORT TABLESPACE failed with error 44 : &#x27;Tablespace not found&#x27; Error_code: 18162017-07-10T21:48:13.649293Z 25 [Warning] Slave: Tablespace is missing for table `fandb`.`dept4`. Error_code: 18122017-07-10T21:48:13.649321Z 25 [ERROR] Error running query, slave SQL thread aborted. Fix the problem, and restart the slave SQL thread with &quot;SLAVE START&quot;. We stopped at log &#x27;mysql-bin.000001&#x27; position 1015slave会停止 修复方法是,将xxx.{ibd,cfg}拷贝到从库,然后12345678910111213141516set sql_log_bin=off;alter table xxx import tablespace;set sql_log_bin=on;(mysql@localhost) [fandb]&gt; set gtid_next=&#x27;5691c701-382a-11e5-bbc4-000c293d13e1:6&#x27;;Query OK, 0 rows affected (0.00 sec)(mysql@localhost) [fandb]&gt; begin;Query OK, 0 rows affected (0.00 sec)(mysql@localhost) [fandb]&gt; commit;Query OK, 0 rows affected (0.00 sec)(mysql@localhost) [fandb]&gt; set gtid_next=&#x27;automatic&#x27;;(mysql@localhost) [fandb]&gt; start slave sql_thread;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"传输表空间","slug":"传输表空间","permalink":"http://fuxkdb.com/tags/%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4/"}]},{"title":"Transportable Tablespace Internals","slug":"Transportable-Tablespace-Internals","date":"2017-08-22T04:24:00.000Z","updated":"2017-08-22T04:31:05.000Z","comments":true,"path":"2017/08/22/Transportable-Tablespace-Internals/","link":"","permalink":"http://fuxkdb.com/2017/08/22/Transportable-Tablespace-Internals/","excerpt":"","text":"Transportable Tablespace Internals以下信息描述了InnoDB的传输表空间复制过程的内部原理和error log中输出的信息 当在目标端执行ALTER TABLE ... DISCARD TABLESPACE命令时: The table is locked in X mode. 表空间会与表分离.The tablespace is detached from the table. 当在源端执行FLUSH TABLES ... FOR EXPORT命令时: The table being flushed for export is locked in shared mode. 清除协调程序线程已停止The purge coordinator thread is stopped. 脏块会被写入磁盘 表的元数据信息会被写入二进制文件.cfg中 此操作的预期错误日志消息:12342013-09-24T13:10:19.903526Z 2 [Note] InnoDB: Sync to disk of &#x27;&quot;test&quot;.&quot;t&quot;&#x27; started.2013-09-24T13:10:19.903586Z 2 [Note] InnoDB: Stopping purge2013-09-24T13:10:19.903725Z 2 [Note] InnoDB: Writing table metadata to &#x27;./test/t.cfg&#x27;2013-09-24T13:10:19.904014Z 2 [Note] InnoDB: Table &#x27;&quot;test&quot;.&quot;t&quot;&#x27; flushed to disk 当在源端执行UNLOCK TABLES命令时: 二进制文件.cfg会被删除 The shared lock on the table or tables being imported is released ,并且清除协调程序线程会重启purge coordinator thread is restarted. 此操作的预期错误日志消息:122013-09-24T13:10:21.181104Z 2 [Note] InnoDB: Deleting the meta-data file &#x27;./test/t.cfg&#x27;2013-09-24T13:10:21.181180Z 2 [Note] InnoDB: Resuming purge 当在目标端执行ALTER TABLE ... IMPORT TABLESPACE时,导入算法会执行如下操作: 将检查每个表空间页是否损坏. 每个页面上的空间ID和日志序列号（LSN）都会更新 标志被验证,LSN被更新为头页. Btree页面更新. 页面状态设置为dirty,以便它将被写入磁盘. 此操作的预期错误日志消息:1234562013-07-18 15:15:01 34960 [Note] InnoDB: Importing tablespace for table &#x27;test/t&#x27; that was exported from host &#x27;ubuntu&#x27;2013-07-18 15:15:01 34960 [Note] InnoDB: Phase I - Update all pages2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk2013-07-18 15:15:01 34960 [Note] InnoDB: Sync to disk - done!2013-07-18 15:15:01 34960 [Note] InnoDB: Phase III - Flush changes to disk2013-07-18 15:15:01 34960 [Note] InnoDB: Phase IV - Flush complete 注意您还可能会收到一个警告，表明丢弃了表空间（如果您丢弃了目标表的表空间）和一条消息，指出由于缺少.ibd文件而无法计算统计信息：1232013-07-18 15:14:38 34960 [Warning] InnoDB: Table &quot;test&quot;.&quot;t&quot; tablespace is set as discarded.2013-07-18 15:14:38 7f34d9a37700 InnoDB: cannot calculate statistics for table &quot;test&quot;.&quot;t&quot; because the .ibd file is missing. For help, please refer tohttp://dev.mysql.com/doc/refman/5.7/en/innodb-troubleshooting.html","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"传输表空间","slug":"传输表空间","permalink":"http://fuxkdb.com/tags/%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4/"}]},{"title":"Transportable Tablespace示例","slug":"Transportable-Tablespace示例","date":"2017-08-22T04:24:00.000Z","updated":"2017-08-22T04:30:20.000Z","comments":true,"path":"2017/08/22/Transportable-Tablespace示例/","link":"","permalink":"http://fuxkdb.com/2017/08/22/Transportable-Tablespace%E7%A4%BA%E4%BE%8B/","excerpt":"Transportable Tablespace示例http://dev.mysql.com/doc/refman/5.7/en/innodb-transportable-tablespace-examples.html 例1:将InnoDB表从一个服务器复制到另一个服务器此过程演示如何将InnoDB表从正在运行的MySQL服务器实例复制到另一个正在运行的实例.经过一些小调整相同过程可用于在同一实例上执行完整表恢复.1.source端1234567891011121314151617181920212223242526272829303132(mysql@localhost) [test]&gt; show table status like &#x27;dept&#x27;\\G*************************** 1. row *************************** Name: dept Engine: InnoDB Version: 10 Row_format: Compact Rows: 0 Avg_row_length: 0 Data_length: 16384Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2016-08-26 14:25:50 Update_time: NULL Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec)(mysql@localhost) [test]&gt; select * from dept;+--------+------------+----------+| deptno | dname | loc |+--------+------------+----------+| 10 | ACCOUNTING | NEW YORK || 20 | RESEARCH | DALLAS || 30 | SALES | CHICAGO || 40 | OPERATIONS | BOSTON |+--------+------------+----------+4 rows in set (0.01 sec) 2.target端123456CREATE TABLE `dept` ( `deptno` int(11) NOT NULL, `dname` varchar(14) DEFAULT NULL, `loc` varchar(13) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 3.在目标端discard tablespace.Before a tablespace can be imported, InnoDB must discard the tablespace that is attached to the receiving table1mysql&gt; alter table dept discard tablespace;","text":"Transportable Tablespace示例http://dev.mysql.com/doc/refman/5.7/en/innodb-transportable-tablespace-examples.html 例1:将InnoDB表从一个服务器复制到另一个服务器此过程演示如何将InnoDB表从正在运行的MySQL服务器实例复制到另一个正在运行的实例.经过一些小调整相同过程可用于在同一实例上执行完整表恢复.1.source端1234567891011121314151617181920212223242526272829303132(mysql@localhost) [test]&gt; show table status like &#x27;dept&#x27;\\G*************************** 1. row *************************** Name: dept Engine: InnoDB Version: 10 Row_format: Compact Rows: 0 Avg_row_length: 0 Data_length: 16384Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2016-08-26 14:25:50 Update_time: NULL Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec)(mysql@localhost) [test]&gt; select * from dept;+--------+------------+----------+| deptno | dname | loc |+--------+------------+----------+| 10 | ACCOUNTING | NEW YORK || 20 | RESEARCH | DALLAS || 30 | SALES | CHICAGO || 40 | OPERATIONS | BOSTON |+--------+------------+----------+4 rows in set (0.01 sec) 2.target端123456CREATE TABLE `dept` ( `deptno` int(11) NOT NULL, `dname` varchar(14) DEFAULT NULL, `loc` varchar(13) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 3.在目标端discard tablespace.Before a tablespace can be imported, InnoDB must discard the tablespace that is attached to the receiving table1mysql&gt; alter table dept discard tablespace;4.在源端, run FLUSH TABLES … FOR EXPORT to quiesce the table and create the .cfg metadata file:1234567(mysql@localhost) [test]&gt; flush tables dept for export;查看error log有如下提示.mysql会先将dept表脏块刷新到磁盘,然后生成.cfg文件.并且以某种方式锁住dept表,通过information_schema.INNODB_TRX和information_schema.INNODB_LOCKS表查不到信息2017-01-04 18:23:42 5174 [Note] InnoDB: Sync to disk of &#x27;&quot;test&quot;.&quot;dept&quot;&#x27; started.2017-01-04 18:23:42 5174 [Note] InnoDB: Stopping purge2017-01-04 18:23:42 5174 [Note] InnoDB: Writing table metadata to &#x27;./test/dept.cfg&#x27;2017-01-04 18:23:42 5174 [Note] InnoDB: Table &#x27;&quot;test&quot;.&quot;dept&quot;&#x27; flushed to disk此时无法对dept表执行DML操作,只能执行只读操作 NoteFLUSH TABLES … FOR EXPORT is available as of MySQL 5.6.6. The statement ensures that changes to the named table have been flushed to disk so that a binary table copy can be made while the server is running. When FLUSH TABLES … FOR EXPORT is run, InnoDB produces a .cfg file in the same database directory as the table. The .cfg file contains metadata used for schema verification when importing the tablespace file. 5.拷贝.ibd文件和.cfg元数据文件到目标端1cp /data/mysqldata/3306/data/test/dept.&#123;ibd,cfg&#125; /data/mysqldata/3307/data/test一定要在shared locks释放之前完成拷贝 6.在源端执行UNLOCK TABLES释放FLUASH TABLES .. FOR EXPORT产生的锁12345(mysql@localhost) [test]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec)2017-01-04 18:34:28 5174 [Note] InnoDB: Deleting the meta-data file &#x27;./test/dept.cfg&#x27;2017-01-04 18:34:28 5174 [Note] InnoDB: Resuming purge 7.在目标端,导入表空间123456789101112131415161718192021mysql&gt; alter table dept import tablespace;Query OK, 0 rows affected (0.02 sec)mysql&gt; select * from dept;+--------+------------+----------+| deptno | dname | loc |+--------+------------+----------+| 10 | ACCOUNTING | NEW YORK || 20 | RESEARCH | DALLAS || 30 | SALES | CHICAGO || 40 | OPERATIONS | BOSTON |+--------+------------+----------+4 rows in set (0.00 sec)2017-01-04 18:35:46 16972 [Note] InnoDB: Importing tablespace for table &#x27;test/dept&#x27; that was exported from host &#x27;master&#x27;2017-01-04 18:35:46 16972 [Note] InnoDB: Phase I - Update all pages2017-01-04 18:35:46 16972 [Note] InnoDB: Sync to disk2017-01-04 18:35:46 16972 [Note] InnoDB: Sync to disk - done!2017-01-04 18:35:46 16972 [Note] InnoDB: Phase III - Flush changes to disk2017-01-04 18:35:46 16972 [Note] InnoDB: Phase IV - Flush complete告警日志里没显示阶段2是啥注意:ALTER TABLE … IMPORT TABLESPACE功能不会对导入的数据实施外键约束。 如果表之间存在外键约束，则应在相同（逻辑）时间点导出所有表。 在这种情况下，您将停止更新表，提交所有事务，获取表上的共享锁，然后执行导出操作。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"传输表空间","slug":"传输表空间","permalink":"http://fuxkdb.com/tags/%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4/"}]},{"title":"Transportable Tablespaces","slug":"Transportable-Tablespaces","date":"2017-08-22T04:23:00.000Z","updated":"2017-08-22T04:35:14.000Z","comments":true,"path":"2017/08/22/Transportable-Tablespaces/","link":"","permalink":"http://fuxkdb.com/2017/08/22/Transportable-Tablespaces/","excerpt":"迁移File-Per-Table TablespacesTransportable Tablespace示例Transportable Tablespace Internals本节介绍如何将file-per-table tablespace从一个server迁移到另一个server,也称为可Transportable Tablespaces功能。 在MySQL 5.7.4之前,只支持非分区的InnoDB表。 从MySQL 5.7.4开始,还支持分区的InnoDB表和各个InnoDB表分区和子分区。 There are many reasons why you might copy an InnoDB file-per-table tablespace to a different database server: 生成报告而不影响生产库 为备库初始化数据 用于数据恢复 相对于mysqldump,传输表空间有个更快的速度 将每个文件的表空间移动到具有更适合系统要求的存储介质的服务器.例如,您可能希望在SSD设备上拥有繁忙的表,或在大容量HDD设备上使用大型表. 限制和使用注意事项 仅当innodb_file_per_table设置为ON（这是MySQL 5.6.6的默认设置）时，传输表空间功能才可以使用。 驻留在共享系统表空间中的表不能使用此功能。 在传输过程中,只有只读操作可以执行 page size要相同.When importing a tablespace, the page size must match the page size of the importing instance. Prior to MySQL 5.7.4, DISCARD TABLESPACE is not supported for partitioned tables meaning that transportable tablespaces is also unsupported. If you run ALTER TABLE … DISCARD TABLESPACE on a partitioned table, the following error is returned: ERROR 1031 (HY000): Table storage engine for ‘part’ doesn’t have this option. As of MySQL 5.7.4, ALTER TABLE … DISCARD TABLESPACE is supported for partitioned InnoDB tables, and ALTER TABLE … DISCARD PARTITION … TABLESPACE is supported for InnoDB table partitions. 当foreign_key_checks设置为1时，对于父 - 子（主键 - 外键）关系的表空间不支持DISCARD TABLESPACE。在放弃父子表的表空间之前，请设置foreign_key_checks = 0。 分区的InnoDB表不支持外键。 ALTER TABLE … IMPORT TABLESPACE不会对导入的数据实施外键约束。 如果表之间存在外键约束，则应在相同（逻辑）时间点导出所有表。 分区的InnoDB表不支持外键。 ALTER TABLE … IMPORT TABLESPACE和ALTER TABLE … IMPORT PARTITION … TABLESPACE不需要.cfg元数据文件来导入表空间。 但是，在不使用.cfg文件导入时，不会执行元数据检查，并且将发出类似于以下内容的警告： 123Message: InnoDB: IO Read error: (2, No such file or directory) Error opening &#x27;.\\test\\t.cfg&#x27;, will attempt to import without schema verification1 row in set (0.00 sec) 没有.cfg文件导入的能力可能更方便，当不需要模式不匹配。 此外，无法使用.cfg文件导入的能力在无法从.ibd文件收集元数据的崩溃恢复方案中很有用。 The ability to import without a .cfg file may be more convenient when no schema mismatches are expected. Additionally, the ability to import without a .cfg file could be useful in crash recovery scenarios in which metadata cannot be collected from an .ibd file. 由于.cfg元数据文件限制，在为分区表导入表空间文件时，不会报告分区类型或分区定义差异的模式不匹配。但会报告列差异。 在子分区表上运行ALTER TABLE … DISCARD PARTITION … TABLESPACE和ALTER TABLE … IMPORT PARTITION … TABLESPACE时，允许使用分区表和子分区表名。指定分区名称时，该分区的子分区将包括在操作中。 在MySQL 5.6或更高版本中，如果两个服务器都具有GA（通用可用性）状态并且它们的版本在同一系列中，则从另一个服务器导入表空间文件是有效的。否则，该文件必须在导入它的服务器上创建。 在复制方案中，在主节点和从节点上必须将innodb_file_per_table设置为ON。 在Windows上，InnoDB以小写形式内部存储数据库，表空间和表名。要避免区分大小写操作系统（如Linux和UNIX）上的导入问题，请使用小写名称创建所有数据库，表空间和表。一种方便的方法是在创建数据库，表空间或表之前，在my.cnf或my.ini文件的[mysqld]节中添加以下行：","text":"迁移File-Per-Table TablespacesTransportable Tablespace示例Transportable Tablespace Internals本节介绍如何将file-per-table tablespace从一个server迁移到另一个server,也称为可Transportable Tablespaces功能。 在MySQL 5.7.4之前,只支持非分区的InnoDB表。 从MySQL 5.7.4开始,还支持分区的InnoDB表和各个InnoDB表分区和子分区。 There are many reasons why you might copy an InnoDB file-per-table tablespace to a different database server: 生成报告而不影响生产库 为备库初始化数据 用于数据恢复 相对于mysqldump,传输表空间有个更快的速度 将每个文件的表空间移动到具有更适合系统要求的存储介质的服务器.例如,您可能希望在SSD设备上拥有繁忙的表,或在大容量HDD设备上使用大型表. 限制和使用注意事项 仅当innodb_file_per_table设置为ON（这是MySQL 5.6.6的默认设置）时，传输表空间功能才可以使用。 驻留在共享系统表空间中的表不能使用此功能。 在传输过程中,只有只读操作可以执行 page size要相同.When importing a tablespace, the page size must match the page size of the importing instance. Prior to MySQL 5.7.4, DISCARD TABLESPACE is not supported for partitioned tables meaning that transportable tablespaces is also unsupported. If you run ALTER TABLE … DISCARD TABLESPACE on a partitioned table, the following error is returned: ERROR 1031 (HY000): Table storage engine for ‘part’ doesn’t have this option. As of MySQL 5.7.4, ALTER TABLE … DISCARD TABLESPACE is supported for partitioned InnoDB tables, and ALTER TABLE … DISCARD PARTITION … TABLESPACE is supported for InnoDB table partitions. 当foreign_key_checks设置为1时，对于父 - 子（主键 - 外键）关系的表空间不支持DISCARD TABLESPACE。在放弃父子表的表空间之前，请设置foreign_key_checks = 0。 分区的InnoDB表不支持外键。 ALTER TABLE … IMPORT TABLESPACE不会对导入的数据实施外键约束。 如果表之间存在外键约束，则应在相同（逻辑）时间点导出所有表。 分区的InnoDB表不支持外键。 ALTER TABLE … IMPORT TABLESPACE和ALTER TABLE … IMPORT PARTITION … TABLESPACE不需要.cfg元数据文件来导入表空间。 但是，在不使用.cfg文件导入时，不会执行元数据检查，并且将发出类似于以下内容的警告： 123Message: InnoDB: IO Read error: (2, No such file or directory) Error opening &#x27;.\\test\\t.cfg&#x27;, will attempt to import without schema verification1 row in set (0.00 sec) 没有.cfg文件导入的能力可能更方便，当不需要模式不匹配。 此外，无法使用.cfg文件导入的能力在无法从.ibd文件收集元数据的崩溃恢复方案中很有用。 The ability to import without a .cfg file may be more convenient when no schema mismatches are expected. Additionally, the ability to import without a .cfg file could be useful in crash recovery scenarios in which metadata cannot be collected from an .ibd file. 由于.cfg元数据文件限制，在为分区表导入表空间文件时，不会报告分区类型或分区定义差异的模式不匹配。但会报告列差异。 在子分区表上运行ALTER TABLE … DISCARD PARTITION … TABLESPACE和ALTER TABLE … IMPORT PARTITION … TABLESPACE时，允许使用分区表和子分区表名。指定分区名称时，该分区的子分区将包括在操作中。 在MySQL 5.6或更高版本中，如果两个服务器都具有GA（通用可用性）状态并且它们的版本在同一系列中，则从另一个服务器导入表空间文件是有效的。否则，该文件必须在导入它的服务器上创建。 在复制方案中，在主节点和从节点上必须将innodb_file_per_table设置为ON。 在Windows上，InnoDB以小写形式内部存储数据库，表空间和表名。要避免区分大小写操作系统（如Linux和UNIX）上的导入问题，请使用小写名称创建所有数据库，表空间和表。一种方便的方法是在创建数据库，表空间或表之前，在my.cnf或my.ini文件的[mysqld]节中添加以下行： 12[mysqld]lower_case_table_names=1 ALTER TABLE … DISCARD TABLESPACE和ALTER TABLE … IMPORT TABLESPACE不支持general tablespace 从MySQL 5.7.9开始，InnoDB表的默认行格式可以使用innodb_default_row_format配置选项配置。如果源服务器上的innodb_default_row_format设置与目标服务器上的设置不同，则试图导入未明确定义行格式（ROW_FORMAT）或使用ROW_FORMAT = DEFAULT的表可能会导致模式不匹配错误。有关相关信息，请参见第15.11.2节“指定表的行格式”。 innodb_default_row_formatThe innodb_default_row_format option, introduced in MySQL 5.7.9, defines the default row format for InnoDB tables (including user-created InnoDB temporary tables). The default setting is DYNAMIC. Other permitted values are COMPACT and REDUNDANT. The COMPRESSED row format, which is not supported for use in the system tablespace, cannot be defined as the default. Newly created tables use the row format defined by innodb_default_row_format when a ROW_FORMAT option is not specified explicitly or when ROW_FORMAT=DEFAULT is used. When a ROW_FORMAT option is not specified explicitly or when ROW_FORMAT=DEFAULT is used, any operation that rebuilds a table also silently changes the row format of the table to the format defined by innodb_default_row_format. For more information, see Section 15.11.2, “Specifying the Row Format for a Table”. Internal InnoDB temporary tables created by the server to process queries use the DYNAMIC row format, regardless of the innodb_default_row_format setting. In MySQL 5.7.8 and earlier, the default row format is COMPACT.REDUNDANT和COMPACT行格式支持最大索引关键字前缀长度为767字节，而DYNAMIC和COMPRESSED行格式支持索引关键字前缀长度为3072字节，如果innodb_large_prefix配置选项启用。 在复制环境中，如果innodb_default_row_format在主服务器上设置为DYNAMIC，并在从服务器上设置为COMPACT，则以下没有显式定义行格式的DDL语句在主服务器上成功，但在从服务器上失败：CREATE TABLE t1 (c1 INT PRIMARY KEY, c2 VARCHAR(5000), KEY i1(c2(3070)));要查看表的行格式，请发出SHOW TABLE STATUS语句或查询INFORMATION_SCHEMA.TABLES。SELECT * FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE NAME LIKE &#39;test/t1&#39; \\G 当导出使用InnoDB表空间加密功能加密的表空间时，InnoDB除了生成.cfg元数据文件外还会生成.cfp文件。在目标服务器上执行ALTER TABLE … IMPORT TABLESPACE操作之前，必须将.cfp文件与.cfg文件和表空间文件一起复制到目标服务器。 .cfp文件包含传输密钥和加密的表空间密钥。在导入时，InnoDB使用传输密钥来解密表空间密钥。有关相关信息，请参见第15.7.10节“InnoDB表空间加密”。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"传输表空间","slug":"传输表空间","permalink":"http://fuxkdb.com/tags/%E4%BC%A0%E8%BE%93%E8%A1%A8%E7%A9%BA%E9%97%B4/"}]},{"title":"调整InnoDB系统表空间大小","slug":"调整InnoDB系统表空间大小","date":"2017-08-11T02:57:00.000Z","updated":"2017-08-11T02:58:07.000Z","comments":true,"path":"2017/08/11/调整InnoDB系统表空间大小/","link":"","permalink":"http://fuxkdb.com/2017/08/11/%E8%B0%83%E6%95%B4InnoDB%E7%B3%BB%E7%BB%9F%E8%A1%A8%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F/","excerpt":"","text":"调整InnoDB系统表空间大小本文介绍如何增大或缩小InnoDB system tablespace 增大InnoDB system tablespace最简单的增大InnoDB system tablespace大小的方法是在一开始配置的时候就指定为自动扩展. 为innodb_data_file_path参数中的最后一个数据文件指定autoextend选项. InnoDB在空间不足时以64MB为单位自动增加该文件的大小. 可以通过设置innodb_autoextend_increment系统变量的值（以兆字节为单位）来更改增量大小. 您可以通过添加另一个数据文件来扩展系统表空间： 1.关闭MySQL2.如果上一个数据文件是使用关键字autoextend定义的,则根据实际增长的大小将其定义更改为使用固定大小. 检查数据文件的大小,将其舍入到1024×1024字节（= 1MB）的最接近的倍数,并在innodb_data_file_path中显式指定舍入后的大小.3.将新的数据文件添加到innodb_data_file_path的末尾,可以指定该文件为自动扩展. 注意,只能将innodb_data_file_path中的最后一个数据文件指定为自动扩展.4.启动MySQL 实际例子: 初始只有一个ibdata1,现在我们想增加一个数据文件 12innodb_data_home_dir =innodb_data_file_path = /ibdata/ibdata1:10M:autoextend 假设ibdata1此时已经增长到988M,那么修改配置为 12innodb_data_home_dir =innodb_data_file_path = /ibdata/ibdata1:988M;/disk2/ibdata2:50M:autoextend 启动MySQL后,ibdata2会被初始化 1232017-08-11T10:27:06.014446+08:00 0 [Note] InnoDB: Need to create a new innodb_system data file &#x27;ibdata2&#x27;.2017-08-11T10:27:06.014567+08:00 0 [Note] InnoDB: Setting file &#x27;./ibdata2&#x27; size to 50 MB. Physically writing the file full; Please wait ...2017-08-11T10:27:06.182464+08:00 0 [Note] InnoDB: File &#x27;./ibdata2&#x27; size is now 50 MB. 缩小InnoDB system tablespace您不能从系统表空间中删除数据文件. 要减少系统表空间大小,请使用以下过程： 1.使用mysqldump来转储所有的InnoDB表,包括位于MySQL数据库中的InnoDB表. 123456789101112131415161718192021222324mysql&gt; SELECT TABLE_NAME from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA=&#x27;mysql&#x27; and ENGINE=&#x27;InnoDB&#x27;;+---------------------------+| TABLE_NAME |+---------------------------+| engine_cost || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || plugin || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type |+---------------------------+ 2.关闭MySQL3.删除所有现有的表空间文件（ .ibd）,包括ibdata和ib_log文件. 不要忘记删除位于MySQL数据库中的表的 .ibd文件.4.删除InnoDB表的任何.frm文件.5.配置新的表空间.6.重启MySQL7.导入dump文件 Note如果您的数据库仅使用InnoDB引擎,可能会更容易地转储所有数据库,停止服务器,删除所有数据库和InnoDB日志文件,重新启动服务器以及导入转储文件.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"MySQL数据库设计规范","slug":"MySQL数据库设计规范","date":"2017-08-10T08:00:00.000Z","updated":"2017-08-10T08:16:04.000Z","comments":true,"path":"2017/08/10/MySQL数据库设计规范/","link":"","permalink":"http://fuxkdb.com/2017/08/10/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/","excerpt":"MySQL数据库设计规范MySQL数据库与Oracle、sqlserver等数据库相比，有其内核上的优势与劣势。我们在使用MySQL数据库的时候需要遵循一定规范，扬长避短。本规范旨在帮助或指导RD、QA、OP等技术人员做出适合线上业务的数据库设计。在数据库变更和处理流程、数据库表设计、SQL编写等方面予以规范，从而为公司业务系统稳定、健康地运行提供保障。 数据库设计以下所有规范会按照【高危】、【强制】、【建议】三个级别进行标注，遵守优先级从高到低。对于不满足【高危】和【强制】两个级别的设计，DBA会强制打回要求修改。 库名1.【强制】库的名称必须控制在32个字符以内，相关模块的表名与表名之间尽量提现join的关系，如user表和user_login表。2.【建议】库的名称格式：业务系统名称子系统名，同一模块使用的表名尽量使用统一前缀。**3.【强制】一般分库名称命名格式是“库通配名编号”，编号从“0”开始递增，比如“wenda001” **以时间进行分库的名称格式是“库通配名时间”4.【强制】创建数据库时必须显式指定字符集，并且字符集只能是utf8或者utf8mb4 创建数据库SQL举例： 1Create database db1 default character set utf8; 表结构1.【强制】表和列的名称必须控制在32个字符以内，表名只能使用字母、数字和下划线，一律小写。2.【建议】表名要求模块名强相关，如师资系统采用”sz”作为前缀，渠道系统采用”qd”作为前缀等。3.【强制】创建表时必须显式指定字符集为utf8或utf8mb4。4.【强制】创建表时必须显式指定表存储引擎类型，如无特殊需求，一律为InnoDB。 当需使用除InnoDB以外的存储引擎时,必须通过DBA审核才能在生产环境中使用 因为Innodb表支持事务、行锁、宕机恢复、MVCC等关系型数据库重要特性，为业界使用最多的MySQL存储引擎。而这是其他大多数存储引擎不具备的，因此首推InnoDB","text":"MySQL数据库设计规范MySQL数据库与Oracle、sqlserver等数据库相比，有其内核上的优势与劣势。我们在使用MySQL数据库的时候需要遵循一定规范，扬长避短。本规范旨在帮助或指导RD、QA、OP等技术人员做出适合线上业务的数据库设计。在数据库变更和处理流程、数据库表设计、SQL编写等方面予以规范，从而为公司业务系统稳定、健康地运行提供保障。 数据库设计以下所有规范会按照【高危】、【强制】、【建议】三个级别进行标注，遵守优先级从高到低。对于不满足【高危】和【强制】两个级别的设计，DBA会强制打回要求修改。 库名1.【强制】库的名称必须控制在32个字符以内，相关模块的表名与表名之间尽量提现join的关系，如user表和user_login表。2.【建议】库的名称格式：业务系统名称子系统名，同一模块使用的表名尽量使用统一前缀。**3.【强制】一般分库名称命名格式是“库通配名编号”，编号从“0”开始递增，比如“wenda001” **以时间进行分库的名称格式是“库通配名时间”4.【强制】创建数据库时必须显式指定字符集，并且字符集只能是utf8或者utf8mb4 创建数据库SQL举例： 1Create database db1 default character set utf8; 表结构1.【强制】表和列的名称必须控制在32个字符以内，表名只能使用字母、数字和下划线，一律小写。2.【建议】表名要求模块名强相关，如师资系统采用”sz”作为前缀，渠道系统采用”qd”作为前缀等。3.【强制】创建表时必须显式指定字符集为utf8或utf8mb4。4.【强制】创建表时必须显式指定表存储引擎类型，如无特殊需求，一律为InnoDB。 当需使用除InnoDB以外的存储引擎时,必须通过DBA审核才能在生产环境中使用 因为Innodb表支持事务、行锁、宕机恢复、MVCC等关系型数据库重要特性，为业界使用最多的MySQL存储引擎。而这是其他大多数存储引擎不具备的，因此首推InnoDB 5.【强制】建表必须有comment6.【强制】表必须有主键,且主键应为顺序增长,如无特殊需求,建议主键为id int或bigint unsigned,且为auto_increment7.【建议】核心表（如用户表，金钱相关的表）必须有行数据的创建时间字段create_time和最后更新时间字段update_time，便于查问题。8.【建议】表中所有字段必须都是NOT NULL属性，业务可以根据需要定义DEFAULT值。因为使用NULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问题。9.【建议】建议对表里的blob、text等大字段，垂直拆分到其他表里，仅在需要读这些对象的时候才去select。10.【建议】反范式设计：把经常需要join查询的字段，在其他表里冗余一份。如user_name属性在user_account，user_loginlog等表里冗余一份，减少join查询。**11.【强制】中间表用于保留中间结果集，名称必须以“tmp”开头。备份表用于备份或抓取源表快照，名称必须以“bak_”开头。中间表和备份表定期清理。 12.【强制】对于超过100W行的大表进行alter table，必须经过DBA审核，并在业务低峰期执行。** 因为alter table会产生表锁，期间阻塞对于该表的所有写入，对于业务可能会产生极大影响。 一个较为规范的建表语句为： 1234567891011121314151617181920CREATE TABLE user ( `id` bigint(11) NOT NULL AUTO_INCREMENT, `user_id` bigint(11) NOT NULL COMMENT ‘用户id’ `username` varchar(45) NOT NULL COMMENT &#x27;真实姓名&#x27;, `email` varchar(30) NOT NULL COMMENT ‘用户邮箱’, `nickname` varchar(45) NOT NULL COMMENT &#x27;昵称&#x27;, `avatar` int(11) NOT NULL COMMENT &#x27;头像&#x27;, `birthday` date NOT NULL COMMENT &#x27;生日&#x27;, `sex` tinyint(4) DEFAULT &#x27;0&#x27; COMMENT &#x27;性别&#x27;, `short_introduce` varchar(150) DEFAULT NULL COMMENT &#x27;一句话介绍自己，最多50个汉字&#x27;, `user_resume` varchar(300) NOT NULL COMMENT &#x27;用户提交的简历存放地址&#x27;, `user_register_ip` int NOT NULL COMMENT ‘用户注册时的源ip’, `create_time` timestamp NOT NULL COMMENT ‘用户记录创建的时间’, `update_time` timestamp NOT NULL COMMENT ‘用户资料修改的时间’, `user_review_status` tinyint NOT NULL COMMENT ‘用户资料审核状态，1为通过，2为审核中，3为未通过，4为还未提交审核’, PRIMARY KEY (`id`), UNIQUE KEY `idx_user_id` (`user_id`), KEY `idx_username`(`username`), KEY `idx_create_time`(`create_time`,`user_review_status`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;网站用户基本信息&#x27;; 列数据类型优化1.【建议】表中的自增列（auto_increment属性），推荐使用bigint类型。因为无符号int存储范围为-2147483648~2147483647（大约21亿左右），溢出后会导致报错。2.【建议】业务中选择性很少的状态status、类型type等字段推荐使用tinytint或者smallint类型节省存储空间。3.【建议】业务中IP地址字段推荐使用int类型，不推荐用char(15)因为int只占4字节，可以用如下函数相互转换，而char(15)占用至少15字节。一旦表数据行数到了1亿，那么要多用1.1G存储空间4.【建议】不推荐使用enum，set因为它们浪费空间，且枚举值写死了，变更不方便。推荐使用tinyint或smallint5.【建议】不推荐使用blob，text等类型它们都比较浪费硬盘和内存空间。在加载表数据时，会读取大字段到内存里从而浪费内存空间，影响系统性能。建议和PM、RD沟通，是否真的需要这么大字段？Innodb中当一行记录超过8098字节时，会将该记录中选取最长的一个字段将其768字节放在原始page里，该字段余下内容放在overflow-page里。不幸的是在compact行格式下，原始page和overflow-page都会加载。6.【建议】存储金钱的字段，建议用int，程序端乘以100和除以100进行存取。因为int占用4字节，而double占用8字节，空间浪费。7.【建议】文本数据尽量用varchar存储因为varchar是变长存储，比char更省空间。MySQL server层规定一行所有文本最多存65535字节，因此在utf8字符集下最多存21844个字符，超过会自动转换为mediumtext字段。而text在utf8字符集下最多存21844个字符，mediumtext最多存2^24/3个字符，longtext最多存2^32个字符。一般建议用varchar类型，字符数不要超过27008.【建议】时间类型尽量选取timestamp因为datetime占用8字节，timestamp仅占用4字节，但是范围为1970-01-01 00:00:01到2038-01-01 00:00:00。更为高阶的方法，选用int来存储时间，使用SQL函数unix_timestamp()和from_unixtime()来进行转换。9.【建议】手机号用varchar(20)理由：涉及到区号或者国家代号，可能出现+-()；手机号不会去做数学运算；varchar可 以支持模糊查询，例如：like“138%” 索引设计1.【强制】InnoDB表必须主键为id int/bigint auto_increment,且主键值禁止被更新。2.【建议】主键的名称以“pk”开头，唯一键以“uk”或“uq”开头，普通索引以“idx”开头，一律使用小写格式，以表名/字段的名称或缩写作为后缀。3.【强制】单个索引中每个索引记录的长度不能超过64KB4.【建议】单个表上的索引个数不能超过7个5.【建议】在建立索引时，多考虑建立联合索引，并把选择性最高的字段放在最前面。如列userid的选择性可由select count(distinct userid)计算出来。 举例: 身份证号的选择性 远高于 性别的选择性 6.【强制】在多表join的SQL里，保证被驱动表的连接列上有索引，这样join执行效率最高。7.【建议】建表或加索引时，保证表里互相不存在冗余索引。对于MySQL来说，如果表里已经存在key(a,b)，则key(a)为冗余索引，需要删除。 分库分表、分区表1.【强制】分区表的分区字段（partition-key）必须有索引，或者是组合索引的首列。2.【强制】单个分区表中的分区（包括子分区）个数不能超过1024。3.【强制】上线前RD或者DBA必须指定分区表的创建、清理策略。4.【强制】访问分区表的SQL必须包含分区键。5.【建议】单个分区文件不超过2G，总大小不超过50G。建议总分区数不超过20个。6.【强制】对于分区表执行alter table操作，必须在业务低峰期执行。7.【强制】采用分库策略的，库的数量不能超过10248.【强制】采用分表策略的，表的数量不能超过40969.【建议】单个分表不超过500W行，ibd文件大小不超过2G，这样才能让数据分布式变得性能更佳。10.【建议】水平分表尽量用取模方式，日志、报表类数据建议采用日期进行分表。 字符集1.【强制】数据库本身库、表、列所有字符集必须保持一致，为utf8或utf8mb4 utf8mb4可以存储emoji表情 2.【强制】前端程序字符集或者环境变量中的字符集，与数据库、表的字符集必须一致，统一为utf8 程序DAO层设计建议1.【建议】新的代码不要用model，推荐使用手动拼SQL+绑定变量传入参数的方式。因为model虽然可以使用面向对象的方式操作db，但是其使用不当很容易造成生成的SQL非常复杂，且model层自己做的强制类型转换性能较差，最终导致数据库性能下降。2.【建议】前端程序连接MySQL或者redis，必须要有连接超时和失败重连机制，且失败重试必须有间隔时间。3.【建议】前端程序报错里尽量能够提示MySQL或redis原生态的报错信息，便于排查错误。4.【建议】对于有连接池的前端程序，必须根据业务需要配置初始、最小、最大连接数，超时时间以及连接回收机制，否则会耗尽数据库连接资源，造成线上事故。5.【建议】对于log或history类型的表，随时间增长容易越来越大，因此上线前RD或者DBA必须建立表数据清理或归档方案。6.【建议】 在应用程序设计阶段，RD必须考虑并规避数据库中主从延迟对于业务的影响。尽量避免从库短时延迟（20秒以内）对业务造成影响，建议强制一致性的读开启事务走主库，或更新后过一段时间再去读从库。7.【建议】多个并发业务逻辑访问同一块数据（innodb表）时，会在数据库端产生行锁甚至表锁导致并发下降，因此建议更新类SQL尽量基于主键去更新。8.【建议】业务逻辑之间加锁顺序尽量保持一致，否则会导致死锁。9.【建议】对于单表读写比大于10:1的数据行或单个列，可以将热点数据放在缓存里（如mecache或redis），加快访问速度，降低MySQL压力。 SQL编写建议DML语句1.【强制】SELECT语句必须指定具体字段名称，禁止写成“*”因为select 会将不该读的数据也从MySQL里读出来，造成网卡压力。且表字段一旦更新，但model层没有来得及更新的话，系统会报错。*2.【强制】insert语句指定具体字段名称，不要写成insert into t1 values(…)，道理同上。3.【建议】insert into…values(XX),(XX),(XX).. 这里XX的值不要超过5000个。值过多虽然上线很很快，但会引起主从同步延迟。 注意:Oracle没有这种语法 4.【建议】能不排序就不排序。减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。Order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。注意Union 与 Union all的却别,前者需要排序,后者不需要5.【建议】in值列表限制在500以内。例如select… where userid in(….500个以内…)，这么做是为了减少底层扫描，减轻数据库压力从而加速查询。6.【建议】事务里批量更新数据需要控制数量，进行必要的sleep，做到少量多次。7.【强制】事务涉及的表必须全部是innodb表。否则一旦失败不会全部回滚，且易造成主从库同步中断。8.【强制】写入和事务发往主库，实时性要求不高的只读SQL发往从库。9.【强制】除静态表或小表（100行以内），DML语句必须有where条件，且使用索引查找。10.【建议】生产环境禁止使用hint，如sql_no_cache，force index，ignore key，straight join等。因为hint是用来强制SQL按照某个执行计划来执行，但随着数据量变化我们无法保证自己当初的预判是正确的，因此我们要相信MySQL优化器！11.【强制】where条件里等号左右字段类型必须一致，否则无法利用索引。 避免隐士转换造成无法使用索引 12.【建议】SELECT|UPDATE|DELETE|REPLACE要有WHERE子句，且WHERE子句的条件必需使用索引查找。13.【强制】生产数据库中强烈不推荐大表上发生全表扫描，但对于100行以下的静态表可以全表扫描。查询数据量不要超过表行数的25%，否则不会利用索引。14.【强制】WHERE 子句中禁止只使用全模糊的LIKE条件进行查找，必须有其他等值或范围查询条件，否则无法利用索引。15.【建议】索引列不要使用函数或表达式，否则无法利用索引。如where length(name)=’Admin’或where user_id+2=10023。16.【建议】减少使用or语句，可将or语句优化为union，然后在各个where条件上建立索引。如where a=1 or b=2优化为where a=1… union …where b=2, key(a),key(b)17.【建议】分页查询，当limit起点较高时，可先用过滤条件进行过滤。如select a,b,c from t1 limit 10000,20;优化为: Select a,b,c from t1 where id&gt;10000 limit 20; 多表连接1.【强制】禁止跨db的join语句。因为这样可以减少模块间耦合，为数据库拆分奠定坚实基础。2.【强制】禁止在业务的更新类SQL语句中使用join，比如update t1 join t2…3.【建议】不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用join来代替子查询。4.【建议】线上环境，多表join不要超过3个表。5.【建议】多表连接查询推荐使用别名，且SELECT列表中要用别名引用字段，数据库.表格式，如select a from db1.table1 alias1 where …6.【建议】在多表join中，尽量选取结果集较小的表作为驱动表，来join其他表。 事务1.【建议】事务里包含SQL不超过5个（支付业务除外）因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等雪崩问题。2.【建议】事务里更新语句尽量基于主键或unique key，如update … where id=XX;3.【建议】尽量把一些典型外部调用移出事务，如调用webservice，访问文件存储等，从而避免事务过长。4.【建议】对于MySQL主从延迟严格敏感的select语句，请开启事务强制访问主库。 线上禁止使用的SQL语句1.【高危】禁用update|delete t1 … where a=XX limit XX;这种带limit的更新语句。因为会导致主从不一致，导致数据错乱。建议加上order by PK2.【高危】禁止使用关联子查询，如update t1 set … where name in(select name from user where…);效率极其低下。3.【强制】禁用procedure、function、trigger、views、event、外键约束。因为他们消耗数据库资源，降低数据库集群可扩展性。推荐都在程序端实现。4.【强制】禁用insert into …on duplicate key update… 在高并发环境下，会造成主从不一致。5.【强制】禁止关联表更新语句，如update t1,t2 where t1.id=t2.id…","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"数据分布不均匀走HASH JOIN导致的性能问题","slug":"数据分布不均匀走HASH-JOIN导致的性能问题","date":"2017-08-09T14:00:00.000Z","updated":"2017-08-09T07:35:57.000Z","comments":true,"path":"2017/08/09/数据分布不均匀走HASH-JOIN导致的性能问题/","link":"","permalink":"http://fuxkdb.com/2017/08/09/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%8D%E5%9D%87%E5%8C%80%E8%B5%B0HASH-JOIN%E5%AF%BC%E8%87%B4%E7%9A%84%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/","excerpt":"这个案例是JAVA开发说一个存储过程跑的很慢，之后我跑这个过程，然后通过脚本抓出了慢的SQL表大小123tb_user_channel --1Wtb_channel_info --1Wbase_data_login_info 19W 就是这条SQL，跑完要7分钟。base_data_login_info本来是@db_link，但是我在本地建了一个同样的表发现还是7分钟左右，所以排除了可能是由于db_link造成问题的可能性123456select count(distinct a.user_name),count(distinct a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform 看一下这个sql返回多少行，结果秒杀，瞬间就出结果了123456select count(*) from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform45122行之后单独跑123456select count(distinct a.user_name),count( a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform或123456select count( a.user_name),count(distinct a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform都是秒杀单独count distinct user_name 或 invest_id 都很快 ，一起count distinct就很慢了那么这时候其实已经可以通过改写SQL提示性能了，改写如下","text":"这个案例是JAVA开发说一个存储过程跑的很慢，之后我跑这个过程，然后通过脚本抓出了慢的SQL表大小123tb_user_channel --1Wtb_channel_info --1Wbase_data_login_info 19W 就是这条SQL，跑完要7分钟。base_data_login_info本来是@db_link，但是我在本地建了一个同样的表发现还是7分钟左右，所以排除了可能是由于db_link造成问题的可能性123456select count(distinct a.user_name),count(distinct a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform 看一下这个sql返回多少行，结果秒杀，瞬间就出结果了123456select count(*) from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform45122行之后单独跑123456select count(distinct a.user_name),count( a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform或123456select count( a.user_name),count(distinct a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform都是秒杀单独count distinct user_name 或 invest_id 都很快 ，一起count distinct就很慢了那么这时候其实已经可以通过改写SQL提示性能了，改写如下1234567with t1 as (select a.user_name, a.invest_id from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform)查看改写后的执行计划12345678910111213141516171819202122232425262728293031323334353637383940select (select count(distinct user_name) from t1),(select count(distinct invest_id) from t1) from dual;Plan hash value: 3790966246 -------------------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time | Inst |IN-OUT|-------------------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | | | 746 (1)| 00:00:09 | | || 1 | SORT AGGREGATE | | 1 | 27 | | | | | || 2 | VIEW | VM_NWVW_2 | 40660 | 1072K| | 1589 (1)| 00:00:20 | | || 3 | SORT GROUP BY | | 40660 | 1072K| 6752K| 1589 (1)| 00:00:20 | | || 4 | VIEW | | 190K| 5021K| | 878 (1)| 00:00:11 | | || 5 | TABLE ACCESS FULL | SYS_TEMP_0FD9D671E_EB8EA | 190K| 9M| | 878 (1)| 00:00:11 | | || 6 | SORT AGGREGATE | | 1 | 27 | | | | | || 7 | VIEW | VM_NWVW_3 | 41456 | 1093K| | 1593 (1)| 00:00:20 | | || 8 | SORT GROUP BY | | 41456 | 1093K| 6752K| 1593 (1)| 00:00:20 | | || 9 | VIEW | | 190K| 5021K| | 878 (1)| 00:00:11 | | || 10 | TABLE ACCESS FULL | SYS_TEMP_0FD9D671E_EB8EA | 190K| 9M| | 878 (1)| 00:00:11 | | || 11 | TEMP TABLE TRANSFORMATION | | | | | | | | || 12 | LOAD AS SELECT | SYS_TEMP_0FD9D671E_EB8EA | | | | | | | ||* 13 | HASH JOIN RIGHT SEMI | | 190K| 22M| | 744 (1)| 00:00:09 | | || 14 | VIEW | VW_NSO_1 | 11535 | 304K| | 258 (1)| 00:00:04 | | ||* 15 | HASH JOIN | | 11535 | 360K| | 258 (1)| 00:00:04 | | ||* 16 | TABLE ACCESS FULL | TB_USER_CHANNEL | 11535 | 157K| | 19 (0)| 00:00:01 | | || 17 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 11767 | 206K| | 238 (0)| 00:00:03 | | || 18 | REMOTE | BASE_DATA_LOGIN_INFO | 190K| 17M| | 486 (1)| 00:00:06 | AGENT | R-&gt;S || 19 | FAST DUAL | | 1 | | | 2 (0)| 00:00:01 | | |------------------------------------------------------------------------------------------------------------------------------- Predicate Information (identified by operation id):--------------------------------------------------- 13 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_RLAT&quot;) 15 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;B&quot;.&quot;CHANNEL_ID&quot;) 16 - filter(&quot;A&quot;.&quot;USER_ID&quot;=5002) Remote SQL Information (identified by operation id):---------------------------------------------------- 18 - SELECT &quot;USER_NAME&quot;,&quot;INVEST_ID&quot;,&quot;STR_DAY&quot;,&quot;CHANNEL_ID&quot;,&quot;PLATFORM&quot; FROM &quot;BASE_DATA_LOGIN_INFO&quot; &quot;A&quot; WHERE &quot;STR_DAY&quot;&lt;=&#x27;20160304&#x27; AND &quot;STR_DAY&quot;&gt;=&#x27;20160301&#x27; AND &quot;PLATFORM&quot; IS NOT NULL (accessing &#x27;AGENT&#x27; )为了探究性能瓶颈我们继续看慢的执行计划12345678910111213141516171819202122232425262728293031323334select count(distinct a.user_name),count(distinct a.invest_id) from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform Plan hash value: 2367445948 -------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | Inst |IN-OUT|-------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 130 | 754 (2)| 00:00:10 | | || 1 | SORT GROUP BY | | 1 | 130 | | | | ||* 2 | HASH JOIN | | 4067K| 504M| 754 (2)| 00:00:10 | | ||* 3 | HASH JOIN | | 11535 | 360K| 258 (1)| 00:00:04 | | ||* 4 | TABLE ACCESS FULL| TB_USER_CHANNEL | 11535 | 157K| 19 (0)| 00:00:01 | | || 5 | TABLE ACCESS FULL| TB_CHANNEL_INFO | 11767 | 206K| 238 (0)| 00:00:03 | | || 6 | REMOTE | BASE_DATA_LOGIN_INFO | 190K| 17M| 486 (1)| 00:00:06 | AGENT | R-&gt;S |------------------------------------------------------------------------------------------------------------- Predicate Information (identified by operation id):--------------------------------------------------- 2 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_RLAT&quot;) 3 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;B&quot;.&quot;CHANNEL_ID&quot;) 4 - filter(&quot;A&quot;.&quot;USER_ID&quot;=5002) Remote SQL Information (identified by operation id):---------------------------------------------------- 6 - SELECT &quot;USER_NAME&quot;,&quot;INVEST_ID&quot;,&quot;STR_DAY&quot;,&quot;CHANNEL_ID&quot;,&quot;PLATFORM&quot; FROM &quot;BASE_DATA_LOGIN_INFO&quot; &quot;A&quot; WHERE &quot;STR_DAY&quot;&lt;=&#x27;20160304&#x27; AND &quot;STR_DAY&quot;&gt;=&#x27;20160301&#x27; AND &quot;PLATFORM&quot; IS NOT NULL (accessing &#x27;AGENT&#x27; )快的执行计划123456789101112131415161718192021222324252627282930313233343536explain plan forselect count( a.user_name),count(distinct a.invest_id) from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platformPlan hash value: 4282421321 ------------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time | Inst |IN-OUT|------------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 40 | | 2982 (1)| 00:00:36 | | || 1 | SORT AGGREGATE | | 1 | 40 | | | | | || 2 | VIEW | VW_DAG_0 | 41456 | 1619K| | 2982 (1)| 00:00:36 | | || 3 | HASH GROUP BY | | 41456 | 4250K| 20M| 2982 (1)| 00:00:36 | | ||* 4 | HASH JOIN RIGHT SEMI| | 190K| 19M| | 744 (1)| 00:00:09 | | || 5 | VIEW | VW_NSO_1 | 11535 | 80745 | | 258 (1)| 00:00:04 | | ||* 6 | HASH JOIN | | 11535 | 360K| | 258 (1)| 00:00:04 | | ||* 7 | TABLE ACCESS FULL| TB_USER_CHANNEL | 11535 | 157K| | 19 (0)| 00:00:01 | | || 8 | TABLE ACCESS FULL| TB_CHANNEL_INFO | 11767 | 206K| | 238 (0)| 00:00:03 | | || 9 | REMOTE | BASE_DATA_LOGIN_INFO | 190K| 17M| | 486 (1)| 00:00:06 | AGENT | R-&gt;S |------------------------------------------------------------------------------------------------------------------------ Predicate Information (identified by operation id):--------------------------------------------------- 4 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_RLAT&quot;) 6 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;B&quot;.&quot;CHANNEL_ID&quot;) 7 - filter(&quot;A&quot;.&quot;USER_ID&quot;=5002) Remote SQL Information (identified by operation id):---------------------------------------------------- 9 - SELECT &quot;USER_NAME&quot;,&quot;INVEST_ID&quot;,&quot;STR_DAY&quot;,&quot;CHANNEL_ID&quot;,&quot;PLATFORM&quot; FROM &quot;BASE_DATA_LOGIN_INFO&quot; &quot;A&quot; WHERE &quot;STR_DAY&quot;&lt;=&#x27;20160304&#x27; AND &quot;STR_DAY&quot;&gt;=&#x27;20160301&#x27; AND &quot;PLATFORM&quot; IS NOT NULL (accessing &#x27;AGENT&#x27; )注意到快的执行计划用的是HASH JOIN SEMI 而 慢的执行计划用的是 HASH JOIN我又跑慢的SQL，想查看等待时间分析问题，结果等待事件却是 SQLNet message to client。。。做个100461234567891011121314151617181920212223242526272829303132333435363738alter session set events ‘10046 trace name context forever, level 8’;alter session set tracefile_identifier=&#x27;fan&#x27;;alter session set max_dump_file_size=unlimited;alter session set events &#x27;10046 trace name context forever, level 8&#x27;;SQL&gt;.............alter session set events &#x27;10046 trace name context off&#x27;;tkprof channel_ora_4917_fan.trc hehe sys=no waits=yes selectcount(distinct a.user_name),count(distinct a.invest_id) from base_data_login_info a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platformcall count cpu elapsed disk query current rows------- ------ -------- ---------- ---------- ---------- ---------- ----------Parse 1 0.00 0.00 0 0 0 0Execute 1 0.00 0.00 0 0 0 0Fetch 2 1092.09 1236.55 0 3643 0 1------- ------ -------- ---------- ---------- ---------- ---------- ----------total 4 1092.09 1236.56 0 3643 0 1Misses in library cache during parse: 0Optimizer mode: ALL_ROWSParsing user id: 84 Number of plan statistics captured: 1Rows (1st) Rows (avg) Rows (max) Row Source Operation---------- ---------- ---------- --------------------------------------------------- 1 1 1 SORT GROUP BY (cr=3643 pr=0 pw=0 time=1236559678 us) 410996039 410996039 410996039 HASH JOIN (cr=3643 pr=0 pw=0 time=406365130 us cost=1006 size=66968010 card=458685) 11535 11535 11535 HASH JOIN (cr=945 pr=0 pw=0 time=199182 us cost=258 size=369120 card=11535) 11535 11535 11535 TABLE ACCESS FULL TB_USER_CHANNEL (cr=67 pr=0 pw=0 time=21452 us cost=19 size=161490 card=11535) 11771 11771 11771 TABLE ACCESS FULL TB_CHANNEL_INFO (cr=878 pr=0 pw=0 time=30291 us cost=238 size=211806 card=11767) 45122 45122 45122 TABLE ACCESS FULL BASE_DATA_LOGIN_INFO (cr=2698 pr=0 pw=0 time=218144 us cost=747 size=2447922 card=21473)Elapsed times include waiting on following events: Event waited on Times Max. Wait Total Waited ---------------------------------------- Waited ---------- ------------ SQL*Net message to client 2 0.00 0.00 SQL*Net message from client 2 50.71 50.71 注意执行计划第二行 变成了40亿！关联列数据分布19W 表连接列123456789101112131415161718192021222324SQL&gt; select channel_id,count(*) from base_data_login_info group by channel_id order by 2;CHANNEL_ID COUNT(*)-------------------------------------------------- ----------011a1 2003a1 3021a1 3006a1 12024h2 16013a1 19007a1 24012a1 25005a1 27EPT01 36028h2 109008a1 139029a1 841009a1 921014a1 1583000a1 1975a0001 2724004a1 5482001a1 16329026h2 16016220 rows selected.in里的1234567891011121314151617181920212223242526select channel_rlat,count(*) from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002 group by channel_rlat order by 2 desc channel_rlat count(*)026h2 10984024h2 7002h2 6023a2 2007s001022001 1007s001022002 1007s001024007 1007s001024009 1007s001022009 1001s001006 1001s001008 1001s001001001 1001s001001003 1001s001001007 1001s001001014 1007s001018003 1007s001018007 1007s001019005 1007s001019008 1001s001002011 1007s001011003 1007s001034 1007s001023005 1007s001011008 1 HASH JOIN 只适合数据分布均匀的列做关联，而这个关联列数据分布极度不均衡，相当于一个小笛卡尔及，在bucket里找死了* 继续深入，为什么本来是半连接，CBO用了HASH JOIN？单独跑这个是很快的123456select a.user_name,a.invest_id from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform 这样就慢了，因为视图合并了1234567select count(distinct a.user_name),count(distinct a.invest_id) from (select a.user_name,a.invest_id from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform ) aCBO把它改写成了12345select count(distinct a.user_name),count(distinct a.invest_id) from base_data_login_info@agent a,(select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) b where a.channel_id=b.channel_rlat and a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27;半连接本身就可以改写成inner join,但是in里的表要distinct,举个例子123456select * from dept where deptno in (select deptno from emp) DEPTNO DNAME LOC---------- -------------- ------------- 10 ACCOUNTING NEW YORK 20 RESEARCH DALLAS 30 SALES CHICAGO可以改写为12select d.* from dept d inner join (select distinct deptno from emp ) e on d.deptno=e.deptno 否则如果不distinct结果集就会有问题12345678910111213141516171819SQL&gt; select d.* from dept d inner join (select deptno from emp ) e 2 on d.deptno=e.deptno ; DEPTNO DNAME LOC---------- -------------- ------------- 10 ACCOUNTING NEW YORK 10 ACCOUNTING NEW YORK 10 ACCOUNTING NEW YORK 20 RESEARCH DALLAS 20 RESEARCH DALLAS 20 RESEARCH DALLAS 20 RESEARCH DALLAS 20 RESEARCH DALLAS 30 SALES CHICAGO 30 SALES CHICAGO 30 SALES CHICAGO 30 SALES CHICAGO 30 SALES CHICAGO 30 SALES CHICAGO已选择14行。但是巧就巧的是，我这个SQL是 count(distinct a.user_name),count(distinct a.invest_id)所以CBO把它改写成了inner join,还不用把in里面先去重，CBO真TM聪明，可惜聪明反被聪明误。我们这个关联列数据分布极度不均匀那我们不想让CBO改写咋办呢 那么我们不让视图合并用 rownum&gt;01234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677select count(distinct a.user_name),count(distinct a.invest_id) from (select a.user_name,a.invest_id from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform and rownum&gt;0) aPlan hash value: 3295380261 -----------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | Inst |IN-OUT|-----------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 54 | 744 (1)| 00:00:09 | | || 1 | SORT GROUP BY | | 1 | 54 | | | | || 2 | VIEW | | 190K| 9M| 744 (1)| 00:00:09 | | || 3 | COUNT | | | | | | | ||* 4 | FILTER | | | | | | | ||* 5 | HASH JOIN RIGHT SEMI| | 190K| 22M| 744 (1)| 00:00:09 | | || 6 | VIEW | VW_NSO_1 | 11535 | 304K| 258 (1)| 00:00:04 | | ||* 7 | HASH JOIN | | 11535 | 360K| 258 (1)| 00:00:04 | | ||* 8 | TABLE ACCESS FULL| TB_USER_CHANNEL | 11535 | 157K| 19 (0)| 00:00:01 | | || 9 | TABLE ACCESS FULL| TB_CHANNEL_INFO | 11767 | 206K| 238 (0)| 00:00:03 | | || 10 | REMOTE | BASE_DATA_LOGIN_INFO | 190K| 17M| 486 (1)| 00:00:06 | AGENT | R-&gt;S |----------------------------------------------------------------------------------------------------------------- Predicate Information (identified by operation id):--------------------------------------------------- 4 - filter(ROWNUM&gt;0) 5 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_RLAT&quot;) 7 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;B&quot;.&quot;CHANNEL_ID&quot;) 8 - filter(&quot;A&quot;.&quot;USER_ID&quot;=5002) Remote SQL Information (identified by operation id):---------------------------------------------------- 10 - SELECT &quot;USER_NAME&quot;,&quot;INVEST_ID&quot;,&quot;STR_DAY&quot;,&quot;CHANNEL_ID&quot;,&quot;PLATFORM&quot; FROM &quot;BASE_DATA_LOGIN_INFO&quot; &quot;A&quot; WHERE &quot;STR_DAY&quot;&lt;=&#x27;20160304&#x27; AND &quot;STR_DAY&quot;&gt;=&#x27;20160301&#x27; AND &quot;PLATFORM&quot; IS NOT NULL (accessing &#x27;AGENT&#x27; )with t1 as (select /*+ materialize */ a.user_name, a.invest_id from base_data_login_info@agent a where a.str_day &lt;= &#x27;20160304&#x27; and a.str_day &gt;= &#x27;20160301&#x27; and a.channel_id in (select channel_rlat from tb_user_channel a, tb_channel_info b where a.channel_id = b.channel_id and a.user_id = 5002) and a.platform = a.platform)select count(distinct user_name) ,count(distinct invest_id) from t1;Plan hash value: 901326807 -----------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | Inst |IN-OUT|-----------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 54 | 1621 (1)| 00:00:20 | | || 1 | TEMP TABLE TRANSFORMATION | | | | | | | || 2 | LOAD AS SELECT | SYS_TEMP_0FD9D6720_EB8EA | | | | | | ||* 3 | HASH JOIN RIGHT SEMI | | 190K| 22M| 744 (1)| 00:00:09 | | || 4 | VIEW | VW_NSO_1 | 11535 | 304K| 258 (1)| 00:00:04 | | ||* 5 | HASH JOIN | | 11535 | 360K| 258 (1)| 00:00:04 | | ||* 6 | TABLE ACCESS FULL | TB_USER_CHANNEL | 11535 | 157K| 19 (0)| 00:00:01 | | || 7 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 11767 | 206K| 238 (0)| 00:00:03 | | || 8 | REMOTE | BASE_DATA_LOGIN_INFO | 190K| 17M| 486 (1)| 00:00:06 | AGENT | R-&gt;S || 9 | SORT GROUP BY | | 1 | 54 | | | | || 10 | VIEW | | 190K| 9M| 878 (1)| 00:00:11 | | || 11 | TABLE ACCESS FULL | SYS_TEMP_0FD9D6720_EB8EA | 190K| 9M| 878 (1)| 00:00:11 | | |----------------------------------------------------------------------------------------------------------------------- Predicate Information (identified by operation id):--------------------------------------------------- 3 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_RLAT&quot;) 5 - access(&quot;A&quot;.&quot;CHANNEL_ID&quot;=&quot;B&quot;.&quot;CHANNEL_ID&quot;) 6 - filter(&quot;A&quot;.&quot;USER_ID&quot;=5002) Remote SQL Information (identified by operation id):---------------------------------------------------- 8 - SELECT &quot;USER_NAME&quot;,&quot;INVEST_ID&quot;,&quot;STR_DAY&quot;,&quot;CHANNEL_ID&quot;,&quot;PLATFORM&quot; FROM &quot;BASE_DATA_LOGIN_INFO&quot; &quot;A&quot; WHERE &quot;STR_DAY&quot;&lt;=&#x27;20160304&#x27; AND &quot;STR_DAY&quot;&gt;=&#x27;20160301&#x27; AND &quot;PLATFORM&quot; IS NOT NULL (accessing &#x27;AGENT&#x27; ) 再解释一下12345678910111213141516171819202122232425create table emp2 as select * from emp;insert into emp2 select * from emp2;create table emp4 as select * from emp2;SQL&gt; select count(distinct a.job),count(distinct a.ename) from emp2 a where a.deptno in (select deptno from emp4); COUNT(DISTINCTA.JOB) COUNT(DISTINCTA.ENAME)-------------------- ---------------------- 4 11等价 于SQL&gt; select count(distinct a.job),count(distinct a.ename) from emp2 a,emp4 b where a.deptno=b.deptno;COUNT(DISTINCTA.JOB) COUNT(DISTINCTA.ENAME)-------------------- ---------------------- 4 11CBO把它改写成了这个 a.deptno=b.deptno hash joinExecution Plan----------------------------------------------------------Plan hash value: 3097773013----------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time |----------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 39 | 8 (13)| 00:00:01 || 1 | SORT GROUP BY | | 1 | 39 | | ||* 2 | HASH JOIN | | 242 | 9438 | 8 (13)| 00:00:01 || 3 | TABLE ACCESS FULL| EMP2 | 22 | 572 | 4 (0)| 00:00:01 || 4 | TABLE ACCESS FULL| EMP4 | 22 | 286 | 3 (0)| 00:00:01 |----------------------------------------------------------------------------","categories":[],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://fuxkdb.com/tags/Oracle/"},{"name":"SQL Tuning","slug":"SQL-Tuning","permalink":"http://fuxkdb.com/tags/SQL-Tuning/"}]},{"title":"视图合并、hash join连接列数据分布不均匀引发的惨案","slug":"视图合并、hash-join连接列数据分布不均匀引发的惨案","date":"2017-08-09T14:00:00.000Z","updated":"2017-08-09T07:36:58.000Z","comments":true,"path":"2017/08/09/视图合并、hash-join连接列数据分布不均匀引发的惨案/","link":"","permalink":"http://fuxkdb.com/2017/08/09/%E8%A7%86%E5%9B%BE%E5%90%88%E5%B9%B6%E3%80%81hash-join%E8%BF%9E%E6%8E%A5%E5%88%97%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%8D%E5%9D%87%E5%8C%80%E5%BC%95%E5%8F%91%E7%9A%84%E6%83%A8%E6%A1%88/","excerpt":"表大小1234567891011121314151617181920212223242526272829303132333435363738394041424344454647SQL&gt; select count(*) from agent.TB_AGENT_INFO; COUNT(*)---------- 1751SQL&gt; select count(*) from TB_CHANNEL_INFO ; COUNT(*)---------- 1807SQL&gt; select count(*) from TB_USER_CHANNEL; COUNT(*)---------- 7269SQL&gt; select count(*) from OSS_USER_STATION; COUNT(*)---------- 2149SQL&gt; select count(*) from tb_user_zgy ; COUNT(*)---------- 43SQL&gt; select count(*) from act.tb_user_agent_relat; COUNT(*)---------- 29612SQL&gt; select count(*) from agent.base_data_user_info ; COUNT(*)---------- 30005SQL&gt; select count(*) from agent.base_data_invest_info; COUNT(*)---------- 3530163 慢的sql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108select a.city, a.agent_id, a.username, a.real_name, phone, zgy_name, login_count, user_count, count(distinct b.invest_id) user_invested, sum(b.order_amount / 100) invest_amount from (select a.city, a.agent_id, a.username, a.real_name, -- 业主姓名 a.phone, -- 业主手机号 d.real_name zgy_name, -- 所属专管员 count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then c.login_name end) login_count, count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then decode(c.status, 1, c.invest_id, null) end) user_count from (select agent_id, city, username, real_name, phone from agent.TB_AGENT_INFO where agent_id in (SELECT agent_id FROM (SELECT distinct * FROM TB_CHANNEL_INFO t START WITH t.CHANNEL_ID in (select CHANNEL_ID from TB_USER_CHANNEL where USER_ID = 596) CONNECT BY PRIOR t.CHANNEL_ID = t.PARENT_CHANNEL_ID) WHERE agent_id IS NOT NULL)) a left join oss_user_station e on a.agent_id = e.agent_id and e.user_type = 0 left join tb_user_zgy d on e.username = d.username left join act.tb_user_agent_relat c on a.agent_id = c.agent_id group by a.city, a.username, a.real_name, a.phone, d.real_name, a.agent_id) a left join (select invest_id, order_amount, agent_id, str_day from agent.base_data_invest_info where str_day &gt;= &#x27;20150801&#x27; and str_day&lt;=&#x27;20160821&#x27;) b on a.agent_id = b.agent_id group by a.city, a.agent_id, a.username, a.real_name, a.phone, a.zgy_name, a.login_count, a.user_count这个查询可以看成两部分，第一部分一堆小表关联的a和唯一的一个大表再做关联man----------------------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time |----------------------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 55M| 6616M| | 3934K (1)| 13:06:52 || 1 | HASH GROUP BY | | 55M| 6616M| | 3934K (1)| 13:06:52 || 2 | VIEW | VW_DAG_1 | 55M| 6616M| | 3934K (1)| 13:06:52 || 3 | HASH GROUP BY | | 55M| 6301M| 7681M| 3934K (1)| 13:06:52 || 4 | VIEW | VM_NWVW_0 | 55M| 6301M| | 2456K (1)| 08:11:15 || 5 | SORT GROUP BY | | 55M| 10G| 11G| 2456K (1)| 08:11:15 ||* 6 | HASH JOIN RIGHT OUTER | | 55M| 10G| | 21643 (2)| 00:04:20 || 7 | TABLE ACCESS FULL | TB_USER_AGENT_RELAT | 27937 | 1200K| | 102 (0)| 00:00:02 ||* 8 | HASH JOIN OUTER | | 3374K| 511M| | 21392 (1)| 00:04:17 ||* 9 | HASH JOIN SEMI | | 1712 | 188K| | 2007 (1)| 00:00:25 ||* 10 | HASH JOIN RIGHT OUTER | | 1712 | 173K| | 32 (0)| 00:00:01 || 11 | TABLE ACCESS FULL | TB_USER_ZGY | 43 | 903 | | 3 (0)| 00:00:01 ||* 12 | HASH JOIN RIGHT OUTER | | 1712 | 138K| | 29 (0)| 00:00:01 ||* 13 | TABLE ACCESS FULL | OSS_USER_STATION | 1075 | 25800 | | 6 (0)| 00:00:01 || 14 | TABLE ACCESS FULL | TB_AGENT_INFO | 1712 | 98K| | 23 (0)| 00:00:01 || 15 | VIEW | VW_NSO_1 | 16271 | 143K| | 1975 (1)| 00:00:24 ||* 16 | VIEW | | 16271 | 143K| | 1975 (1)| 00:00:24 || 17 | HASH UNIQUE | | 16271 | 8882K| 10M| 1975 (1)| 00:00:24 ||* 18 | CONNECT BY WITHOUT FILTERING (UNIQUE)| | | | | | ||* 19 | HASH JOIN RIGHT SEMI | | 530 | 146K| | 29 (0)| 00:00:01 ||* 20 | TABLE ACCESS FULL | TB_USER_CHANNEL | 600 | 7800 | | 7 (0)| 00:00:01 || 21 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 || 22 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 ||* 23 | TABLE ACCESS FULL | BASE_DATA_INVEST_INFO | 3374K| 148M| | 19375 (1)| 00:03:53 |----------------------------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):--------------------------------------------------- 6 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 8 - access(&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;(+)) 9 - access(&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;) 10 - access(&quot;C&quot;.&quot;USERNAME&quot;=&quot;D&quot;.&quot;USERNAME&quot;(+)) 12 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 13 - filter(&quot;C&quot;.&quot;USER_TYPE&quot;(+)=0) 16 - filter(&quot;AGENT_ID&quot; IS NOT NULL) 18 - access(&quot;T&quot;.&quot;PARENT_CHANNEL_ID&quot;=PRIOR &quot;T&quot;.&quot;CHANNEL_ID&quot;) 19 - access(&quot;T&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_ID&quot;) 20 - filter(&quot;USER_ID&quot;=596) 23 - filter(&quot;STR_DAY&quot;(+)&gt;=&#x27;20150801&#x27; AND &quot;STR_DAY&quot;(+)&lt;=&#x27;20160821&#x27;)","text":"表大小1234567891011121314151617181920212223242526272829303132333435363738394041424344454647SQL&gt; select count(*) from agent.TB_AGENT_INFO; COUNT(*)---------- 1751SQL&gt; select count(*) from TB_CHANNEL_INFO ; COUNT(*)---------- 1807SQL&gt; select count(*) from TB_USER_CHANNEL; COUNT(*)---------- 7269SQL&gt; select count(*) from OSS_USER_STATION; COUNT(*)---------- 2149SQL&gt; select count(*) from tb_user_zgy ; COUNT(*)---------- 43SQL&gt; select count(*) from act.tb_user_agent_relat; COUNT(*)---------- 29612SQL&gt; select count(*) from agent.base_data_user_info ; COUNT(*)---------- 30005SQL&gt; select count(*) from agent.base_data_invest_info; COUNT(*)---------- 3530163 慢的sql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108select a.city, a.agent_id, a.username, a.real_name, phone, zgy_name, login_count, user_count, count(distinct b.invest_id) user_invested, sum(b.order_amount / 100) invest_amount from (select a.city, a.agent_id, a.username, a.real_name, -- 业主姓名 a.phone, -- 业主手机号 d.real_name zgy_name, -- 所属专管员 count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then c.login_name end) login_count, count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then decode(c.status, 1, c.invest_id, null) end) user_count from (select agent_id, city, username, real_name, phone from agent.TB_AGENT_INFO where agent_id in (SELECT agent_id FROM (SELECT distinct * FROM TB_CHANNEL_INFO t START WITH t.CHANNEL_ID in (select CHANNEL_ID from TB_USER_CHANNEL where USER_ID = 596) CONNECT BY PRIOR t.CHANNEL_ID = t.PARENT_CHANNEL_ID) WHERE agent_id IS NOT NULL)) a left join oss_user_station e on a.agent_id = e.agent_id and e.user_type = 0 left join tb_user_zgy d on e.username = d.username left join act.tb_user_agent_relat c on a.agent_id = c.agent_id group by a.city, a.username, a.real_name, a.phone, d.real_name, a.agent_id) a left join (select invest_id, order_amount, agent_id, str_day from agent.base_data_invest_info where str_day &gt;= &#x27;20150801&#x27; and str_day&lt;=&#x27;20160821&#x27;) b on a.agent_id = b.agent_id group by a.city, a.agent_id, a.username, a.real_name, a.phone, a.zgy_name, a.login_count, a.user_count这个查询可以看成两部分，第一部分一堆小表关联的a和唯一的一个大表再做关联man----------------------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time |----------------------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 55M| 6616M| | 3934K (1)| 13:06:52 || 1 | HASH GROUP BY | | 55M| 6616M| | 3934K (1)| 13:06:52 || 2 | VIEW | VW_DAG_1 | 55M| 6616M| | 3934K (1)| 13:06:52 || 3 | HASH GROUP BY | | 55M| 6301M| 7681M| 3934K (1)| 13:06:52 || 4 | VIEW | VM_NWVW_0 | 55M| 6301M| | 2456K (1)| 08:11:15 || 5 | SORT GROUP BY | | 55M| 10G| 11G| 2456K (1)| 08:11:15 ||* 6 | HASH JOIN RIGHT OUTER | | 55M| 10G| | 21643 (2)| 00:04:20 || 7 | TABLE ACCESS FULL | TB_USER_AGENT_RELAT | 27937 | 1200K| | 102 (0)| 00:00:02 ||* 8 | HASH JOIN OUTER | | 3374K| 511M| | 21392 (1)| 00:04:17 ||* 9 | HASH JOIN SEMI | | 1712 | 188K| | 2007 (1)| 00:00:25 ||* 10 | HASH JOIN RIGHT OUTER | | 1712 | 173K| | 32 (0)| 00:00:01 || 11 | TABLE ACCESS FULL | TB_USER_ZGY | 43 | 903 | | 3 (0)| 00:00:01 ||* 12 | HASH JOIN RIGHT OUTER | | 1712 | 138K| | 29 (0)| 00:00:01 ||* 13 | TABLE ACCESS FULL | OSS_USER_STATION | 1075 | 25800 | | 6 (0)| 00:00:01 || 14 | TABLE ACCESS FULL | TB_AGENT_INFO | 1712 | 98K| | 23 (0)| 00:00:01 || 15 | VIEW | VW_NSO_1 | 16271 | 143K| | 1975 (1)| 00:00:24 ||* 16 | VIEW | | 16271 | 143K| | 1975 (1)| 00:00:24 || 17 | HASH UNIQUE | | 16271 | 8882K| 10M| 1975 (1)| 00:00:24 ||* 18 | CONNECT BY WITHOUT FILTERING (UNIQUE)| | | | | | ||* 19 | HASH JOIN RIGHT SEMI | | 530 | 146K| | 29 (0)| 00:00:01 ||* 20 | TABLE ACCESS FULL | TB_USER_CHANNEL | 600 | 7800 | | 7 (0)| 00:00:01 || 21 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 || 22 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 ||* 23 | TABLE ACCESS FULL | BASE_DATA_INVEST_INFO | 3374K| 148M| | 19375 (1)| 00:03:53 |----------------------------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):--------------------------------------------------- 6 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 8 - access(&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;(+)) 9 - access(&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;) 10 - access(&quot;C&quot;.&quot;USERNAME&quot;=&quot;D&quot;.&quot;USERNAME&quot;(+)) 12 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 13 - filter(&quot;C&quot;.&quot;USER_TYPE&quot;(+)=0) 16 - filter(&quot;AGENT_ID&quot; IS NOT NULL) 18 - access(&quot;T&quot;.&quot;PARENT_CHANNEL_ID&quot;=PRIOR &quot;T&quot;.&quot;CHANNEL_ID&quot;) 19 - access(&quot;T&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_ID&quot;) 20 - filter(&quot;USER_ID&quot;=596) 23 - filter(&quot;STR_DAY&quot;(+)&gt;=&#x27;20150801&#x27; AND &quot;STR_DAY&quot;(+)&lt;=&#x27;20160821&#x27;)尝试单独跑 a,很快12345678910111213141516171819202122232425262728293031323334353637383940(select a.city, a.agent_id, a.username, a.real_name, -- 业主姓名 a.phone, -- 业主手机号 d.real_name zgy_name, -- 所属专管员 count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then c.login_name end) login_count, count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then decode(c.status, 1, c.invest_id, null) end) user_count from (select agent_id, city, username, real_name, phone from agent.TB_AGENT_INFO where agent_id in (SELECT agent_id FROM (SELECT distinct * FROM TB_CHANNEL_INFO t START WITH t.CHANNEL_ID in (select CHANNEL_ID from TB_USER_CHANNEL where USER_ID = 596) CONNECT BY PRIOR t.CHANNEL_ID = t.PARENT_CHANNEL_ID) WHERE agent_id IS NOT NULL)) a left join oss_user_station e on a.agent_id = e.agent_id and e.user_type = 0 left join tb_user_zgy d on e.username = d.username left join act.tb_user_agent_relat c on a.agent_id = c.agent_id group by a.city, a.username, a.real_name, a.phone, d.real_name, a.agent_id) a单独跑a很快，和b合在一起就很慢，那么怀疑是由于视图合并，导致了a内部的表提前去和b关联，引发了性能问题。尝试禁止视图合并可以使用rownum&gt;0,或no_merge hint 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110select a.city, a.agent_id, a.username, a.real_name, phone, zgy_name, login_count, user_count, count(distinct b.invest_id) user_invested, sum(b.order_amount / 100) invest_amount from (select * from (select a.city, a.agent_id, a.username, a.real_name, -- 业主姓名 a.phone, -- 业主手机号 d.real_name zgy_name, -- 所属专管员 count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then c.login_name end) login_count, count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then decode(c.status, 1, c.invest_id, null) end) user_count from (select agent_id, city, username, real_name, phone from agent.TB_AGENT_INFO where agent_id in (SELECT agent_id FROM (SELECT distinct * FROM TB_CHANNEL_INFO t START WITH t.CHANNEL_ID in (select CHANNEL_ID from TB_USER_CHANNEL where USER_ID = 596) CONNECT BY PRIOR t.CHANNEL_ID = t.PARENT_CHANNEL_ID) WHERE agent_id IS NOT NULL)) a left join oss_user_station e on a.agent_id = e.agent_id and e.user_type = 0 left join tb_user_zgy d on e.username = d.username left join act.tb_user_agent_relat c on a.agent_id = c.agent_id group by a.city, a.username, a.real_name, a.phone, d.real_name, a.agent_id) where rownum&gt;0)a left join (select invest_id, order_amount, agent_id, str_day from agent.base_data_invest_info where str_day &gt;= &#x27;20150801&#x27; and str_day&lt;=&#x27;20160821&#x27;) b on a.agent_id = b.agent_id group by a.city, a.agent_id, a.username, a.real_name, a.phone, a.zgy_name, a.login_count, a.user_countkuai-----------------------------------------------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time |-----------------------------------------------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 823M| 96G| | 23M (1)| 78:59:52 || 1 | HASH GROUP BY | | 823M| 96G| | 23M (1)| 78:59:52 || 2 | VIEW | VW_DAG_0 | 823M| 96G| | 23M (1)| 78:59:52 || 3 | HASH GROUP BY | | 823M| 98G| 112G| 23M (1)| 78:59:52 ||* 4 | HASH JOIN OUTER | | 823M| 98G| 26M| 41358 (6)| 00:08:17 || 5 | VIEW | | 259K| 23M| | 11090 (1)| 00:02:14 || 6 | COUNT | | | | | | ||* 7 | FILTER | | | | | | || 8 | VIEW | | 259K| 23M| | 11090 (1)| 00:02:14 || 9 | SORT GROUP BY | | 259K| 38M| 41M| 11090 (1)| 00:02:14 ||* 10 | HASH JOIN | | 259K| 38M| | 2111 (1)| 00:00:26 ||* 11 | VIEW | | 16271 | 143K| | 1975 (1)| 00:00:24 || 12 | HASH UNIQUE | | 16271 | 8882K| 10M| 1975 (1)| 00:00:24 ||* 13 | CONNECT BY WITHOUT FILTERING (UNIQUE)| | | | | | ||* 14 | HASH JOIN RIGHT SEMI | | 530 | 146K| | 29 (0)| 00:00:01 ||* 15 | TABLE ACCESS FULL | TB_USER_CHANNEL | 600 | 7800 | | 7 (0)| 00:00:01 || 16 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 || 17 | TABLE ACCESS FULL | TB_CHANNEL_INFO | 1807 | 476K| | 22 (0)| 00:00:01 ||* 18 | HASH JOIN OUTER | | 27937 | 4037K| | 134 (0)| 00:00:02 ||* 19 | HASH JOIN RIGHT OUTER | | 1712 | 173K| | 32 (0)| 00:00:01 || 20 | TABLE ACCESS FULL | TB_USER_ZGY | 43 | 903 | | 3 (0)| 00:00:01 ||* 21 | HASH JOIN RIGHT OUTER | | 1712 | 138K| | 29 (0)| 00:00:01 ||* 22 | TABLE ACCESS FULL | OSS_USER_STATION | 1075 | 25800 | | 6 (0)| 00:00:01 || 23 | TABLE ACCESS FULL | TB_AGENT_INFO | 1712 | 98K| | 23 (0)| 00:00:01 || 24 | TABLE ACCESS FULL | TB_USER_AGENT_RELAT | 27937 | 1200K| | 102 (0)| 00:00:02 ||* 25 | TABLE ACCESS FULL | BASE_DATA_INVEST_INFO | 3374K| 109M| | 19375 (1)| 00:03:53 |-----------------------------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):--------------------------------------------------- 4 - access(&quot;A&quot;.&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;(+)) 7 - filter(ROWNUM&gt;0) 10 - access(&quot;AGENT_ID&quot;=&quot;AGENT_ID&quot;) 11 - filter(&quot;AGENT_ID&quot; IS NOT NULL) 13 - access(&quot;T&quot;.&quot;PARENT_CHANNEL_ID&quot;=PRIOR &quot;T&quot;.&quot;CHANNEL_ID&quot;) 14 - access(&quot;T&quot;.&quot;CHANNEL_ID&quot;=&quot;CHANNEL_ID&quot;) 15 - filter(&quot;USER_ID&quot;=596) 18 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 19 - access(&quot;C&quot;.&quot;USERNAME&quot;=&quot;D&quot;.&quot;USERNAME&quot;(+)) 21 - access(&quot;AGENT_ID&quot;=&quot;C&quot;.&quot;AGENT_ID&quot;(+)) 22 - filter(&quot;C&quot;.&quot;USER_TYPE&quot;(+)=0) 25 - filter(&quot;STR_DAY&quot;(+)&gt;=&#x27;20150801&#x27; AND &quot;STR_DAY&quot;(+)&lt;=&#x27;20160821&#x27;) 用no_merge hint禁止视图合并也可以123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263select a.city, a.agent_id, a.username, a.real_name, phone, zgy_name, login_count, user_count, count(distinct b.invest_id) user_invested, sum(b.order_amount / 100) invest_amount from (select /*+ no_merge */ a.city, a.agent_id, a.username, a.real_name, -- 业主姓名 a.phone, -- 业主手机号 d.real_name zgy_name, -- 所属专管员 count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then c.login_name end) login_count, count(distinct case when c.str_day &lt;= &#x27;20160821&#x27; then decode(c.status, 1, c.invest_id, null) end) user_count from (select /*+ qb_name(sb) */ agent_id, city, username, real_name, phone from agent.TB_AGENT_INFO where agent_id in (SELECT agent_id FROM (SELECT distinct * FROM TB_CHANNEL_INFO t START WITH t.CHANNEL_ID in (select CHANNEL_ID from TB_USER_CHANNEL where USER_ID = 596) CONNECT BY PRIOR t.CHANNEL_ID = t.PARENT_CHANNEL_ID) WHERE agent_id IS NOT NULL)) a left join oss_user_station e on a.agent_id = e.agent_id and e.user_type = 0 left join tb_user_zgy d on e.username = d.username left join (select * from act.tb_user_agent_relat c) c on a.agent_id = c.agent_id group by a.city, a.username, a.real_name, a.phone, d.real_name, a.agent_id) a left join (select invest_id, order_amount, agent_id, str_day from agent.base_data_invest_info where str_day &gt;= &#x27;20150801&#x27; and str_day&lt;=&#x27;20160821&#x27;) b on a.agent_id = b.agent_id group by a.city, a.agent_id, a.username, a.real_name, a.phone, a.zgy_name, a.login_count, a.user_count 至此sql从一个小时都跑不完，到最后两秒跑完，工作已经完成，但是单从慢的执行计划中并没有看出什么问题。有聚合函数group by走hash没有错，虽然有全表扫描带*但是要么过滤性太差，要么不是性能瓶颈。那为什么总共300多w就跑不完了呢 慢的执行计划做一个10046123456789101112131415161718192021222324252627Number of plan statistics captured: 1Rows (1st) Rows (avg) Rows (max) Row Source Operation---------- ---------- ---------- --------------------------------------------------- 0 0 0 HASH GROUP BY (cr=0 pr=0 pw=0 time=278 us cost=3934270 size=6937507584 card=55059584) 0 0 0 VIEW VW_DAG_1 (cr=0 pr=0 pw=0 time=111 us cost=3934270 size=6937507584 card=55059584) 0 0 0 HASH GROUP BY (cr=0 pr=0 pw=0 time=108 us cost=3934270 size=6607150080 card=55059584) 0 0 0 VIEW VM_NWVW_0 (cr=0 pr=0 pw=0 time=32 us cost=2456206 size=6607150080 card=55059584) 0 0 0 SORT GROUP BY (cr=0 pr=0 pw=0 time=31 us cost=2456206 size=11177095552 card=55059584) 148234852 148234852 148234852 HASH JOIN RIGHT OUTER (cr=34882 pr=0 pw=0 time=34098445 us cost=21643 size=11177095552 card=55059584) 29651 29651 29651 TABLE ACCESS FULL TB_USER_AGENT_RELAT (cr=332 pr=0 pw=0 time=8201 us cost=102 size=1229228 card=27937) 703556 703556 703556 HASH JOIN OUTER (cr=34550 pr=0 pw=0 time=1518631 us cost=21392 size=536480628 card=3374092) 612 612 612 HASH JOIN SEMI (cr=272 pr=0 pw=0 time=31359 us cost=2007 size=193456 card=1712) 1751 1751 1751 HASH JOIN RIGHT OUTER (cr=100 pr=0 pw=0 time=11404 us cost=32 size=178048 card=1712) 43 43 43 TABLE ACCESS FULL TB_USER_ZGY (cr=2 pr=0 pw=0 time=103 us cost=3 size=903 card=43) 1751 1751 1751 HASH JOIN RIGHT OUTER (cr=98 pr=0 pw=0 time=6664 us cost=29 size=142096 card=1712) 1312 1312 1312 TABLE ACCESS FULL OSS_USER_STATION (cr=15 pr=0 pw=0 time=420 us cost=6 size=25800 card=1075) 1751 1751 1751 TABLE ACCESS FULL TB_AGENT_INFO (cr=83 pr=0 pw=0 time=1804 us cost=23 size=101008 card=1712) 612 612 612 VIEW VW_NSO_1 (cr=172 pr=0 pw=0 time=19720 us cost=1975 size=146439 card=16271) 612 612 612 VIEW (cr=172 pr=0 pw=0 time=19351 us cost=1975 size=146439 card=16271) 613 613 613 HASH UNIQUE (cr=172 pr=0 pw=0 time=19224 us cost=1975 size=9095489 card=16271) 1215 1215 1215 CONNECT BY WITHOUT FILTERING (UNIQUE) (cr=172 pr=0 pw=0 time=16687 us) 603 603 603 HASH JOIN RIGHT SEMI (cr=97 pr=0 pw=0 time=4922 us cost=29 size=149990 card=530) 603 603 603 TABLE ACCESS FULL TB_USER_CHANNEL (cr=22 pr=0 pw=0 time=550 us cost=7 size=7800 card=600) 1807 1807 1807 TABLE ACCESS FULL TB_CHANNEL_INFO (cr=75 pr=0 pw=0 time=1615 us cost=22 size=487890 card=1807) 1807 1807 1807 TABLE ACCESS FULL TB_CHANNEL_INFO (cr=75 pr=0 pw=0 time=1133 us cost=22 size=487890 card=1807) 1631878 1631878 1631878 TABLE ACCESS FULL BASE_DATA_INVEST_INFO (cr=34278 pr=0 pw=0 time=950767 us cost=19375 size=155208232 card=3374092)id 6 1亿4千多万，一个多小时也没跑出来并且temp撑爆了第 43 行出现错误:ORA-01652: 无法通过 128 (在表空间 TEMP 中) 扩展 temp 段一亿四千多万，b表才300万，sql group by之前也不过一百多万的结果 根据 6 -access(“AGENT_ID”=”C”.”AGENT_ID”(+)) 查看c和b表agent_id数据分布1select agent_id,count(*) from act.tb_user_agent_relat group by agent_id order by 2 desc 最多的6827行，最少的1行1select agent_id,count(*) from agent.base_data_invest_info group by agent_id order by 2 desc 最多50w，最少1行又一次进了hash join链接列数据分布不均匀的坑，hash join只适合数据分布均匀的列做链接条件 做个oradebug short_stack12345678910111213141516171819202122232425262728293031323334SQL&gt; select unique sid from v$mystat; SID---------- 1132SQL&gt; select p.spid from v$process p ,v$session s where s.paddr=p.addr and s.sid=1132;SPID------------------------------------------------28539 oradebug setospid 28539SQL&gt; oradebug short_stackksedsts()+465&lt;-ksdxfstk()+32&lt;-ksdxcb()+1927&lt;-sspuser()+112&lt;-__sighandler()&lt;-io_submit()+7&lt;-skgfqio()+1275&lt;-ksfd_skgfqio()+894&lt;-ksfdgo()+423&lt;-ksfdaio()+2290&lt;-kcflbi()+906&lt;-kcbldio()+3104&lt;-kcblsltio()+530&lt;-stsIssueWrite()+118&lt;-stsGetBlock()+442&lt;-sdbinb()+135&lt;-sdbput()+1042&lt;-smbwrt()+247&lt;-smbput()+2503&lt;-sorput()+93&lt;-qesaEvaAndPutDistAggOpns()+590&lt;-qergsRowP()+430&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-qerhjGenProbeHashTable()+718&lt;-kdstf11011010000km()+673&lt;-kdsttgr()+153241&lt;-qertbFetch()+2455&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-qergsFetch()+757&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-opifch2()+2766&lt;-kpoal8()+2833&lt;-opiodr()+917&lt;-ttcpip()+2183&lt;-opitsk()+1710&lt;-opiino()+969&lt;-opiodr()+917&lt;-opidrv()+570&lt;-sou2o()+103&lt;-opimai_real()+133&lt;-ssthrdmain()+265&lt;-main()+201&lt;-__libc_start_main()+244SQL&gt; SQL&gt; SQL&gt; SQL&gt; SQL&gt; oradebug short_stackksedsts()+465&lt;-ksdxfstk()+32&lt;-ksdxcb()+1927&lt;-sspuser()+112&lt;-__sighandler()&lt;-io_submit()+7&lt;-skgfqio()+1275&lt;-ksfd_skgfqio()+894&lt;-ksfdgo()+423&lt;-ksfdaio()+2290&lt;-kcflbi()+906&lt;-kcbldio()+3104&lt;-kcblsltio()+530&lt;-stsIssueWrite()+118&lt;-stsGetBlock()+442&lt;-sdbinb()+135&lt;-sdbput()+1042&lt;-smbwrt()+247&lt;-smbput()+2503&lt;-sorput()+93&lt;-qesaEvaAndPutDistAggOpns()+590&lt;-qergsRowP()+430&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-kdstf11011010000km()+673&lt;-kdsttgr()+153241&lt;-qertbFetch()+2455&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-qergsFetch()+757&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-opifch2()+2766&lt;-kpoal8()+2833&lt;-opiodr()+917&lt;-ttcpip()+2183&lt;-opitsk()+1710&lt;-opiino()+969&lt;-opiodr()+917&lt;-opidrv()+570&lt;-sou2o()+103&lt;-opimai_real()+133&lt;-ssthrdmain()+265&lt;-main()+201&lt;-__libc_start_main()+244SQL&gt; SQL&gt; SQL&gt; SQL&gt; SQL&gt; oradebug short_stackksedsts()+465&lt;-ksdxfstk()+32&lt;-ksdxcb()+1927&lt;-sspuser()+112&lt;-__sighandler()&lt;-qergsRowP()+2161&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-kdstf11011010000km()+673&lt;-kdsttgr()+153241&lt;-qertbFetch()+2455&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-qergsFetch()+757&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-opifch2()+2766&lt;-kpoal8()+2833&lt;-opiodr()+917&lt;-ttcpip()+2183&lt;-opitsk()+1710&lt;-opiino()+969&lt;-opiodr()+917&lt;-opidrv()+570&lt;-sou2o()+103&lt;-opimai_real()+133&lt;-ssthrdmain()+265&lt;-main()+201&lt;-__libc_start_main()+244SQL&gt; oradebug short_stackksedsts()+465&lt;-ksdxfstk()+32&lt;-ksdxcb()+1927&lt;-sspuser()+112&lt;-__sighandler()&lt;-lmebco()+63&lt;-qesaSimpleCompare()+73&lt;-smbput()+913&lt;-sorput()+93&lt;-qergsRowP()+1067&lt;-qerhjWalkHashBucket()+397&lt;-qerhjGenProbeHashTable()+1571&lt;-qerhjGenProbeHashTable()+718&lt;-kdstf11011010000km()+673&lt;-kdsttgr()+153241&lt;-qertbFetch()+2455&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-rwsfcd()+103&lt;-qerhjFetch()+1661&lt;-qergsFetch()+757&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-qervwFetch()+139&lt;-qerghFetch()+315&lt;-opifch2()+2766&lt;-kpoal8()+2833&lt;-opiodr()+917&lt;-ttcpip()+2183&lt;-opitsk()+1710&lt;-opiino()+969&lt;-opiodr()+917&lt;-opidrv()+570&lt;-sou2o()+103&lt;-opimai_real()+133&lt;-ssthrdmain()+265&lt;-main()+201&lt;-__libc_start_main()+244可以看到qerhjWalkHashBucketqerhjWalkHashBucket就表示在做hash join的过程中需要遍历hash bucket中的数据，当链接列数据分布不均，某些值特别多时，遍历其hash bucket的成本也就非常高，如果pga放不下了，就会放到temp进行磁盘io，这就是性能瓶颈的原因，这个例子把30g的temp表空间都撑爆了，可见hash bucket有多大！ 做个SQL MONITOR，也可以看出，瓶颈在id 6。如果做一个sql rpt也可以发现sql执行过程中的每妙逻辑读实际并不高，因为时间都花在了遍历hash bucket中","categories":[],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://fuxkdb.com/tags/Oracle/"},{"name":"SQL Tuning","slug":"SQL-Tuning","permalink":"http://fuxkdb.com/tags/SQL-Tuning/"}]},{"title":"抓跑得慢的SQL，查看正在跑的sql的等待事件","slug":"抓跑得慢的SQL，查看正在跑的sql的等待事件","date":"2017-08-09T13:00:00.000Z","updated":"2017-08-09T07:31:22.000Z","comments":true,"path":"2017/08/09/抓跑得慢的SQL，查看正在跑的sql的等待事件/","link":"","permalink":"http://fuxkdb.com/2017/08/09/%E6%8A%93%E8%B7%91%E5%BE%97%E6%85%A2%E7%9A%84SQL%EF%BC%8C%E6%9F%A5%E7%9C%8B%E6%AD%A3%E5%9C%A8%E8%B7%91%E7%9A%84sql%E7%9A%84%E7%AD%89%E5%BE%85%E4%BA%8B%E4%BB%B6/","excerpt":"","text":"12345678910111213141516171819202122232425select (sysdate-a.logon_time)*24*60 minutes, a.username, a.BLOCKING_INSTANCE, a.BLOCKING_SESSION, a.program, a.machine, a.osuser, a.status, a.sid, a.serial#, a.event, a.p1, a.p2, a.p3, a.sql_id, a.sql_child_number, b.sql_text from v$session a, v$sql b where a.sql_address = b.address and a.sql_hash_value = b.hash_value and a.sql_child_number=b.child_number and a.event not in (&#x27;SQL*Net message from client&#x27;,&#x27;Space Manager: slave idle wait&#x27;) -- and a.username like &#x27;%USERNAME%&#x27; order by 1 desc; 在查执行计划1Select * from table(dbms_xplan.display_cursor(&#x27;91prd0dh0bs8d&#x27;,0,&#x27;ALL&#x27;));","categories":[],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://fuxkdb.com/tags/Oracle/"},{"name":"SQL Script","slug":"SQL-Script","permalink":"http://fuxkdb.com/tags/SQL-Script/"}]},{"title":"Hexo博客配置","slug":"Hexo博客配置","date":"2017-08-05T08:40:53.000Z","updated":"2017-08-05T09:30:32.000Z","comments":true,"path":"2017/08/05/Hexo博客配置/","link":"","permalink":"http://fuxkdb.com/2017/08/05/Hexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE/","excerpt":"Hexo博客配置基于hexo-theme-hiker主题 先看myblog下的_config.yml完整配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Fan()subtitle:description:author: a dba&#x27;s bloglanguage: zh-CNtimezone: Asia/Shanghai# URL## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;url: http://yoursite.comroot: /Fandb.github.iopermalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: true tab_replace:# Home page setting# path: Root path for your blogs index page. (default = &#x27;&#x27;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &#x27;&#x27; per_page: 6 order_by: -date# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hiker# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: github: https://github.com/Fanduzi/Fandb.github.io.git coding: https://git.coding.net/Fandb/Fandb.blog.git branch: master Note 冒号后面要空一格","text":"Hexo博客配置基于hexo-theme-hiker主题 先看myblog下的_config.yml完整配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Fan()subtitle:description:author: a dba&#x27;s bloglanguage: zh-CNtimezone: Asia/Shanghai# URL## If your site is put in a subdirectory, set url as &#x27;http://yoursite.com/child&#x27; and root as &#x27;/child/&#x27;url: http://yoursite.comroot: /Fandb.github.iopermalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: true tab_replace:# Home page setting# path: Root path for your blogs index page. (default = &#x27;&#x27;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &#x27;&#x27; per_page: 6 order_by: -date# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hiker# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: github: https://github.com/Fanduzi/Fandb.github.io.git coding: https://git.coding.net/Fandb/Fandb.blog.git branch: master Note 冒号后面要空一格 解释部分区块设置site1234567# Sitetitle: Fan()subtitle:description:author: a dba&#x27;s bloglanguage: zh-CNtimezone: Asia/Shanghai 此处控制 language设置为zh-CN,否则会产生问题,比如会显示成法文 timezone: Asia/Shanghai 没啥可解释的了 writing123456789101112131415# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: true tab_replace: 这里控制代码块的显示,唯一要注意的是auto_detect,默认是false,不会检测语言代码,从而根据不同的语言产生不同的代码高亮 Home page setting12345678# Home page setting# path: Root path for your blogs index page. (default = &#x27;&#x27;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &#x27;&#x27; per_page: 6 order_by: -date per_page: 6 表示一页显示6篇文章 Extensions1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hiker theme: hiker 选择你想要的themes文件夹下的主题,你可以在themes文件夹下下载多个主题,然后在配置文件中选择一个使用 12TiM@TiMdeMacBook-Pro  ~/Documents/myblog/themes  lshiker landscape 主题配置文件设置先看完整配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# ---------------------------------------------------------------# Site Information Settings# ---------------------------------------------------------------# Header Menumenu: Home: / Archives: archives #Categories: /categories Tags: /tags About: /aboutrss: /atom.xmlsince: 2013# Set default keywords (Use a comma to separate)keywords: &quot;&quot;# Put your favicon.ico or avatar.jpg into `hexo-site/themes/hiker/source/` directory.avatar: enable: true border: true width: 124 height: 124 top: 0 url: css/images/mylogo.jpeg# Homepage# eg. home_background_image: [css/images/home-bg.jpg, http://t.cn/RMbvEza]home_background_image: enable: true rolling: true url: [css/images/1.jpg, css/images/2.jpg, css/images/3.jpg, css/images/4.jpg]home_logo_image: enable: false border: false url: css/images/homelogo.jpg# AboutPage backgroundabout_big_image: css/images/pose.jpg# Archive paginationarchive_pagination: true# Post Article&#x27;s Contentpost_catalog: enable: true# Contentfancybox: true# Sidebarsidebar: rightwidgets:- social- category- tag- tagcloud- archive- recent_posts# Social Links# Key is the name of FontAwsome icon.# Value is the target link (E.g. GitHub: https://github.com/iTimeTraveler)social: Github: https://github.com/Fanduzi Weibo: http://weibo.com/1278787855/profile?topnav=1&amp;wvr=6 Twitter: https://twitter.com/FanAshic Facebook: Google-plus: Instagram: Pinterest: Flickr: email: 18501341937@163.com# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false# comment ShortName, you can choose only ONE to display.gentie_productKey: #your-gentie-product-keyduoshuo_shortname:disqus_shortname:livere_shortname: MTAyMC8yOTQ4MS82MDQ5uyan_uid:wumii:# Code Highlight theme# Available value:# default | normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: default# Article theme color# Available value:# random | orange | blue | red | green | blacktheme_color: random# display widgets at the bottom of index pages (pagination == 2)index_widgets: - category - tagcloud - archive# widget behaviorarchive_type: &#x27;monthly&#x27;show_count: true# Google Webmaster tools verification setting# See: https://www.google.com/webmasters/google_site_verification:baidu_site_verification:qihu_site_verification:# Miscellaneousgoogle_analytics:gauges_analytics:baidu_analytics:tencent_analytics:twitter:google_plus:fb_admins:fb_app_id:# Facebook SDK Support.# https://github.com/iissnan/hexo-theme-next/pull/410facebook_sdk: enable: false app_id: #&lt;app_id&gt; fb_admin: #&lt;user_id&gt; like_button: #true webmaster: #true# CNZZ countcnzz_siteid: 1260716016# busuanzi count# http://busuanzi.ibruce.info/show_busuanzi_view_counts: true# donation buttondonate: enable: true message: &#x27;如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!&#x27; wechatImage: https://raw.githubusercontent.com/Fanduzi/Fandb.github.io/master/css/images/WechatIMG65.jpeg alipayImage: https://raw.githubusercontent.com/Fanduzi/Fandb.github.io/master/css/images/WechatIMG64.jpeg Header menu12345678# Header Menumenu: Home: / Archives: archives #Categories: /categories Tags: /tags About: /aboutrss: /atom.xml 注释掉的不会在菜单中显示 Tags,About参考http://theme-next.iissnan.com/theme-settings.html#tags-page我的配置与效果12345678910111213141516171819 TiM@TiMdeMacBook-Pro  ~/Documents/myblog/source/tags  more index.md---title: Tagclouddate: 2017-08-04 23:06:48type: &quot;tags&quot;---# [MySQL](https://fanduzi.github.io/Fandb.github.io/tags/MySQL/)- [PMM](https://fanduzi.github.io/Fandb.github.io/tags/PMM/)- [MHA](https://fanduzi.github.io/Fandb.github.io/tags/MHA/)- [Sysbench](https://fanduzi.github.io/Fandb.github.io/tags/Sysbench/) TiM@TiMdeMacBook-Pro  ~/Documents/myblog/source/tags  cd .. TiM@TiMdeMacBook-Pro  ~/Documents/myblog/source  cd about TiM@TiMdeMacBook-Pro  ~/Documents/myblog/source/about  more index.md---title: aboutdate: 2017-08-04 22:40:02---About Me一个菜&lt;U+1F413&gt;dba 自定义title图片12345678# Put your favicon.ico or avatar.jpg into `hexo-site/themes/hiker/source/` directory.avatar: enable: true border: true width: 124 height: 124 top: 0 url: css/images/mylogo.jpeg url: css/images/mylogo.jpeg 控制 Homepage123456# Homepage# eg. home_background_image: [css/images/home-bg.jpg, http://t.cn/RMbvEza]home_background_image: enable: true rolling: true url: [css/images/1.jpg, css/images/2.jpg, css/images/3.jpg, css/images/4.jpg] url: [css/images/1.jpg, css/images/2.jpg, css/images/3.jpg, css/images/4.jpg] 控制这里的图片滚动 你可以将自己喜欢的图片放到 1myblog/themes/hiker/source/css/images 搜索12345# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false 要安装12cd myblognpm install hexo-generator-json-content save然后就可以看到搜索按钮(博客内文章搜索) 其他的看我的配置就大概能看懂了,不解释了 需要注意的文章格式默认文章是没有标题的,并且是全部显示没有read more按钮 要添加文章标题需要在文章开头添加 12345---title: 你的文章标题datte: 2017-08-05 10:10:10tags: [你的标签,你的标签2]--- 而read more则是在文章你想要开始隐藏的位置前添加 1&lt;!-- more --&gt; 文章中的图片建议我仍然放到了 1myblog/themes/hiker/source/css/images 然后hexo d上传代码后,去找一个图片地址,类似于这样 1![img](https://raw.githubusercontent.com/Fanduzi/Fandb.github.io/master/css/images/hexo1.jpg)","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://fuxkdb.com/tags/Hexo/"}]},{"title":"使用Hexo + github快速搭建个人博客","slug":"使用Hexo-+-github快速搭建个人博客","date":"2017-08-05T07:55:12.000Z","updated":"2017-08-05T08:36:37.000Z","comments":true,"path":"2017/08/05/使用Hexo-+-github快速搭建个人博客/","link":"","permalink":"http://fuxkdb.com/2017/08/05/%E4%BD%BF%E7%94%A8Hexo-+-github%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","excerpt":"使用Hexo + github快速搭建个人博客安装依赖软件安装git1sudo brew install git 安装Node.jsMac下最简单的做法便是直接下载pkg文件进行安装，最新版本的下载地址如下，选择后缀为pkg的文件下载安装即可：https://nodejs.org/download/release/latest/安装完成后修改环境变量123vi ~/.bash_profile添加export PATH=/usr/local/bin:$PATH 将npm源替换成淘宝源1npm config set registry http://registry.npm.taobao.org/ 安装Hexo安装前先介绍几个hexo常用的命令1234$ hexo g #完整命令为hexo generate，用于生成静态文件$ hexo s #完整命令为hexo server，用于启动服务器，主要用来本地预览$ hexo d #完整命令为hexo deploy，用于将本地文件发布到github上$ hexo n #完整命令为hexo new，用于新建一篇文章利用 npm 命令安装：1sudo npm install -g hexo报错可尝试1npm install --unsafe-perm -g hexo","text":"使用Hexo + github快速搭建个人博客安装依赖软件安装git1sudo brew install git 安装Node.jsMac下最简单的做法便是直接下载pkg文件进行安装，最新版本的下载地址如下，选择后缀为pkg的文件下载安装即可：https://nodejs.org/download/release/latest/安装完成后修改环境变量123vi ~/.bash_profile添加export PATH=/usr/local/bin:$PATH 将npm源替换成淘宝源1npm config set registry http://registry.npm.taobao.org/ 安装Hexo安装前先介绍几个hexo常用的命令1234$ hexo g #完整命令为hexo generate，用于生成静态文件$ hexo s #完整命令为hexo server，用于启动服务器，主要用来本地预览$ hexo d #完整命令为hexo deploy，用于将本地文件发布到github上$ hexo n #完整命令为hexo new，用于新建一篇文章利用 npm 命令安装：1sudo npm install -g hexo报错可尝试1npm install --unsafe-perm -g hexo 本地建立博客安装完成后，新建一个目录如 myblog 用于存放博客，切换到该目录下执行以下指令，Hexo 即会在目标文件夹初步生成博客所需要的所有文件：1hexo init然后切换到该目录下执行如下命令，安装所需要的依赖：1sudo npm install或12345678910111213npm install hexo-generator-index --savenpm install hexo-generator-archive --savenpm install hexo-generator-category --savenpm install hexo-generator-tag --savenpm install hexo-server --savenpm install hexo-deployer-git --savenpm install hexo-deployer-heroku --savenpm install hexo-deployer-rsync --savenpm install hexo-deployer-openshift --savenpm install hexo-renderer-marked --savenpm install hexo-renderer-stylus --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save修改theme网上有大量开发者们分享的模板可供选择使用，将它们的 Git 仓库 Clone 以后放到博客目录下的 themes 文件夹中即可：Github Hexo Themes有哪些好看的 Hexo 主题？本博客的搭建我选择了使用该主题：https://github.com/iTimeTraveler/hexo-theme-hiker1git clone https://github.com/iTimeTraveler/hexo-theme-hiker.git themes/hiker修改配置文件1234567cd myblogvi _config.yml将这里修改为hiker,默认为landspace# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hiker 生成静态文件,并查看12$ hexo g$ hexo s hexo s默认使用4000端口,如果冲突可以改为hexo s -p4001 然后用浏览器访问http://localhost:4001/，此时，你应该看到了一个漂亮的博客了，当然这个博客只是在本地的，别人是看不到的 部署本地文件到github首先去github创建repository,次数省略.我创建的repository叫Fandb.github.io1234567891011cd myblogvi _config.yml改成如下内容# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepository: github: https://github.com/Fanduzi/Fandb.github.io.git coding: https://git.coding.net/Fandb/Fandb.blog.gitbranch: master 发布到Github12$ hexo g$ hexo d执行上面的第二个命令，可能会要你输入用户名和密码，皆为注册Github时的数据，输入密码是不显示任何东西的，输入完毕回车即可。此时，我们的博客已经搭建起来，并发布到Github上了，这时可以登陆自己的Github查看代码是否已经推送到对应Repository，在浏览器访问huangjunhui.github.io就能看到自己的博客了。第一次访问地址，可能访问不了，您可以在几分钟后进行访问，一般不超过10分钟。 需要注意的是,需要将这里设置为master branch并保存,才能访问你的页面 参考 http://opiece.me/2015/04/09/hexo-guide/http://www.jianshu.com/p/6902dd4a0e75","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://fuxkdb.com/tags/Hexo/"}]},{"title":"Percona Monitoring and Management架构","slug":"Percona Monitoring and Management架构","date":"2017-08-02T14:00:00.000Z","updated":"2017-08-04T15:45:39.000Z","comments":true,"path":"2017/08/02/Percona Monitoring and Management架构/","link":"","permalink":"http://fuxkdb.com/2017/08/02/Percona%20Monitoring%20and%20Management%E6%9E%B6%E6%9E%84/","excerpt":"Percona Monitoring and Management架构PMM基于简单的client-server模型,可实现高效的扩展性,它包含以下模块: PMM Client,安装在任何你希望被监控的数据库服务器上.它会手机服务器指标和查询分析数据以提供一份完整的性能概览.数据被收集并发送到PMM Server PMM Server是PMM的核心部分,它聚合手机的数据,并以Web界面的表格,仪表盘和图形的形式展现 这些模块被封装以提供简单的安装和使用给用户,无需关心它的内部实现方法.但是，如果要利用PMM的全部潜力，内部结构就很重要. PMM Client PMM Server 部署方案 简单场景 典型场景 PMM是旨在无缝协同工作的工具集合。 一些是由Percona开发的，一些是第三方开源工具。","text":"Percona Monitoring and Management架构PMM基于简单的client-server模型,可实现高效的扩展性,它包含以下模块: PMM Client,安装在任何你希望被监控的数据库服务器上.它会手机服务器指标和查询分析数据以提供一份完整的性能概览.数据被收集并发送到PMM Server PMM Server是PMM的核心部分,它聚合手机的数据,并以Web界面的表格,仪表盘和图形的形式展现 这些模块被封装以提供简单的安装和使用给用户,无需关心它的内部实现方法.但是，如果要利用PMM的全部潜力，内部结构就很重要. PMM Client PMM Server 部署方案 简单场景 典型场景 PMM是旨在无缝协同工作的工具集合。 一些是由Percona开发的，一些是第三方开源工具。 Note 整个客户端 - 服务器模型不太可能发生变化，但组合每个组件的工具集可能随产品而变化。 下图说明了PMM当前的结构： PMM ClientPMM客户端软件包适用于大多数流行的Linux发行版： DEB用于基于Debian的发行版（包括Ubuntu等） Red Hat Enterprise Linux衍生产品的RPM（包括CentOS，Oracle Linux，Amazon Linux等） 还有可以在任何Linux系统上使用的通用tarball二进制文件。 有关详细信息，请参阅安装PMM客户端。 PMM客户端软件包包含以下内容： pmm-admin是一个用于管理PMM Client的命令行工具,例如:添加删除数你想要监控的数据库实例 percona-qan-agent是一个用于管理 Query Analytics (QAN) agent的服务 is a service that manages the Query Analytics (QAN) agent as it collects query performance data. It also connects with QAN API in PMM Server and sends over collected data. node-exporter是一个Prometheus exporter用于收集系统指标For more information, see https://github.com/percona/node_exporter. mysqld_exporter是一个Prometheus exporter用于收集MySQL server指标.For more information, see https://github.com/percona/mysqld_exporter. mongodb_exporter是一个Prometheus exporter用于收集MongoDB server指标.For more information, see https://github.com/percona/mongodb_exporter. proxysql_exporter是一个Prometheus exporter用于收集ProxySQL性能指标. For more information, see https://github.com/percona/proxysql_exporter. PMM ServerPMM服务器将作为您的中央监控主机的机器运行。 它通过以下方式作为设备分发： Docker image that you can use to run a container Open Virtual Appliance（OVA），您可以在VirtualBox或其他管理程序中运行 Amazon Machine Image (AMI) that you can run via Amazon Web Services (AWS) 有关更多信息，请参阅Running PMM Server. PMM服务器由以下工具组成： Query Analytics (QAN)使你能够分析MySQL查询性能. 除客户端QAN代理外，还包括以下内容： QAN API是用于存储和访问在PMM客户端上运行的percona-qan-agent收集的查询数据的后端。 QAN Web App是一个web程序用于可视化收集的Query Analytics数据 Metrics Monitor (MM)提供对MySQL或MongoDB服务器实例至关重要的指标的历史视图。 它包括以下内容： Prometheus是一个第三方的时间序列数据库,连接到PMM Client上运行的exporters并聚合收集到的指标.更多信息请参阅Prometheus Docs [1]. Consulprovides an API that a PMM Client can use to remotely list,add, and remove hosts for Prometheus. It also stores monitoring metadata. For more information, see Consul Docs [2]. Warning Although the Consul web UI is accessible, do not make any changes to the configuration. Grafana是第三方仪表板和图形构建器，用于在直观的Web界面中可视化由Prometheus汇总的数据。 有关更多信息，请参阅Grafana Docs[3]. Percona Dashboards是由Percona开发的Grafana仪表板. Orchestrator是一个MySQL的复制拓扑管理和可视化工具。 有关详细信息，请参阅Orchestrator Manual [4]. 所有工具都可以通过PMM Server web界面(登录页面)访问.For more information, see Using the Percona Monitoring and Management Platform. 部署方案PMM旨在针对各种环境进行扩展。 根据您的基础架构的大小和复杂性，您可以通过多种方式进行部署。 简单场景如果您只有一个MySQL或MongoDB服务器，则可以在此数据库主机上安装和运行两个模块（PMM客户端和PMM服务器）。 典型场景将典型的MySQL和MongoDB服务器实例分布在不同的主机上。 在这种情况下，您可以在专用监控主机上运行PMM Server，并在要监视的每个数据库主机上安装PMM Client。 来自主机的数据将聚合在PMM服务器上。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"PMM部署遇到的坑","slug":"PMM部署遇到的坑","date":"2017-08-01T14:00:00.000Z","updated":"2017-12-11T02:54:44.000Z","comments":true,"path":"2017/08/01/PMM部署遇到的坑/","link":"","permalink":"http://fuxkdb.com/2017/08/01/PMM%E9%83%A8%E7%BD%B2%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"","text":"PMM部署遇到的坑 系统 内核版本 CentOS release 6.4 (Final) 2.6.32-358.el6.x86_64 1.公司环境pull不下来 在自己的环境pull下来然后save image 123456789[root@slave oracle]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/percona/pmm-server 1.2.0 eb82a0e154c8 2 weeks ago 1.266 GBdocker.io/percona/pmm-server latest eb82a0e154c8 2 weeks ago 1.266 GB[root@slave oracle]# docker save eb82a0e154c8 &gt; pmm-server.tar[root@slave oracle]# scp pmm-server.tar 10.4.2.43:~/root@10.4.2.43&#x27;s password: pmm-server.tar 100% 1232MB 902.5KB/s 23:18 在原环境导入 1[root@test2 ~]# docker load &lt; pmm-server.tar repostory和tag是 1234[root@test2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE&lt;none&gt; &lt;none&gt; abdf7c1b7a63 2 weeks ago 1.266 GB&lt;none&gt; &lt;none&gt; 3690474eb5b4 11 months ago 0 B 修改tag 12345[root@test2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE&lt;none&gt; &lt;none&gt; abdf7c1b7a63 2 weeks ago 1.266 GB&lt;none&gt; &lt;none&gt; 3690474eb5b4 11 months ago 0 B[root@test2 ~]# docker tag abdf7c1b7a63 docker.io/percona/pmm-server:1.2.0 2.docker容器无法启动 no such file or directory statusCode=404 这个搜了半天不知道是什么原因,怀疑是内核版本太低 因为CentOS6.4自带内核版本是2.6.32-358.23.2.el6.x86_64，而Docker要求内核版本大于3.0，推荐3.8以上的内核 https://yq.aliyun.com/ziliao/48262 遂升级内核 1wget http://elrepo.org/linux/kernel/el6/x86_64/RPMS/kernel-lt-3.10.107-1.el6.elrepo.x86_64.rpm 如果连接不对,自己去http://elrepo.org/linux/kernel/el6/x86_64/RPMS/看一眼,找一个合适的 1rpm -ivh kernel-lt-3.10.107-1.el6.elrepo.x86_64.rpm 修改grub.conf 1234567891011121314151617181920212223242526vi /etc/grub.conf# grub.conf generated by anaconda## Note that you do not have to rerun grub after making changes to this file# NOTICE: You have a /boot partition. This means that# all kernel and initrd paths are relative to /boot/, eg.# root (hd0,0)# kernel /vmlinuz-version ro root=/dev/mapper/VolGroup-LogVol00# initrd /initrd-[generic-]version.img#boot=/dev/sdadefault=1timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (3.10.107-1.el6.elrepo.x86_64) root (hd0,0) kernel /vmlinuz-3.10.107-1.el6.elrepo.x86_64 ro root=/dev/mapper/VolGroup-LogVol00 rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=VolGroup/LogVol00 rd_NO_DM rhgb quiet numa=off elevator=deadline initrd /initramfs-3.10.107-1.el6.elrepo.x86_64.imgtitle CentOS (2.6.32-358.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/mapper/VolGroup-LogVol00 rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_LVM_LV=VolGroup/LogVol00 rd_NO_DM rhgb quiet numa=off elevator=deadline initrd /initramfs-2.6.32-358.el6.x86_64.img 现在title CentOS (3.10.107-1.el6.elrepo.x86_64)在 0 号位置,所以将default=1改为default=0重启os 重启后 1234[root@test2 ~]# cat /etc/redhat-release CentOS release 6.9 (Final)[root@test2 ~]# uname -r3.10.107-1.el6.elrepo.x86_64 再次创建container成功 12345678910111213141516171819202122232425[root@test2 ~]# docker load &lt; pmm-server.tar [root@test2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE&lt;none&gt; &lt;none&gt; abdf7c1b7a63 2 weeks ago 1.266 GB&lt;none&gt; &lt;none&gt; 3690474eb5b4 11 months ago 0 B[root@test2 ~]# docker tag abdf7c1b7a63 docker.io/percona/pmm-server:1.2.0[root@test2 ~]# docker create \\&gt; -v /opt/prometheus/data \\&gt; -v /opt/consul-data \\&gt; -v /var/lib/mysql \\&gt; -v /var/lib/grafana \\&gt; --name pmm-data \\&gt; percona/pmm-server:1.2.0 /bin/true094c63bd911b5139a267abe7939e5c4442cdc857970dedaccb9ae0cb5f165fc9[root@test2 ~]# docker run -d \\&gt; -p 80:80 \\&gt; --volumes-from pmm-data \\&gt; --name pmm-server \\&gt; --restart always \\&gt; percona/pmm-server:1.2.069195dca404bc607fa12a9cd6436a9786a71dcf226a0e4c1d6bf0b9879a14f03[root@test2 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69195dca404b percona/pmm-server:1.2.0 &quot;/opt/entrypoint.sh&quot; 11 seconds ago Up 9 seconds 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server 094c63bd911b percona/pmm-server:1.2.0 &quot;/bin/true&quot; 21 seconds ago pmm-data Note PMM-server选择一个内核版本搞的服务器就行 PMM-client无所谓 3.pmm-data 状态为Exited内核版本低 4.MySQL dashboard没数据防火墙没关123456pmm-admin check-networkcentos 7停止： systemctl disable firewalld禁用： systemctl stop firewalld 5.容器时区为UTC与我们系统CST差八个小时进入容器1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimehttp://www.cnblogs.com/w2206/p/6904446.html","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"部署PMM","slug":"部署PMM","date":"2017-08-01T14:00:00.000Z","updated":"2018-05-03T08:31:22.000Z","comments":true,"path":"2017/08/01/部署PMM/","link":"","permalink":"http://fuxkdb.com/2017/08/01/%E9%83%A8%E7%BD%B2PMM/","excerpt":"","text":"部署PMM 运行PMM Server 通过Docker运行 安装PMM Client Connect PMM Client to PMM Server Start data collection 1.在主机上运行PMM Server，用于访问收集的数据，查看基于时间的图表，并执行性能分析。可以通过三种方式运行PMM Server: Run PMM Server using Docker Run PMM Server as a virtual appliance Run PMM Server using Amazon Machine Image (AMI) 这里只介绍通过Docker方式运行PMM Server Running PMM Server Using DockerPMM服务器的Docker映像公开托管在https://hub.docker.com/r/percona/pmm-server/。 如果要从Docker映像运行PMM Server，则主机必须能够运行Docker 1.12.6或更高版本，并具有网络访问权限。 12345678安装docker首先安装epel源wget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-6.repoyum clean allyum makecacheyum -y install docker-io yum upgrade device-mapper-libs -yservice docker start 修改docker容器存储路径,默认存储在/var/lib/docker下.这里使用软连接方式修改123456service stop dockercd /var/lib/dockercp -r * /data/docker/mv /var/lib/docker /var/lib/docker.bakln -s /var/lib/docker /data/dockerservice start docker 禁用防火墙12停止： systemctl disable firewalld禁用： systemctl stop firewalld关闭或者设置 IPTABLES123#PMM$IPTABLES -A INPUT -p tcp --dport 42002 -j ACCEPT$IPTABLES -A INPUT -p tcp --dport 42000 -j ACCEPT这两个端口号是哪来的的呢? 是在pmm-admin check-network看到的(官方文档也有描述)123456* Connection: Client &lt;-- Server-------------- ---------- --------------------- ------- ---------- ---------SERVICE TYPE NAME REMOTE ENDPOINT STATUS HTTPS/TLS PASSWORD -------------- ---------- --------------------- ------- ---------- ---------linux:metrics pt_slave1 120.27.136.247:42000 OK YES - mysql:metrics pt_slave1 120.27.136.247:42002 OK YES - 安装Percona-toolkit (Query Analytics需要)12yum install percona-xtrabackup-24.x86_64 percona-xtrabackup-24-debuginfo.x86_64 percona-xtrabackup-test-24.x86_64 percona-toolkit.x86_64 percona-toolkit-debuginfo.x86_64 percona-toolkit.x86_64 -yyum install qpress* -y创建专用用户赋予权限12GRANT SELECT, PROCESS, SUPER, REPLICATION CLIENT, RELOAD ON *.* TO &#x27;pmm&#x27;@&#x27; localhost&#x27; IDENTIFIED BY &#x27;pass&#x27; WITH MAX_USER_CONNECTIONS 10;GRANT SELECT, UPDATE, DELETE, DROP ON performance_schema.* TO &#x27;pmm&#x27;@&#x27; localhost&#x27;;mysql:metrics 需要REPLICATION CLIENT权限mysql:queries 需要SUPER权限具体参考 [What privileges are required to monitor a MySQL instance?(https://www.percona.com/doc/percona-monitoring-and-management/faq.html#id11) Step 1.创建PMM Data Container1234567$ docker create \\ -v /opt/prometheus/data \\ -v /opt/consul-data \\ -v /var/lib/mysql \\ -v /var/lib/grafana \\ --name pmm-data \\ percona/pmm-server:latest /bin/true Note 如果再本地找不到,Docker会从Dockerhub拉取image 确保你在使用最新版本的Docker This container does not run, it simply exists to make sure you retain all PMM data when you upgrade to a newer pmm-server image. Do not remove or re-create this container, unless you intend to wipe out all PMM data and start over. 上述命令执行以下工作: docker create命令指示Docker守护程序从映像创建一个容器. -v选项初始化容器的数据卷. --name选项为可用于引用Docker网络中的容器的容器分配一个自定义名称。 在次数为：pmm-data. percona/pmm-server:1.5.2是导出容器的映像的名称和版本标签. /bin/true是容器运行命令 Step 2.Create and Run the PMM Server Container123456789101112docker run -d \\ -p 80:80 \\ --volumes-from pmm-data \\ --name pmm-server \\ -e SERVER_USER=hehe \\ -e SERVER_PASSWORD=ninainaide \\ -e METRICS_RETENTION=4320h \\ -e METRICS_MEMORY=35651584 \\ -e METRICS_RESOLUTION=3s \\ -e DISABLE_TELEMETRY=true \\ --restart always \\ percona/pmm-server:1.5.2 上述命令执行以下工作: docker run命令指示docker守护程序从映像运行容器。 -d选项以分离模式（即后台）启动容器。 -p选项映射用于访问PMM服务器Web UI的端口。 例如，如果端口80不可用，则可以使用-p 8080：80将着陆页映射到端口8080。 --volumes-from选项从pmm-data容器中装载卷（请参阅步骤1.创建一个PMM数据容器）。 —name选项为可用于引用Docker网络中的容器的容器分配一个自定义名称。 在这种情况下：pmm-server。 —restart选项定义容器的重新启动策略。 设置它始终确保Docker守护程序在启动时启动容器，并在容器退出时重新启动它。 percona / pmm-server:1.5.2是导出容器的映像的名称和版本标签。 -e SERVER_USER SERVER_PASSWORD是为了安全，设置访问PMM web页面所需的用户名和密码(否则任何有你pmm地址的人都可以访问) Security Features in Percona Monitoring and Management -e METRICS_MEMORY 默认prometheus使用768M内存存储最近的data chunks 使用此参数设置 具体参考How to control memory consumption for PMM? -e METRICS_RESOLUTION pmm-server 和 client 网络不太好的话把这个值设大一点 1~5s 超过5s promethes无法启动 具体参考What resolution is used for metrics? 2.在所有需要监控的服务器安装PMM Client123安装percona源rpm -ivh https://www.percona.com/downloads/percona-release/redhat/0.1-4/percona-release-0.1-4.noarch.rpmsudo yum install pmm-client 3.Connect PMM Client to PMM Server在安装完PMM Client后,它并不会自动连接PMM Server. 要将客户端连接到PMM服务器，请使用pmm-admin config –server命令指定IP地址。 例如，如果PMM服务器在192.168.100.1上运行，并且在IP 192.168.200.1的计算机上安装了PMM Client： 123456$ sudo pmm-admin config --server 192.168.100.1 --server-user jsmith --server-password pass1234OK, PMM server is alive.PMM Server | 192.168.100.1Client Name | ubuntu-amd64Client Address | 192.168.200.1 Note If you changed the default port 80 when running PMM Server, specify it after the server’s IP address. For example: 1$ sudo pmm-admin config --server 192.168.100.1:8080 --server-user jsmith --server-password pass1234 可以添加–client-name参数指定客户端名称,否则为主机名,但是像ECS那样的默认主机名iZbp1igpeohje7z5ugkdr9Z肯定不是我们想要的 4.Start data collectionAfter you connect the client to PMM Server, enable data collection from the database instance by adding a monitoring service. To enable general system metrics, MySQL metrics, and MySQL query analytics, run: 1sudo pmm-admin add mysql --user mysql --password mysql --socket /data/mysqldata/3306/mysql.sock To enable general system metrics, MongoDB metrics, and MongoDB query analytics, run: 1sudo pmm-admin --dev-enable add mongodb Note MongoDB查询分析是实验性的，在添加时需要–dev-enable选项。 没有此选项，则只会添加一般系统指标和MongoDB指标. To enable ProxySQL performance metrics, run: 1sudo pmm-admin add proxysql:metrics 要查看正在监控的内容，请运行: 1$ sudo pmm-admin list 举例你开启了系统和MongoDB指标监控,会产生类似下面的输出: 12345678910111213$ sudo pmm-admin listpmm-admin 1.1.3PMM Server | 192.168.100.1Client Name | ubuntu-amd64Client Address | 192.168.200.1Service manager | linux-systemd---------------- ----------- ----------- -------- ---------------- --------SERVICE TYPE NAME LOCAL PORT RUNNING DATA SOURCE OPTIONS---------------- ----------- ----------- -------- ---------------- --------linux:metrics mongo-main 42000 YES -mongodb:metrics mongo-main 42003 YES localhost:27017 有关添加实例的更多信息，请运行pmm-admin add —help. 装完一定要用pmm-admin check-network检查,下面的是正常的123456789101112131415161718192021222324252627282930313233[root@iZbp1igpeohje7z5ugkdr9Z log]# pmm-admin check-networkPMM Network StatusServer Address | xx.xx.xx.xxClient Address | xx.xx.xx.xx * System TimeNTP Server (0.pool.ntp.org) | 2017-09-28 15:44:39 +0800 CSTPMM Server | 2017-09-28 07:44:38 +0000 GMTPMM Client | 2017-09-28 15:44:39 +0800 CSTPMM Server Time Drift | OKPMM Client Time Drift | OKPMM Client to PMM Server Time Drift | OK* Connection: Client --&gt; Server-------------------- ------- SERVER SERVICE STATUS -------------------- ------- Consul API OKPrometheus API OKQuery Analytics API OKConnection duration | 31.032856msRequest duration | 31.557981msFull round trip | 62.590837ms* Connection: Client &lt;-- Server-------------- ---------- -------------------- ------- ---------- ---------SERVICE TYPE NAME REMOTE ENDPOINT STATUS HTTPS/TLS PASSWORD -------------- ---------- -------------------- ------- ---------- ---------linux:metrics pt_slave5 101.37.14.213:42000 OK YES - mysql:metrics pt_slave5 101.37.14.213:42002 OK YES - 修改容器时区(你试试不改时区grafana邮件告警的时候时间对吗 :) )1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 重启12345678910111213141516171819[root@test2 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69195dca404b percona/pmm-server:1.5.2 &quot;/opt/entrypoint.sh&quot; 28 hours ago Exited (128) 6 minutes ago 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server 094c63bd911b percona/pmm-server:1.5.2 &quot;/bin/true&quot; 28 hours ago pmm-data [root@test2 ~]# docker start 69195dca404bError response from daemon: Cannot start container 69195dca404b: Error getting container 69195dca404bc607fa12a9cd6436a9786a71dcf226a0e4c1d6bf0b9879a14f03 from driver devicemapper: Error mounting &#x27;/dev/mapper/docker-253:0-6947028-69195dca404bc607fa12a9cd6436a9786a71dcf226a0e4c1d6bf0b9879a14f03&#x27; on &#x27;/var/lib/docker/devicemapper/mnt/69195dca404bc607fa12a9cd6436a9786a71dcf226a0e4c1d6bf0b9879a14f03&#x27;: device or resource busyError: failed to start containers: [69195dca404b]umount /var/lib/docker/devicemapper/mnt/69195dca404bc607fa12a9cd6436a9786a71dcf226a0e4c1d6bf0b9879a14f03[root@test2 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69195dca404b percona/pmm-server:1.5.2 &quot;/opt/entrypoint.sh&quot; 28 hours ago Exited (128) 8 minutes ago 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server 094c63bd911b percona/pmm-server:1.5.2 &quot;/bin/true&quot; 28 hours ago pmm-data [root@test2 ~]# docker start 69195dca404b69195dca404b[root@test2 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69195dca404b percona/pmm-server:1.5.2 &quot;/opt/entrypoint.sh&quot; 28 hours ago Up 5 seconds 0.0.0.0:80-&gt;80/tcp, 443/tcp pmm-server 094c63bd911b percona/pmm-server:1.5.2 &quot;/bin/true&quot; 28 hours ago pmm-data","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"PMM","slug":"PMM","permalink":"http://fuxkdb.com/tags/PMM/"}]},{"title":"MHA在监控和故障转移时都做了什么","slug":"MHA在监控和故障转移时都做了什么","date":"2017-07-24T14:00:00.000Z","updated":"2017-08-04T15:10:40.000Z","comments":true,"path":"2017/07/24/MHA在监控和故障转移时都做了什么/","link":"","permalink":"http://fuxkdb.com/2017/07/24/MHA%E5%9C%A8%E7%9B%91%E6%8E%A7%E5%92%8C%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E6%97%B6%E9%83%BD%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88/","excerpt":"MHA在监控和故障转移时都做了什么以下是MHA(masterha_manager)在监控和故障切换上的基本流程 验证复制配置和识别当前主库 通过连接配置文件中描述的所有主机来识别当前主库.你不必手动指明那个主句是主库,MHA会自动检查复制设置并识别当前主库. 注意:MHA本身不能构建复制环境,MHA监控已存在的复制环境 If any slave is dead at this stage, terminating the script for safety reasons(If any slave is dead, MHA can not recover the dead slave, of course). 开启监控时任何slave发生故障都会导致监控退出,MHA并不能修复从库 如果任何必要的脚本没有安装在所有Node,MHA abort而不会启动监控","text":"MHA在监控和故障转移时都做了什么以下是MHA(masterha_manager)在监控和故障切换上的基本流程 验证复制配置和识别当前主库 通过连接配置文件中描述的所有主机来识别当前主库.你不必手动指明那个主句是主库,MHA会自动检查复制设置并识别当前主库. 注意:MHA本身不能构建复制环境,MHA监控已存在的复制环境 If any slave is dead at this stage, terminating the script for safety reasons(If any slave is dead, MHA can not recover the dead slave, of course). 开启监控时任何slave发生故障都会导致监控退出,MHA并不能修复从库 如果任何必要的脚本没有安装在所有Node,MHA abort而不会启动监控 监控主库 监控阶段MHA会持续监控主库直到其发生故障,MHA并不会监控从库.停止/重启/添加/移除从库不会对当前的MHA监控产生任何影响.但是注意当你添加或移除从库后应当更新MHA配置文件并重启MHA 监测主库故障 如果MHA连续三次连接主库失败,将会进入此阶段 如果你在配置文件中指定了secondary_check_script脚本,MHA会调用次脚本二次验证master状态看看是不是真的挂了 以下步骤也由masterha_master_switch命令执行. 您可以使用与masterha_manager相同的参数. 再次验证从库配置 再次读取配置文件,并连接所有主机并验证当前故障主机状态和所有此主库的从库.如果检测到任何无效的复制配置（即某些从站从不同的主站复制），那么MHA将在此处停止。 您可以通过在配置文件中设置ignore_fail参数来更改此行为。 这一步是出于安全原因。 由于masterha_manager可能运行了很长时间（周/月），因此复制配置可能会更改，因此通常建议进行双重检查。 检查上一个故障转移状态如果最后一个故障转移以错误结束，或最近一次故障转移最近完成，则MHA将在此停止，并且不会启动故障转移。 您可以通过masterha_manager命令中的ignore_last_failover和wait_on_failover_error参数更改此行为。 Shutting down failed master server (optional) 如果你在配置文件中定义了 master_ip_failover_script and/or shutdown_script ,MHA会调用这些脚本 当前主库(死掉的主库)的vip会通过master_ip_failover_script漂移到新主库 关闭故障主库的服务器,避免脑裂 Recovering a new master 从崩溃的主机中保存二进制日志事件（如果可能） 如果可以通过SSH访问崩溃的主库,可以从最新的slave的end_log_pos（Read_Master_Log_Pos）位置开始复制二进制日志. 选举新主库 基于配置文件中的设置和当前MySQL的设置 如果你希望某些从库作为优先备选主库,可以在配置中指定candidate_master=1 如果你希望某些从库永远不会成为新主库,可以在配置中指定no_master=1 识别最新的从众(接收了最新relay log的从库) 恢复并提升新主库 生成差异日志传输到新主库 应用这些差异日志到新主库 如果再次阶段产生任何错误(如,duplicate key error),MHA aborts,之后的恢复步骤,包括恢复其余从库将不会发生 激活新主库 如果您在配置文件中定义了master_ip_failover_script，则MHA会调用该脚本 您可以执行任何操作，例如激活当前主控的IP地址，创建特权用户等 恢复其他从库 恢复其余从库 并行的为所有从库生成差异日志 并行的为所有从库应用差异日志 change master到新主库,并start slave 即使在此阶段发生恢复错误,MHA也不会终止.故障的从库将不会从新主库复制,而其他成功恢复的从库可以启动复制 通知(可选) 如果你在配置文件中定义了report_script,MHA会调用此脚本,再次脚本中你可以做任何你想做的事,例如: 发送邮件 停止新主库得定时备份任务(因为我们一般希望备份在从库做,新主库应取消定时备份任务) 更细内部管理工具状态,等等 MHA在在线切换是做了什么 以下步骤可以通过masterha_master_switch命令（使用–master_state = alive参数）完成。 验证复制设置并识别当前主库 通过连接配置文件中描述的所有主机来识别当前主库 在当前主库执行FLUSH TABLES(可选).这一操作用来最小化之后的FLUSH TABLES WITH READ LOCK时间 检查MHA主库监控和failover是否正在运行 检查是否符合以下条件 所有从库上的IO线程正在运行 所有从库上的SQL线程正在运行 所有从库的Seconds_Behind_Master小于2秒(可以用过—running_updates_limit=N更改) 在主库上,通过show processlist输出中没有执行超过2秒的update语句 识别新主库 新主库可以通过”—new_master_host”参数指定.否则吗,将从配置文件中自动识别新的主库. 源主库和新主库必须拥有一致的过滤规则(binlog-do-db and binlog-ignore-db) 拒绝当前主库的写入 如果您在配置文件中定义了master_ip_online_change_script参数，MHA会调用该脚本。 您可以优雅地阻止写入（即删除写入用户，执行SET GLOBAL read_only = 1等） 在当前主机上执行FLUSH TABLE WITH READ LOCK来阻止所有写入（可以使用–skip_lock_all_tables参数跳过） 等待所有从库追上复制进度 这里会使用MASTER_LOG_POS()函数 赋予新主库写入权限 执行show master status来获取新主库得file和position 如果你在配置文件中定义了master_ip_online_change_script,MHA会调用次脚本.你可以在脚本里执行创建用户,set GLOBAL read_only=0等操作 为其他所有从库切换新主库 所有从库并发的执行change master,start slave","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"MHA配置文件","slug":"MHA 配置文件","date":"2017-07-20T14:00:00.000Z","updated":"2017-08-04T15:43:08.000Z","comments":true,"path":"2017/07/20/MHA 配置文件/","link":"","permalink":"http://fuxkdb.com/2017/07/20/MHA%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"MHA 配置文件 创建application配置文件 创建global配置文件 Binlog server 创建application配置文件示例","text":"MHA 配置文件 创建application配置文件 创建global配置文件 Binlog server 创建application配置文件示例manager_host$ cat /etc/app1.cnf 12345678910111213141516171819[server default]# mysql user and passworduser=rootpassword=mysqlpass# working directory on the managermanager_workdir=/var/log/masterha/app1# manager log filemanager_log=/var/log/masterha/app1/app1.log# working directory on MySQL serversremote_workdir=/var/log/masterha/app1[server1]hostname=host1[server2]hostname=host2[server3]hostname=host3 所有参数必须遵循”param = value”语法。 例如，以下参数设置不正确。 1234[server1]hostname=host1# incorrect: must be &quot;no_master=1&quot; 应该写成no_master=1no_master [server default]区块内的参数对所有[serverN]生效,[serverN]应该配置每个server特定的参数 创建global配置文件如果你计划使用一个MHA Manager管理两个或更多的applications(多个主从对),可以创建一个global配置文件管理通用参数.MHA Manager默认会去/etc/masterha_default.cnf读取global配置文件 global配置文件示例: Global configuration file (/etc/masterha_default.cnf) 1234567891011[server default]user=rootpassword=rootpassssh_user=rootmaster_binlog_dir= /var/lib/mysqlremote_workdir=/data/log/masterhasecondary_check_script= masterha_secondary_check -s remote_host1 -s remote_host2ping_interval=3master_ip_failover_script=/script/masterha/master_ip_failovershutdown_script= /script/masterha/power_managerreport_script= /script/masterha/send_master_failover_mail 以上参数在两个application间是公用的 每个application需要单独指定一个配置文件.以下示例为app1(host1-4)和app2(host11-14)的配置文件 app1: manager_host$ cat /etc/app1.cnf 123456789101112131415161718[server default]manager_workdir=/var/log/masterha/app1manager_log=/var/log/masterha/app1/app1.log[server1]hostname=host1candidate_master=1[server2]hostname=host2candidate_master=1[server3]hostname=host3[server4]hostname=host4no_master=1 以上配置单独指定了app1的manager_workdir和manager_log等信息.对于管理多个application的情况,应当单独为每个app设置manager_workdir和manager_log app2: manager_host$ cat /etc/app2.cnf 123456789101112131415161718[server default]manager_workdir=/var/log/masterha/app2manager_log=/var/log/masterha/app2/app2.log[server1]hostname=host11candidate_master=1[server2]hostname=host12candidate_master=1[server3]hostname=host13[server4]hostname=host14no_master=1 如果你同时在global配置文件和application配置文件中设置了同一个参数,那么后者(application)会覆盖前者. Binlog server从MHA 0.56版本开始,MHA支持新区块[binlogN].在binlog区块你可以定义mysqlbinlog streaming servers.当MHA使用基于GTID的failover,MHA会检查binlog servers,如果binlog server的日志&gt;其他从库,MHA会从binlog server获取binlog event应用到slave.当MHA使用基于传统复制的failover,MHA会忽略binlog servers. 下面是示例配置 manager_host$ cat /etc/app1.cnf 12345678910111213141516171819202122232425[server default]# mysql user and passworduser=rootpassword=mysqlpass# working directory on the managermanager_workdir=/var/log/masterha/app1# manager log filemanager_log=/var/log/masterha/app1/app1.log# working directory on MySQL serversremote_workdir=/var/log/masterha/app1[server1]hostname=host1[server2]hostname=host2[server3]hostname=host3[binlog1]hostname=binlog_host1[binlog2]hostname=binlog_host2","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"masterha_manager参数说明","slug":"masterha_manager参数说明","date":"2017-07-20T14:00:00.000Z","updated":"2017-08-04T15:45:59.000Z","comments":true,"path":"2017/07/20/masterha_manager参数说明/","link":"","permalink":"http://fuxkdb.com/2017/07/20/masterha_manager%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/","excerpt":"masterha_manager: Command to run MHA ManagerMHA Manager can be started by executing masterha_manager command. 12# masterha_manager --conf=/etc/conf/masterha/app1.cnf masterha_manager takes below arguments. Common arguments","text":"masterha_manager: Command to run MHA ManagerMHA Manager can be started by executing masterha_manager command. 12# masterha_manager --conf=/etc/conf/masterha/app1.cnf masterha_manager takes below arguments. Common arguments –conf=(config file path) Application and local scope configuration file. This argument is mandatory. –global_conf=(global config file path) Global scope configuration file. Default is /etc/masterha_default.cnf –manager_workdir, –workdir Same as manager_workdir parameter. –manager_log, –log_output Same as manager_log parameter. Monitoring specific arguments –wait_on_monitor_error=(seconds) 如果再监控过程中发生了错误,masterha_manager sleep wait_on_monitor_error 秒然后退出.默认值是0.此功能主要用于从守护程序脚本执行自动化主监控和故障转移。 在重新启动监视之前等待一段时间的错误是非常合理的。 This functionality is mainly introduced for doing automated master monitoring and failover from daemon scripts. It is pretty reasonable for waiting for some time on errors before restarting monitoring again. –ignore_fail_on_start 当有slave 节点宕掉时，默认是启动不了的，加上 –ignore_fail_on_start 即使有节点宕掉也能启动MHA，加上该参数会忽略启动文件中配置ignore_fail=1的server Failover specific arguments –last_failover_minute=(minutes) 1当最近的一个failover 切换发生在last_failover_minute(默认为8小时) 之内，MHA manager 将不会在切换。因为它会认为有些问题没有得到解决。如果设置了 --ignore_last_failover 参数，参数(--last_failover_minute) 将会失效 –ignore_last_failover 1如果最近failover 失败，MHA 将不会再次开始failover机制，因为这个问题可能再次发生。常规步骤:手动清理failover 错误文件，此文件一般在manager_workdir/app_name.failover.error文件，然后在启动failover机制。如果设置此参数，MHA 将会继续failover 不管上次的failover状态 –wait_on_failover_error=(seconds) 在failover的过程，当发出错误了，masterha_manager 等待 wait_no_failover_error 的时间后，退出。如果设置为了0，直接退出。这个好处，是当后台运行master monitor 和 failover scripts的时候，masterha_manager 可以在 wait_no_failover_error 时间到达之前重启监控 –remove_dead_master_conf 12如果设置此参数，当成功failover后，MHA manager将会自动删除配置文件中关于dead master的配置选项。 eg: For example, if the dead master&#x27;s hostname is host1 and it belongs to the section of server1, the entire part of the server1 will be removed from the configuration file. By default, the configuration file is not modified at all. After MHA finishes failover, the section of the dead master still exists. If you start masterha_manager immediately (this includes automatic restart from any daemon program), masterha_manager stops with an error that &quot;there is a dead slave&quot; (previous dead master). You might want to change this behavior especially if you want to continuously monitor and failover MySQL master automatically. In such cases, --remove_dead_master_conf argument is helpful.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"MHA Tutorial","slug":"MHA Tutorial","date":"2017-07-19T15:00:00.000Z","updated":"2017-08-04T15:45:47.000Z","comments":true,"path":"2017/07/19/MHA Tutorial/","link":"","permalink":"http://fuxkdb.com/2017/07/19/MHA%20Tutorial/","excerpt":"MHA Tutorial简单故障转移构建复制环境MHA不会帮你搭建复制环境,所以你需要自己搭建复制.换句话说,你可以在已有环境中使用MHA.举个例子,假设有四台主机:host1,host2,host3,host4.主库运行在host1,两个从库分别运行在host2和host3,而host4负责运行MHA Manager. 在host1-host4安装MHA NodeSee Installing MHA Node","text":"MHA Tutorial简单故障转移构建复制环境MHA不会帮你搭建复制环境,所以你需要自己搭建复制.换句话说,你可以在已有环境中使用MHA.举个例子,假设有四台主机:host1,host2,host3,host4.主库运行在host1,两个从库分别运行在host2和host3,而host4负责运行MHA Manager. 在host1-host4安装MHA NodeSee Installing MHA Node 在host4安装MHA ManagerSee Installing MHA Manager .监控节点需要同时安装MHA Node和MHA Manager 创建配置文件1234567891011121314151617181920manager_host$ cat /etc/app1.cnf[server default]# mysql user and passworduser=rootpassword=mysqlpassssh_user=root# working directory on the managermanager_workdir=/var/log/masterha/app1# working directory on MySQL serversremote_workdir=/var/log/masterha/app1[server1]hostname=host1[server2]hostname=host2[server3]hostname=host3 无需指定host1为master,MHA会自动探测出谁是master 检查SSH互信MHA Manager内部通过SSH调用包含在MHA Node包中的程序.MHA Node同样通过SSH(scp)传递差异中继日志到no-latest slave.为了使这些过程不需要手动交互,必须要配置SSH公钥验证.MHA Manager提供了一个简单的脚本”masterha_check_ssh”来验证各个节点是非配置好了SSH互信. 12345678910111213141516171819# masterha_check_ssh --conf=/etc/app1.cnfSat May 14 14:42:19 2011 - [warn] Global configuration file /etc/masterha_default.cnf not found. Skipping.Sat May 14 14:42:19 2011 - [info] Reading application default configurations from /etc/app1.cnf..Sat May 14 14:42:19 2011 - [info] Reading server configurations from /etc/app1.cnf..Sat May 14 14:42:19 2011 - [info] Starting SSH connection tests..Sat May 14 14:42:19 2011 - [debug] Connecting via SSH from root@host1(192.168.0.1) to root@host2(192.168.0.2)..Sat May 14 14:42:20 2011 - [debug] ok.Sat May 14 14:42:20 2011 - [debug] Connecting via SSH from root@host1(192.168.0.1) to root@host3(192.168.0.3)..Sat May 14 14:42:20 2011 - [debug] ok.Sat May 14 14:42:21 2011 - [debug] Connecting via SSH from root@host2(192.168.0.2) to root@host1(192.168.0.1)..Sat May 14 14:42:21 2011 - [debug] ok.Sat May 14 14:42:21 2011 - [debug] Connecting via SSH from root@host2(192.168.0.2) to root@host3(192.168.0.3)..Sat May 14 14:42:21 2011 - [debug] ok.Sat May 14 14:42:22 2011 - [debug] Connecting via SSH from root@host3(192.168.0.3) to root@host1(192.168.0.1)..Sat May 14 14:42:22 2011 - [debug] ok.Sat May 14 14:42:22 2011 - [debug] Connecting via SSH from root@host3(192.168.0.3) to root@host2(192.168.0.2)..Sat May 14 14:42:22 2011 - [debug] ok.Sat May 14 14:42:22 2011 - [info] All SSH connection tests passed successfully. 检查复制配置为了使MHA可以工作,所有在配置文件中定义的主从需要正常运行.MHA Manager提供了一个命令masterha_check_repl来快速检查复制运行状况。 123manager_host$ masterha_check_repl --conf=/etc/app1.cnf...MySQL Replication Health is OK. 如果您在此处遇到任何错误，请检查日志并解决问题。 当前主库不能是从库(意思是不能使其他mysql的从库)，所有其他从库必须从主库复制。 TypicalErrors页面可能有助于修复设置错误。 启动Manager1nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /data/masterha/app1/log/manager.log 2&gt;&amp;1 &amp; 如果所有配置都有效,masterha_manager会持续检查MySQL主库的可用性,知道主库宕机.默认masterha_manager会打印错误到标准输出,但是可以在配置文件中通过manager_log配置参数更改位置.典型的masterha_manager错误是MySQL复制配置无效,ssh_user没有足够的权限(最小要求是有读取relay log权限和写remote_workdir权限).默认masterha_manager在前台运行.如果发送SIGINT(Ctrl+C)到masterha_manager,masterha_manager将停止监控并退出 检查Manager状态MHA Manager开始监控MySQL master后不会打印任何日志信息知道主库无法访问或者Manager本身被终止.可以通过masterha_check_status命令检查MHA Manager运行状态. 12manager_host$ masterha_check_status --conf=/etc/app1.cnfapp1 (pid:5057) is running(0:PING_OK), master:host1 “app1”是MHA内部处理的应用程序名称，它是配置文件的前缀名称。 如果manager被停止或配置文件无效，将返回以下错误。 12manager_host$ masterha_check_status --conf=/etc/app1.cnfapp1 is stopped(1:NOT_RUNNING). 停止Manager你可以通过masterha_stop停止MHA Manager 12manager_host$ masterha_stop --conf=/etc/app1.cnfStopped app1 successfully. 测试master failoverNow MHA Manager monitors MySQL master server availability. Next, let’s test that master failover works correctly. To simulate this, you can simply kill mysqld on the master. 12host1$ killall -9 mysqld mysqld_safe On some distributions like Ubuntu, mysqld will be automatically restarted by angel process. If mysqld restarts very quickly (a few seconds), pings from MHA will succeed again before MHA starts failover. In such cases, failover does not start. If restarting mysqld takes long time (i.e. taking 2 minutes for InnoDB crash recovery), failover will start. If you have difficulties for testing killing mysqld or if you want to test Linux kernel side problem, invoking kernel panic is easy. 12host1# echo c &gt; /proc/sysrq-trigger Check logs on MHA manager, and verify that host2 becomes new mater, and host3 replicates from host2. When failover completes (or ends with errors), MHA Manager process stops. This is an expected behavior. If you want to run MHA Manager permanently, please read “Running MHA Manager in background” section.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"MHA Overview","slug":"MHA Overview","date":"2017-07-19T14:00:00.000Z","updated":"2017-08-04T15:45:52.000Z","comments":true,"path":"2017/07/19/MHA Overview/","link":"","permalink":"http://fuxkdb.com/2017/07/19/MHA%20Overview/","excerpt":"MHA概览MHA能够实现自动化的master failover和从库升级为new master,通常这一过程只需要10-30秒的停机时间.MHA最大程度保证了数据一致性,对性能零影响,易于安装,并且不需要改变现有部署情况. MHA支持计划性在线主库切换,在短时间内(0.5-2秒)的停机时间(仅限阻塞写入)，将当前运行的主机安全地更改为新主机. MHA提供以下功能,并且可用于需要高可用性,数据完整性和近乎不间断主库维护的许多部署中. Automated master monitoring and failover","text":"MHA概览MHA能够实现自动化的master failover和从库升级为new master,通常这一过程只需要10-30秒的停机时间.MHA最大程度保证了数据一致性,对性能零影响,易于安装,并且不需要改变现有部署情况. MHA支持计划性在线主库切换,在短时间内(0.5-2秒)的停机时间(仅限阻塞写入)，将当前运行的主机安全地更改为新主机. MHA提供以下功能,并且可用于需要高可用性,数据完整性和近乎不间断主库维护的许多部署中. Automated master monitoring and failover MHA可以在现有的复制环境中监控主库状态,在检测到主库故障时执行自动切换.MHA通过从拥有最新数据的slave识别与其他slave的数据差异,获取差异relay log并应用至所有需要同步的slave来保证数据一致性.MHA可以在几秒钟内完成故障转移:9-12秒钟可以检测到主库故障,7-10秒内关闭主机以避免脑裂(可选择不power off主服务器),几秒钟内将差异中级日志应用于新主库. 总停机时间通常为10-30秒.可以在配置文件中指定候选主库.由于MHA保证了从库间的数据一致性,因此任何从库都能提升为新的主库.(主要能够连通主服务器,获取日志,理论上主和所有从库都能保证一致性) 交互式Master Failover MHA可以手动（非自动），交互式故障切换，而不开启主库监控。 默认是开启masterha_manager监控,发现主库故障时会自动failover和选择新主库,这里说的是不开masterha_manager,手动使用masterha_master_switch切换主库.交互式是指切换过程中需要用户输入yes/no来选择是否继续执行 非交互式Master Failover MHA也支持非交互式自动Master Failover而不使用MHA监控主库.这个功能对于已经配置了其他MySQL master监控软件的情景.例如,你可以使用Pacemaker(Heartbeat)来监测Master故障和vip接管,然后使用MHA做master failover和将slave提升为新主库 这里意思是不使用masterha_manager监控主库状态,而是我们已经有了其他监控软件,但是在发生故障时调用masterha_master_switch来进行处理 在线切换主库 当现有主库需要做硬件维护等操作时,可以使用MHA更优雅的切换主库,MHA仅阻塞0.5-2秒的写入操作即可完成主库切换. Difficulties of Master Failover主故障转移并不像看起来那么简单。以最经典的一主多从架构举例,当master出现故障,通常我们会选择一个和主库数据一致(或者最接近的)的从库作为新的主库,然后将其他从库change master到新主库开启复制.这个过程可能会很复杂,即便可以找到一个与原主库一致的从库做为新的主库,其他从库也未必是同步的,如果直接change master而没有先补齐数据差异,就会造成数据不一致. MHA旨在尽可能快的完全自动化主库故障转移和恢复过程,而无需任何备机.恢复过程包括选择新主库,识别从库间relay log差异,将必要的events应用到新主库,同步其他从库并与新主库建立并启动复制.MHA通常可以在10-30秒的停机时间内进行故障转移(10秒内检测到主库故障,可选的7-10秒内关闭主库服务器以避免脑裂,几秒钟用于恢复数据至一致),具体时间取决于复制延迟度. MHA提供自动和手动故障切换命令.自动故障切换命令“masterha_manager（MHA Manager）”由master monitoring和master failover组成。 masterha_manager会永久监控主服务器的可用性。 如果MHA Manager无法到达主服务器，则它将自动启动非交互式故障切换过程。 手动故障转移命令“masterha_master_switch”会检查master是否出现了故障(dead)。 如果master dead，masterha_master_switch将其中一个从库作为新的主库（您可以选择一个首选主机），并启动恢复和故障切换。 在内部，它做的更多，但是您只执行一个命令，而无需自己执行复杂的恢复和故障转移操作。 MHA架构When a master crashes, MHA recovers rest slaves as below. 从库上的relay log中,主库的binary log positions写在end_log_pos部分(example).通过比较从库relay log中最后的end_log_pos,我们可以确定哪些relay log events没有被发送到从库.MHA通过这个机制恢复从库.In addition to basic algorithms covered in the slides at the MySQL Conf 2011, MHA does some optimizations and enhancements, such as generating differential relay logs very quickly (indenpendent from relay log file size), making recovery work with row based formats, etc. MHA组件MHA由MHA Manager和MHA Node组成. Manager可以管理多个 复制集群 masterha_manager:提供实现自动故障检测和故障转移的命令 masterha_check_ssh 检查MHA的SSH配置状况masterha_check_repl 检查MySQL复制状况masterha_manger 启动MHAmasterha_check_status 检测当前MHA运行状态masterha_master_monitor 检测master是否宕机masterha_master_switch 控制故障转移（自动或者手动）masterha_conf_host 添加或删除配置的server信息 Node部署在所有节点 save_binary_logs 保存和复制master的二进制日志apply_diff_relay_logs 识别差异的中继日志事件并将其差异的事件应用于其他的slavefilter_mysqlbinlog 去除不必要的ROLLBACK事件（MHA已不再使用这个工具）purge_relay_logs 清除中继日志（不会阻塞SQL线程） MHA Manager具有如监控MySQL master和控制故障转移等程序 MHA Node具有一些帮助主库进行故障转移的脚本,例如:解析MySQL binary/relay log;识别relay log位置,进而判断哪些event需要应用到其他从库;应用events到目标从库等. MHA Node运行在每个MySQL服务器上 当MHA Manager进行故障切换时,MHA Manager通过SSH连接MHA节点,并在需要时执行MHA节点命令. 自定义扩展MHA有几个用户可以自行扩展的脚本.例如,MHA可以调用任何自定义脚本切换VIP. secondary_check_script：用于检查来自多个网络路由的主机可用性 master_ip_failover_script：用于更新应用程序使用的主机的IP地址(vip) shutdown_script：用于强制关闭主服务器 report_script：发送报告(邮件) init_conf_load_script：用于加载初始配置参数 master_ip_online_change_script：更新主IP地址(vip)。 当通过masterha_master_switch执行在线主库切换时,需要此脚本来切换vip secondary_check_script通常来说，非常推荐使用两个或更多的网络线路检测MySQL master服务器的可用性。默认情况下，MHA Manager只是用单个线路检测：从Manager到Master。但是这个并不推荐。MHA通过调用一个额外的脚本（通过secondary_check_script参数指定），实际上可以有两个或更多的检查线路。配置示例： secondary_check_script =masterha_secondary_check -s remote_host1 -s remote_host2 masterha_secondary_check在MHA Manager包中已经包含。内置的masterha_secondary_check 脚本在大多数情况下都是比较好的，这里也可以调用其他任意脚本。 在上述示例中，MHA Manager通过Manager-(A)-&gt;remote_host1-(B)-&gt;master_host和 Manager-(A)-&gt;remote_host2-(B)-&gt;master_host检测MySQL master服务器。如果在两个线路中都会出现：connection A是成功的，但是B是不成功的，则masterha_secondary_check会退出（返回0），并且MHAManager决定MySQL master真的挂掉了，然后会进行failover。如果A是不成功的，masterha_secondary_check会退出（返回2），并且MHAManager猜测发生网络问题，不会进行failover。如果B是成功的，masterha_secondary_check会退出（返回3），并且MHAManager 推断MySQL master是存活的，不会进行failover。 一般来讲，remote_host1 和 remote_host2 应该位于不同的网段（从MHA Manager和MySQL服务器）。 MHA 调用secondary_check_script参数定义的脚本，自动传一下参数(所以不需要再配置文件中设置以下参数)。 masterha_secondary_check 在许多情形下是适宜的，但是如果你需要更多功能可以自己写一些网络监测脚本。 –user=(远程hosts的SSH用户名。ssh_user 参数的值会被传入) –master_host=(master的hostname) –master_ip=(master的ip 地址) –master_port=(master的端口号) 内置的masterha_secondary_check 脚本依赖于IO::Socket::INETPerl包，从Perl v5.6.0开始已经默认包含。masterha_secondary_check 脚本也是通过SSH连接到远程服务器，所以也需要SSH公共密钥验证。还有，masterha_secondary_check 脚本尝试从远程服务器（set by -s）到MySQLmaster建立TCP连接。my.cnf中设置的max_connections参数不会影响，如果TCP连接成功，master上的aborted_connects值会加1。 master_ip_failover_script在常见的HA环境中，许多情形下会在master上分配一个虚拟IP地址。如果master挂掉，HA软件（比如：Pacemaker）会接管虚拟IP到备机上。 另一个常见的处理方式是创建一个全局性的目录数据库，该数据库包含应用名称和读写IP地址之间的映射（i.e. {app1_master1, 192.168.0.1}, {app_master2, 192.168.0.2}, …）。这种情形下，在当前master挂掉之后需要更新目录数据库。 两者方式各有优势和劣势。MHA不强制使用其中一种方式，允许用户使用任何一种IP地址故障切换的解决办法，master_ip_failover_script参数被用于解决这个。 换句话说，需要编写一个脚本使应用透明的连接到new master，必须定义在master_ip_failover_script 参数中。实例如下： 1master_ip_failover_script= /usr/local/sample/bin/master_ip_failover 示例及保本位于（MHA Manager包）/samples/scripts/master_ip_failover。 示例脚本在MHA Managertarball和GitHup分支中包含。 MHA Manager 调用master_ip_failover_script 三次。第一次在进入master监控之前（脚本有效性检查），第二次是在调用shutdown_script脚本之前，第三次是在应用差异relay log到new master之后。MHA Manager 传入以下参数（不需要在配置文件中配置）。 Checking 阶段 –command=status –ssh_user=(当前master的ssh username) –orig_master_host=(当前master 的 hostname) –orig_master_ip=(当前 master的 ip地址) –orig_master_port=(当前master的端口号) 当前master shutdown阶段 –command=stop 或 stopssh –ssh_user=(dead master的 ssh username，如果可以通过SSH访问) –orig_master_host=(current(dead) master的 hostname) –orig_master_ip=(current(dead) master的 ip 地址) –orig_master_port=(current(dead) master的 端口号) New master 激活阶段 –command=start –ssh_user=(new master的 ssh username) –orig_master_host=(dead master的 hostname) –orig_master_ip=(dead master的 ip 地址) –orig_master_port=(dead master的端口号) –new_master_host=(new master的 hostname) –new_master_ip=(new master的 ip 地址) –new_master_port(new master的端口号) –new_master_user=(new master的用户) –new_master_password(new master的密码) 如果在master上使用了一个共享的虚拟IP地址，只要在之后shutdown_script中关掉了机器，在shutdown阶段可能不需要做任何事。在newmaster激活阶段，可以在new master上指定虚拟IP。 如果使用了目录数据库的方式，在master shutdown阶段需要delete或者update dead master的记录。在new master激活阶段，可以insert/update new master的记录。还有，可以做任何你需要做的事，使得应用可以写入new master。比如，SET GLOBAL read_only=0，创建拥有write权限的数据库用户，等等。 MHA Manager 检查该脚本退出的code（return code）。如果脚本返回的code为0或者10，MHA Manager 继续操作。 如果脚本返回的code不是0或者10，MHA Manager终止，不会继续故障切换。参数默认为空。 report_scriptfailover完成或者因为错误结束的时候可能需要发送报告（i.e. e-mail）。report_script 用于这个目的。MHA Manager传入以下参数。 –orig_master_host=(dead master的 hostname) –new_master_host=(new master的 hostname) –new_slave_hosts=(new slaves 的 hostnames，通过逗号分隔) –subject=(mail subject) –body=(body) 参数默认为空。 示例脚本位于(MHA Manager 包)/samples/scripts/send_report。示例脚本在MHA Managertarball 和 GitHub分支中包含。 init_conf_load_script当不想在配置文件中设置纯文本（i.e. password和repl_password）的时候可以使用该脚本。通过从该脚本返回 “name=value” 对，可以覆盖全局配置文件的参数。示例如下。 123#!/usr/bin/perlprint &quot;password=$ROOT_PASS\\n&quot;;print &quot;repl_password=$REPL_PASS\\n&quot;; master_ip_online_change_script类似于master_ip_failover_script 参数，但是该参数不是用于master故障切换的命令，用于master在线切换命令（masterha_master_switch–master_state=alive）。传入以下参数。 Current master阻塞写入阶段 –command=stop 或 stopssh –orig_master_host=(current master的 hostname) –orig_master_ip=(current master的 ip 地址) –orig_master_port=(current master的端口号) –orig_master_user=(current master的用户) –orig_master_password=(current master的密码) –orig_master_ssh_user=(从0.56支持, current master的 ssh user) –orig_master_is_new_slave=(从0.56支持, 表示原来的master是否会成为新的slave) New master 赋write权限阶段 –command=start –orig_master_host=(orig master的hostname) –orig_master_ip=(orig master的 ip 地址) –orig_master_port=(orig master的端口号) –new_master_host=(new master的 hostname) –new_master_ip=(new master的ip 地址) –new_master_port(new master的端口号) –new_master_user=(new master的用户) –new_master_password=(new master的密码) –new_master_ssh_user=(从0.56支持, new master的 ssh user) 当前master阻塞写阶段，MHA 在当前master上执行 FLUSH TABLES WITH READ LOCK 。在new master赋wirte权限阶段，可以做和master_ip_failover_script一样的事情。比如，创建有权限的用户，执行SET GLOBAL read_only=0，更新目录数据库，等等。如果脚本返回的code不是0或10，则MHA Manager 会终止，并且不会继续master切换。 默认参数为空。 示例脚本位于（MHA Manager包）/samples/scripts/master_ip_online_change。 典型用例一主多从1234 M(RW) M(RW), promoted from S1 | | +------+------+ --(master crash)--&gt; +-----+------+S1(R) S2(R) S3(R) S2(R) S3(R) 最常见的复制场景 一主多从,其中一个从库在异地1234 M(RW) M(RW), promoted from S1 | | +------+------+ --(master crash)--&gt; +------+------+S1(R) S2(R) Sr(R,no_master=1) S2(R) Sr(R,no_master=1) 通常我们可能会需要在异地数据中心搭建一个从库,但当主库宕机时,我们并不希望该从库被提升为新主库,而是希望一个与原主库同机房的从库作为新主库.可以通过在配置文件中向Sr区块添加no_master=1参数来从候选名单中移除Sr 一主多从,一个候选主库1234 M(RW)-----S0(R,candidate_master=1) M(RW), promoted from S0 | | +------+------+ --(master crash)--&gt; +------+------+S1(R) S2(R) S1(R) S2(R) 有时我们想指定一个特定的从库作为新主库.可以通过在配置文件中向希望的从库区块下添加candidate_master=1来实现这一需求 双主1234 M(RW)&lt;---&gt;M2(R,candidate_master=1) M(RW), promoted from M2 | | +------+------+ --(master crash)--&gt; +------+------+S(R) S2(R) S1(R) S2(R) 双主,M读写,M2只读, M下挂两个从库. 切换后M2作为新主 S1,S2 change master到M2. 只要M2是只读的,MHA就支持多主配置 MHA Manager supports multi-master configurations as long as all non-primary masters (M2 in this figure) are read-only. 三层复制1234567 M(RW) M(RW), promoted from S1 | | +------+------+ --(master crash)--&gt; +------+------+S1(R) S2(R) Sr(R) S2(R) Sr(R) | | + + Sr2 Sr2 M主库RW模式,S1 S2为本地只读从库,Sr为远程只读从库,Sr2为Sr的从库.这种结构下MHA仍然可以支持master failover.在配置文件配置master和所有二层的从库(在这个例子添加M,S1,S2和Sr到配置文件,但是不要添加Sr2到配置文件中).如果有master宕机,MHA自动提升二层从库(S1或S3或Sr.取决于优先级)为新主库,并恢复其他二层从库. 第三次的从库Sr2不受MHA管理,不过只要Sr2的主库可用,Sr2仍然可以继续复制. 如果Sr宕机,Sr2将不能继续复制,因为Sr2的主库是Sr.MHA不能回复Sr2 This is where support for global transaction id is desired. Hopefully this is less serious than master crash.每台明白,意思是这就是为什么需要GTID?有了GTID Sr2就可以change master到新主了吧 三层复制,双主1234567891011 M1(host1,RW) &lt;-----------------&gt; M2(host2,read-only) | | +------+------+ +S1(host3,R) S2(host4,R) S3(host5,R)=&gt; After failover M2(host2,RW) |+--------------+--------------+S1(host3,R) S2(host4,R) S3(host5,R) MHA通用支持这种结构.在这个结构下host5是一个第三层的slave,所以MHA不能管理host5(MHA does not execute CHANGE MASTER on host5 when the primary master host1 fails).当host1 dow,host2将会成为新主库,所以host5可以keep replication from host2 without doing anything. 下面是配置文件示例. 12345678910111213141516171819[server default]multi_tier_slave=1[server1]hostname=host1candidate_master=1[server2]hostname=host2candidate_master=1[server3]hostname=host3[server4]hostname=host4[server5]hostname=host5","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MHA","slug":"MHA","permalink":"http://fuxkdb.com/tags/MHA/"}]},{"title":"Sysbench出图","slug":"Sysbench出图","date":"2017-07-06T14:00:00.000Z","updated":"2018-04-24T09:59:31.000Z","comments":true,"path":"2017/07/06/Sysbench出图/","link":"","permalink":"http://fuxkdb.com/2017/07/06/Sysbench%E5%87%BA%E5%9B%BE/","excerpt":"Plot-Sysbenchprepare12345678910111213141516171819202122232425262728293031323334353637383940sysbench /tmp/sysbench-master/src/lua/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysql55/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=30 \\--events=5000000 --report-interval=5 prepare##--table-size=五百万,一个表五百万 10个表 五千万##--threads=30 开30个线程并发prepareInitializing worker threads...Creating table &#x27;sbtest6&#x27;...Creating table &#x27;sbtest4&#x27;...Creating table &#x27;sbtest5&#x27;...Creating table &#x27;sbtest10&#x27;...Creating table &#x27;sbtest8&#x27;...Creating table &#x27;sbtest3&#x27;...Creating table &#x27;sbtest9&#x27;...Creating table &#x27;sbtest1&#x27;...Creating table &#x27;sbtest2&#x27;...Creating table &#x27;sbtest7&#x27;...Inserting 5000000 records into &#x27;sbtest4&#x27;Inserting 5000000 records into &#x27;sbtest5&#x27;Inserting 5000000 records into &#x27;sbtest6&#x27;Inserting 5000000 records into &#x27;sbtest8&#x27;Inserting 5000000 records into &#x27;sbtest3&#x27;Inserting 5000000 records into &#x27;sbtest9&#x27;Inserting 5000000 records into &#x27;sbtest10&#x27;Inserting 5000000 records into &#x27;sbtest7&#x27;Inserting 5000000 records into &#x27;sbtest1&#x27;Inserting 5000000 records into &#x27;sbtest2&#x27;Creating a secondary index on &#x27;sbtest6&#x27;...Creating a secondary index on &#x27;sbtest9&#x27;...Creating a secondary index on &#x27;sbtest4&#x27;...Creating a secondary index on &#x27;sbtest8&#x27;...Creating a secondary index on &#x27;sbtest3&#x27;...Creating a secondary index on &#x27;sbtest7&#x27;...Creating a secondary index on &#x27;sbtest2&#x27;...Creating a secondary index on &#x27;sbtest1&#x27;...Creating a secondary index on &#x27;sbtest5&#x27;...Creating a secondary index on &#x27;sbtest10&#x27;...","text":"Plot-Sysbenchprepare12345678910111213141516171819202122232425262728293031323334353637383940sysbench /tmp/sysbench-master/src/lua/oltp_read_write.lua --mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysql55/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 --threads=30 \\--events=5000000 --report-interval=5 prepare##--table-size=五百万,一个表五百万 10个表 五千万##--threads=30 开30个线程并发prepareInitializing worker threads...Creating table &#x27;sbtest6&#x27;...Creating table &#x27;sbtest4&#x27;...Creating table &#x27;sbtest5&#x27;...Creating table &#x27;sbtest10&#x27;...Creating table &#x27;sbtest8&#x27;...Creating table &#x27;sbtest3&#x27;...Creating table &#x27;sbtest9&#x27;...Creating table &#x27;sbtest1&#x27;...Creating table &#x27;sbtest2&#x27;...Creating table &#x27;sbtest7&#x27;...Inserting 5000000 records into &#x27;sbtest4&#x27;Inserting 5000000 records into &#x27;sbtest5&#x27;Inserting 5000000 records into &#x27;sbtest6&#x27;Inserting 5000000 records into &#x27;sbtest8&#x27;Inserting 5000000 records into &#x27;sbtest3&#x27;Inserting 5000000 records into &#x27;sbtest9&#x27;Inserting 5000000 records into &#x27;sbtest10&#x27;Inserting 5000000 records into &#x27;sbtest7&#x27;Inserting 5000000 records into &#x27;sbtest1&#x27;Inserting 5000000 records into &#x27;sbtest2&#x27;Creating a secondary index on &#x27;sbtest6&#x27;...Creating a secondary index on &#x27;sbtest9&#x27;...Creating a secondary index on &#x27;sbtest4&#x27;...Creating a secondary index on &#x27;sbtest8&#x27;...Creating a secondary index on &#x27;sbtest3&#x27;...Creating a secondary index on &#x27;sbtest7&#x27;...Creating a secondary index on &#x27;sbtest2&#x27;...Creating a secondary index on &#x27;sbtest1&#x27;...Creating a secondary index on &#x27;sbtest5&#x27;...Creating a secondary index on &#x27;sbtest10&#x27;...sysbenchtest库大小12[root@uz6535 mysql55]# du -sh sysbenchtest/12G sysbenchtest/ run123456sysbench \\/tmp/sysbench-master/src/lua/oltp_read_write.lua \\--mysql-user=root --mysql-password=mysql --mysql-port=3306 \\--mysql-socket=/data/mysql55/mysql.sock --mysql-host=localhost \\--mysql-db=sysbenchtest --tables=10 --table-size=5000000 \\--threads=30 --report-interval=5 --time=300 run &gt; binlog_off.txt输出的原始结果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[ 5s ] thds: 30 tps: 73.76 qps: 1538.67 (r/w/o: 1084.35/300.82/153.51) lat (ms,95%): 733.00 err/s: 0.00 reconn/s: 0.00[ 10s ] thds: 30 tps: 75.80 qps: 1519.27 (r/w/o: 1062.85/304.81/151.61) lat (ms,95%): 590.56 err/s: 0.00 reconn/s: 0.00[ 15s ] thds: 30 tps: 58.39 qps: 1183.57 (r/w/o: 831.24/235.55/116.78) lat (ms,95%): 926.33 err/s: 0.00 reconn/s: 0.00[ 20s ] thds: 30 tps: 48.21 qps: 961.19 (r/w/o: 672.13/192.64/96.42) lat (ms,95%): 1129.24 err/s: 0.00 reconn/s: 0.00[ 25s ] thds: 30 tps: 43.57 qps: 863.96 (r/w/o: 606.66/170.15/87.14) lat (ms,95%): 1109.09 err/s: 0.00 reconn/s: 0.00[ 30s ] thds: 30 tps: 21.70 qps: 418.24 (r/w/o: 291.07/83.77/43.41) lat (ms,95%): 2585.31 err/s: 0.00 reconn/s: 0.00[ 35s ] thds: 30 tps: 19.99 qps: 420.45 (r/w/o: 299.09/81.37/39.99) lat (ms,95%): 3267.19 err/s: 0.00 reconn/s: 0.00[ 40s ] thds: 30 tps: 26.41 qps: 511.99 (r/w/o: 352.53/106.64/52.82) lat (ms,95%): 2198.52 err/s: 0.00 reconn/s: 0.00[ 45s ] thds: 30 tps: 30.20 qps: 608.26 (r/w/o: 427.04/120.81/60.41) lat (ms,95%): 2279.14 err/s: 0.00 reconn/s: 0.00[ 50s ] thds: 30 tps: 20.58 qps: 438.48 (r/w/o: 308.03/89.29/41.15) lat (ms,95%): 1973.38 err/s: 0.00 reconn/s: 0.00[ 55s ] thds: 30 tps: 26.03 qps: 486.74 (r/w/o: 338.97/95.71/52.06) lat (ms,95%): 2279.14 err/s: 0.00 reconn/s: 0.00[ 60s ] thds: 30 tps: 46.98 qps: 955.28 (r/w/o: 670.77/190.54/93.97) lat (ms,95%): 1170.65 err/s: 0.00 reconn/s: 0.00[ 65s ] thds: 30 tps: 40.62 qps: 803.33 (r/w/o: 562.03/160.06/81.23) lat (ms,95%): 1533.66 err/s: 0.00 reconn/s: 0.00[ 70s ] thds: 30 tps: 40.49 qps: 814.29 (r/w/o: 569.19/164.13/80.97) lat (ms,95%): 1903.57 err/s: 0.00 reconn/s: 0.00[ 75s ] thds: 30 tps: 38.90 qps: 779.99 (r/w/o: 550.83/151.36/77.80) lat (ms,95%): 1589.90 err/s: 0.00 reconn/s: 0.00[ 80s ] thds: 30 tps: 30.30 qps: 617.64 (r/w/o: 428.84/128.19/60.61) lat (ms,95%): 2009.23 err/s: 0.00 reconn/s: 0.00[ 85s ] thds: 30 tps: 21.46 qps: 418.45 (r/w/o: 292.27/83.25/42.93) lat (ms,95%): 2449.36 err/s: 0.00 reconn/s: 0.00[ 90s ] thds: 30 tps: 37.61 qps: 755.12 (r/w/o: 527.08/152.82/75.21) lat (ms,95%): 1771.29 err/s: 0.00 reconn/s: 0.00[ 95s ] thds: 30 tps: 32.54 qps: 651.63 (r/w/o: 457.56/128.98/65.08) lat (ms,95%): 1869.60 err/s: 0.00 reconn/s: 0.00[ 100s ] thds: 30 tps: 42.28 qps: 826.39 (r/w/o: 577.77/164.27/84.36) lat (ms,95%): 1453.01 err/s: 0.00 reconn/s: 0.00[ 105s ] thds: 30 tps: 30.84 qps: 653.14 (r/w/o: 460.72/130.55/61.87) lat (ms,95%): 2198.52 err/s: 0.00 reconn/s: 0.00[ 110s ] thds: 30 tps: 49.66 qps: 975.39 (r/w/o: 684.23/191.85/99.31) lat (ms,95%): 1149.76 err/s: 0.00 reconn/s: 0.00[ 115s ] thds: 30 tps: 39.18 qps: 779.72 (r/w/o: 540.44/160.93/78.35) lat (ms,95%): 1376.60 err/s: 0.00 reconn/s: 0.00[ 120s ] thds: 30 tps: 19.13 qps: 370.96 (r/w/o: 258.79/74.11/38.05) lat (ms,95%): 3095.38 err/s: 0.00 reconn/s: 0.00[ 125s ] thds: 30 tps: 21.44 qps: 460.58 (r/w/o: 325.15/92.36/43.07) lat (ms,95%): 3267.19 err/s: 0.00 reconn/s: 0.00[ 130s ] thds: 30 tps: 29.84 qps: 581.09 (r/w/o: 404.44/116.97/59.68) lat (ms,95%): 3040.14 err/s: 0.00 reconn/s: 0.00[ 135s ] thds: 30 tps: 23.97 qps: 467.19 (r/w/o: 328.38/90.86/47.95) lat (ms,95%): 3326.55 err/s: 0.00 reconn/s: 0.00[ 140s ] thds: 30 tps: 25.41 qps: 516.94 (r/w/o: 363.70/102.43/50.81) lat (ms,95%): 2449.36 err/s: 0.00 reconn/s: 0.00[ 145s ] thds: 30 tps: 26.00 qps: 518.58 (r/w/o: 363.19/103.40/52.00) lat (ms,95%): 2778.39 err/s: 0.00 reconn/s: 0.00[ 150s ] thds: 30 tps: 22.56 qps: 447.75 (r/w/o: 314.40/88.25/45.11) lat (ms,95%): 2539.17 err/s: 0.00 reconn/s: 0.00[ 155s ] thds: 30 tps: 23.66 qps: 485.03 (r/w/o: 333.80/103.92/47.31) lat (ms,95%): 2405.65 err/s: 0.00 reconn/s: 0.00[ 160s ] thds: 30 tps: 22.00 qps: 427.88 (r/w/o: 305.20/78.68/44.00) lat (ms,95%): 2932.60 err/s: 0.00 reconn/s: 0.00[ 165s ] thds: 30 tps: 25.84 qps: 521.81 (r/w/o: 362.14/108.00/51.68) lat (ms,95%): 2632.28 err/s: 0.00 reconn/s: 0.00[ 170s ] thds: 30 tps: 40.39 qps: 818.20 (r/w/o: 575.66/161.76/80.78) lat (ms,95%): 2082.91 err/s: 0.00 reconn/s: 0.00[ 175s ] thds: 30 tps: 35.98 qps: 697.47 (r/w/o: 485.77/139.73/71.97) lat (ms,95%): 2045.74 err/s: 0.00 reconn/s: 0.00[ 180s ] thds: 30 tps: 18.50 qps: 391.91 (r/w/o: 277.32/77.59/37.00) lat (ms,95%): 2680.11 err/s: 0.00 reconn/s: 0.00[ 185s ] thds: 30 tps: 26.36 qps: 520.54 (r/w/o: 363.80/104.03/52.72) lat (ms,95%): 1973.38 err/s: 0.00 reconn/s: 0.00[ 190s ] thds: 30 tps: 32.40 qps: 641.60 (r/w/o: 447.40/129.40/64.80) lat (ms,95%): 1903.57 err/s: 0.00 reconn/s: 0.00[ 195s ] thds: 30 tps: 32.60 qps: 668.65 (r/w/o: 465.04/138.41/65.21) lat (ms,95%): 2405.65 err/s: 0.00 reconn/s: 0.00[ 200s ] thds: 30 tps: 23.99 qps: 469.08 (r/w/o: 328.12/92.98/47.99) lat (ms,95%): 2985.89 err/s: 0.00 reconn/s: 0.00[ 205s ] thds: 30 tps: 35.41 qps: 715.31 (r/w/o: 506.48/138.02/70.81) lat (ms,95%): 1903.57 err/s: 0.00 reconn/s: 0.00[ 210s ] thds: 30 tps: 28.97 qps: 571.75 (r/w/o: 396.95/116.87/57.93) lat (ms,95%): 1836.24 err/s: 0.00 reconn/s: 0.00[ 215s ] thds: 30 tps: 27.81 qps: 538.34 (r/w/o: 374.30/108.43/55.61) lat (ms,95%): 2539.17 err/s: 0.00 reconn/s: 0.00[ 220s ] thds: 30 tps: 41.66 qps: 855.75 (r/w/o: 605.81/166.62/83.31) lat (ms,95%): 1213.57 err/s: 0.00 reconn/s: 0.00[ 225s ] thds: 30 tps: 43.58 qps: 844.54 (r/w/o: 584.88/172.51/87.15) lat (ms,95%): 2120.76 err/s: 0.00 reconn/s: 0.00[ 230s ] thds: 30 tps: 29.80 qps: 613.46 (r/w/o: 433.04/120.81/59.61) lat (ms,95%): 1533.66 err/s: 0.00 reconn/s: 0.00[ 235s ] thds: 30 tps: 35.47 qps: 716.43 (r/w/o: 503.19/142.29/70.95) lat (ms,95%): 2198.52 err/s: 0.00 reconn/s: 0.00[ 240s ] thds: 30 tps: 35.54 qps: 702.84 (r/w/o: 487.17/144.58/71.09) lat (ms,95%): 1506.29 err/s: 0.00 reconn/s: 0.00[ 245s ] thds: 30 tps: 28.79 qps: 581.14 (r/w/o: 410.41/113.15/57.57) lat (ms,95%): 1938.16 err/s: 0.00 reconn/s: 0.00[ 250s ] thds: 30 tps: 19.20 qps: 393.79 (r/w/o: 272.59/82.80/38.40) lat (ms,95%): 3151.62 err/s: 0.00 reconn/s: 0.00[ 255s ] thds: 30 tps: 31.60 qps: 608.62 (r/w/o: 427.62/117.80/63.20) lat (ms,95%): 2493.86 err/s: 0.00 reconn/s: 0.00[ 260s ] thds: 30 tps: 29.00 qps: 588.06 (r/w/o: 411.64/118.41/58.01) lat (ms,95%): 2159.29 err/s: 0.00 reconn/s: 0.00[ 265s ] thds: 30 tps: 34.99 qps: 705.06 (r/w/o: 494.30/140.77/69.99) lat (ms,95%): 1836.24 err/s: 0.00 reconn/s: 0.00[ 270s ] thds: 30 tps: 16.41 qps: 333.18 (r/w/o: 229.72/70.64/32.82) lat (ms,95%): 2778.39 err/s: 0.00 reconn/s: 0.00[ 275s ] thds: 30 tps: 32.99 qps: 670.70 (r/w/o: 472.39/132.34/65.97) lat (ms,95%): 3040.14 err/s: 0.00 reconn/s: 0.00[ 280s ] thds: 30 tps: 38.99 qps: 755.84 (r/w/o: 527.09/150.77/77.98) lat (ms,95%): 1771.29 err/s: 0.00 reconn/s: 0.00[ 285s ] thds: 30 tps: 36.41 qps: 720.35 (r/w/o: 503.91/143.63/72.82) lat (ms,95%): 1561.52 err/s: 0.00 reconn/s: 0.00[ 290s ] thds: 30 tps: 35.80 qps: 740.01 (r/w/o: 523.00/145.40/71.60) lat (ms,95%): 1678.14 err/s: 0.00 reconn/s: 0.00[ 295s ] thds: 30 tps: 32.94 qps: 662.28 (r/w/o: 466.61/129.78/65.89) lat (ms,95%): 1973.38 err/s: 0.00 reconn/s: 0.00[ 300s ] thds: 30 tps: 32.27 qps: 639.95 (r/w/o: 439.33/136.09/64.54) lat (ms,95%): 1869.60 err/s: 0.00 reconn/s: 0.00SQL statistics: queries performed: read: 139958 write: 39988 other: 19994 total: 199940 transactions: 9997 (33.18 per sec.) queries: 199940 (663.60 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 301.2969s total number of events: 9997Latency (ms): min: 13.53 avg: 901.57 max: 5107.53 95th percentile: 2238.47 sum: 9012972.28Threads fairness: events (avg/stddev): 333.2333/6.55 execution time (avg/stddev): 300.4324/0.32shell处理一下,文件名 最好为测试的条件 比如binlog_off binlog_on thread_30 thread9012345678grep &quot;^\\[&quot; binlog_off.txt |awk -F&#x27;(&#x27; &#x27;&#123;print $1&#125;&#x27;|sed -e &quot;s/\\[\\ /\\&#123;&#x27;time&#x27;:&#x27;/g&quot; -e &quot;s/\\ \\]//g&quot; -e &quot;s/:\\ /&#x27;:&#x27;/g&quot; -e &quot;s/\\ /&#x27;,&#x27;/g&quot; -e &quot;s/,&#x27;$/\\&#125;/g&quot;&#123;&#x27;time&#x27;:&#x27;5s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;14.95&#x27;,&#x27;qps&#x27;:&#x27;391.01&#x27;&#125;&#123;&#x27;time&#x27;:&#x27;10s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;27.33&#x27;,&#x27;qps&#x27;:&#x27;549.58&#x27;&#125;&#123;&#x27;time&#x27;:&#x27;15s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;29.01&#x27;,&#x27;qps&#x27;:&#x27;585.07&#x27;&#125;&#123;&#x27;time&#x27;:&#x27;20s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;27.51&#x27;,&#x27;qps&#x27;:&#x27;528.04&#x27;&#125;&#123;&#x27;time&#x27;:&#x27;25s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;24.80&#x27;,&#x27;qps&#x27;:&#x27;503.08&#x27;&#125;&#123;&#x27;time&#x27;:&#x27;30s&#x27;,&#x27;thds&#x27;:&#x27;30&#x27;,&#x27;tps&#x27;:&#x27;34.22&#x27;,&#x27;qps&#x27;:&#x27;688.40&#x27;&#125;处理成字典的样子,然后用python的 eval函数就能转换成字典了.我这里只取了输出结果的tps qps thds部分数据 出图用charts库,我用的jupyter notebook直接执行的测试机跑完文件那下来,统一命名测试机跑出来的结果为.txt,处理完的为.out1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import chartsimport subprocessimport osclass MyCharts: def __init__(self,file_list,qpsortps,title,interval=None): self.file_list=file_list self.qpsortps=qpsortps self.interval=interval self.title=title self.options = &#123; &#x27;title&#x27;:&#123;&#x27;text&#x27;:self.title&#125;, &#x27;xAxis&#x27;:&#123; &#x27;labels&#x27;:&#123; &#x27;step&#x27;:1 &#125;, &#125;, &#x27;yAxis&#x27;:&#123; &#x27;title&#x27;:&#123;&#x27;text&#x27;:self.qpsortps&#125;, &#125;, &#x27;chart&#x27;: &#123; &#x27;type&#x27;: &#x27;line&#x27;, &#x27;zoomType&#x27;: &#x27;x&#x27;, &#x27;panning&#x27;: &#x27;true&#x27;, &#x27;panKey&#x27;: &#x27;shift&#x27; &#125;, &#125; def getOne(self,file): with open(file,&#x27;r&#x27;) as data: dicList=[] for line in data: dic=eval(line.strip()) if self.interval: if int(dic[&#x27;time&#x27;][:-1])%self.interval == 0: dicList.append(dic) else: dicList.append(dic) timeList,qpsortpsList=[],[] for i in dicList: timeList.append(int(i[&#x27;time&#x27;][:-1])) if self.qpsortps == &#x27;qps&#x27;: qpsortpsList.append(float(i[&#x27;qps&#x27;])) else: qpsortpsList.append(float(i[&#x27;tps&#x27;])) qpsortpsData=list(zip(timeList,qpsortpsList)) temp = file[file.rindex(&#x27;/&#x27;)+1:file.rindex(&#x27;.&#x27;)] return &#123;&#x27;data&#x27;:qpsortpsData,&#x27;name&#x27;:temp&#125; def getMulti(self): s1List = [] for file in self.file_list: s1List.append(self.getOne(file)) return s1Listdef etl(file_list): for file in file_list: file_txt=&#x27;/Users/TiM/sysbench_file/&#x27;+file+&#x27;.txt&#x27; file_out=&#x27;/Users/TiM/sysbench_file/&#x27;+file+&#x27;.out&#x27; if not os.path.exists(file_out): cmd=&#x27;&#x27;&#x27;grep &quot;^\\[&quot; %s \\ |awk -F&#x27;(&#x27; &#x27;&#123;print $1&#125;&#x27;|sed -e &quot;s/\\[\\ /\\&#123;&#x27;time&#x27;:&#x27;/g&quot; -e \\ &quot;s/\\ \\]//g&quot; -e &quot;s/:\\ /&#x27;:&#x27;/g&quot; -e &quot;s/\\ /&#x27;,&#x27;/g&quot; -e &quot;s/,&#x27;$/\\&#125;/g&quot; \\ &gt; %s&#x27;&#x27;&#x27; % (file_txt,file_out) subprocess.call(cmd,shell=True)etl([&#x27;oltp_point_select&#x27;,&#x27;oltp_read_only&#x27;,&#x27;oltp_read_write&#x27;]) file_list = [ &#x27;/Users/TiM/sysbench_file/oltp_read_write.out&#x27;, &#x27;/Users/TiM/sysbench_file/oltp_point_select.out&#x27;, &#x27;/Users/TiM/sysbench_file/oltp_read_only.out&#x27; ]char_1=MyCharts(file_list,&#x27;qps&#x27;,&#x27;16G 4core Benchmark&#x27;)res=char_1.getMulti()charts.plot(res,options=char_1.options,show=&#x27;inline&#x27;,)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Sysbench","slug":"Sysbench","permalink":"http://fuxkdb.com/tags/Sysbench/"}]},{"title":"docopt详解","slug":"docopt详解","date":"2017-06-28T14:00:00.000Z","updated":"2017-08-07T02:18:00.000Z","comments":true,"path":"2017/06/28/docopt详解/","link":"","permalink":"http://fuxkdb.com/2017/06/28/docopt%E8%AF%A6%E8%A7%A3/","excerpt":"12345678910111213141516Naval Fate.Usage: naval_fate ship new &lt;name&gt;... naval_fate ship &lt;name&gt; move &lt;x&gt; &lt;y&gt; [--speed=&lt;kn&gt;] naval_fate ship shoot &lt;x&gt; &lt;y&gt; naval_fate mine (set|remove) &lt;x&gt; &lt;y&gt; [--moored|--drifting] naval_fate -h | --help naval_fate --versionOptions: -h --help Show this screen. --version Show version. --speed=&lt;kn&gt; Speed in knots [default: 10]. --moored Moored (anchored) mine. --drifting Drifting mine. 该示例描述了可执行的naval_fate的界面，可以使用命令(ship，new，move等)的不同组合，选项(-h，–help，–speed = 等)和 位置参数(，，). 示例使用方括号“[]”，圆括号“()”，管道“|” 和省略号“…”来描述可选的，必需的，相互排斥的和重复的元素. 一起，这些元素形成有效的使用模式，每个都以程序的名称naval_fate开头. Below the usage patterns，有一个包含说明的选项列表. 它们描述一个选项是否具有短/长形式(-h，–help)，选项是否具有参数(-speed = )，以及该参数是否具有默认值([default:10]). Usage模式在Usage:(不区分大小写) 关键字 间出现,并且显示的空了一行的部分 被解释为 usage pattern在”Usage:”后出现的第一个单词被解释为程序的名字. 下面是一个最简单的示例,该示例没有任何命令行参数1#### Usage: my_program程序可以使用用于描述模式的各种元素列出几个模式:123456Usage: my_program command --option &lt;argument&gt; my_program [&lt;optional-argument&gt;] my_program --another-option=&lt;with-argument&gt; my_program (--either-that-option | &lt;or-this-argument&gt;) my_program &lt;repeating-argument&gt; &lt;repeating-argument&gt;... ARGUMENT","text":"12345678910111213141516Naval Fate.Usage: naval_fate ship new &lt;name&gt;... naval_fate ship &lt;name&gt; move &lt;x&gt; &lt;y&gt; [--speed=&lt;kn&gt;] naval_fate ship shoot &lt;x&gt; &lt;y&gt; naval_fate mine (set|remove) &lt;x&gt; &lt;y&gt; [--moored|--drifting] naval_fate -h | --help naval_fate --versionOptions: -h --help Show this screen. --version Show version. --speed=&lt;kn&gt; Speed in knots [default: 10]. --moored Moored (anchored) mine. --drifting Drifting mine. 该示例描述了可执行的naval_fate的界面，可以使用命令(ship，new，move等)的不同组合，选项(-h，–help，–speed = 等)和 位置参数(，，). 示例使用方括号“[]”，圆括号“()”，管道“|” 和省略号“…”来描述可选的，必需的，相互排斥的和重复的元素. 一起，这些元素形成有效的使用模式，每个都以程序的名称naval_fate开头. Below the usage patterns，有一个包含说明的选项列表. 它们描述一个选项是否具有短/长形式(-h，–help)，选项是否具有参数(-speed = )，以及该参数是否具有默认值([default:10]). Usage模式在Usage:(不区分大小写) 关键字 间出现,并且显示的空了一行的部分 被解释为 usage pattern在”Usage:”后出现的第一个单词被解释为程序的名字. 下面是一个最简单的示例,该示例没有任何命令行参数1#### Usage: my_program程序可以使用用于描述模式的各种元素列出几个模式:123456Usage: my_program command --option &lt;argument&gt; my_program [&lt;optional-argument&gt;] my_program --another-option=&lt;with-argument&gt; my_program (--either-that-option | &lt;or-this-argument&gt;) my_program &lt;repeating-argument&gt; &lt;repeating-argument&gt;... ARGUMENT 以”&lt;”开始,”&gt;”结尾 和 大写字母的被解释为 位置参数 ####-o --option以一个或两个破折号开头的单词(除了“ - ”，“ - ”之外)分别被解释为短(一个字母)或长选项. 短选项可以“堆叠”，这意味着-abc等同于-a -b -c. 长选项可以在空格或等于“=”之后指定参数: –input = ARG相当于 - 输入ARG 短选项可以在可选空格之后指定参数:-f FILE等效于-fFILE 注意，写入 –input(与–input = ARG相比)是容易产生歧义的，这意味着不可能知道ARG是选项的参数还是位置参数. 在Usage:中，只有在提供该选项的description(如下所述)时，这个选项才会被解释为参数选项. 否则它将被解释为一个选项和单独的位置参数.同样,使用-f FILE和-fFILE也可能产生歧义,后者可能是一组断选项的堆叠(就像之前的例子-abc是-a -b -c的堆叠),或者是一个带参数得选项.只有在提供了该选项的description时，这些符号才会被被解释为一个带参数的选项. command所有其他不符合–options或的约定的单词都被解释为(sub)command. [optional elements] 可选元素被方括号”[]”包围起来的元素(options, arguments, commands)被标记为可选项.Usage: my_program [command --option &lt;argument&gt;]等价于Usage: my_program [command] [--option] [&lt;argument&gt;] (required elements) 必要元素默认情况下需要所有元素，如果不包括在括号“[]”中. 但是，有时候，必须使用“”()“来明确标记元素. 例如，当您需要组合相互排斥的元素(请参阅下一节):1Usage: my_program (--either-this &lt;and-that&gt; | &lt;or-this&gt;)另一个用例是当您需要指定如果存在一个元素，则需要另外一个元素:1Usage: my_program [(&lt;one-argument&gt; &lt;another-argument&gt;)]在这种情况下，有效的程序调用可以是无参数，也可以是2个参数. element|another相互排斥的元素可以用管道“|”分隔开 如下:1Usage: my_program go (--up | --down | --left | --right)当需要相互排斥的情况之一时，使用括号“()”对元素进行分组. 当不需要相互排斥的情况时，使用括号“[]”分组元素: Note, that specifying several patterns works exactly like pipe “|”, that is:12Usage: my_program run [--fast] my_program jump [--high]等价于1Usage: my_program (run [--fast] | jump [--high]) element…使用省略号“…”来指定左侧的参数(或参数组)可以重复一次或多次:12Usage: my_program open &lt;file&gt;... my_program move (&lt;from&gt; &lt;to&gt;)...您可以灵活地指定所需的参数数. 以下是需要零个或多个参数的3种(冗余)方法:123Usage: my_program [&lt;file&gt;...] my_program [&lt;file&gt;]... my_program [&lt;file&gt; [&lt;file&gt; ...]] 一个或多个参数1Usage: my_program &lt;file&gt;...两个或多个参数1Usage: my_program &lt;file&gt; &lt;file&gt;... [options]“[options]”是一种快捷方式，可以避免在模式中列出所有选项(带有说明的选项列表). 例如:12345Usage: my_program [options] &lt;path&gt;--all List everything.--long Long output.--human-readable Display in human-readable format.等价于12345Usage: my_program [--all --long --human-readable] &lt;path&gt;--all List everything.--long Long output.--human-readable Display in human-readable format.如果您有很多选项，并且所有选项都适用于其中一种模式，这将非常有用. 或者，如果您有短版本和长版本的选项(specified in option description part)，you can list either of them in a pattern: 12345Usage: my_program [-alh] &lt;path&gt;-a, --all List everything.-l, --long Long output.-h, --human-readable Display in human-readable format. [–]双重破折号“ - ”，当不是选项的一部分时，通常用作分隔选项和位置参数的约定，以便处理例如文件名可能被误认为选项的情况. 为了支持这个约定，在位置参数之前添加“[ - ]”到你的模式中.1Usage: my_program [options] [--] &lt;file&gt;...Apart from this special meaning, “–” is just a normal command, so you can apply any previously-described operations, for example, make it required (by dropping brackets “[ ]”) [-]一个单独的破折号“ - ”，当不是选项的一部分时，常常被用来表示一个程序应该处理stdin而不是一个文件. 如果你想遵循这个惯例，在你的模式中添加“[ - ]”. “ - ”本身只是一个正常的命令，您可以使用任何意义. Option descriptions选项描述包含您放在使用模式下的选项列表.即使在usage pattern中没有产生歧义也可以指定他们选项的描述允许指定: 互为同义词的短选项和长选项 带参数得选项 选项参数得默认值 The rules are as follows:以“ - ”或“ – ”(不包括空格)开头的每一行都被视为一个选项描述，例如: 1234Options: --verbose # GOOD -o FILE # GOODOther: --bad # BAD, line does not start with dash &quot;-&quot; 错误的示例,不是以&#x27;-&#x27;开头的行 要指定一个选项有一个参数，请在空格(或等于“=”符号)后面放置一个描述该参数的单词，如下所示. 遵循或UPPER-CASE约定的选项参数. 如果要分离选项，可以使用逗号. 在下面的示例中，两行均有效，但建议使用单一样式.12-o FILE --output=FILE # without comma, with &quot;=&quot; sign-i &lt;file&gt;, --input &lt;file&gt; # with comma, without &quot;=&quot; sign 使用两个空格分隔选项和其描述信息123456--verbose MORE text. # BAD, will be treated as if verbose # option had an argument MORE, so use # 2 spaces instead-q Quit. # GOOD-o FILE Output file. # GOOD--stdout Use stdout. # GOOD, 2 spaces如果要为参数的选项设置默认值，请将其放入选项的描述中，格式为[default:].123--coefficient=K The K coefficient [default: 2.95]--output=FILE Output file [default: test.txt]--directory=DIR Some directory [default: ./]","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://fuxkdb.com/tags/Python/"}]},{"title":"传统复制在线切换到GTID模式","slug":"传统复制在线切换到GTID模式","date":"2017-06-17T14:00:00.000Z","updated":"2017-08-07T02:28:10.000Z","comments":true,"path":"2017/06/17/传统复制在线切换到GTID模式/","link":"","permalink":"http://fuxkdb.com/2017/06/17/%E4%BC%A0%E7%BB%9F%E5%A4%8D%E5%88%B6%E5%9C%A8%E7%BA%BF%E5%88%87%E6%8D%A2%E5%88%B0GTID%E6%A8%A1%E5%BC%8F/","excerpt":"传统复制切换到GTID模式5.7.6以后参数gtid_mode可以动态修改GTID_MODE: OFF 彻底关闭GTID,如果关闭状态的备库接受到带GTID的事务,则复制中断 OFF_PERMISSIVE 可以认为是关闭GTID前的过渡阶段,主库在设置成该值后不再生成GTID,备库在接受到带GTID 和不带GTID的事务都可以容忍主库在关闭GTID时,执行事务会产生一个Anonymous_Gtid事件,会在备库执行:SET @@SESSION.GTID_NEXT= ‘ANONYMOUS’备库在执行匿名事务时,就不会去尝试生成本地GTID了 ON_PERMISSIVE 可以认为是打开GTID前的过渡阶段,主库在设置成该值后会产生GTID,同时备库依然容忍带GTID和不带GTID的事务 ON 完全打开GTID,如果打开状态的备库接受到不带GTID的事务,则复制中断 准备工作1.拓扑中的所有服务器都必须使用MySQL 5.7.6或更高版本. 除非拓扑中的所有服务器都使用此版本,否则无法在任何单个服务器上启用GTID事务.2.所有服务器都将gtid_mode设置为默认值OFF. The following procedure can be paused at any time and later resumed where it was, or reversed by jumping to the corresponding step of Section 16.1.5.3, “Disabling GTID Transactions Online”, the online procedure to disable GTIDs. This makes the procedure fault-tolerant because any unrelated issues that may appear in the middle of the procedure can be handled as usual, and then the procedure continued where it was left off.切换过程可以再任意时刻停止,并稍后继续执行 注意在每一步执行完全完成后再执行下一步","text":"传统复制切换到GTID模式5.7.6以后参数gtid_mode可以动态修改GTID_MODE: OFF 彻底关闭GTID,如果关闭状态的备库接受到带GTID的事务,则复制中断 OFF_PERMISSIVE 可以认为是关闭GTID前的过渡阶段,主库在设置成该值后不再生成GTID,备库在接受到带GTID 和不带GTID的事务都可以容忍主库在关闭GTID时,执行事务会产生一个Anonymous_Gtid事件,会在备库执行:SET @@SESSION.GTID_NEXT= ‘ANONYMOUS’备库在执行匿名事务时,就不会去尝试生成本地GTID了 ON_PERMISSIVE 可以认为是打开GTID前的过渡阶段,主库在设置成该值后会产生GTID,同时备库依然容忍带GTID和不带GTID的事务 ON 完全打开GTID,如果打开状态的备库接受到不带GTID的事务,则复制中断 准备工作1.拓扑中的所有服务器都必须使用MySQL 5.7.6或更高版本. 除非拓扑中的所有服务器都使用此版本,否则无法在任何单个服务器上启用GTID事务.2.所有服务器都将gtid_mode设置为默认值OFF. The following procedure can be paused at any time and later resumed where it was, or reversed by jumping to the corresponding step of Section 16.1.5.3, “Disabling GTID Transactions Online”, the online procedure to disable GTIDs. This makes the procedure fault-tolerant because any unrelated issues that may appear in the middle of the procedure can be handled as usual, and then the procedure continued where it was left off.切换过程可以再任意时刻停止,并稍后继续执行 注意在每一步执行完全完成后再执行下一步 从传统复制模式切换到GTID模式1.在每个sever执行:1SET @@GLOBAL.ENFORCE_GTID_CONSISTENCY = WARN; 让服务器与您的正常工作负载运行一段时间并监视日志. 如果此步骤在日志中导致任何警告,请调整应用程序,以使其仅使用兼容GTID的功能,并且不会产生任何警告. Important这是第一个重要步骤. 您必须确保在进入下一步骤之前不会在错误日志中生成警告. 2.在每个sever执行:1SET @@GLOBAL.ENFORCE_GTID_CONSISTENCY = ON; 3.在每个sever执行:1SET @@GLOBAL.GTID_MODE = OFF_PERMISSIVE; 哪个服务器首先执行此语句无关紧要,但重要的是所有服务器在任何服务器开始下一步之前完成此步骤. 4.在每个sever执行:1SET @@GLOBAL.GTID_MODE = ON_PERMISSIVE; 哪个服务器首先执行此语句无关紧要. 5.在每个服务器上,等待状态变量ONGOING_ANONYMOUS_TRANSACTION_COUNT为零. 可以使用如下方式查询:1SHOW STATUS LIKE &#x27;ONGOING_ANONYMOUS_TRANSACTION_COUNT&#x27;;ONGOING_ANONYMOUS_TRANSACTION_COUNT显示已标记为匿名的正在进行的事务的数量 注意:在从库上,理论上这个值可能显示为零,之后又显示为非零值.这并不是问题,只要显示为零一次即可 6.等待生成到步骤5的所有事务复制到所有服务器. 您可以在不停止更新的情况下执行此操作：唯一重要的是所有anonymous transactions都被复制了. 有关检查所有匿名事务已复制到所有服务器的一种方法,请参见第16.1.5.4节“验证匿名事务的复制”. 7.If you use binary logs for anything other than replication, for example point in time backup and restore, wait until you do not need the old binary logs having transactions without GTIDs. For instance, after step 6 has completed, you can execute FLUSH LOGS on the server where you are taking backups. Then either explicitly take a backup or wait for the next iteration of any periodic backup routine you may have set up. Ideally, wait for the server to purge all binary logs that existed when step 6 was completed. Also wait for any backup taken before step 6 to expire. ImportantThis is the second important point. It is vital to understand that binary logs containing anonymous transactions, without GTIDs cannot be used after the next step. After this step, you must be sure that transactions without GTIDs do not exist anywhere in the topology. 8.在每个sever执行:1SET @@GLOBAL.GTID_MODE = ON; 9.在每个sever的my.cnf配置文件中添加12gtid-mode=ONENFORCE_GTID_CONSISTENCY = ON 现在可以保证所有事务都具有GTID（除了在步骤5或更早版本中生成的已经处理的事务之外）. 要开始使用GTID协议,以便稍后执行自动故障切换,请在每个从站上执行以下操作. 或者,如果使用多源复制,请对每个通道执行此操作,并包括FOR CHANNEL通道子句：123STOP SLAVE [FOR CHANNEL &#x27;channel&#x27;];CHANGE MASTER TO MASTER_AUTO_POSITION = 1 [FOR CHANNEL &#x27;channel&#x27;];START SLAVE [FOR CHANNEL &#x27;channel&#x27;]; 参考 https://yq.aliyun.com/articles/41200","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL复制","slug":"MySQL复制","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%8D%E5%88%B6/"}]},{"title":"主从切换实例(传统复制)","slug":"主从切换实例(传统复制)","date":"2017-06-13T14:00:00.000Z","updated":"2017-08-07T02:27:33.000Z","comments":true,"path":"2017/06/13/主从切换实例(传统复制)/","link":"","permalink":"http://fuxkdb.com/2017/06/13/%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2%E5%AE%9E%E4%BE%8B(%E4%BC%A0%E7%BB%9F%E5%A4%8D%E5%88%B6)/","excerpt":"主从切换案例M为主库,read writeS* 为从库,read only现在要对M进行硬件维护,提升S1为主库,接管业务读写. M维护完成后作为从库,如下图首先可以进行如下切换 (A)再进行如下切换(B) A切换步骤首先在S1制造”错误”1234set session sql_log_bin=0;create table t_error_maker(id int);set session sql_log_bin=1;drop table t_error_maker;通过在session级别关闭写入binlog,建表,开启写入binlog,删表 制造异常, 当S11 S12 S13都执行到drop语句时,会报错停止sql_thread.通过这种方式,可以让它们停止在同一个位置.S121show master status 获取File PositionS11 S13123stop slave;change master到S12上start slave;S1212set global sql_slave_skip_counter=1;start slave sql_thread; B切换步骤M 停止业务写操作123set global read_only=on; 此时只有super权限用户能写入set global super_read_only=on;禁止super权限用户写#这里没有通过修改用户密码的方式是因为修改用户密码对已经连接上来的用户无效等M S1 S12 跑一致后(File Position相同) 停S12 sql_thread, 将业务写入操作接入S1最后M1234set global read_only=off;set global super_read_only=off;change master 到S12start slaveS121start slave sql_thread; 以上步骤通过脚本完成的话,可以做到对业务造成很小的影响脚本实例如下,注释掉了B步骤,因为需要配合切换业务写入操作","text":"主从切换案例M为主库,read writeS* 为从库,read only现在要对M进行硬件维护,提升S1为主库,接管业务读写. M维护完成后作为从库,如下图首先可以进行如下切换 (A)再进行如下切换(B) A切换步骤首先在S1制造”错误”1234set session sql_log_bin=0;create table t_error_maker(id int);set session sql_log_bin=1;drop table t_error_maker;通过在session级别关闭写入binlog,建表,开启写入binlog,删表 制造异常, 当S11 S12 S13都执行到drop语句时,会报错停止sql_thread.通过这种方式,可以让它们停止在同一个位置.S121show master status 获取File PositionS11 S13123stop slave;change master到S12上start slave;S1212set global sql_slave_skip_counter=1;start slave sql_thread; B切换步骤M 停止业务写操作123set global read_only=on; 此时只有super权限用户能写入set global super_read_only=on;禁止super权限用户写#这里没有通过修改用户密码的方式是因为修改用户密码对已经连接上来的用户无效等M S1 S12 跑一致后(File Position相同) 停S12 sql_thread, 将业务写入操作接入S1最后M1234set global read_only=off;set global super_read_only=off;change master 到S12start slaveS121start slave sql_thread; 以上步骤通过脚本完成的话,可以做到对业务造成很小的影响脚本实例如下,注释掉了B步骤,因为需要配合切换业务写入操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165# -*- coding: utf-8 -*- #coding=utf-8import pymysqlimport timefrom warnings import filterwarningsfrom progressive.bar import Barerror_message=u&quot;Error &#x27;Unknown table &#x27;fandb.t_error_maker&#x27;&#x27; on query. Default database: &#x27;fandb&#x27;. Query: &#x27;DROP TABLE `t_error_maker` /* generated by server */&#x27;&quot;def dec_progressive(func): def progess(*args, **kwargs): global i i += 100/9 bar.cursor.restore() bar.draw(value=i) return func(*args, **kwargs) return progess#创建数据库连接函数@dec_progressivedef get_conn(host,port,user,password,db=&#x27;performance_schema&#x27;,charset=&#x27;utf8&#x27;): return pymysql.connect(host=host, port=int(port), user=user,password=password,db=db,charset=charset)#制造复制异常函数@dec_progressivedef error_maker(host,port,user,password,db,charset): conn=get_conn(host=host, port=int(port), user=user,password=password,db=db,charset=charset) cursor = conn.cursor() cursor.execute(&quot;set session sql_log_bin=0;&quot;) cursor.execute(&quot;create table t_error_maker(id int)&quot;) cursor.execute(&quot;set session sql_log_bin=1&quot;) cursor.execute(&quot;drop table t_error_maker&quot;) cursor.close() conn.close()#获取slave statusdef get_slave_status(conn,sql): cursor = conn.cursor(pymysql.cursors.DictCursor) cursor.execute(sql) result = cursor.fetchone() return result cursor.close()#获取master status@dec_progressivedef get_master_status(conn): cursor = conn.cursor(pymysql.cursors.DictCursor) cursor.execute(&quot;show master status;&quot;) result = cursor.fetchone() return result cursor.close()#执行change master语句@dec_progressivedef change_master(conn,change_string): cursor = conn.cursor() cursor.execute(&quot;stop slave;&quot;) cursor.execute(change_string) cursor.execute(&quot;start slave;&quot;) cursor.close()#修复slave错误@dec_progressivedef repair_slave(conn): cursor = conn.cursor() cursor.execute(&quot;set global sql_slave_skip_counter=1;&quot;) cursor.execute(&quot;start slave sql_thread;&quot;) cursor.close() #设置read only@dec_progressivedef set_read_only(conn,switch): cursor = conn.cursor() if switch == &#x27;on&#x27;: cursor.execute(&quot;set global read_only=on;&quot;) elif switch == &#x27;off&#x27;: cursor.execute(&quot;set global read_only=off;&quot;) cursor.close()#启停 sql_thread@dec_progressivedef set_sql_thread(conn,switch): cursor = conn.cursor() if switch == &#x27;off&#x27;: cursor.execute(&quot;stop slave sql_thread;&quot;) elif switch == &#x27;on&#x27;: cursor.execute(&quot;start slave sql_thread;&quot;) cursor.close()#判断 39 40 41是否都因为drop t_error_maker停止@dec_progressivedef get_error_status(): while True: Last_SQL_Error_39 = get_slave_status(conn39,&#x27;show slave status;&#x27;)[&#x27;Last_SQL_Error&#x27;] Last_SQL_Error_40 = get_slave_status(conn40,&#x27;show slave status;&#x27;)[&#x27;Last_SQL_Error&#x27;] Last_SQL_Error_41 = get_slave_status(conn41,&#x27;show slave status;&#x27;)[&#x27;Last_SQL_Error&#x27;] if Last_SQL_Error_39 == Last_SQL_Error_40 == Last_SQL_Error_41 == error_message: break else: time.sleep(1)if __name__ == &#x27;__main__&#x27;: MAX_VALUE = 100 bar = Bar(max_value=MAX_VALUE, fallback=True) bar.cursor.clear_lines(2) bar.cursor.save() i=0 #不显示MySQL的warning filterwarnings(&#x27;ignore&#x27;,category=pymysql.Warning) #连接36 制造复制异常函数 print(u&quot;连接3306 制造复制异常函数&quot;) error_maker(host=&#x27;172.16.65.36&#x27;, port=3306, user=&#x27;root&#x27;,password=&#x27;mysql&#x27;,db=&#x27;fandb&#x27;,charset=&#x27;utf8&#x27;) conn39 = get_conn(&#x27;10.0.1.39&#x27;,3306,&#x27;root&#x27;,&#x27;mysql&#x27;) conn40 = get_conn(&#x27;10.0.1.40&#x27;,3306,&#x27;root&#x27;,&#x27;mysql&#x27;) conn41 = get_conn(&#x27;10.0.1.41&#x27;,3306,&#x27;root&#x27;,&#x27;mysql&#x27;) #判断 39 40 41是否都因为drop t_error_maker停止 print(u&quot;判断 39 40 41是否都因为drop t_error_maker停止&quot;) get_error_status() #获取40 master status 以供39 41切换 print(u&quot;获取40 master status 以供39 41切换&quot;) master_status_40 = get_master_status(conn40) File_40,Position_40 = master_status_40[&#x27;File&#x27;],master_status_40[&#x27;Position&#x27;] change_string = &quot;&quot;&quot; change master to master_host=&#x27;10.0.1.40&#x27;, master_port=3306, master_user=&#x27;repl&#x27;, master_password=&#x27;repl&#x27;, master_log_file=&#x27;%s&#x27;, master_log_pos=%d; &quot;&quot;&quot; % (File_40,Position_40) #39 41切换到40 print(u&quot;39,41切换到40&quot;) change_master(conn39,change_string) print(u&quot;39切换到40成功&quot;) change_master(conn41,change_string) print(u&quot;41切换到40成功&quot;) #修复40 slave print(u&quot;修复40 slave&quot;) repair_slave(conn40) # conn35 = get_conn(&#x27;172.16.65.35&#x27;,3306,&#x27;root&#x27;,&#x27;mysql&#x27;) # conn36 = get_conn(&#x27;172.16.65.36&#x27;,3306,&#x27;root&#x27;,&#x27;mysql&#x27;) # #35 设置read only # set_read_only(conn35,switch=&#x27;on&#x27;) # #判断35 36 40 是否同步 # while True: # res35 = get_slave_status(conn35,&#x27;show master status;&#x27;) # res36 = get_slave_status(conn36,&#x27;show slave status;&#x27;) # res40 = get_slave_status(conn40,&#x27;show slave status;&#x27;) # File_35,Position_35 = res35[&#x27;File&#x27;],res35[&#x27;Position&#x27;] # File_36,Position_36 = res36[&#x27;Relay_Master_Log_File&#x27;],res36[&#x27;Exec_Master_Log_Pos&#x27;] # File_40,Position_40 = res40[&#x27;Relay_Master_Log_File&#x27;],res40[&#x27;Exec_Master_Log_Pos&#x27;] # if File_35 == File_36 == File_40 and Position_35 == Position_36 == Position_40: # break # else: # time.sleep(1) # #停40 sql_thread # set_sql_thread(conn40,switch=&#x27;off&#x27;) # #写接入36 # #35 read_only=off # set_read_only(conn35,switch=&#x27;off&#x27;) # master_status_40 = get_master_status(conn40) # File_40,Position_40 = master_status_40[&#x27;File&#x27;],master_status_40[&#x27;Position&#x27;] # change_string = &quot;&quot;&quot; # change master to # master_host=&#x27;10.0.1.40&#x27;, # master_port=3306, # master_user=&#x27;repl&#x27;, # master_password=&#x27;repl&#x27;, # master_log_file=&#x27;%s&#x27;, # master_log_pos=%d; # &quot;&quot;&quot; % (File_40,Position_40) # change_master(conn35,change_string) # #起40 sql_thread # set_sql_thread(conn40,switch=&#x27;on&#x27;) bar.cursor.restore() bar.draw(value=100)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL复制","slug":"MySQL复制","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%8D%E5%88%B6/"}]},{"title":"Tpcc-MySQL","slug":"Tpcc-MySQL","date":"2017-05-07T14:00:00.000Z","updated":"2017-08-05T16:02:32.000Z","comments":true,"path":"2017/05/07/Tpcc-MySQL/","link":"","permalink":"http://fuxkdb.com/2017/05/07/Tpcc-MySQL/","excerpt":"Tpcc-MySQL下载安装tpcc-mysql123cd /usr/localwget https://github.com/Percona-Lab/tpcc-mysql/archive/master.zipmv tpcc-mysql-master tpcc-mysql 安装之前需要保证将mysql_config添加到$PATH 环境变量中1cd src ; make ( you should have mysql_config available in $PATH) 初始化数据123456789101112131415161718192021[mysql@master ~]$ mysqladmin -umysql -p -S /data/mysqldata/3306/mysql.sock create tpcc10Enter password: [mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock tpcc10 &lt; /usr/local/tpcc-mysql/create_table.sql Warning: Using a password on the command line interface can be insecure.[mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock -e &quot; show tables from tpcc10;&quot;Warning: Using a password on the command line interface can be insecure.+------------------+| Tables_in_tpcc10 |+------------------+| customer || district || history || item || new_orders || order_line || orders || stock || warehouse |+------------------+[mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock tpcc10 &lt; /usr/local/tpcc-mysql/add_fkey_idx.sql Warning: Using a password on the command line interface can be insecure.","text":"Tpcc-MySQL下载安装tpcc-mysql123cd /usr/localwget https://github.com/Percona-Lab/tpcc-mysql/archive/master.zipmv tpcc-mysql-master tpcc-mysql 安装之前需要保证将mysql_config添加到$PATH 环境变量中1cd src ; make ( you should have mysql_config available in $PATH) 初始化数据123456789101112131415161718192021[mysql@master ~]$ mysqladmin -umysql -p -S /data/mysqldata/3306/mysql.sock create tpcc10Enter password: [mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock tpcc10 &lt; /usr/local/tpcc-mysql/create_table.sql Warning: Using a password on the command line interface can be insecure.[mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock -e &quot; show tables from tpcc10;&quot;Warning: Using a password on the command line interface can be insecure.+------------------+| Tables_in_tpcc10 |+------------------+| customer || district || history || item || new_orders || order_line || orders || stock || warehouse |+------------------+[mysql@master ~]$ mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock tpcc10 &lt; /usr/local/tpcc-mysql/add_fkey_idx.sql Warning: Using a password on the command line interface can be insecure. 装载数据1234567891011121314151617181920212223242526272829/usr/local/tpcc-mysql/tpcc_load -hlocalhost -d tpcc10 -u mysql -p mysql -w 10各列含义为： |hostname:port| |dbname| |user| |password| |WAREHOUSES选项 warehouse 意为指定测试库下的仓库数量, 在生产环境中建议设置至少100以上这里设置10个仓库**************************************** TPCC-mysql Data Loader ****************************************option h with value &#x27;localhost&#x27;option d with value &#x27;tpcc10&#x27;option u with value &#x27;mysql&#x27;option p with value &#x27;mysql&#x27;option w with value &#x27;10&#x27;&lt;Parameters&gt; [server]: localhost [port]: 3306 [DBname]: tpcc10 [user]: mysql [pass]: mysql [warehouse]: 10TPCC Data Load Started...Loading Item .................................................. 5000.................................................. 10000.................................................. 15000.................................................. 20000.................................................. 25000.................................................. 30000......DATA LOADING COMPLETED SUCCESSFULLY. 压力测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100[mysql@master tpcc-mysql]$ /usr/local/tpcc-mysql/tpcc_start -h 172.16.120.130 -P 3306 -d tpcc10 -u mysql -p mysql -w 10 -c 10 -r 10 -l 360 -i 10各列含义为： |hostname| |port| |dbname| |user| |password| |WAREHOUSES| |CONNECTIONS| |WARMUP TIME| |BENCHMARK TIME|****************************************** ###easy### TPC-C Load Generator ******************************************option h with value &#x27;172.16.120.130&#x27;option P with value &#x27;3306&#x27;option d with value &#x27;tpcc10&#x27;option u with value &#x27;mysql&#x27;option p with value &#x27;mysql&#x27;option w with value &#x27;10&#x27;option c with value &#x27;10&#x27;option r with value &#x27;10&#x27;option l with value &#x27;360&#x27;option i with value &#x27;10&#x27;&lt;Parameters&gt; [server]: 172.16.120.130 [port]: 3306 [DBname]: tpcc10 [user]: mysql [pass]: mysql [warehouse]: 10 [connection]: 10 [rampup]: 10 (sec.) [measure]: 360 (sec.)RAMP-UP TIME.(10 sec.)MEASURING START. 10, trx: 99, 95%: 1276.031, 99%: 1651.666, max_rt: 1900.823, 99|2176.948, 9|453.968, 9|2345.216, 9|4414.681 20, trx: 118, 95%: 820.803, 99%: 1096.353, max_rt: 1103.197, 118|756.232, 13|256.137, 12|1141.656, 14|2324.185 30, trx: 154, 95%: 641.000, 99%: 725.352, max_rt: 806.858, 157|607.721, 15|459.590, 15|1153.335, 15|2572.398 40, trx: 207, 95%: 493.000, 99%: 600.507, max_rt: 655.846, 207|329.540, 20|248.943, 21|980.139, 19|1454.455 50, trx: 225, 95%: 508.894, 99%: 1075.549, max_rt: 1184.488, 221|303.606, 23|263.231, 22|1438.822, 24|1140.145 60, trx: 276, 95%: 375.332, 99%: 444.495, max_rt: 702.731, 281|374.427, 28|332.414, 28|868.740, 28|1169.240 70, trx: 276, 95%: 412.445, 99%: 561.394, max_rt: 613.595, 275|331.192, 27|254.446, 27|1023.836, 28|986.823 80, trx: 346, 95%: 285.151, 99%: 385.697, max_rt: 480.219, 343|465.637, 36|446.044, 35|663.491, 34|836.714 90, trx: 361, 95%: 304.106, 99%: 418.414, max_rt: 454.055, 365|261.836, 35|306.751, 36|706.509, 37|451.790 100, trx: 352, 95%: 329.310, 99%: 379.285, max_rt: 596.875, 350|228.144, 35|314.441, 35|874.110, 35|434.005 110, trx: 391, 95%: 264.116, 99%: 364.263, max_rt: 524.591, 393|300.455, 40|237.326, 39|870.451, 39|368.716 120, trx: 390, 95%: 283.110, 99%: 380.422, max_rt: 796.105, 390|209.881, 39|212.044, 39|649.781, 39|361.857 130, trx: 481, 95%: 211.891, 99%: 273.448, max_rt: 320.599, 478|205.557, 48|401.033, 48|545.170, 48|299.260 140, trx: 394, 95%: 261.676, 99%: 355.964, max_rt: 500.034, 397|457.157, 40|228.973, 40|921.951, 39|338.248 150, trx: 383, 95%: 283.788, 99%: 454.587, max_rt: 566.393, 384|280.592, 37|183.698, 38|848.897, 39|399.204 160, trx: 451, 95%: 231.870, 99%: 340.538, max_rt: 484.727, 448|260.694, 46|361.268, 46|660.337, 44|466.192 170, trx: 462, 95%: 228.493, 99%: 344.949, max_rt: 514.245, 459|181.838, 45|238.782, 44|635.492, 47|380.900 180, trx: 501, 95%: 201.077, 99%: 264.590, max_rt: 367.453, 504|187.646, 51|209.077, 51|592.223, 50|312.695 190, trx: 487, 95%: 208.182, 99%: 248.247, max_rt: 385.329, 487|156.600, 48|268.224, 49|599.426, 48|369.467 200, trx: 549, 95%: 198.565, 99%: 246.544, max_rt: 289.273, 547|125.297, 55|149.780, 55|579.936, 55|288.928 210, trx: 525, 95%: 183.039, 99%: 252.746, max_rt: 348.508, 527|151.534, 53|157.159, 53|538.393, 53|342.113 220, trx: 498, 95%: 212.272, 99%: 454.723, max_rt: 649.610, 499|247.239, 49|373.921, 49|850.496, 50|307.415 230, trx: 593, 95%: 165.673, 99%: 224.628, max_rt: 254.276, 592|161.555, 60|217.229, 59|554.132, 59|274.591 240, trx: 562, 95%: 172.712, 99%: 222.220, max_rt: 462.752, 563|119.360, 56|363.567, 56|444.219, 57|438.265 250, trx: 613, 95%: 156.842, 99%: 210.249, max_rt: 336.516, 609|210.124, 61|162.821, 63|443.805, 60|302.938 260, trx: 581, 95%: 160.451, 99%: 234.803, max_rt: 310.168, 584|134.567, 58|205.125, 57|503.892, 58|354.136 270, trx: 657, 95%: 142.558, 99%: 184.193, max_rt: 236.276, 657|114.116, 66|163.854, 66|398.327, 66|302.011 280, trx: 625, 95%: 154.188, 99%: 325.488, max_rt: 402.501, 627|305.710, 62|220.527, 62|490.638, 63|306.516 290, trx: 648, 95%: 138.147, 99%: 186.747, max_rt: 257.152, 641|176.512, 66|97.194, 65|459.044, 65|268.244 300, trx: 587, 95%: 157.030, 99%: 207.498, max_rt: 356.098, 589|177.653, 58|96.041, 58|439.606, 58|304.784 310, trx: 606, 95%: 158.067, 99%: 200.416, max_rt: 261.933, 609|190.645, 61|147.659, 61|957.198, 61|312.263 320, trx: 641, 95%: 149.149, 99%: 190.587, max_rt: 377.275, 640|111.658, 64|86.510, 64|398.594, 64|255.898 330, trx: 616, 95%: 141.707, 99%: 186.970, max_rt: 325.279, 612|127.490, 61|143.177, 62|539.403, 61|329.158 340, trx: 616, 95%: 146.626, 99%: 205.335, max_rt: 424.379, 621|144.290, 62|114.400, 62|521.001, 62|460.688 350, trx: 659, 95%: 139.143, 99%: 187.250, max_rt: 274.776, 660|121.001, 66|115.034, 66|370.938, 66|260.560 360, trx: 675, 95%: 136.871, 99%: 165.425, max_rt: 249.625, 673|147.049, 67|131.501, 67|462.549, 68|260.771STOPPING THREADS..........&lt;Raw Results&gt; [0] sc:9 lt:16596 rt:0 fl:0 avg_rt: 118.3 (5) [1] sc:678 lt:15928 rt:0 fl:0 avg_rt: 37.0 (5) [2] sc:201 lt:1459 rt:0 fl:0 avg_rt: 51.4 (5) [3] sc:0 lt:1659 rt:0 fl:0 avg_rt: 349.9 (80) [4] sc:1 lt:1661 rt:0 fl:0 avg_rt: 265.4 (20) in 360 sec.&lt;Raw Results2(sum ver.)&gt; [0] sc:9 lt:16596 rt:0 fl:0 [1] sc:678 lt:15928 rt:0 fl:0 [2] sc:201 lt:1459 rt:0 fl:0 [3] sc:0 lt:1659 rt:0 fl:0 [4] sc:1 lt:1661 rt:0 fl:0 &lt;Constraint Check&gt; (all must be [OK]) [transaction percentage] Payment: 43.48% (&gt;=43.0%) [OK] Order-Status: 4.35% (&gt;= 4.0%) [OK] Delivery: 4.34% (&gt;= 4.0%) [OK] Stock-Level: 4.35% (&gt;= 4.0%) [OK] [response time (at least 90% passed)] New-Order: 0.05% [NG] * Payment: 4.08% [NG] * Order-Status: 12.11% [NG] * Delivery: 0.00% [NG] * Stock-Level: 0.06% [NG] *&lt;TpmC&gt; 2767.500 TpmC --每分钟 除60就是tps 含义123456710, trx: 12920, 95%: 9.483, 99%: 18.738, max_rt: 213.169, 12919|98.778, 1292|101.096, 1293|443.955, 1293|670.84210 - 从基准测试开始到现在的秒数trx: 12920 - 在给定的间隔内（在这种情况下,在过去的10秒）内执行的新订单交易. 基本上这是每个间隔的吞吐量. 越多越好95％：9.483： - 每次给定间隔的新订单交易的95％响应时间. 在这种情况下是9.483秒99％：18.738： - 每次给定间隔的新订单交易的99％响应时间. 在这种情况下是18.738秒max_rt：213.169： - 每个给定间隔的新订单交易的最大响应时间. 在这种情况下是213.169秒其余的：12919 | 98.778,1292 | 101.096,1293 | 443.955,1293 | 670.842是其他类型的事务的吞吐量和最大响应时间,可以忽略 gnuplot出图/usr/local/tpcc-mysql/scripts 目录下有一个analyze.sh 修改了一下,如果用gnuplot出图,则为12345678 cat tpcc.txt | grep -v HY000 | grep -v payment | grep -v neword | awk -v timeslot=1 &#x27; BEGIN &#123; FS=&quot;[,():]&quot;; s=0; cntr=0 &#125; /MEASURING START/ &#123; s=1&#125; /STOPPING THREADS/ &#123;s=0&#125; /0/ &#123; if (s==1) &#123; cntr++; &#125; if ( cntr==timeslot ) &#123; printf (&quot;%d %d\\n&quot;,$1,$3) ; cntr=0;&#125;&#125;&#x27; &gt;&gt; tpcc-graphic-data.txt10 9920 11830 15440 20750 225只输出时间和trx列gnuplot.cnf12345678910set terminal gif small size 800,600 #指定输出成gif图片，且图片大小为550×25set output &quot;tpcc.gif&quot; #指定输出gif图片的文件名set title &quot;MySQL Performance&quot; #图片标题set style data lines #显示网格set xlabel &quot;Time/s&quot; #X轴标题set ylabel &quot;Transactions&quot; #Y轴标题set grid #显示网格plot \\&quot;tpcc-graphic-data.txt&quot; using 1:2 title &quot;Total throughput&quot; with lines #从tpcc-graphic-data.txt文件中读取第一列和第二列作为X轴和Y轴数据，示例名&quot;Total throughput&quot;出图cat gnuplot.cnf | gnuplot highcharts出图修改analyze.sh123456789 cat tpcc.txt | grep -v HY000 | grep -v payment | grep -v neword | awk -v timeslot=1 &#x27; BEGIN &#123; FS=&quot;[,():]&quot;; s=0; cntr=0 &#125; /MEASURING START/ &#123; s=1&#125; /STOPPING THREADS/ &#123;s=0&#125; /0/ &#123; if (s==1) &#123; cntr++; &#125; if ( cntr==timeslot ) &#123; printf (&quot;&#x27;&#123;\\&#x27;time\\&#x27;:&#x27;%d&#x27;,\\&#x27;trx\\&#x27;:&#x27;%d&#125;\\n&quot;,$1,$3) ; cntr=0;&#125;&#125;&#x27; &#123;&#x27;time&#x27;:10,&#x27;trx&#x27;:99&#125;&#123;&#x27;time&#x27;:20,&#x27;trx&#x27;:118&#125;&#123;&#x27;time&#x27;:30,&#x27;trx&#x27;:154&#125;&#123;&#x27;time&#x27;:40,&#x27;trx&#x27;:207&#125;&#123;&#x27;time&#x27;:50,&#x27;trx&#x27;:225&#125;&#123;&#x27;time&#x27;:60,&#x27;trx&#x27;:276&#125; 出图123456789101112131415161718192021import chartsdef getData(file): with open(file,&#x27;r&#x27;) as data: dicList=[] for line in data: dic=eval(line.strip()) dicList.append(dic) timeList,trxList=[],[] for i in dicList: timeList.append(i[&#x27;time&#x27;]) trxList.append(i[&#x27;trx&#x27;]) dataList=list(zip(timeList,trxList)) return dataList options = &#123; &#x27;title&#x27;:&#123;&#x27;text&#x27;:&#x27;TPCC-MySQL&#x27;&#125;, &#x27;yAxis&#x27;:&#123;&#x27;title&#x27;:&#123;&#x27;text&#x27;:&#x27;trx&#x27;&#125;&#125; &#125;charts.plot(getData(&#x27;C:\\\\Users\\\\Fan\\\\tpcc.txt&#x27;),name=&#x27;10 warehouse 10 connect&#x27;,options=options,show=&#x27;inline&#x27;,) 不过要注意的是 这里是 trx是10秒的累计,每十秒一个trx 关于tpcc_loadtpcc_load用上面的方法是很慢的,实际上tpcc-mysql/下有一个脚本load_multi_schema.sh. 修改一下后12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455tpcc_load -h127.0.0.1 -d tpcc10 -u root -p &quot;&quot; -w 1000各列含义为： |hostname:port| |dbname| |user| |password| |WAREHOUSES选项 warehouse 意为指定测试库下的仓库数量, 在生产环境中建议设置至少100以上$1 将要创建的数据库名$2 仓库数量$3 schema数量,这里为1$4 数据库ip这里mysql用户名密码socket写死了[root@master2 tpcc-mysql]# more load_multi_schema.sh #export LD_LIBRARY_PATH=/data/opt/bin/mysql-5.7.11-linux-glibc2.5-x86_64/lib/export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mysql/lib/DBNAME=$1WH=$2NSCHEMA=$3HOST=$4STEP=5 #step应该小于WHschema=0while [ $schema -lt $NSCHEMA ] #主循环do DBFULLNAME=$&#123;DBNAME&#125;_$&#123;schema&#125; echo &quot;Creating schema $DBFULLNAME&quot; mysqladmin -umysql -pmysql -S /data/mysqldata/3306/mysql.sock -f drop $DBFULLNAME mysqladmin -umysql -pmysql -S /data/mysqldata/3306/mysql.sock create $DBFULLNAME mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock $DBFULLNAME &lt; create_table.sql mysql -umysql -pmysql -S /data/mysqldata/3306/mysql.sock $DBFULLNAME &lt; add_fkey_idx.sql mkdir -p out#以上是创建数据库,跑初始化脚本./tpcc_load -h$HOST -d $DBFULLNAME -u mysql -p mysql -w $WH -l 1 -m 1 -n $WH &gt;&gt; out/1_$DBFULLNAME.out &amp;#Usage: tpcc_load -h server_host -P port -d database_name -u mysql_user -p mysql_password -w warehouses -l part -m min_wh -n max_wh#* [part]: 1=ITEMS 2=WAREHOUSE 3=CUSTOMER 4=ORDERS#-m 1 就是建ITEMSx=1#在循环里建剩下的-m 1 -n 步长, 一次循环建多少个while [ $x -le $WH ] #主循环里的嵌套循环do echo $x $(( $x + $STEP - 1 ))./tpcc_load -h$HOST -d $DBFULLNAME -u mysql -p mysql -w $WH -l 2 -m $x -n $(( $x + $STEP - 1 )) &gt;&gt; out/2_$DBFULLNAME.$x.out &amp;./tpcc_load -h$HOST -d $DBFULLNAME -u mysql -p mysql -w $WH -l 3 -m $x -n $(( $x + $STEP - 1 )) &gt;&gt; out/3_$DBFULLNAME.$x.out &amp;./tpcc_load -h$HOST -d $DBFULLNAME -u mysql -p mysql -w $WH -l 4 -m $x -n $(( $x + $STEP - 1 )) &gt;&gt; out/4_$DBFULLNAME.$x.out &amp; x=$(( $x + $STEP ))donefor job in `jobs -p`doecho $job wait $jobdoneschema=$(( $schema + 1 ))done现在load是并行的12345678910111213141516171819[mysql@master2 tpcc-mysql]$ ./load_multi_schema.sh tpcc 10 1 192.168.98.128Creating schema tpcc_0Warning: Using a password on the command line interface can be insecure.Database &quot;tpcc_0&quot; droppedWarning: Using a password on the command line interface can be insecure.Warning: Using a password on the command line interface can be insecure.Warning: Using a password on the command line interface can be insecure.1 56 1028496284972849828499285002850128502[mysql@master2 tpcc-mysql]$ out目录有日志","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"Tpcc-MySQL","slug":"Tpcc-MySQL","permalink":"http://fuxkdb.com/tags/Tpcc-MySQL/"}]},{"title":"MySQL升级","slug":"MySQL升级","date":"2017-03-03T14:00:00.000Z","updated":"2020-09-24T03:35:06.875Z","comments":true,"path":"2017/03/03/MySQL升级/","link":"","permalink":"http://fuxkdb.com/2017/03/03/MySQL%E5%8D%87%E7%BA%A7/","excerpt":"Upgrading Mysql 支持的升级方法 支持的升级路径 Before You Begin Performing an In-Place Upgrade Supported Upgrade Method支持的升级方法包括: In-Place Upgrade:涉及关闭旧版本的MySQL,将旧的MySQL二进制文件或软件包替换成新版本的MySQL,重新启动MySQL使用原来的data directory,并执行mysql_upgrade. Logical Upgrade:涉及使用mysqldump导出旧版本MySQL的所有数据,安装新版本的MySQL,导入dump文件,并执行mysql_upgrade. 支持的升级路径除非另有说明，否则支持以下升级路径： Upgrading from a release series version to a newer release series version is supported.可以从5.7.9升级到5.7.10,或者直接从5.7.9升级到5.7.11 Upgrading one release level is supported. For example, upgrading from 5.6 to 5.7 is supported.可以从5.6升级到5.7 Upgrading more than one release level is supported, but only if you upgrade one release level at a time.升级多个版本是支持的但只能一次升级一个版本,比如5.5 先升级 5.6 再升级到5.7 直接从5.5升级到5.7是不建议且不支持的以下条件适用于所有升级路径： 支持通用可用性（GA）状态版本之间的升级。 不支持在里程碑版本之间（或从里程碑版本到GA版本）的升级。 例如，不支持从5.7.7升级到5.7.8，因为GA状态释放也不受支持(这段可能写错了)。 对于已达到GA状态的MySQL版本系列的版本之间的升级，可以在具有相同架构的系统上在不同版本之间移动MySQL格式文件和数据文件。 这对于在里程碑版本之间的升级不一定是真的。 使用里程碑版本是您自己的风险。 Before You Begin升级之前,建议查看如下信息,并执行建议的步骤: 升级前,备份数据,尤其是system库. 请查看版本说明，提供有关MySQL 5.7中新增功能的信息或与早期MySQL版本中发现的功能不同的信息。 其中一些更改可能导致不兼容。对于已在MySQL 5.7中添加，弃用或删除的MySQL服务器变量和选项的列表，请参见第1.5节“在MySQL 5.7中添加，弃用或删除的服务器和状态变量及选项”。如果使用以上任何项目，升级需要更改配置。 查看第2.11.1.1节“影响升级到MySQL 5.7的更改”。本节介绍可能需要在升级之前或之后执行操作的更改。 检查第2.11.3节“检查表或索引是否必须重新构建”，以了解您当前版本的MySQL与要升级的版本之间是否对表格式或字符集或归类进行了更改。如果这些更改导致MySQL版本之间不兼容，则需要使用第2.11.4节“重建或修复表或索引”中的说明升级受影响的表。 如果使用复制，请查看第18.4.3节“升级复制设置”。 如果您使用XA事务与InnoDB，请在升级之前运行XA RECOVER以检查未提交的XA事务。如果返回结果，则通过发出XA COMMIT或XA ROLLBACK语句来提交或回滚XA事务。 如果您的MySQL安装包含大量数据，在就地升级后可能需要很长时间进行转换，您可能会发现创建一个“虚拟”数据库实例，用于评估可能需要的转换和涉及的工作执行它们。制作一份包含mysql数据库完整副本的MySQL实例的副本，以及没有数据的所有其他数据库。在此虚拟实例上运行升级过程，以查看可能需要执行哪些操作，以便在原始数据库实例上执行实际数据转换时，可以更好地评估所涉及的工作。 当您安装或升级到新版本的MySQL时，建议重建和重新安装MySQL语言接口。这适用于MySQL接口，如PHP mysql扩展，Perl DBD :: mysql模块和Python MySQLdb模块。","text":"Upgrading Mysql 支持的升级方法 支持的升级路径 Before You Begin Performing an In-Place Upgrade Supported Upgrade Method支持的升级方法包括: In-Place Upgrade:涉及关闭旧版本的MySQL,将旧的MySQL二进制文件或软件包替换成新版本的MySQL,重新启动MySQL使用原来的data directory,并执行mysql_upgrade. Logical Upgrade:涉及使用mysqldump导出旧版本MySQL的所有数据,安装新版本的MySQL,导入dump文件,并执行mysql_upgrade. 支持的升级路径除非另有说明，否则支持以下升级路径： Upgrading from a release series version to a newer release series version is supported.可以从5.7.9升级到5.7.10,或者直接从5.7.9升级到5.7.11 Upgrading one release level is supported. For example, upgrading from 5.6 to 5.7 is supported.可以从5.6升级到5.7 Upgrading more than one release level is supported, but only if you upgrade one release level at a time.升级多个版本是支持的但只能一次升级一个版本,比如5.5 先升级 5.6 再升级到5.7 直接从5.5升级到5.7是不建议且不支持的以下条件适用于所有升级路径： 支持通用可用性（GA）状态版本之间的升级。 不支持在里程碑版本之间（或从里程碑版本到GA版本）的升级。 例如，不支持从5.7.7升级到5.7.8，因为GA状态释放也不受支持(这段可能写错了)。 对于已达到GA状态的MySQL版本系列的版本之间的升级，可以在具有相同架构的系统上在不同版本之间移动MySQL格式文件和数据文件。 这对于在里程碑版本之间的升级不一定是真的。 使用里程碑版本是您自己的风险。 Before You Begin升级之前,建议查看如下信息,并执行建议的步骤: 升级前,备份数据,尤其是system库. 请查看版本说明，提供有关MySQL 5.7中新增功能的信息或与早期MySQL版本中发现的功能不同的信息。 其中一些更改可能导致不兼容。对于已在MySQL 5.7中添加，弃用或删除的MySQL服务器变量和选项的列表，请参见第1.5节“在MySQL 5.7中添加，弃用或删除的服务器和状态变量及选项”。如果使用以上任何项目，升级需要更改配置。 查看第2.11.1.1节“影响升级到MySQL 5.7的更改”。本节介绍可能需要在升级之前或之后执行操作的更改。 检查第2.11.3节“检查表或索引是否必须重新构建”，以了解您当前版本的MySQL与要升级的版本之间是否对表格式或字符集或归类进行了更改。如果这些更改导致MySQL版本之间不兼容，则需要使用第2.11.4节“重建或修复表或索引”中的说明升级受影响的表。 如果使用复制，请查看第18.4.3节“升级复制设置”。 如果您使用XA事务与InnoDB，请在升级之前运行XA RECOVER以检查未提交的XA事务。如果返回结果，则通过发出XA COMMIT或XA ROLLBACK语句来提交或回滚XA事务。 如果您的MySQL安装包含大量数据，在就地升级后可能需要很长时间进行转换，您可能会发现创建一个“虚拟”数据库实例，用于评估可能需要的转换和涉及的工作执行它们。制作一份包含mysql数据库完整副本的MySQL实例的副本，以及没有数据的所有其他数据库。在此虚拟实例上运行升级过程，以查看可能需要执行哪些操作，以便在原始数据库实例上执行实际数据转换时，可以更好地评估所涉及的工作。 当您安装或升级到新版本的MySQL时，建议重建和重新安装MySQL语言接口。这适用于MySQL接口，如PHP mysql扩展，Perl DBD :: mysql模块和Python MySQLdb模块。 执行In-Place升级要执行In-Place升级:1.查看在2.11.1.1章节,”Changes Affecting Upgrades MySQL5.7”描述的升级所造成的改变https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html2.Configure MySQL to perform a slow shutdown by setting innodb_fast_shutdown to 0. For example:1mysql -u root -p --execute=&quot;SET GLOBAL innodb_fast_shutdown=0&quot; With a slow shutdown, InnoDB performs a full purge and change buffer merge before shutting down, which ensures that data files are fully prepared in case of file format differences between releases.3.关闭数据库1mysqladmin -u root -p shutdown 4.将MySQL二进制文件或软件包替换成新的就是说把旧版本的mysql软件换成新的1234lrwxrwxrwx 1 root mysql 47 Mar 3 03:53 mysql -&gt; mysql-5.7.17/mysql-5.7.17-linux-glibc2.5-x86_64drwxr-xr-x 13 root root 4096 Mar 3 03:44 mysql55drwxrwxr-x 13 root mysql 4096 Dec 15 2014 mysql5.6drwxr-xr-x 3 root root 4096 Mar 3 03:42 mysql-5.7.17 这里可以这样做,替换的时候使用ln -s 建一个连接,这样每次替换都只是改变连接取消连接unlink5.启动新版本的MySQL1mysqld_safe --user=mysql --datadir=/path/to/existing-datadir 这里要保证数据文件目录指定对了6.执行mysql_upgrade1mysql_upgrade -u -p mysql_upgrade检查所有数据库中的所有表与当前版本的MySQL不兼容。 mysql_upgrade还升级mysql系统数据库，以便您可以利用新的权限或功能。 翻译有问题,反正就是要检查所有数据库的所有表,所以你的数据库如果很大的话,这个升级过程会非常久.可以使用 -s选项只升级系统表 Notemysql_upgrade should not be used when the server is running with –gtid-mode=ON. See GTID mode and mysql_upgrade for more information. mysql_upgrade does not upgrade the contents of the help tables. For upgrade instructions, see Section 6.1.10, “Server-Side Help”. 7.关闭并重启MySQL确保对系统标的改变生效12mysqladmin -u root -p shutdownmysqld_safe --user=mysql --datadir=/path/to/existing-datadir","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL升级","slug":"MySQL升级","permalink":"http://fuxkdb.com/tags/MySQL%E5%8D%87%E7%BA%A7/"}]},{"title":"MySQL误DROP表恢复,innodb_file_per_table=OFF","slug":"MySQL误DROP表恢复,innodb_file_per_table=OFF","date":"2016-12-27T14:00:00.000Z","updated":"2017-08-05T16:01:57.000Z","comments":true,"path":"2016/12/27/MySQL误DROP表恢复,innodb_file_per_table=OFF/","link":"","permalink":"http://fuxkdb.com/2016/12/27/MySQL%E8%AF%AFDROP%E8%A1%A8%E6%81%A2%E5%A4%8D,innodb_file_per_table=OFF/","excerpt":"MySQL误DROP表恢复,innodb_file_per_table=OFF原文https://twindb.com/recover-after-drop-table-innodb_file_per_table-is-off/ Undrop-for-innodb安装详见https://github.com/chhabhaiya/undrop-for-innodb 介绍误操作是不可避免的.错误的DROP DATABASE或DROP TABLE会摧毁重要的数据,更悲剧的是备份又不可用.恢复方法取决于InnoDB是将所有数据保存在单个ibdata1中还是每个表都有自己的表空间.在这篇文章中,我们将考虑innodb_file_per_table = OFF的情况.此选项假定所有表都存储在通用文件中,通常位于/var/lib/mysql/ibdata1. 误删表恢复这里使用sakila库,假设我们误删除了actor表123456789101112131415161718192021222324252627282930313233343536mysql&gt; SELECT * FROM actor LIMIT 10;+----------+------------+--------------+---------------------+| actor_id | first_name | last_name | last_update |+----------+------------+--------------+---------------------+| 1 | PENELOPE | GUINESS | 2006-02-15 04:34:33 || 2 | NICK | WAHLBERG | 2006-02-15 04:34:33 || 3 | ED | CHASE | 2006-02-15 04:34:33 || 4 | JENNIFER | DAVIS | 2006-02-15 04:34:33 || 5 | JOHNNY | LOLLOBRIGIDA | 2006-02-15 04:34:33 || 6 | BETTE | NICHOLSON | 2006-02-15 04:34:33 || 7 | GRACE | MOSTEL | 2006-02-15 04:34:33 || 8 | MATTHEW | JOHANSSON | 2006-02-15 04:34:33 || 9 | JOE | SWANK | 2006-02-15 04:34:33 || 10 | CHRISTIAN | GABLE | 2006-02-15 04:34:33 |+----------+------------+--------------+---------------------+10 rows in set (0.02 sec)查看一下checksummysql&gt; CHECKSUM TABLE actor;+--------------+------------+| Table | Checksum |+--------------+------------+| sakila.actor | 1702520518 |+--------------+------------+1 row in set (0.01 sec)mysql&gt; select count(*) from actor;+----------+| count(*) |+----------+| 200 |+----------+1 row in set (0.01 sec)mysql&gt; DROP TABLE actor;Query OK, 0 rows affected (0.03 sec)","text":"MySQL误DROP表恢复,innodb_file_per_table=OFF原文https://twindb.com/recover-after-drop-table-innodb_file_per_table-is-off/ Undrop-for-innodb安装详见https://github.com/chhabhaiya/undrop-for-innodb 介绍误操作是不可避免的.错误的DROP DATABASE或DROP TABLE会摧毁重要的数据,更悲剧的是备份又不可用.恢复方法取决于InnoDB是将所有数据保存在单个ibdata1中还是每个表都有自己的表空间.在这篇文章中,我们将考虑innodb_file_per_table = OFF的情况.此选项假定所有表都存储在通用文件中,通常位于/var/lib/mysql/ibdata1. 误删表恢复这里使用sakila库,假设我们误删除了actor表123456789101112131415161718192021222324252627282930313233343536mysql&gt; SELECT * FROM actor LIMIT 10;+----------+------------+--------------+---------------------+| actor_id | first_name | last_name | last_update |+----------+------------+--------------+---------------------+| 1 | PENELOPE | GUINESS | 2006-02-15 04:34:33 || 2 | NICK | WAHLBERG | 2006-02-15 04:34:33 || 3 | ED | CHASE | 2006-02-15 04:34:33 || 4 | JENNIFER | DAVIS | 2006-02-15 04:34:33 || 5 | JOHNNY | LOLLOBRIGIDA | 2006-02-15 04:34:33 || 6 | BETTE | NICHOLSON | 2006-02-15 04:34:33 || 7 | GRACE | MOSTEL | 2006-02-15 04:34:33 || 8 | MATTHEW | JOHANSSON | 2006-02-15 04:34:33 || 9 | JOE | SWANK | 2006-02-15 04:34:33 || 10 | CHRISTIAN | GABLE | 2006-02-15 04:34:33 |+----------+------------+--------------+---------------------+10 rows in set (0.02 sec)查看一下checksummysql&gt; CHECKSUM TABLE actor;+--------------+------------+| Table | Checksum |+--------------+------------+| sakila.actor | 1702520518 |+--------------+------------+1 row in set (0.01 sec)mysql&gt; select count(*) from actor;+----------+| count(*) |+----------+| 200 |+----------+1 row in set (0.01 sec)mysql&gt; DROP TABLE actor;Query OK, 0 rows affected (0.03 sec) 从ibdata1恢复误删除表虽然表被删了,但是数据可能依然存在于ibdata1中,直到这部分数据被覆盖.所以要赶紧停止MySQL1234[mysql@master 3307]$ mysqladmin -umysql -p -S /data/mysqldata/3307/mysql.sock shutdownEnter password: 161227 14:13:59 mysqld_safe mysqld from pid file /data/mysqldata/3307/mysql.pid ended[1]+ Done mysqld_safe --defaults-file=/data/mysqldata/3307/my.cnf 解析InnoDB tablespaceInnoDB通过B+树存储数据.每个表都通过主键聚簇,所有字段数据都存储在索引叶块中.如果表有secondary index,则每个键具有索引.每个索引通过index_id标识.如果我们想要恢复一个表,就要找出属于特定index_id的所有pagestream_parser读取InnoDB tablespace,并按照type和index_id排序InnoDB pages stream_parser reads InnoDB tablespace and sorts InnoDB pages per type and per index_id. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[mysql@master undrop-for-innodb]$ ./stream_parser -f /data/mysqldata/3307/data/ibdata1 Opening file: /data/mysqldata/3307/data/ibdata1File information:ID of device containing file: 64768inode number: 104900755protection: 100660 (regular file)number of hard links: 1user ID of owner: 27group ID of owner: 27device ID (if special file): 0blocksize for filesystem I/O: 4096number of blocks allocated: 4194304time of last access: 1482819237 Tue Dec 27 14:13:57 2016time of last modification: 1482819237 Tue Dec 27 14:13:57 2016time of last status change: 1482819237 Tue Dec 27 14:13:57 2016total size, in bytes: 2147483648 (2.000 GiB)Size to process: 2147483648 (2.000 GiB)Worker(0): 5.47% done. 2016-12-27 14:24:21 ETA(in 00:00:18). Processing speed: 104.000 MiB/secWorker(0): 13.67% done. 2016-12-27 14:24:14 ETA(in 00:00:10). Processing speed: 168.000 MiB/secWorker(0): 22.66% done. 2016-12-27 14:24:13 ETA(in 00:00:08). Processing speed: 184.000 MiB/secWorker(0): 28.52% done. 2016-12-27 14:24:18 ETA(in 00:00:12). Processing speed: 120.000 MiB/secWorker(0): 35.94% done. 2016-12-27 14:24:15 ETA(in 00:00:08). Processing speed: 152.000 MiB/secWorker(0): 43.75% done. 2016-12-27 14:24:15 ETA(in 00:00:07). Processing speed: 160.000 MiB/secWorker(0): 51.17% done. 2016-12-27 14:24:15 ETA(in 00:00:06). Processing speed: 152.000 MiB/secWorker(0): 57.81% done. 2016-12-27 14:24:16 ETA(in 00:00:06). Processing speed: 136.000 MiB/secWorker(0): 64.45% done. 2016-12-27 14:24:16 ETA(in 00:00:05). Processing speed: 136.000 MiB/secWorker(0): 71.48% done. 2016-12-27 14:24:16 ETA(in 00:00:04). Processing speed: 144.000 MiB/secWorker(0): 78.91% done. 2016-12-27 14:24:15 ETA(in 00:00:02). Processing speed: 152.000 MiB/secWorker(0): 85.16% done. 2016-12-27 14:24:16 ETA(in 00:00:02). Processing speed: 128.000 MiB/secWorker(0): 92.19% done. 2016-12-27 14:24:16 ETA(in 00:00:01). Processing speed: 144.000 MiB/secAll workers finished in 13 secData from database pages is saved by the stream_parser to folder pages-ibdata1:[mysql@master undrop-for-innodb]$ tree pages-ibdata1/pages-ibdata1/├── FIL_PAGE_INDEX│ ├── 0000000000000001.page│ ├── 0000000000000002.page│ ├── 0000000000000003.page│ ├── 0000000000000004.page│ ├── 0000000000000005.page│ ├── 0000000000000011.page│ ├── 0000000000000012.page│ ├── 0000000000000013.page│ ├── 0000000000000014.page│ ├── 0000000000000015.page│ ├── 0000000000000016.page│ ├── 0000000000000017.page│ ├── 0000000000000018.page│ ├── 0000000000000019.page│ ├── 0000000000000020.page│ ├── 0000000000000021.page│ ├── 0000000000000022.page│ ├── 0000000000000023.page│ └── 18446744069414584320.page└── FIL_PAGE_TYPE_BLOB2 directories, 19 files 现在从InnoDB tablespace取出的每个index_id都被保存在单独的文件中.我们可以使用c_parser从page中取回数据,但是我们需要知道actor表的index_id是多少.我们可以通过字典表SYS_TABLES和SYS_INDEXES中查出actor表的信息 SYS_TABLES字典表总是存储在index_id为1的文件中,即pages-ibdata1/FIL_PAGE_INDEX./0000000000000001.page If MySQL had enough time to flush changes to disk then add -D option which means “find deleted records”. The dictionary is always in REDUNDANT format, so we specify option -4: 12345[mysql@master undrop-for-innodb]$ ./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000001.page -t dictionary/SYS_TABLES.sql | grep sakila/actorSET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/SYS_TABLES&#x27; REPLACE INTO TABLE `SYS_TABLES` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;SYS_TABLES\\t&#x27; (`NAME`, `ID`, `N_COLS`, `TYPE`, `MIX_ID`, `MIX_LEN`, `CLUSTER_NAME`, `SPACE`);000000000712 0F0000014802C8 SYS_TABLES &quot;sakila/actor&quot; 20 4 1 0 64 &quot;&quot; 0000000000712 0F0000014802C8 SYS_TABLES &quot;sakila/actor&quot; 20 4 1 0 64 &quot;&quot; 0 在表名”sakila/actor”后面的列就是table_id列,这里查出actor表的table_id是20接着要查出actor表的主键索引的index_id,通过解析0000000000000003.page文件可以取出SYS_INDEXES字典表的数据(这个表包含index_id和table_id信息).1234567[mysql@master undrop-for-innodb]$ ./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000003.page -t dictionary/SYS_INDEXES.sql | grep 20SET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/SYS_INDEXES&#x27; REPLACE INTO TABLE `SYS_INDEXES` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;SYS_INDEXES\\t&#x27; (`TABLE_ID`, `ID`, `NAME`, `N_FIELDS`, `TYPE`, `SPACE`, `PAGE_NO`);000000000712 0F000001480145 SYS_INDEXES 20 22 &quot;PRIMARY&quot; 1 3 0 4294967295000000000712 0F0000014801B7 SYS_INDEXES 20 23 &quot;idx\\_actor\\_last\\_name&quot; 1 0 0 4294967295000000000712 0F000001480145 SYS_INDEXES 20 22 &quot;PRIMARY&quot; 1 3 0 4294967295000000000712 0F0000014801B7 SYS_INDEXES 20 23 &quot;idx\\_actor\\_last\\_name&quot; 1 0 0 4294967295从结果可以看出,主键的index_id是22.因此我们将从0000000000000022.page文件中取回actor表的数据,需要actor表的建表语句123456[mysql@master undrop-for-innodb]$ ./c_parser -6f pages-ibdata1/FIL_PAGE_INDEX/0000000000000022.page -t sakila/actor.sql | head -5-- Page id: 320, Format: COMPACT, Records list: Valid, Expected records: (200 200)000000000709 89000001430110 actor 1 &quot;PENELOPE&quot; &quot;GUINESS&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143011A actor 2 &quot;NICK&quot; &quot;WAHLBERG&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430124 actor 3 &quot;ED&quot; &quot;CHASE&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143012E actor 4 &quot;JENNIFER&quot; &quot;DAVIS&quot; &quot;2006-02-15 04:34:33&quot;从结果来看我们找到了数据,下面通过c_parser生成load data infile语句和数据1234[mysql@master undrop-for-innodb]$ mkdir -p dumps/default[mysql@master undrop-for-innodb]$ ./c_parser -6f pages-ibdata1/FIL_PAGE_INDEX/0000000000000022.page -t sakila/actor.sql &gt; dumps/default/actor 2&gt; dumps/default/actor_load.sql[mysql@master undrop-for-innodb]$ ls dumps/default/actor actor_load.sql.sql文件包含load data语句123[mysql@master undrop-for-innodb]$ cat dumps/default/actor_load.sqlSET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/actor&#x27; REPLACE INTO TABLE `actor` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;actor\\t&#x27; (`actor_id`, `first_name`, `last_name`, `last_update`);另一个文件包含数据12345678910111213[mysql@master undrop-for-innodb]$ more dumps/default/actor-- Page id: 320, Format: COMPACT, Records list: Valid, Expected records: (200 200)000000000709 89000001430110 actor 1 &quot;PENELOPE&quot; &quot;GUINESS&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143011A actor 2 &quot;NICK&quot; &quot;WAHLBERG&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430124 actor 3 &quot;ED&quot; &quot;CHASE&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143012E actor 4 &quot;JENNIFER&quot; &quot;DAVIS&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430138 actor 5 &quot;JOHNNY&quot; &quot;LOLLOBRIGIDA&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430142 actor 6 &quot;BETTE&quot; &quot;NICHOLSON&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143014C actor 7 &quot;GRACE&quot; &quot;MOSTEL&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430156 actor 8 &quot;MATTHEW&quot; &quot;JOHANSSON&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430160 actor 9 &quot;JOE&quot; &quot;SWANK&quot; &quot;2006-02-15 04:34:33&quot;000000000709 8900000143016A actor 10 &quot;CHRISTIAN&quot; &quot;GABLE&quot; &quot;2006-02-15 04:34:33&quot;000000000709 89000001430174 actor 11 &quot;ZERO&quot; &quot;CAGE&quot; &quot;2006-02-15 04:34:33&quot; ##恢复数据启动数据库1mysqld_safe --defaults-file=/data/mysqldata/3307/my.cnf &amp;创建表123mysql&gt; use sakilaDatabase changedmysql&gt; source sakila/actor.sqlload回数据12345678910111213141516171819202122mysql&gt; source dumps/default/actor_load.sqlQuery OK, 0 rows affected (0.00 sec)Query OK, 400 rows affected (0.01 sec)Records: 400 Deleted: 0 Skipped: 0 Warnings: 0mysql&gt; select count(*) from actor;+----------+| count(*) |+----------+| 200 |+----------+1 row in set (0.00 sec)checksum和之前也是一样的mysql&gt; CHECKSUM TABLE actor;+--------------+------------+| Table | Checksum |+--------------+------------+| sakila.actor | 1702520518 |+--------------+------------+1 row in set (0.00 sec)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"恢复表结构","slug":"恢复表结构","date":"2016-12-27T14:00:00.000Z","updated":"2017-08-05T16:00:29.000Z","comments":true,"path":"2016/12/27/恢复表结构/","link":"","permalink":"http://fuxkdb.com/2016/12/27/%E6%81%A2%E5%A4%8D%E8%A1%A8%E7%BB%93%E6%9E%84/","excerpt":"恢复表结构此文档适用于.frm文件丢失,并且没有备份情况下恢复出建表语句 恢复InnoDB字典表首先创建用于恢复的字典辅助表12345678910111213141516mysql&gt; create database sakila_recovered;Query OK, 1 row affected (0.01 sec)[mysql@master undrop-for-innodb]$ cat dictionary/SYS_* | mysql -umysql -pmysql -S /data/mysqldata/3307/mysql.sock sakila_recoveredWarning: Using a password on the command line interface can be insecure.mysql&gt; show tables;+----------------------------+| Tables_in_sakila_recovered |+----------------------------+| SYS_COLUMNS || SYS_FIELDS || SYS_INDEXES || SYS_TABLES |+----------------------------+4 rows in set (0.00 sec)字典表存储在ibdata1中,所以要解析它123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[mysql@master undrop-for-innodb]$ ./stream_parser -f /data/mysqldata/3307/data/ibdata1 Opening file: /data/mysqldata/3307/data/ibdata1File information:ID of device containing file: 64768inode number: 104900755protection: 100660 (regular file)number of hard links: 1user ID of owner: 27group ID of owner: 27device ID (if special file): 0blocksize for filesystem I/O: 4096number of blocks allocated: 4194304time of last access: 1482819237 Tue Dec 27 14:13:57 2016time of last modification: 1482819237 Tue Dec 27 14:13:57 2016time of last status change: 1482819237 Tue Dec 27 14:13:57 2016total size, in bytes: 2147483648 (2.000 GiB)Size to process: 2147483648 (2.000 GiB)Worker(0): 5.47% done. 2016-12-27 14:24:21 ETA(in 00:00:18). Processing speed: 104.000 MiB/secWorker(0): 13.67% done. 2016-12-27 14:24:14 ETA(in 00:00:10). Processing speed: 168.000 MiB/secWorker(0): 22.66% done. 2016-12-27 14:24:13 ETA(in 00:00:08). Processing speed: 184.000 MiB/secWorker(0): 28.52% done. 2016-12-27 14:24:18 ETA(in 00:00:12). Processing speed: 120.000 MiB/secWorker(0): 35.94% done. 2016-12-27 14:24:15 ETA(in 00:00:08). Processing speed: 152.000 MiB/secWorker(0): 43.75% done. 2016-12-27 14:24:15 ETA(in 00:00:07). Processing speed: 160.000 MiB/secWorker(0): 51.17% done. 2016-12-27 14:24:15 ETA(in 00:00:06). Processing speed: 152.000 MiB/secWorker(0): 57.81% done. 2016-12-27 14:24:16 ETA(in 00:00:06). Processing speed: 136.000 MiB/secWorker(0): 64.45% done. 2016-12-27 14:24:16 ETA(in 00:00:05). Processing speed: 136.000 MiB/secWorker(0): 71.48% done. 2016-12-27 14:24:16 ETA(in 00:00:04). Processing speed: 144.000 MiB/secWorker(0): 78.91% done. 2016-12-27 14:24:15 ETA(in 00:00:02). Processing speed: 152.000 MiB/secWorker(0): 85.16% done. 2016-12-27 14:24:16 ETA(in 00:00:02). Processing speed: 128.000 MiB/secWorker(0): 92.19% done. 2016-12-27 14:24:16 ETA(in 00:00:01). Processing speed: 144.000 MiB/secAll workers finished in 13 secData from database pages is saved by the stream_parser to folder pages-ibdata1:[mysql@master undrop-for-innodb]$ tree pages-ibdata1/pages-ibdata1/├── FIL_PAGE_INDEX│ ├── 0000000000000001.page│ ├── 0000000000000002.page│ ├── 0000000000000003.page│ ├── 0000000000000004.page│ ├── 0000000000000005.page│ ├── 0000000000000011.page│ ├── 0000000000000012.page│ ├── 0000000000000013.page│ ├── 0000000000000014.page│ ├── 0000000000000015.page│ ├── 0000000000000016.page│ ├── 0000000000000017.page│ ├── 0000000000000018.page│ ├── 0000000000000019.page│ ├── 0000000000000020.page│ ├── 0000000000000021.page│ ├── 0000000000000022.page│ ├── 0000000000000023.page│ └── 18446744069414584320.page└── FIL_PAGE_TYPE_BLOB2 directories, 19 files","text":"恢复表结构此文档适用于.frm文件丢失,并且没有备份情况下恢复出建表语句 恢复InnoDB字典表首先创建用于恢复的字典辅助表12345678910111213141516mysql&gt; create database sakila_recovered;Query OK, 1 row affected (0.01 sec)[mysql@master undrop-for-innodb]$ cat dictionary/SYS_* | mysql -umysql -pmysql -S /data/mysqldata/3307/mysql.sock sakila_recoveredWarning: Using a password on the command line interface can be insecure.mysql&gt; show tables;+----------------------------+| Tables_in_sakila_recovered |+----------------------------+| SYS_COLUMNS || SYS_FIELDS || SYS_INDEXES || SYS_TABLES |+----------------------------+4 rows in set (0.00 sec)字典表存储在ibdata1中,所以要解析它123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[mysql@master undrop-for-innodb]$ ./stream_parser -f /data/mysqldata/3307/data/ibdata1 Opening file: /data/mysqldata/3307/data/ibdata1File information:ID of device containing file: 64768inode number: 104900755protection: 100660 (regular file)number of hard links: 1user ID of owner: 27group ID of owner: 27device ID (if special file): 0blocksize for filesystem I/O: 4096number of blocks allocated: 4194304time of last access: 1482819237 Tue Dec 27 14:13:57 2016time of last modification: 1482819237 Tue Dec 27 14:13:57 2016time of last status change: 1482819237 Tue Dec 27 14:13:57 2016total size, in bytes: 2147483648 (2.000 GiB)Size to process: 2147483648 (2.000 GiB)Worker(0): 5.47% done. 2016-12-27 14:24:21 ETA(in 00:00:18). Processing speed: 104.000 MiB/secWorker(0): 13.67% done. 2016-12-27 14:24:14 ETA(in 00:00:10). Processing speed: 168.000 MiB/secWorker(0): 22.66% done. 2016-12-27 14:24:13 ETA(in 00:00:08). Processing speed: 184.000 MiB/secWorker(0): 28.52% done. 2016-12-27 14:24:18 ETA(in 00:00:12). Processing speed: 120.000 MiB/secWorker(0): 35.94% done. 2016-12-27 14:24:15 ETA(in 00:00:08). Processing speed: 152.000 MiB/secWorker(0): 43.75% done. 2016-12-27 14:24:15 ETA(in 00:00:07). Processing speed: 160.000 MiB/secWorker(0): 51.17% done. 2016-12-27 14:24:15 ETA(in 00:00:06). Processing speed: 152.000 MiB/secWorker(0): 57.81% done. 2016-12-27 14:24:16 ETA(in 00:00:06). Processing speed: 136.000 MiB/secWorker(0): 64.45% done. 2016-12-27 14:24:16 ETA(in 00:00:05). Processing speed: 136.000 MiB/secWorker(0): 71.48% done. 2016-12-27 14:24:16 ETA(in 00:00:04). Processing speed: 144.000 MiB/secWorker(0): 78.91% done. 2016-12-27 14:24:15 ETA(in 00:00:02). Processing speed: 152.000 MiB/secWorker(0): 85.16% done. 2016-12-27 14:24:16 ETA(in 00:00:02). Processing speed: 128.000 MiB/secWorker(0): 92.19% done. 2016-12-27 14:24:16 ETA(in 00:00:01). Processing speed: 144.000 MiB/secAll workers finished in 13 secData from database pages is saved by the stream_parser to folder pages-ibdata1:[mysql@master undrop-for-innodb]$ tree pages-ibdata1/pages-ibdata1/├── FIL_PAGE_INDEX│ ├── 0000000000000001.page│ ├── 0000000000000002.page│ ├── 0000000000000003.page│ ├── 0000000000000004.page│ ├── 0000000000000005.page│ ├── 0000000000000011.page│ ├── 0000000000000012.page│ ├── 0000000000000013.page│ ├── 0000000000000014.page│ ├── 0000000000000015.page│ ├── 0000000000000016.page│ ├── 0000000000000017.page│ ├── 0000000000000018.page│ ├── 0000000000000019.page│ ├── 0000000000000020.page│ ├── 0000000000000021.page│ ├── 0000000000000022.page│ ├── 0000000000000023.page│ └── 18446744069414584320.page└── FIL_PAGE_TYPE_BLOB2 directories, 19 files现在我们要从生成的page文件中抽取字典记录,我们县创建一个目录1mkdir -p dumps/defaultAnd now we can generate table dumps and LOAD INFILE commands to load the dumps. We also need to specify -D option to c_parser because the records we need were deleted from the dictionary when the table was dropped.现在开始恢复字典表SYS_TABLES1234./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000001.page \\ -t dictionary/SYS_TABLES.sql \\ &gt; dumps/default/SYS_TABLES \\ 2&gt; dumps/default/SYS_TABLES.sqlSYS_INDEXES1234./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000003.page \\ -t dictionary/SYS_INDEXES.sql \\ &gt; dumps/default/SYS_INDEXES \\ 2&gt; dumps/default/SYS_INDEXES.sqlSYS_COLUMNS1234./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000002.page \\ -t dictionary/SYS_COLUMNS.sql \\ &gt; dumps/default/SYS_COLUMNS \\ 2&gt; dumps/default/SYS_COLUMNS.sqlSYS_FIELDS1234./c_parser -4Df pages-ibdata1/FIL_PAGE_INDEX/0000000000000004.page \\ -t dictionary/SYS_FIELDS.sql \\ &gt; dumps/default/SYS_FIELDS \\ 2&gt; dumps/default/SYS_FIELDS.sql生成了包含数据的文件和包含load data infile语句的sql文件123[mysql@master undrop-for-innodb]$ ls dumps/default/SYS_COLUMNS SYS_FIELDS SYS_INDEXES SYS_TABLESSYS_COLUMNS.sql SYS_FIELDS.sql SYS_INDEXES.sql SYS_TABLES.sql恢复字典表数据cat dumps/default/*.sql | mysql -umysql -pmysql -S /data/mysqldata/3307/mysql.sock sakila_recovered 编译sys_parsersys_parser is a tool that reads dictionary from tables stored in MySQL and generates CREATE TABLE structure for a table. To compile it we will need MySQL libraries and development files. Depending on a distribution they may be in -devel or -dev package. On RedHat based system you can check it with command yum provides “*/mysql_config” . On my server it was package mysql-community-devel. If all necessary packages are installed compilation boils down to simple command:123[mysql@master undrop-for-innodb]$ make sys_parser/usr/local/mysql/bin/mysql_configcc -o sys_parser sys_parser.c `mysql_config --cflags` `mysql_config --libs` 恢复表结构12345678[mysql@master undrop-for-innodb]$ ./sys_parser -u mysql -p mysql -d sakila_recovered sakila/actorCREATE TABLE `actor`( `actor_id` SMALLINT UNSIGNED NOT NULL, `first_name` VARCHAR(45) CHARACTER SET &#x27;utf8&#x27; COLLATE &#x27;utf8_general_ci&#x27; NOT NULL, `last_name` VARCHAR(45) CHARACTER SET &#x27;utf8&#x27; COLLATE &#x27;utf8_general_ci&#x27; NOT NULL, `last_update` TIMESTAMP NOT NULL, PRIMARY KEY (`actor_id`)) ENGINE=InnoDB; 遗憾的是好像sys_parser不能指定sock,只能用默认端口号3306还有一些注意事项:1.InnoDB不存储您可以在frm文件中找到的所有信息。 例如，如果一个字段是AUTO_INCREMENT，InnoDB字典什么也不知道。因此，sys_parser将不会恢复该属性。 如果有任何字段或表级注释，他们将丢失2.sys_parser生成适合进一步数据恢复的表结构。 它可以但不会恢复二级索引，外键。3.InnoDB不存储DECIMAL类型作为二进制字符串。 它不存储DECIMAL字段的精度。 这样信息就会丢失。 For example, table payment uses DECIMAL to store money.1234567891011# ./sys_parser -u root -p qwerty -d sakila_recovered sakila/paymentCREATE TABLE `payment`( `payment_id` SMALLINT UNSIGNED NOT NULL, `customer_id` SMALLINT UNSIGNED NOT NULL, `staff_id` TINYINT UNSIGNED NOT NULL, `rental_id` INT, `amount` DECIMAL(6,0) NOT NULL, `payment_date` DATETIME NOT NULL, `last_update` TIMESTAMP NOT NULL, PRIMARY KEY (`payment_id`)) ENGINE=InnoDB;幸运的是，Oracle计划扩展InnoDB字典，最终摆脱了.frm文件。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"MySQL误DROP表恢复,innodb_file_per_table=ON","slug":"MySQL误DROP表恢复,innodb_file_per_table=ON","date":"2016-12-26T14:00:00.000Z","updated":"2017-08-05T16:02:05.000Z","comments":true,"path":"2016/12/26/MySQL误DROP表恢复,innodb_file_per_table=ON/","link":"","permalink":"http://fuxkdb.com/2016/12/26/MySQL%E8%AF%AFDROP%E8%A1%A8%E6%81%A2%E5%A4%8D,innodb_file_per_table=ON/","excerpt":"MySQL误DROP表恢复,innodb_file_per_table=ON 参考https://twindb.com/recover-after-drop-table-innodb_file_per_table-is-on/ Undrop-for-innodb安装详见https://github.com/chhabhaiya/undrop-for-innodb 删除Country表并恢复使用sakila示例数据库作为例子,sakila相当于Oracle的SCOTTinnodb每个表包含两个文件.frm文件包含建表语句.ibd文件包含数据123[mysql@master sakila]$ ll country.*-rw-r----- 1 mysql mysql 8652 Aug 26 14:25 country.frm-rw-r----- 1 mysql mysql 98304 Aug 26 14:25 country.ibd 接下来删除country表,删之前看一下checksum1234567891011121314151617SESSION_A&gt; use sakilaDatabase changedSESSION_A&gt; select count(*) from country;+----------+| count(*) |+----------+| 109 |+----------+1 row in set (0.01 sec)SESSION_A&gt;checksum table country;+---------------------+------------+| Table | Checksum |+---------------------+------------+| sakila.country | 2039770088 |+---------------------+------------+1 row in set (0.00 sec)接下来删除country表12SESSION_A&gt;drop table country;Query OK, 0 rows affected (0.01 sec) drop table后,.frm和.ibd文件也被删除了12[mysql@master sakila]$ ll country*ls: cannot access country*: No such file or directory","text":"MySQL误DROP表恢复,innodb_file_per_table=ON 参考https://twindb.com/recover-after-drop-table-innodb_file_per_table-is-on/ Undrop-for-innodb安装详见https://github.com/chhabhaiya/undrop-for-innodb 删除Country表并恢复使用sakila示例数据库作为例子,sakila相当于Oracle的SCOTTinnodb每个表包含两个文件.frm文件包含建表语句.ibd文件包含数据123[mysql@master sakila]$ ll country.*-rw-r----- 1 mysql mysql 8652 Aug 26 14:25 country.frm-rw-r----- 1 mysql mysql 98304 Aug 26 14:25 country.ibd 接下来删除country表,删之前看一下checksum1234567891011121314151617SESSION_A&gt; use sakilaDatabase changedSESSION_A&gt; select count(*) from country;+----------+| count(*) |+----------+| 109 |+----------+1 row in set (0.01 sec)SESSION_A&gt;checksum table country;+---------------------+------------+| Table | Checksum |+---------------------+------------+| sakila.country | 2039770088 |+---------------------+------------+1 row in set (0.00 sec)接下来删除country表12SESSION_A&gt;drop table country;Query OK, 0 rows affected (0.01 sec) drop table后,.frm和.ibd文件也被删除了12[mysql@master sakila]$ ll country*ls: cannot access country*: No such file or directory 恢复表这是情况比较复杂,因为误删除了表,此时如果数据库还是活动的,那么很有可能country表的数据会被rewrite.此时应该立即停止数据库,并把曾经包含country表数据的磁盘分区设为read-only以免被覆盖.在这里,我们只停止MySQL就好了12345[mysql@master ~]$ mysqladmin -S /data/mysqldata/3306/mysql.sock -umysql -pmysql shutdownWarning: Using a password on the command line interface can be insecure.[mysql@master ~]$ ps -ef | grep mysqld | grep -v grep[mysql@master ~]$ 尽管我们使用的是独立表空间,但是数据字典仍然存储在ibdata1中. 使用stream_parser解析数据文件12345678910111213141516171819202122232425[mysql@master undrop-for-innodb]$ ./stream_parser -f /data/mysqldata/3306/data/ibdata1 Opening file: /data/mysqldata/3306/data/ibdata1File information:ID of device containing file: 64768inode number: 106448290protection: 100640 (regular file)number of hard links: 1user ID of owner: 27group ID of owner: 27device ID (if special file): 0blocksize for filesystem I/O: 4096number of blocks allocated: 4194304time of last access: 1482762932 Mon Dec 26 22:35:32 2016time of last modification: 1482762932 Mon Dec 26 22:35:32 2016time of last status change: 1482762932 Mon Dec 26 22:35:32 2016total size, in bytes: 2147483648 (2.000 GiB)Size to process: 2147483648 (2.000 GiB)Worker(0): 3.90% done. 2016-12-26 22:36:46 ETA(in 00:00:27). Processing speed: 71.922 MiB/secWorker(0): 30.46% done. 2016-12-26 22:36:22 ETA(in 00:00:02). Processing speed: 544.000 MiB/secWorker(0): 55.46% done. 2016-12-26 22:36:22 ETA(in 00:00:01). Processing speed: 512.000 MiB/secWorker(0): 75.39% done. 2016-12-26 22:36:23 ETA(in 00:00:01). Processing speed: 408.000 MiB/secWorker(0): 92.96% done. 2016-12-26 22:36:23 ETA(in 00:00:00). Processing speed: 360.000 MiB/secAll workers finished in 5 sec 解析之后会生成如下目录和文件123456789101112131415161718192021222324252627282930[mysql@master undrop-for-innodb]$ tree pages-ibdata1/pages-ibdata1/├── FIL_PAGE_INDEX│ ├── 0000000000000001.page│ ├── 0000000000000002.page│ ├── 0000000000000003.page│ ├── 0000000000000004.page│ ├── 0000000000000005.page│ ├── 0000000000000011.page│ ├── 0000000000000012.page│ ├── 0000000000000013.page│ ├── 0000000000000014.page│ ├── 0000000000000015.page│ ├── 0000000000000016.page│ ├── 0000000000000529.page│ ├── 0000000000000655.page│ ├── 0000000000000657.page│ ├── 0000000000000658.page│ ├── 0000000000000659.page│ ├── 0000000000000661.page│ ├── 0000000000000665.page│ ├── 0000000000000666.page│ ├── 0000000000000667.page│ ├── 0000000000000669.page│ ├── 0000000000000680.page│ ├── 0000000000000718.page│ └── 18446744069414584320.page└── FIL_PAGE_TYPE_BLOB2 directories, 24 files为了找出TABLE_ID和INDEX_ID,我们需要查看SYS_TABLES和SYS_INDEXES数据字典.我们会从ibdata1中取出数据.因为数据字典记录总是冗余的,所以我们要执行 -4选项,同时我们假定mysql已经将变化刷到磁盘了.所以指定-D选项,表示找出删除的记录.SYS_TABLES表的信息存储在index_id=1的文件中,即pages-ibdata1/FIL_PAGE_INDEX./0000000000000001.page12345[mysql@master undrop-for-innodb]$ ./c_parser -4Df ./pages-ibdata1/FIL_PAGE_INDEX/0000000000000001.page -t ./dictionary/SYS_TABLES.sql | grep countrySET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/SYS_TABLES&#x27; REPLACE INTO TABLE `SYS_TABLES` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;SYS_TABLES\\t&#x27; (`NAME`, `ID`, `N_COLS`, `TYPE`, `MIX_ID`, `MIX_LEN`, `CLUSTER_NAME`, `SPACE`);00000004FA1A 150000037222EC SYS_TABLES &quot;sakila/country&quot; 714 3 65 0 80 &quot;&quot; 70300000004FA1A 150000037222EC SYS_TABLES &quot;sakila/country&quot; 714 3 65 0 80 &quot;&quot; 703我们可以看到country表的table_id是714.接下来我们要找到country表的主键索引.我们需要查看0000000000000003.page文件中的SYS_INDEXES表数据 (SYS_INDEXES table contains mapping between table_id and index_id).1234[mysql@master undrop-for-innodb]$ ./c_parser -4Df ./pages-ibdata1/FIL_PAGE_INDEX/0000000000000003.page -t ./dictionary/SYS_INDEXES.sql | grep 714SET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/SYS_INDEXES&#x27; REPLACE INTO TABLE `SYS_INDEXES` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;SYS_INDEXES\\t&#x27; (`TABLE_ID`, `ID`, `NAME`, `N_FIELDS`, `TYPE`, `SPACE`, `PAGE_NO`);00000004FA1A 150000037221CC SYS_INDEXES 714 729 &quot;PRIMARY&quot; 1 3 703 4294967295我们可以看到country表的index_id是729 由于没有可用数据的文件，我们将扫描所有存储设备作为原始设备，并查找适合数据库页面的预期结构的数据。 顺便说一句，这种方法可以用于已经损坏的数据文件。 如果一些数据被损坏，恢复工具可以执行部分数据恢复。 在工具的选项中，我们指定设备的名称和设备大小（可以是近似值）。 Since there is no file with data available, we will scan through all the storage device as raw device and look for data that fit in expected structure of the database pages. By the way, this approach can be taken in case we have corrupted data files. If some data is corrupted, recovery tool can perform partial data recovery. In the options of the tool we specify name of the device and device size (can be approximate). 这里我的数据放在/dev/mapper/centos-oracle 下 所以root用户scan 30G数据,磁盘要有足够空间,并且请注意不要在曾经包含被删除表.ibd文件所挂载的磁盘下执行下面的命令,因为他会生成大量文件,可能导致rewrite.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@master undrop-for-innodb]# ./stream_parser -f /dev/mapper/centos-oracle -t 20000000kOpening file: /dev/mapper/centos-oracleFile information:ID of device containing file: 5inode number: 15178protection: 60660 (block device)number of hard links: 1user ID of owner: 0group ID of owner: 6device ID (if special file): 64770blocksize for filesystem I/O: 4096number of blocks allocated: 0time of last access: 1482761058 Mon Dec 26 22:04:18 2016time of last modification: 1481179251 Thu Dec 8 14:40:51 2016time of last status change: 1481179251 Thu Dec 8 14:40:51 2016total size, in bytes: 0 (0.000 exp(+0))Size to process: 20480000000 (19.073 GiB)Worker(0): 1.06% done. 2016-12-26 22:41:21 ETA(in 00:01:36). Processing speed: 199.621 MiB/secWorker(0): 2.09% done. 2016-12-26 22:42:58 ETA(in 00:03:11). Processing speed: 99.806 MiB/secWorker(0): 3.11% done. 2016-12-26 22:41:22 ETA(in 00:01:34). Processing speed: 199.612 MiB/secWorker(0): 4.13% done. 2016-12-26 22:42:57 ETA(in 00:03:07). Processing speed: 99.807 MiB/secWorker(0): 5.15% done. 2016-12-26 22:41:23 ETA(in 00:01:32). Processing speed: 199.617 MiB/sec...[mysql@master undrop-for-innodb]$ tree pages-centos-oracle/pages-centos-oracle/├── FIL_PAGE_INDEX│ ├── 0000000000000001.page│ ├── 0000000000000002.page│ ├── 0000000000000003.page│ ├── 0000000000000004.page│ ├── 0000000000000005.page│ ├── 0000000000000011.page│ ├── 0000000000000012.page│ ├── 0000000000000013.page│ ├── 0000000000000014.page│ ├── 0000000000000015.page│ ├── 0000000000000016.page│ ├── 0000000000000092.page│ ├── 0000000000000347.page│ ├── 0000000000000400.page│ ├── 0000000000000407.page│ ├── 0000000000000511.page│ ├── 0000000000000529.page│ ├── 0000000000000655.page│ ├── 0000000000000657.page│ ├── 0000000000000658.page│ ├── 0000000000000659.page│ ├── 0000000000000661.page│ ├── 0000000000000665.page│ ├── 0000000000000666.page│ ├── 0000000000000667.page│ ├── 0000000000000669.page│ ├── 0000000000000680.page│ ├── 0000000000000718.page│ ├── 0000000000000729.page│ └── 18446744069414584320.page└── FIL_PAGE_TYPE_BLOB2 directories, 30 files0000000000000729.page使我们需要的.这里需要country的建表语句country.sql,所以说请备份好元数据.这里undrop-for-innodb自带了sakila的建表语句以供实验使用12345678[mysql@master undrop-for-innodb]$ ./c_parser -6f pages-centos-oracle/FIL_PAGE_INDEX/0000000000000729.page -t sakila/country.sql | head -5SET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/country&#x27; REPLACE INTO TABLE `country` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;country\\t&#x27; (`country_id`, `country`, `last_update`);-- Page id: 3, Format: COMPACT, Records list: Valid, Expected records: (109 109)00000004FA13 90000002820110 country 1 &quot;Afghanistan&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 9000000282011B country 2 &quot;Algeria&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 90000002820126 country 3 &quot;American Samoa&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 90000002820131 country 4 &quot;Angola&quot; &quot;2006-02-15 04:44:00&quot;看起来我们找到了数据,下面通过c_parser生成load data infile语句和数据12[mysql@master undrop-for-innodb]$ mkdir -p dumps/default[mysql@master undrop-for-innodb]$ ./c_parser -6f pages-centos-oracle/FIL_PAGE_INDEX/0000000000000729.page -t sakila/country.sql &gt; dumps/default/country 2&gt; dumps/default/country_load.sql查看一下生成的文件12345678910111213141516171819[mysql@master undrop-for-innodb]$ cd dumps/default/[mysql@master default]$ lscountry country_load.sql这是包含数据的文件[mysql@master default]$ more country -- Page id: 3, Format: COMPACT, Records list: Valid, Expected records: (109 109)00000004FA13 90000002820110 country 1 &quot;Afghanistan&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 9000000282011B country 2 &quot;Algeria&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 90000002820126 country 3 &quot;American Samoa&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 90000002820131 country 4 &quot;Angola&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 9000000282013C country 5 &quot;Anguilla&quot; &quot;2006-02-15 04:44:00&quot;00000004FA13 90000002820147 country 6 &quot;Argentina&quot; &quot;2006-02-15 04:44:00&quot;这是生成的写好的load data infile语句[mysql@master default]$ more country_load.sql SET FOREIGN_KEY_CHECKS=0;LOAD DATA LOCAL INFILE &#x27;/var/lib/mysql/undrop-for-innodb/dumps/default/country&#x27; REPLACE INTO TABLE `country` FIELDS TERMINATED BY &#x27;\\t&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES STARTING BY &#x27;country\\t&#x27; (`country_id`, `country`, `last_update`); 开始恢复数据启动数据库1mysqld_safe --defaults-file=/data/mysqldata/3306/my.cnf &amp;创建表123(mysql@localhost) [(none)]&gt; use sakilaDatabase changed(mysql@localhost) [sakila]&gt; source sakila/country.sqlload回数据12345678910111213141516171819202122(mysql@localhost) [sakila]&gt; source dumps/default/country_load.sqlQuery OK, 0 rows affected (0.00 sec)Query OK, 109 rows affected (0.01 sec)Records: 109 Deleted: 0 Skipped: 0 Warnings: 0(mysql@localhost) [sakila]&gt; SELECT COUNT(*) FROM country;+----------+| COUNT(*) |+----------+| 109 |+----------+1 row in set (0.00 sec)checksum和之前也是一样的(mysql@localhost) [sakila]&gt; CHECKSUM TABLE country;+---------------------+------------+| Table | Checksum |+---------------------+------------+| sakila.country | 2039770088 |+---------------------+------------+1 row in set (0.00 sec)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"},{"name":"MySQL备份恢复","slug":"MySQL备份恢复","permalink":"http://fuxkdb.com/tags/MySQL%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D/"}]},{"title":"Auto_increment 产生 GAP的原因","slug":"Auto_increment-产生-GAP的原因","date":"2016-12-20T14:00:00.000Z","updated":"2017-08-05T16:00:44.000Z","comments":true,"path":"2016/12/20/Auto_increment-产生-GAP的原因/","link":"","permalink":"http://fuxkdb.com/2016/12/20/Auto_increment-%E4%BA%A7%E7%94%9F-GAP%E7%9A%84%E5%8E%9F%E5%9B%A0/","excerpt":"原文http://thenoyes.com/littlenoise/?p=187 Auto_increment 产生 GAP的原因Why are there gaps in my auto_increment sequence, even if there are no deletes or rolled back transactions?Is it a bug?The manual says, “For lock modes 1 or 2, gaps may occur between successive statements because for bulk inserts the exact number of auto-increment values required by each statement may not be known and overestimation is possible.”Where does that overestimation come from?An example to illustrate:123456789101112131415161718192021222324252627282930DROP TABLE IF EXISTS t;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB SELECT NULL AS a;/* #1 */ INSERT INTO t SELECT NULL FROM t;/* #2 */ INSERT INTO t SELECT NULL FROM t;/* #3 */ INSERT INTO t SELECT NULL FROM t;/* #4 */ INSERT INTO t SELECT NULL FROM t;SELECT * FROM t;+----+| a |+----+| 1 || 2 || 3 || 4 || 6 || 7 || 8 || 9 || 13 || 14 || 15 || 16 || 17 || 18 || 19 || 20 |+----+16 rows in set (0.02 sec)","text":"原文http://thenoyes.com/littlenoise/?p=187 Auto_increment 产生 GAP的原因Why are there gaps in my auto_increment sequence, even if there are no deletes or rolled back transactions?Is it a bug?The manual says, “For lock modes 1 or 2, gaps may occur between successive statements because for bulk inserts the exact number of auto-increment values required by each statement may not be known and overestimation is possible.”Where does that overestimation come from?An example to illustrate:123456789101112131415161718192021222324252627282930DROP TABLE IF EXISTS t;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB SELECT NULL AS a;/* #1 */ INSERT INTO t SELECT NULL FROM t;/* #2 */ INSERT INTO t SELECT NULL FROM t;/* #3 */ INSERT INTO t SELECT NULL FROM t;/* #4 */ INSERT INTO t SELECT NULL FROM t;SELECT * FROM t;+----+| a |+----+| 1 || 2 || 3 || 4 || 6 || 7 || 8 || 9 || 13 || 14 || 15 || 16 || 17 || 18 || 19 || 20 |+----+16 rows in set (0.02 sec)Notice that 5 and 10-12 are missing. If we did another insert, we’d be missing 21-27 (try it and see!) Here’s a model of what MySQL is doing: Create the table and simultaneously insert a single row. That is the row where a=1. 1: Insert as many rows as there are in the table (it’s one row, but MySQL doesn’t know that.) Grab a chunk of auto_increment values. How many in the chunk? One - the value ‘2’. Insert it (one row inserted). No more rows to insert, so all done. 2: Insert as many rows as there are in the table (it’s two rows, but MySQL doesn’t know that.) Grab a chunk of auto_increment values. How many in the chunk? One - the value ‘3’. Insert it (one row inserted). Still more rows to insert. Grab another chunk, twice as big as before - two values, ‘4’ and ‘5’. Insert the ‘4’ (two rows inserted). No more rows to insert. Discard the left over ‘5’. 3: Insert as many rows as there are in the table (it’s four rows, but MySQL doesn’t know that.) Grab a chunk of auto_increment values. How many in the chunk? One - the value ‘6’. Insert it (one row inserted). Still more rows to insert. Grab another chunk, twice as big as before - two values, ‘7’ and ‘8’. Insert them (three rows inserted). Still more rows to insert. Grab another chunk, twice as big as before - four values, ‘9’, ‘10’, ‘11’, ‘12’. Insert the ‘9’ (four rows inserted). No more rows to insert. Discard the left over ‘10’, ‘11’, and ‘12’. 4: Insert as many rows as there are in the table (it’s eight rows, but MySQL doesn’t know that.) Grab a chunk of auto_increment values. How many in the chunk? One - the value ‘13’. Insert it (one row inserted). Still more rows to insert. Grab another chunk, twice as big as before - two values, ‘14’ and ‘15’. Insert them (three rows inserted). Still more rows to insert. Grab another chunk, twice as big as before - four values, ‘16’, ‘17’, ‘18’, ‘19’. Insert them (seven rows inserted). Still more rows to insert. Grab another chunk, twice as big as before - eight values, ‘20’, ‘21’, ‘22’, …, ‘27’. Insert the ‘20’ (eight rows inserted). No more rows to insert. Discard the left over ‘21’, ‘22’, etc. The gap can get as big as 65535 (I didn’t look in the code to confirm that, it’s just what running the test above a few more times seems to suggest). When innodb_autoinc_lock_mode=1, there can be gaps between statements, but not within a statement, because there’s a lock on the auto_increment until we’re done. #4 above is guaranteed to get 8 consecutive values, you just might not be sure where they are going to start (well, now you are, because you read this post). When innodb_autoinc_lock_mode=2, there can be gaps within a statement, because the auto_increment is not locked. Imagine we’re in the middle of #4 above. Our statement is inserting the ‘14’ and ‘15’, when another statement comes along wanting just a single auto_increment value. It gets the ‘16’. Now it’s our turn to ask for another chunk, and we get ‘17’, ‘18’, ‘19’, ‘20’. While we’re doing those, another statement comes along and steals our ‘21’. So the last row for our statement is ‘22’.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"AUTO-INC锁和AUTO_INCREMENT在InnoDB中处理方式","slug":"AUTO-INC锁和AUTO_INCREMENT在InnoDB中处理方式","date":"2016-12-15T14:00:00.000Z","updated":"2017-08-05T16:01:10.000Z","comments":true,"path":"2016/12/15/AUTO-INC锁和AUTO_INCREMENT在InnoDB中处理方式/","link":"","permalink":"http://fuxkdb.com/2016/12/15/AUTO-INC%E9%94%81%E5%92%8CAUTO_INCREMENT%E5%9C%A8InnoDB%E4%B8%AD%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/","excerpt":"AUTO-INC Locks An AUTO-INC lock is a special table-level lock taken by transactions inserting into tables with AUTO_INCREMENT columns. In the simplest case, if one transaction is inserting values into the table, any other transactions must wait to do their own inserts into that table, so that rows inserted by the first transaction receive consecutive primary key values. The innodb_autoinc_lock_mode configuration option controls the algorithm used for auto-increment locking. It allows you to choose how to trade off between predictable sequences of auto-increment values and maximum concurrency for insert operations. AUTO-INC锁是当向使用含有AUTO_INCREMENT列的表中插入数据时需要获取的一种特殊的表级锁在最简单的情况下，如果一个事务正在向表中插入值，则任何其他事务必须等待对该表执行自己的插入操作，以便第一个事务插入的行的值是连续的。innodb_autoinc_lock_mode配置选项控制用于自动增量锁定的算法。 它允许您选择如何在可预测的自动递增值序列和插入操作的最大并发性之间进行权衡。 AUTO_INCREMENT Handling in InnoDBInnoDB提供了一个可配置的锁定机制，可以显着提高使用AUTO_INCREMENT列向表中添加行的SQL语句的可伸缩性和性能。 要对InnoDB表使用AUTO_INCREMENT机制，必须将AUTO_INCREMENT列定义为索引的一部分，以便可以对表执行相当于索引的SELECT MAX（ai_col）查找以获取最大列值。 通常，这是通过使列成为某些表索引的第一列来实现的。 本节介绍AUTO_INCREMENT锁定模式的行为，对不同AUTO_INCREMENT锁定模式设置的使用含义，以及InnoDB如何初始化AUTO_INCREMENT计数器。 InnoDB AUTO_INCREMENT锁定模式 InnoDB AUTO_INCREMENT锁定模式使用含义 InnoDB AUTO_INCREMENT计数器初始化","text":"AUTO-INC Locks An AUTO-INC lock is a special table-level lock taken by transactions inserting into tables with AUTO_INCREMENT columns. In the simplest case, if one transaction is inserting values into the table, any other transactions must wait to do their own inserts into that table, so that rows inserted by the first transaction receive consecutive primary key values. The innodb_autoinc_lock_mode configuration option controls the algorithm used for auto-increment locking. It allows you to choose how to trade off between predictable sequences of auto-increment values and maximum concurrency for insert operations. AUTO-INC锁是当向使用含有AUTO_INCREMENT列的表中插入数据时需要获取的一种特殊的表级锁在最简单的情况下，如果一个事务正在向表中插入值，则任何其他事务必须等待对该表执行自己的插入操作，以便第一个事务插入的行的值是连续的。innodb_autoinc_lock_mode配置选项控制用于自动增量锁定的算法。 它允许您选择如何在可预测的自动递增值序列和插入操作的最大并发性之间进行权衡。 AUTO_INCREMENT Handling in InnoDBInnoDB提供了一个可配置的锁定机制，可以显着提高使用AUTO_INCREMENT列向表中添加行的SQL语句的可伸缩性和性能。 要对InnoDB表使用AUTO_INCREMENT机制，必须将AUTO_INCREMENT列定义为索引的一部分，以便可以对表执行相当于索引的SELECT MAX（ai_col）查找以获取最大列值。 通常，这是通过使列成为某些表索引的第一列来实现的。 本节介绍AUTO_INCREMENT锁定模式的行为，对不同AUTO_INCREMENT锁定模式设置的使用含义，以及InnoDB如何初始化AUTO_INCREMENT计数器。 InnoDB AUTO_INCREMENT锁定模式 InnoDB AUTO_INCREMENT锁定模式使用含义 InnoDB AUTO_INCREMENT计数器初始化 InnoDB AUTO_INCREMENT锁定模式本节介绍用于生成自动递增值的AUTO_INCREMENT锁定模式的行为，以及每种锁定模式如何影响复制。 自动递增锁定模式在启动时使用innodb_autoinc_lock_mode配置参数进行配置。 以下术语用于描述innodb_autoinc_lock_mode设置: “INSERT-like” statements(类INSERT语句)所有可以向表中增加行的语句,包括INSERT, INSERT ... SELECT, REPLACE, REPLACE ... SELECT, and LOAD DATA.包括“simple-inserts”, “bulk-inserts”, and “mixed-mode” inserts. “Simple inserts”可以预先确定要插入的行数（当语句被初始处理时）的语句。 这包括没有嵌套子查询的单行和多行INSERT和REPLACE语句，但不包括INSERT ... ON DUPLICATE KEY UPDATE。 “Bulk inserts”事先不知道要插入的行数（和所需自动递增值的数量）的语句。 这包括INSERT ... SELECT，REPLACE ... SELECT和LOAD DATA语句，但不包括纯INSERT。 InnoDB在处理每行时一次为AUTO_INCREMENT列分配一个新值。 “Mixed-mode inserts”这些是“Simple inserts”语句但是指定一些（但不是全部）新行的自动递增值。 示例如下，其中c1是表t1的AUTO_INCREMENT列：INSERT INTO t1 (c1,c2) VALUES (1,&#39;a&#39;), (NULL,&#39;b&#39;), (5,&#39;c&#39;), (NULL,&#39;d&#39;); 另一种类型的“Mixed-mode inserts”是INSERT ... ON DUPLICATE KEY UPDATE，其在最坏的情况下实际上是INSERT语句随后又跟了一个UPDATE，其中AUTO_INCREMENT列的分配值不一定会在 UPDATE 阶段使用 innodb_autoinc_lock_mode = 0 (“traditional” lock mode)传统的锁定模式提供了在MySQL 5.1中引入innodb_autoinc_lock_mode配置参数之前存在的相同行为。传统的锁定模式选项用于向后兼容性，性能测试以及解决“Mixed-mode inserts”的问题，因为语义上可能存在差异。 在此锁定模式下，所有“INSERT-like”语句获得一个特殊的表级AUTO-INC锁，用于插入具有AUTO_INCREMENT列的表。此锁定通常保持到语句结束（不是事务结束），以确保为给定的INSERT语句序列以可预测和可重复的顺序分配自动递增值，并确保自动递增由任何给定语句分配的值是连续的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778SESSION_A&gt;DROP TABLE IF EXISTS t;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB;Query OK, 0 rows affected (0.01 sec)SESSION_A&gt;insert into t values(1),(3),(4),(5),(6),(7);Query OK, 6 rows affected (0.01 sec)Records: 6 Duplicates: 0 Warnings: 0SESSION_A&gt;select * from t;+---+| a |+---+| 1 || 3 || 4 || 5 || 6 || 7 |+---+6 rows in set (0.00 sec)SESSION_A&gt;select @@innodb_autoinc_lock_mode;+----------------------------+| @@innodb_autoinc_lock_mode |+----------------------------+| 0 |+----------------------------+1 row in set (0.00 sec)A B C 三个会话事务隔离级别都是 RRSESSION_A&gt;select @@global.tx_isolation,@@session.tx_isolation;+-----------------------+------------------------+| @@global.tx_isolation | @@session.tx_isolation |+-----------------------+------------------------+| REPEATABLE-READ | REPEATABLE-READ |+-----------------------+------------------------+SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;delete from t where a&gt;4;Query OK, 3 rows affected (0.00 sec)B会话被锁,这是由于会话 A 产生的 gap lockSESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t values(null); --注意这里因为是 null, 锁需要在内存中分配 AUTO-INCREMENT 值C 会话被阻塞SESSION_C&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_C&gt;insert into t values(2); --这里插入2,没有 gap lock 也被锁了(mysql@localhost) [fandb]&gt; (mysql@localhost) [fandb]&gt; select trx_id,trx_state,trx_requested_lock_id,trx_weight,trx_mysql_thread_id,trx_query, trx_operation_state from information_schema.INNODB_TRX;+--------+-----------+-----------------------+------------+---------------------+----------------------------+-----------------------+| trx_id | trx_state | trx_requested_lock_id | trx_weight | trx_mysql_thread_id | trx_query | trx_operation_state |+--------+-----------+-----------------------+------------+---------------------+----------------------------+-----------------------+| 321912 | LOCK WAIT | 321912:701 | 3 | 7 | insert into t values(2) | setting auto-inc lock || 321911 | LOCK WAIT | 321911:690:3:1 | 3 | 2 | insert into t values(null) | inserting || 321906 | RUNNING | NULL | 5 | 1 | NULL | NULL |+--------+-----------+-----------------------+------------+---------------------+----------------------------+-----------------------+3 rows in set (0.00 sec)可以看到,SESSION_C是等待自增锁，一直处于setting auto-inc lock状态(mysql@localhost) [fandb]&gt; select * from information_schema.INNODB_LOCKS;+----------------+-------------+-----------+-----------+-------------+------------+------------+-----------+----------+------------------------+| lock_id | lock_trx_id | lock_mode | lock_type | lock_table | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+-------------+------------+------------+-----------+----------+------------------------+| 321912:701 | 321912 | AUTO_INC | TABLE | `fandb`.`t` | NULL | NULL | NULL | NULL | NULL || 321911:701 | 321911 | AUTO_INC | TABLE | `fandb`.`t` | NULL | NULL | NULL | NULL | NULL || 321911:690:3:1 | 321911 | X | RECORD | `fandb`.`t` | PRIMARY | 690 | 3 | 1 | supremum pseudo-record || 321906:690:3:1 | 321906 | X | RECORD | `fandb`.`t` | PRIMARY | 690 | 3 | 1 | supremum pseudo-record |+----------------+-------------+-----------+-----------+-------------+------------+------------+-----------+----------+------------------------+4 rows in set (0.00 sec) 在statement-based replication的情况下，这意味着当在从服务器上复制SQL语句时，自动增量列使用与主服务器上相同的值。多个INSERT语句的执行结果是确定性的，SLAVE再现与MASTER相同的数据。如果由多个INSERT语句生成的自动递增值交错，则两个并发INSERT语句的结果将是不确定的，并且不能使用基于语句的复制可靠地传播到从属服务器。 为了解释清楚,查看下面的例子:12345CREATE TABLE t1 ( c1 INT(11) NOT NULL AUTO_INCREMENT, c2 VARCHAR(10) DEFAULT NULL, PRIMARY KEY (c1)) ENGINE=InnoDB;假设有两个事务正在运行，每个事务都将行插入到具有AUTO_INCREMENT列的表中。 一个事务正在使用插入1000行的INSERT … SELECT语句，另一个事务正在使用插入一行的“Simple inserts”语句:12Tx1: INSERT INTO t1 (c2) SELECT 1000 rows from another table ...Tx2: INSERT INTO t1 (c2) VALUES (&#x27;xxx&#x27;);InnoDB不能预先得知有多少行会从TX1的select部分获取到,所以在事务进行过程中,InnoDB一次只会为AUTO_INCREMENT列分配一个值.通过一个表级锁的控制,保证了在同一时刻只有一个引用表t1的INSERT语句可以执行,直到整个INSERT语句结束,并且由不同语句生成自动递增数不会交错由Tx1 INSERT ... SELECT语句生成的自动递增值将是连续的，并且Tx2中的INSERT语句使用的（单个）自动递增值将小于或大于用于Tx1的所有那些值，具体取决于 那个语句先执行。 只要SQL语句在从二进制日志（当使用基于语句的复制或在恢复方案中）重放时以相同的顺序执行，结果将与Tx1和Tx2首次运行时的结果相同。 因此，持续至语句结束的表级锁定( table-level locks)保证了在statement-based replication中对auto-increment列的插入数据的安全性. 但是，当多个事务同时执行insert语句时，这些表级锁定会限制并发性和可伸缩性。 在前面的示例中，如果没有表级锁，则Tx2中用于INSERT的自动递增列的值取决于语句执行的确切时间。 如果Tx2的INSERT在Tx1的INSERT正在运行时（而不是在它开始之前或完成之后）执行，则由两个INSERT语句分配的特定自动递增值将是不确定的，并且可能每次运行都会得到不同的值 在连续锁定模式下，InnoDB可以避免为“Simple inserts”语句使用表级AUTO-INC锁，其中行数是预先已知的，并且仍然保留基于语句的复制的确定性执行和安全性。 如果不使用二进制日志作为恢复或复制的一部分来重放SQL语句，则可以使用interleaved lock模式来消除所有使用表级AUTO-INC锁，以实现更大的并发性和性能,其代价是由于并发的语句交错执行,同一语句生成的AUTO-INCREMENT值可能会产生GAP innodb_autoinc_lock_mode = 1 (“consecutive” lock mode)这是默认的锁定模式.在这个模式下,“bulk inserts”仍然使用AUTO-INC表级锁,并保持到语句结束.这适用于所有INSERT ... SELECT，REPLACE ... SELECT和LOAD DATA语句。同一时刻只有一个语句可以持有AUTO-INC锁. “Simple inserts”（要插入的行数事先已知）通过在mutex（轻量锁）的控制下获得所需数量的自动递增值来避免表级AUTO-INC锁， 它只在分配过程的持续时间内保持，而不是直到语句完成。 不使用表级AUTO-INC锁，除非AUTO-INC锁由另一个事务保持。 如果另一个事务保持AUTO-INC锁，则“简单插入”等待AUTO-INC锁，如同它是一个“批量插入”。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162SESSION_A&gt;DROP TABLE IF EXISTS t;Query OK, 0 rows affected (0.01 sec)SESSION_A&gt;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB;Query OK, 0 rows affected (0.01 sec)SESSION_A&gt;insert into t values(1),(3),(4),(5),(6),(7);Query OK, 6 rows affected (0.01 sec)Records: 6 Duplicates: 0 Warnings: 0SESSION_A&gt;select @@innodb_autoinc_lock_mode;+----------------------------+| @@innodb_autoinc_lock_mode |+----------------------------+| 1 |+----------------------------+1 row in set (0.00 sec)SESSION_A&gt;select * from t;+---+| a |+---+| 1 || 3 || 4 || 5 || 6 || 7 |+---+6 rows in set (0.00 sec)SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;delete from t where a&gt;4;Query OK, 3 rows affected (0.00 sec)会话 B, 被 GAP LOCK 阻塞SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t values(null); --由于是`simple-insert`且`innodb_autoinc_lock_mode=1`,所以并不需要AUTO-INC表级锁会话 C 成功插入没有阻塞SESSION_C&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_C&gt;insert into t values(2); --由于它也是`simple-insert`且`innodb_autoinc_lock_mode=1`所以不需要获取AUTO-INC表级锁,没有阻塞成功插入Query OK, 1 row affected (0.00 sec)C会话rollback,B会话改为使用“Bulk inserts”SESSION_C&gt;rollback;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t select null;此时 C 会话又被阻塞了SESSION_C&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_C&gt;insert into t values(2); --这验证了官方文档中的说法`If another transaction holds an AUTO-INC lock, a “simple insert” waits for the AUTO-INC lock, as if it were a “bulk insert”.`Query OK, 1 row affected (41.17 sec) 此锁定模式确保,当行数不预先知道的INSERT存在时(并且自动递增值在语句过程执行中分配)由任何“类INSERT”语句分配的所有自动递增值是连续的，并且对于基于语句的复制(statement-based replication)操作是安全的。 这种锁定模式显著地提高了可扩展性,并且保证了对于基于语句的复制(statement-based replication)的安全性.此外，与“传统”锁定模式一样，由任何给定语句分配的自动递增数字是连续的。 与使用自动递增的任何语句的“传统”模式相比，语义没有变化.但有一个特例: The exception is for “mixed-mode inserts”, where the user provides explicit values for an AUTO_INCREMENT column for some, but not all, rows in a multiple-row “simple insert”. For such inserts, InnoDB allocates more auto-increment values than the number of rows to be inserted. However, all values automatically assigned are consecutively generated (and thus higher than) the auto-increment value generated by the most recently executed previous statement. “Excess” numbers are lost. innodb_autoinc_lock_mode = 2 (“interleaved” lock mode)在这种锁定模式下,所有类INSERT(“INSERT-like” )语句都不会使用表级AUTO-INC lock,并且可以同时执行多个语句。这是最快和最可扩展的锁定模式，但是当使用基于语句的复制或恢复方案时，从二进制日志重播SQL语句时，这是不安全的。 在此锁定模式下，自动递增值保证在所有并发执行的“类INSERT”语句中是唯一且单调递增的。但是，由于多个语句可以同时生成数字（即，跨语句交叉编号），为任何给定语句插入的行生成的值可能不是连续的。 如果执行的语句是“simple inserts”，其中要插入的行数已提前知道，则除了“混合模式插入”之外，为单个语句生成的数字不会有间隙。然而，当执行“批量插入”时，在由任何给定语句分配的自动递增值中可能存在间隙。 InnoDB AUTO_INCREMENT锁定模式使用含义 在复制环节中使用自增列如果你在使用基于语句的复制(statement-based replication)请将innodb_autoinc_lock_mode设置为0或1，并在主从上使用相同的值。 如果使用innodb_autoinc_lock_mode = 2（“interleaved”）或主从不使用相同的锁定模式的配置，自动递增值不能保证在从机上与主机上相同。 如果使用基于行的或混合模式的复制，则所有自动增量锁定模式都是安全的，因为基于行的复制对SQL语句的执行顺序不敏感（混合模式会在遇到不安全的语句是使用基于行的复制模式）。 “Lost” auto-increment values and sequence gaps在所有锁定模式（0,1和2）中，如果生成自动递增值的事务回滚，那些自动递增值将“丢失”。 一旦为自动增量列生成了值，无论是否完成“类似INSERT”语句以及包含事务是否回滚，都不能回滚。 这种丢失的值不被重用。 因此，存储在表的AUTO_INCREMENT列中的值可能存在间隙。 Specifying NULL or 0 for the AUTO_INCREMENT column在所有锁定模式（0,1和2）中，如果用户在INSERT中为AUTO_INCREMENT列指定NULL或0，InnoDB会将该行视为未指定值，并为其生成新值。 为AUTO_INCREMENT列分配一个负值在所有锁定模式（0,1和2）中，如果您为AUTO_INCREMENT列分配了一个负值，则不会定义自动增量机制的行为。 如果AUTO_INCREMENT值大于指定整数类型的最大整数在所有锁定模式（0,1和2）中，如果值大于可以存储在指定整数类型中的最大整数，则不定义自动递增机制的行为。 Gaps in auto-increment values for “bulk inserts”当innodb_autoinc_lock_mode设置为0（“traditional”）或1（“consecutive”）时,任何给定语句生成的自动递增值是连续的，没有间隙，因为表级AUTO-INC锁会持续到 语句结束,并且一次只能执行一个这样的语句。 当innodb_autoinc_lock_mode设置为2（“interleaved”）时，在“bulk inserts”生成的自动递增值中可能存在间隙，但只有在并发执行“INSERT-Like”语句时才会产生这种情况。 对于锁定模式1或2，在连续语句之间可能出现间隙，因为对于批量插入，每个语句所需的自动递增值的确切数目可能不为人所知，并且可能进行过度估计。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697select @@innodb_autoinc_lock_mode;+----------------------------+| @@innodb_autoinc_lock_mode |+----------------------------+| 0 |+----------------------------+DROP TABLE IF EXISTS t;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB SELECT NULL AS a;/* #1 */ INSERT INTO t SELECT NULL FROM t;/* #2 */ INSERT INTO t SELECT NULL FROM t;/* #3 */ INSERT INTO t SELECT NULL FROM t;/* #4 */ INSERT INTO t SELECT NULL FROM t;SELECT * FROM t;+----+| a |+----+| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 || 10 || 11 || 12 || 13 || 14 || 15 || 16 |+----+当innodb_autoinc_lock_mode=0 类INSERT语句产生的自动递增值都是连续的select @@innodb_autoinc_lock_mode;+----------------------------+| @@innodb_autoinc_lock_mode |+----------------------------+| 1 |+----------------------------+1 row in set (0.00 sec)DROP TABLE IF EXISTS t;CREATE TABLE t (a bigint unsigned auto_increment primary key) ENGINE=InnoDB SELECT NULL AS a;/* #1 */ INSERT INTO t SELECT NULL FROM t;/* #2 */ INSERT INTO t SELECT NULL FROM t;/* #3 */ INSERT INTO t SELECT NULL FROM t;/* #4 */ INSERT INTO t SELECT NULL FROM t;SELECT * FROM t;+----+| a |+----+| 1 || 2 || 3 || 4 || 6 || 7 || 8 || 9 || 13 || 14 || 15 || 16 || 17 || 18 || 19 || 20 |+----+出现了间隙gap, 5和10-12都没了,下面来解释产生这种情况的原因:/* #1 */ 这是第一次INSERT,此时表中只有一行(创建表时的那一行),但是MySQL不知道有多少行.然后MySQL Grab a chunk of auto_increment values 在chunk中有多少？ 一 只有一个,即&#x27;2&#x27;,将其插入表中.没有更多的行插入，所以一切完成。/* #2 */ 这是第二次INSERT,此时表中有两行(1,2),但是MySQL不知道有多少行.MySQL Grab a chunk of auto_increment values 在chunk中有多少？ 一 只有一个,即&#x27;3&#x27;,将其插入表中.还有需要插入的行,所以Grab another chunk,这次是前一次的两倍大小 在chunk中有多少？ 一 两个,&#x27;4&#x27;和&#x27;5&#x27;. 插入&#x27;4&#x27;.没有更多的行插入，所以一切完成,&#x27;5&#x27;被舍弃,但是此时 AUTO_INCREMENT的下一个值是6了/* #3 */这是第三次INSERT,此时表中有四行(1,2,3,4),但是MySQL不知道有多少行.- Grab a chunk of auto_increment values. How many in the chunk? One - the value &#x27;6&#x27;. Insert it (one row inserted).- Still more rows to insert. Grab another chunk, twice as big as before - two values, &#x27;7&#x27; and &#x27;8&#x27;. Insert them (three rows inserted).- Still more rows to insert. Grab another chunk, twice as big as before - four values, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;11&#x27;, &#x27;12&#x27;. Insert the &#x27;9&#x27; (four rows inserted).- No more rows to insert. Discard the left over &#x27;10&#x27;, &#x27;11&#x27;, and &#x27;12&#x27;.#4: Insert as many rows as there are in the table (it&#x27;s eight rows, but MySQL doesn&#x27;t know that.)- Grab a chunk of auto_increment values. How many in the chunk? One - the value &#x27;13&#x27;. Insert it (one row inserted).- Still more rows to insert. Grab another chunk, twice as big as before - two values, &#x27;14&#x27; and &#x27;15&#x27;. Insert them (three rows inserted).- Still more rows to insert. Grab another chunk, twice as big as before - four values, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;19&#x27;. Insert them (seven rows inserted).- Still more rows to insert. Grab another chunk, twice as big as before - eight values, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, ..., &#x27;27&#x27;. Insert the &#x27;20&#x27; (eight rows inserted).- No more rows to insert. Discard the left over &#x27;21&#x27;, &#x27;22&#x27;, etc.所以这就是 gap 产生的原因 由“mixed-mode inserts”分配的自动递增值考虑一下场景,在“mixed-mode insert”中,其中一个“simple insert”语句指定了一些（但不是全部）行的AUTO-INCREMENT值。 这样的语句在锁模式0,1和2中表现不同。例如，假设c1是表t1的AUTO_INCREMENT列，并且最近自动生成的序列号是100。12345mysql&gt; CREATE TABLE t1 ( -&gt; c1 INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY, -&gt; c2 CHAR(1) -&gt; ) ENGINE = INNODB; Now, consider the following “mixed-mode insert” statement:1mysql&gt; INSERT INTO t1 (c1,c2) VALUES (1,&#x27;a&#x27;), (NULL,&#x27;b&#x27;), (5,&#x27;c&#x27;), (NULL,&#x27;d&#x27;); 当innodb_autoinc_lock_mode=0时:123456789mysql&gt; SELECT c1, c2 FROM t1 ORDER BY c2;+-----+------+| c1 | c2 |+-----+------+| 1 | a || 101 | b || 5 | c || 102 | d |+-----+------+ 下一个可用的auto-increment值103.因为innodb_autoinc_lock_mode=0时,auto-increment值一次只分配一个,而不是在开始时全部分配.不论是否有并发的其他类INSERT语句同时执行,都会是这样的结果 当innodb_autoinc_lock_mode=1时:123456789mysql&gt; SELECT c1, c2 FROM t1 ORDER BY c2;+-----+------+| c1 | c2 |+-----+------+| 1 | a || 101 | b || 5 | c || 102 | d |+-----+------+不同于innodb_autoinc_lock_mode=0时的情况,此时下一个可用的auto-increment值105,因为auto-increment值在语句一开始就分配了,分配了四个,但是只用了俩.不论是否有并发的其他类INSERT语句同时执行,都会是这样的结果 当innodb_autoinc_lock_mode=2时:123456789mysql&gt; SELECT c1, c2 FROM t1 ORDER BY c2;+-----+------+| c1 | c2 |+-----+------+| 1 | a || x | b || 5 | c || y | d |+-----+------+x和y的值是唯一的，并大于任何先前生成的行。 然而，x和y的具体值取决于通过并发执行语句生成的自动增量值的数量。 最后考虑下面的情况,当最近的 AUTO-INCREMENT 值为4时,执行下面的语句:1mysql&gt; INSERT INTO t1 (c1,c2) VALUES (1,&#x27;a&#x27;), (NULL,&#x27;b&#x27;), (5,&#x27;c&#x27;), (NULL,&#x27;d&#x27;);无论innodb_autoinc_lock_mode如何设置,都会报错duplicate-key error 23000 (Can’t write; duplicate key in table)因为5已经分配给了(NULL, ‘b’),所以导致插入(5, ‘C’)时报错 在INSERT语句序列的中间修改AUTO_INCREMENT列值在所有锁定模式（0,1和2）中，在INSERT语句序列中间修改AUTO_INCREMENT列值可能会导致duplicate key错误。 1234567891011121314151617181920212223242526272829mysql&gt; CREATE TABLE t1 ( -&gt; c1 INT NOT NULL AUTO_INCREMENT, -&gt; PRIMARY KEY (c1) -&gt; ) ENGINE = InnoDB;mysql&gt; INSERT INTO t1 VALUES(0), (0), (3); -- 0 0分配两个值1,2. 手动指定3,则此时AUTO_INCREMENT为3,下一个值为4mysql&gt; SELECT c1 FROM t1;+----+| c1 |+----+| 1 || 2 || 3 |+----+mysql&gt; UPDATE t1 SET c1 = 4 WHERE c1 = 1;mysql&gt; SELECT c1 FROM t1;+----+| c1 |+----+| 2 || 3 || 4 |+----+mysql&gt; INSERT INTO t1 VALUES(0); --由于分配值为4,所以报错duplicate keyERROR 1062 (23000): Duplicate entry &#x27;4&#x27; for key &#x27;PRIMARY&#x27; InnoDB AUTO_INCREMENT计数器初始化本章节讨论 InnoDB如何初始化AUTO_INCREMENT计数器如果你为一个Innodb表创建了一个AUTO_INCREMENT列,则InnoDB数据字典中的表句柄包含一个称为自动递增计数器的特殊计数器，用于为列分配新值。 此计数器仅存在于内存中，而不存储在磁盘上。 要在服务器重新启动后初始化自动递增计数器，InnoDB将在首次插入行到包含AUTO_INCREMENT列的表时执行以下语句的等效语句。1SELECT MAX(ai_col) FROM table_name FOR UPDATE;InnoDB增加语句检索的值，并将其分配给表和表的自动递增计数器。 默认情况下，值增加1.此默认值可以由auto_increment_increment配置设置覆盖。 如果表为空，InnoDB使用值1.此默认值可以由auto_increment_offset配置设置覆盖。 如果在自动递增计数器初始化前使用SHOW TABLE STATUS语句查看表, InnoDB将初始化计数器值,但不会递增该值.这个值会储存起来以备之后的插入语句使用.这个初始化过程使用了一个普通的排它锁来读取表中自增列的最大值. InnoDB遵循相同的过程来初始化新创建的表的自动递增计数器。 在自动递增计数器初始化之后，如果您未明确指定AUTO_INCREMENT列的值，InnoDB会递增计数器并将新值分配给该列。如果插入显式指定列值的行，并且该值大于当前计数器值，则将计数器设置为指定的列值。 只要服务器运行，InnoDB就使用内存中自动递增计数器。当服务器停止并重新启动时，InnoDB会重新初始化每个表的计数器，以便对表进行第一次INSERT，如前所述。 服务器重新启动还会取消CREATE TABLE和ALTER TABLE语句中的AUTO_INCREMENT = N表选项的效果","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"Innodb事务隔离级别","slug":"Innodb事务隔离级别","date":"2016-12-13T14:00:00.000Z","updated":"2017-08-17T07:37:52.000Z","comments":true,"path":"2016/12/13/Innodb事务隔离级别/","link":"","permalink":"http://fuxkdb.com/2016/12/13/Innodb%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","excerpt":"Innodb事务隔离级别REPEATABLE READ This is the default isolation level for InnoDB. Consistent reads within the same transaction read the snapshot established by the first read. This means that if you issue several plain (nonlocking) SELECT statements within the same transaction, these SELECT statements are consistent also with respect to each other. 这是 InnoDB 默认的事务隔离级别,同一事物通过第一次创建的快照来构造一致性读.这意味着如果你在同一会话执行多次查询(非锁定读),那么每次获取的结果都是彼此相等的 For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. 对于锁定读,(SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, 和 DELETE语句,是否锁定取决于此次查询(UPDATE DELETE 也是一种查询)是否通过唯一条件或范围条件查询使用唯一索引 For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. For a unique index with a unique search condition,InnoDB只锁索引记录,不锁 gap For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by other sessions into the gaps covered by the range READ COMMITTED Each consistent read, even within the same transaction, sets and reads its own fresh snapshot. For information about consistent reads read view在 innodb 如何避免幻读中已经介绍过 For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE statements, and DELETE statements, InnoDB locks only index records, not the gaps before them, and thus permits the free insertion of new records next to locked records. Gap locking is only used for foreign-key constraint checking and duplicate-key checking. locking reads(SELECT with FOR UPDATE or LOCK IN SHARE MODE),UPDATE 语句,DELETE语句,InnoDB 都只会锁index records(也就是 record lock),不会产生 gap lock,因此允许在 gap 中插入新的 records.Gap locking只在外键约束检查和重复值检查时产生 Because gap locking is disabled, phantom problems may occur, as other sessions can insert new rows into the gaps 因为gap locking被 disabled,幻读问题可能会产生,其他会话可以在 gap 中插入新的记录","text":"Innodb事务隔离级别REPEATABLE READ This is the default isolation level for InnoDB. Consistent reads within the same transaction read the snapshot established by the first read. This means that if you issue several plain (nonlocking) SELECT statements within the same transaction, these SELECT statements are consistent also with respect to each other. 这是 InnoDB 默认的事务隔离级别,同一事物通过第一次创建的快照来构造一致性读.这意味着如果你在同一会话执行多次查询(非锁定读),那么每次获取的结果都是彼此相等的 For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. 对于锁定读,(SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE, 和 DELETE语句,是否锁定取决于此次查询(UPDATE DELETE 也是一种查询)是否通过唯一条件或范围条件查询使用唯一索引 For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. For a unique index with a unique search condition,InnoDB只锁索引记录,不锁 gap For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key locks to block insertions by other sessions into the gaps covered by the range READ COMMITTED Each consistent read, even within the same transaction, sets and reads its own fresh snapshot. For information about consistent reads read view在 innodb 如何避免幻读中已经介绍过 For locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE), UPDATE statements, and DELETE statements, InnoDB locks only index records, not the gaps before them, and thus permits the free insertion of new records next to locked records. Gap locking is only used for foreign-key constraint checking and duplicate-key checking. locking reads(SELECT with FOR UPDATE or LOCK IN SHARE MODE),UPDATE 语句,DELETE语句,InnoDB 都只会锁index records(也就是 record lock),不会产生 gap lock,因此允许在 gap 中插入新的 records.Gap locking只在外键约束检查和重复值检查时产生 Because gap locking is disabled, phantom problems may occur, as other sessions can insert new rows into the gaps 因为gap locking被 disabled,幻读问题可能会产生,其他会话可以在 gap 中插入新的记录 If you use READ COMMITTED, you must use row-based binary logging. 如果使用READ COMMITTED隔离级别,必须设置binlog_format为row举个例子,假设两张表 T1和 T212345678910111213141516171819202122CREATE TABLE t1 ( id int(11) NOT NULL) ENGINE=InnoDBCREATE TABLE t2 ( id int(11) NOT NULL) ENGINE=InnoDBselect * from t1;+----+| id |+----+| 1 || 2 |+----+ select * from t2;+----+| id |+----+| 2 |+----+ 假设事务隔离级别为read commited,假设发生如下顺序的事务begin;//事务1insert into t2 select max(id) from t1;//事务1，t2增加了一个2begin;//事务2insert into t1(id) values(10);//事务2，t1增加了一个10commit;//事务2commit;//事务1此时12345678910111213141516select * from t1;+----+| id |+----+| 1 || 2 || 10 |+----+ select * from t2;+----+| id |+----+| 2 || 2 |+----+ 假如binlog模式是statement模式，因为事务2先提交，且binlog是串行的，那么在binlog看来，整体事务为begin;//事务2insert into t1(id) values(10);//事务2，t1增加了一个10commit;//事务2begin;//事务1insert into t2 select from max(id) from t1;//事务1，t2增加了一个10commit;//事务1 此时12345678910111213141516select * from t1;+----+| id |+----+| 1 || 2 || 10 |+----+ select * from t2;+----+| id |+----+| 2 || 10 |+----+这就造成了主从不一致,因此read commited事务隔离级别下要使用 RBR 模式 使用READ COMMITTED隔离级别产生的额外影响: For UPDATE or DELETE statements, InnoDB holds locks only for rows that it updates or deletes. Record locks for nonmatching rows are released after MySQL has evaluated the WHERE condition. This greatly reduces the probability of deadlocks, but they can still happen. 对于 UPDATE 和 DELETE 语句,InnoDB只对更新和删除涉及到的行持有锁,而对于在 where 条件过滤后不匹配的行源会释放锁(释放Record locks).这显著的减少了发生死锁的可能性,但并不是说不会再出现死锁. For UPDATE statements, if a row is already locked, InnoDB performs a “semi-consistent” read, returning the latest committed version to MySQL so that MySQL can determine whether the row matches the WHERE condition of the UPDATE. If the row matches (must be updated), MySQL reads the row again and this time InnoDB either locks it or waits for a lock on it. 对于 UPDATE 语句,如果一行记录已经被锁定, InnoDB会执行一个”semi-consistent”读MySQL+InnoDB semi-consitent read原理及实现分析,获取最后一次提交的数据版本,MySQL以这一版本的数据过滤where 条件判断是否匹配UPDATE语句,如果匹配,那么这些行将被更新, MySQL再次读取行源并对这些匹配的行加锁或者等待别人释放锁以加锁 考虑下面的例子:创建表1234CREATE TABLE t (a INT NOT NULL, b INT) ENGINE = InnoDB;INSERT INTO t VALUES (1,2),(2,3),(3,2),(4,3),(5,2);COMMIT; In this case, table has no indexes, so searches and index scans use the hidden clustered index for record locking在这个例子中,表 t 没有索引,所以查询和索引扫描使用隐藏的聚簇索引来进行加锁 SESSION_A12SET autocommit = 0;UPDATE t SET b = 5 WHERE b = 3;SESSION_B12SET autocommit = 0;UPDATE t SET b = 4 WHERE b = 2; As InnoDB executes each UPDATE, it first acquires an exclusive lock for each row, and then determines whether to modify it. If InnoDB does not modify the row, it releases the lock. Otherwise, InnoDB retains the lock until the end of the transaction. This affects transaction processing as follows. InnoDB 执行每次UPDATE操作,都会首先对每一行获取排它锁,然后再决定是否修改.如果InnoDB不修改这一行,就会释放锁,否则InnoDB会持有锁直到事物结束 当时使用默认的REPEATABLE READ事务隔离级别,SESSION_A的更细获取 x-locks(排它锁)且不会释放锁:12345x-lock(1,2); retain x-lockx-lock(2,3); update(2,3) to (2,5); retain x-lockx-lock(3,2); retain x-lockx-lock(4,3); update(4,3) to (4,5); retain x-lockx-lock(5,2); retain x-lockSESSION_B的更新此时就会被阻塞(因为SESSION_A在每一行都加了排它锁),直到SESSION_A提交或回滚1x-lock(1,2); block and wait for first UPDATE to commit or roll back 如果事务隔离级别为READ COMMITTED,SESSION_A获取x-locks(排它锁)后会释放那些不被修改的行的锁:12345x-lock(1,2); unlock(1,2)x-lock(2,3); update(2,3) to (2,5); retain x-lockx-lock(3,2); unlock(3,2)x-lock(4,3); update(4,3) to (4,5); retain x-lockx-lock(5,2); unlock(5,2)SESSION_B执行更新语句,此时InnoDB做了一个“semi-consistent” read,获取并返回了最后一次提交的数据版本给 MySQL,so MySQL 可以判断 where 条件过滤后的数据是否需要被更新,找到匹配的行后,MySQL再次读取行源并对这些匹配的行加锁12345x-lock(1,2); update(1,2) to (1,4); retain x-lockx-lock(2,3); unlock(2,3)x-lock(3,2); update(3,2) to (3,4); retain x-lockx-lock(4,3); unlock(4,3)x-lock(5,2); update(5,2) to (5,4); retain x-lock 使用READ COMMITTED事务隔离级别与启用废弃的参数innodb_locks_unsafe_for_binlog是一样的,除了一下几点: innodb_locks_unsafe_for_binlog是一个 GLOBAL 参数,会影响所有会话,然后事务隔离级别既可以针对全局也可以针对单独会话进行设置 innodb_locks_unsafe_for_binlog只可以在MySQL 启动时设置,然后事务隔离级别级别既可以在启动时设置也可以在运行时设置 所以设置事务隔离级别为READ COMMITTED相比修改参数innodb_locks_unsafe_for_binlog具有更好的灵活性 READ UNCOMMITTED SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent. This is also called a dirty read. Otherwise, this isolation level works like READ COMMITTED. ###SERIALIZABLE This level is like REPEATABLE READ, but InnoDB implicitly converts all plain SELECT statements to SELECT … LOCK IN SHARE MODE if autocommit is disabled. If autocommit is enabled, the SELECT is its own transaction. It therefore is known to be read only and can be serialized if performed as a consistent (nonlocking) read and need not block for other transactions. (To force a plain SELECT to block if other transactions have modified the selected rows, disable autocommit.) 这个级别有点像REPEATABLE READ,但是当autocommit禁用时时InnoDB明确的转换所有 SELECT 语句为SELECT ... LOCK IN SHARE MODE.","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"MySQL(InnoDB)如何避免幻读","slug":"MySQL(InnoDB)如何避免幻读","date":"2016-12-12T14:00:00.000Z","updated":"2017-08-17T08:24:59.000Z","comments":true,"path":"2016/12/12/MySQL(InnoDB)如何避免幻读/","link":"","permalink":"http://fuxkdb.com/2016/12/12/MySQL(InnoDB)%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB/","excerpt":"幻读Phantom Rows The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 幻读问题是指一个事务的两次不同时间的相同查询返回了不同的的结果集。例如:一个 select 语句执行了两次，但是在第二次返回了第一次没有返回的行,那么这些行就是“phantom” row. read view(或者说 MVCC)实现了一致性不锁定读(Consistent Nonlocking Reads)，从而避免了幻读实验1:开两个窗口设置12345set session tx_isolation=&#x27;REPEATABLE-READ&#x27;;select @@session.autocommit;select @@global.tx_isolation,@@session.tx_isolation;create table read_view(text varchar(50));insert into read_view values(&#x27;init&#x27;); 两个会话开始事务12345SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec) SESSION_A执行一个查询，这个查询可以访问任何表，这个查询的目的是创建一个当前时间点的快照START TRANSACTION WITH CONSISTENT SNAPSHOT;也可以达到同样的效果 12345678910SESSION_A&gt;select * from dept;+--------+------------+----------+| deptno | dname | loc |+--------+------------+----------+| 10 | ACCOUNTING | NEW YORK || 20 | RESEARCH | DALLAS || 30 | SALES | CHICAGO || 40 | OPERATIONS | BOSTON |+--------+------------+----------+4 rows in set (0.00 sec) SESSION_B 插入一条记录并提交12345SESSION_B&gt;insert into read_view values(&#x27;after session A select&#x27;);Query OK, 1 row affected (0.01 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec) SESSION_A12345678910111213141516171819SESSION_A&gt;select * from read_view;+------+| text |+------+| init |+------+1 row in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+------------------------+| text |+------------------------+| init || after session A select |+------------------------+2 rows in set (0.00 sec)由于 SESSION_A 第一次的查询开始于 SESSION_B 插入数据前，通过创建了一个以SELECT操作的时间为基准点的 read view,避免了幻读的产生所以在 SESSION_A 的事务结束前,无法看到 SESSION_B 对表 read_view 做出的任何更改 (insert,delete,update)","text":"幻读Phantom Rows The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row. 幻读问题是指一个事务的两次不同时间的相同查询返回了不同的的结果集。例如:一个 select 语句执行了两次，但是在第二次返回了第一次没有返回的行,那么这些行就是“phantom” row. read view(或者说 MVCC)实现了一致性不锁定读(Consistent Nonlocking Reads)，从而避免了幻读实验1:开两个窗口设置12345set session tx_isolation=&#x27;REPEATABLE-READ&#x27;;select @@session.autocommit;select @@global.tx_isolation,@@session.tx_isolation;create table read_view(text varchar(50));insert into read_view values(&#x27;init&#x27;); 两个会话开始事务12345SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec) SESSION_A执行一个查询，这个查询可以访问任何表，这个查询的目的是创建一个当前时间点的快照START TRANSACTION WITH CONSISTENT SNAPSHOT;也可以达到同样的效果 12345678910SESSION_A&gt;select * from dept;+--------+------------+----------+| deptno | dname | loc |+--------+------------+----------+| 10 | ACCOUNTING | NEW YORK || 20 | RESEARCH | DALLAS || 30 | SALES | CHICAGO || 40 | OPERATIONS | BOSTON |+--------+------------+----------+4 rows in set (0.00 sec) SESSION_B 插入一条记录并提交12345SESSION_B&gt;insert into read_view values(&#x27;after session A select&#x27;);Query OK, 1 row affected (0.01 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec) SESSION_A12345678910111213141516171819SESSION_A&gt;select * from read_view;+------+| text |+------+| init |+------+1 row in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+------------------------+| text |+------------------------+| init || after session A select |+------------------------+2 rows in set (0.00 sec)由于 SESSION_A 第一次的查询开始于 SESSION_B 插入数据前，通过创建了一个以SELECT操作的时间为基准点的 read view,避免了幻读的产生所以在 SESSION_A 的事务结束前,无法看到 SESSION_B 对表 read_view 做出的任何更改 (insert,delete,update) 实验2两个会话开始事务12345SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec) SESSION_B 在 SESSION_A 创建read view 前插入数据12345SESSION_B&gt;insert into read_view values(&#x27;before Session_A select&#x27;);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec) SESSION_A1234567891011121314151617181920212223SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)SESSION_A&gt;commit -&gt; ;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)由于 SESSION_A 第一次查询开始于 SESSION_B 对表做出更改并提交后,所以这次的 read view 包含了 SESSION_B 所做出的更改 在官方文档中写道http://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html A consistent read means that InnoDB uses multi-versioning to present to a query a snapshot of the database at a point in time. The query sees the changes made by transactions that committed before that point of time, and no changes made by later or uncommitted transactions. The exception to this rule is that the query sees the changes made by earlier statements within the same transaction. This exception causes the following anomaly: If you update some rows in a table, a SELECT sees the latest version of the updated rows, but it might also see older versions of any rows. If other sessions simultaneously update the same table, the anomaly means that you might see the table in a state that never existed in the database. 一致性读是通过 MVCC 为查询提供了一个基于时间的点的快照。这个查询只能看到在自己之前提交的数据，而在查询开始之后提交的数据是不可以看到的。一个特例是,这个查询可以看到于自己开始之后的同一个事务产生的变化。这个特例会产生一些反常的现象 If the transaction isolation level is REPEATABLE READ (the default level), all consistent reads within the same transaction read the snapshot established by the first such read in that transaction. You can get a fresher snapshot for your queries by committing the current transaction and after that issuing new queries. 在默认隔离级别REPEATABLE READ下，同一事务的所有一致性读只会读取第一次查询时创建的快照 实验3两个会话开始事务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071SESSION_A开始事务并创建快照SESSION_A&gt;START TRANSACTION WITH CONSISTENT SNAPSHOT;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)SESSION_B&gt;insert into read_view values(&#x27;anomaly&#x27;),(&#x27;anomaly&#x27;);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0SESSION_B&gt;update read_view set text=&#x27;INIT&#x27; where text=&#x27;init&#x27;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select |+-------------------------+3 rows in set (0.00 sec)SESSION_A更新了它并没有&quot;看&quot;到的行SESSION_A&gt;update read_view set text=&#x27;anomaly!&#x27; where text=&#x27;anomaly&#x27;;Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| init || after session A select || before Session_A select || anomaly! || anomaly! |+-------------------------+5 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| INIT || after session A select || before Session_A select || anomaly! || anomaly! |+-------------------------+5 rows in set (0.00 sec)观察实验步骤可以发现，在倒数第二次查询中，出现了一个并不存在的状态 the anomaly means that you might see the table in a state that never existed in the database 这里A的前后两次读，均为快照读，而且是在同一个事务中。但是B先插入直接提交，此时A再update，update属于当前读，所以可以作用于新插入的行，并且将修改行的当前版本号设为A的事务号，所以第二次的快照读，是可以读取到的，因为同事务号。这种情况符合MVCC的规则，如果要称为一种幻读也非不可，算为一个特殊情况来看待吧。 With READ COMMITTED isolation level, each consistent read within a transaction sets and reads its own fresh snapshot. 在 read commit 隔离级别下，同一事务的每个一致性读sets and reads its own fresh snapshot. 实验4修改事务隔离级别set session tx_isolation=&#39;READ-COMMITTED&#39;两个会话开始事务12345678910111213141516171819202122232425262728293031323334353637SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| INIT || after session A select || before Session_A select || anomaly! || anomaly! |+-------------------------+5 rows in set (0.00 sec)SESSION_B&gt;insert into read_view values(&#x27;hehe&#x27;);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from read_view;+-------------------------+| text |+-------------------------+| INIT || after session A select || before Session_A select || anomaly! || anomaly! || hehe |+-------------------------+6 rows in set (0.00 sec)read commit 每次读取都是新的快照 InnoDB通过Nextkey lock解决了当前读时的幻读问题Innodb行锁分为:| 类型 | 说明 ||—————-|——————————————————————————————————————————————|| Record Lock: | 在索引上对单行记录加锁. || Gap Lock: | 锁定一个范围的记录,但不包括记录本身.锁加在未使用的空闲空间上,可能是两个索引记录之间，也可能是第一个索引记录之前或最后一个索引之后的空间. || Next-Key Lock: | 行锁与间隙锁组合起来用就叫做Next-Key Lock。锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 | 实验5创建表123456(mysql@localhost) [fandb]&gt; create table t5(id int,key(id));Query OK, 0 rows affected (0.02 sec)SESSION_A&gt;insert into t5 values(1),(4),(7),(10);Query OK, 4 rows affected (0.00 sec)Records: 4 Duplicates: 0 Warnings: 0 开始实验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 4 || 7 || 10 |+------+4 rows in set (0.00 sec)SESSION_A&gt;select * from t5 where id=7 for update;+------+| id |+------+| 7 |+------+1 row in set (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t5 values(2);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t5 values(12);Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t5 values(5); --被阻塞^CCtrl-C -- sending &quot;KILL QUERY 93&quot; to server ...Ctrl-C -- query aborted.^[[AERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;insert into t5 values(7); --被阻塞^CCtrl-C -- sending &quot;KILL QUERY 93&quot; to server ...Ctrl-C -- query aborted.ERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;insert into t5 values(9); --被阻塞^CCtrl-C -- sending &quot;KILL QUERY 93&quot; to server ...Ctrl-C -- query aborted.ERROR 1317 (70100): Query execution was interruptedSESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 4 || 7 || 10 |+------+4 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t5;+------+| id |+------+| 1 || 2 || 4 || 7 || 10 || 12 |+------+6 rows in set (0.00 sec)当以当前读模式select * from t5 where id=7 for update;获取 id=7的数据时,产生了 Next-Key Lock,锁住了4-10范围和 id=7单个record从而阻塞了 SESSION_B在这个范围内插入数据，而在除此之外的范围内是可以插入数据的。在倒数第二个查询中,因为 read view 的存在，避免了我们看到 2和12两条数据，避免了幻读同时因为 Next-Key Lock 的存在,阻塞了其他回话插入数据，因此当前模式读不会产生幻读(select for update 是以当前读模式获取数据) 尽量使用唯一索引,因为唯一索引会把Next-Key Lock降级为Record Lock实验6创建表(mysql@localhost) [fandb]&gt; create table t6(id int primary key);Query OK, 0 rows affected (0.02 sec) SESSION_A&gt;insert into t6 values(1),(4),(7),(10);Query OK, 4 rows affected (0.00 sec)Records: 4 Duplicates: 0 Warnings: 0 开始实验12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061SESSION_A&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 7 || 10 |+----+4 rows in set (0.00 sec)SESSION_A&gt;select * from t6 where id=7 for update;+----+| id |+----+| 7 |+----+1 row in set (0.00 sec)SESSION_B&gt;begin;Query OK, 0 rows affected (0.00 sec)SESSION_B&gt;insert into t6 values(5); --插入成功没有阻塞Query OK, 1 row affected (0.00 sec)SESSION_B&gt;insert into t6 values(8); --插入成功没有阻塞Query OK, 1 row affected (0.00 sec)SESSION_B&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 7 || 10 |+----+4 rows in set (0.00 sec)SESSION_A&gt;commit;Query OK, 0 rows affected (0.00 sec)SESSION_A&gt;select * from t6;+----+| id |+----+| 1 || 4 || 5 || 7 || 8 || 10 |+----+6 rows in set (0.00 sec)当 id 列有唯一索引,Next-Key Lock 会降级为 Records Lock","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"Auto_increment详解","slug":"Auto_increment详解","date":"2016-07-31T14:00:00.000Z","updated":"2017-08-05T16:00:59.000Z","comments":true,"path":"2016/07/31/Auto_increment详解/","link":"","permalink":"http://fuxkdb.com/2016/07/31/Auto_increment%E8%AF%A6%E8%A7%A3/","excerpt":"Auto_incrementMysql AUTO_INCREMENT 1.Innodb表的自动增长列可以手工插入，但是插入的值如果是空或者0，则实际插入的将是自动增长后的值1234567891011121314151617181920212223242526mysql&gt; create table t1(id int not null auto_increment primary key,name varchar(10));Query OK, 0 rows affected (0.06 sec)mysql&gt; desc t1;+-------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(10) | YES | | NULL | |+-------+-------------+------+-----+---------+----------------+2 rows in set (0.01 sec)mysql&gt; insert into t1 values(0,&#x27;fanboshi&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t1 values(null,&#x27;duyalan&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan |+----+----------+2 rows in set (0.00 sec) 2.可以通过alter table t1 auto_incremenrt=n 语句强制设置自动增长列的初始值，默认从1开始，但是该强制的默认值是保留在内存中的，如果该值在使用之前数据库重新启动，那么这个强制的默认值就会丢失，就需要数据库启动后重新设置12345678910111213141516mysql&gt; alter table t1 auto_increment=5;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; insert into t1 values(null,&#x27;handudu&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan || 5 | handudu |+----+----------+3 rows in set (0.00 sec) 3.可以是用last_insert_id()查询当前线程最后插入记录使用的值。如果一次插入多条记录，那么返回的是第一条记录使用的自动增长值。","text":"Auto_incrementMysql AUTO_INCREMENT 1.Innodb表的自动增长列可以手工插入，但是插入的值如果是空或者0，则实际插入的将是自动增长后的值1234567891011121314151617181920212223242526mysql&gt; create table t1(id int not null auto_increment primary key,name varchar(10));Query OK, 0 rows affected (0.06 sec)mysql&gt; desc t1;+-------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(10) | YES | | NULL | |+-------+-------------+------+-----+---------+----------------+2 rows in set (0.01 sec)mysql&gt; insert into t1 values(0,&#x27;fanboshi&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t1 values(null,&#x27;duyalan&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan |+----+----------+2 rows in set (0.00 sec) 2.可以通过alter table t1 auto_incremenrt=n 语句强制设置自动增长列的初始值，默认从1开始，但是该强制的默认值是保留在内存中的，如果该值在使用之前数据库重新启动，那么这个强制的默认值就会丢失，就需要数据库启动后重新设置12345678910111213141516mysql&gt; alter table t1 auto_increment=5;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; insert into t1 values(null,&#x27;handudu&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan || 5 | handudu |+----+----------+3 rows in set (0.00 sec) 3.可以是用last_insert_id()查询当前线程最后插入记录使用的值。如果一次插入多条记录，那么返回的是第一条记录使用的自动增长值。 123456789mysql&gt; select last_insert_id();+------------------+| last_insert_id() |+------------------+| 5 |+------------------+1 row in set (0.00 sec)注意last_insert_id()是所有表auto_increment的最新插入值，因此在并发的情况下，获取某表的最新插入auto_increment可能出现错误 4.对于innodb表，自动增长列必须是索引，且必须是组合索引的第一列，且一个表只能有一个auto_increment属性。mysql&gt; create table t2(id int not null auto_increment,name varchar(10));ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key123456789101112131415161718192021222324252627282930313233343536373839非主键mysql&gt; create table t2(id int not null auto_increment,name varchar(10),index(id));Query OK, 0 rows affected (0.09 sec)mysql&gt; mysql&gt; show index from t2;+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| t2 | 1 | id | 1 | id | A | 0 | NULL | NULL | | BTREE | | |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+1 row in set (0.00 sec)mysql&gt; show index from t2;+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| t2 | 1 | id | 1 | id | A | 0 | NULL | NULL | | BTREE | | |+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+1 row in set (0.00 sec)不是主键，只是有索引mysql&gt; insert into t2 values(1,&#x27;fan&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t2 values(2,&#x27;fan&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t2;+----+------+| id | name |+----+------+| 1 | fan || 2 | fan |+----+------+2 rows in set (0.00 sec)如果是组合索引，也必须是组合索引的第一列mysql&gt; create table t3(id1 int not null auto_increment,id2 int,name varchar(10),index(id2,id1));ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key但是对于MyISAM表，自动增长列可以使组合索引的其他列，这样插入记录后，自动增长列是按照组合索引的前面几列进行排序后递增的。123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; create table t3_myisam(id1 int not null auto_increment,id2 int,name varchar(10),index(id2,id1)) engine=myisam;Query OK, 0 rows affected (0.04 sec)mysql&gt; insert into t3_myisam(id2,name) values(3,&#x27;fanboshi&#x27;),(1,&#x27;duyalan&#x27;),(1,&#x27;daduzi&#x27;),(2,&#x27;fan&#x27;),(5,&#x27;hehe&#x27;),(6,&#x27;keke&#x27;);Query OK, 6 rows affected (0.03 sec)Records: 6 Duplicates: 0 Warnings: 0mysql&gt; select * from t3_myisam;+-----+------+----------+| id1 | id2 | name |+-----+------+----------+| 1 | 3 | fanboshi || 1 | 1 | duyalan || 2 | 1 | daduzi || 1 | 2 | fan || 1 | 5 | hehe || 1 | 6 | keke |+-----+------+----------+6 rows in set (0.00 sec)好像看不出啥规律再插入一次mysql&gt; insert into t3_myisam(id2,name) values(3,&#x27;fanboshi&#x27;),(1,&#x27;duyalan&#x27;),(1,&#x27;daduzi&#x27;),(2,&#x27;fan&#x27;),(5,&#x27;hehe&#x27;),(6,&#x27;keke&#x27;);Query OK, 6 rows affected (0.00 sec)Records: 6 Duplicates: 0 Warnings: 0mysql&gt; select * from t3_myisam order by id2,id1;+-----+------+----------+| id1 | id2 | name |+-----+------+----------+| 1 | 1 | duyalan || 2 | 1 | daduzi || 3 | 1 | duyalan || 4 | 1 | daduzi || 1 | 2 | fan || 2 | 2 | fan || 1 | 3 | fanboshi || 2 | 3 | fanboshi || 1 | 5 | hehe || 2 | 5 | hehe || 1 | 6 | keke || 2 | 6 | keke |+-----+------+----------+12 rows in set (0.00 sec)id2=1有四个，所以id1有1,2,3,4id2=2有俩，id1=1,2自动增长列id1作为组合索引的第二列，对该表插入一些记录后，可以发现自动增长列是按照组合索引第一列id2进行排序后分组递增的 5.MyISAM 及INNODB表，表中auto_increment最大值被删除，将不会被重用。就是说会跳号123456789101112131415161718192021222324252627282930mysql&gt; insert into t1(name) values(&#x27;hehe&#x27;);Query OK, 1 row affected (0.02 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan || 5 | handudu || 6 | hehe |+----+----------+4 rows in set (0.00 sec)mysql&gt; delete from t1 where id=6;Query OK, 1 row affected (0.08 sec)mysql&gt; insert into t1(name) values(&#x27;keke&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 2 | duyalan || 5 | handudu || 7 | keke |+----+----------+4 rows in set (0.00 sec) 6.用”WHERE auto_col IS NULL”条件选择出新插入的行，即在INSERT后马上用: SELECT * FROM t4 WHERE id IS NULL;选择出来的将是新插入的行，而非真正的满足”id IS NULL”条件的行。但你要是再执行一次上述查询，则返回的又变成了真正的满足”a IS NULL”条件的行，由于a是主键，因此肯定会返回空集。这看上去很诡异是吗，不过MySQL也不想这么干，为了支持 ODBC标准不过可以将SQL_AUTO_IS_NULL设为0来禁止这一用法。此方法获取last_insert_id不推荐1234567891011121314151617181920212223242526272829303132333435mysql&gt; insert into t1(name) values(&#x27;new&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1 where id is null;Empty set (0.00 sec)mysql&gt; show variables like &#x27;sql_auto_is_null&#x27;;+------------------+-------+| Variable_name | Value |+------------------+-------+| sql_auto_is_null | OFF |+------------------+-------+1 row in set (0.00 sec)mysql&gt; set session sql_auto_is_null=on;Query OK, 0 rows affected (0.02 sec)mysql&gt; show variables like &#x27;sql_auto_is_null&#x27;;+------------------+-------+| Variable_name | Value |+------------------+-------+| sql_auto_is_null | ON |+------------------+-------+1 row in set (0.00 sec)mysql&gt; select * from t1 where id is null;+----+------+| id | name |+----+------+| 8 | new |+----+------+1 row in set (0.01 sec)mysql&gt; select * from t1 where id is null;Empty set (0.00 sec) 7.AUTO_INCREMENT属性也给复制带来了麻烦。一般情况下复制AUTO_INCREMENT属性能正确工作，但以下情况还是有问题： INSERT DELAYED … VALUES(LAST_INSERT_ID())不能被正确复制 存储过程插入的使用AUTO_INCREMENT属性的记录不能被正确复制 通过”ALTER TABLE”命令增加AUTO_INCREMENT属性时在主从节点上产生的值可能是不一样的，因为这个各行AUTO_INCREMENT属性的值取决于物理上的存储顺序。 8.对于replication的master-master方式 为防止auto_increment字段的重复，可做如下设置A服务器的my.cnf设置如下： auto_increment_offset = 1auto_increment_increment = 2 这样A的auto_increment字段产生的数值是：1, 3, 5, 7, … B服务器的my.cnf设置如下： auto_increment_offset = 2auto_increment_increment = 2 这样B的auto_increment字段产生的数值是：2, 4, 6, 8, … 3和节点A offset=1 incr=3B offset=2 incr=3C offset=3 incr=3 8.根据官方的说明：If the value of auto_increment_offset is greater than that of auto_increment_increment, the value of auto_increment_offset is ignored. （如果auto_increment_offset的值大于auto_increment_increment的值，则auto_increment_offset的值会被忽略）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162mysql&gt; show variables like &#x27;auto_increment%&#x27;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| auto_increment_increment | 1 || auto_increment_offset | 1 |+--------------------------+-------+2 rows in set (0.00 sec)mysql&gt; set session auto_increment_offset=5;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like &#x27;auto_increment%&#x27;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| auto_increment_increment | 1 || auto_increment_offset | 5 |+--------------------------+-------+2 rows in set (0.00 sec)mysql&gt; create table t5 like t1;Query OK, 0 rows affected (0.07 sec)mysql&gt; desc t5;+-------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(10) | YES | | NULL | |+-------+-------------+------+-----+---------+----------------+2 rows in set (0.00 sec)mysql&gt; insert into t5(name) values(&#x27;fanboshi&#x27;);Query OK, 1 row affected (0.01 sec)mysql&gt; select * from t5;+----+----------+| id | name |+----+----------+| 1 | fanboshi |+----+----------+1 row in set (0.00 sec)mysql&gt; set session auto_increment_increment=5;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t5(name) values(&#x27;duyalan&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t5(name) values(&#x27;heheda&#x27;);Query OK, 1 row affected (0.02 sec)mysql&gt; select * from t5;+----+----------+| id | name |+----+----------+| 1 | fanboshi || 5 | duyalan || 10 | heheda |+----+----------+3 rows in set (0.00 sec)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://fuxkdb.com/tags/MySQL/"}]},{"title":"Hello World","slug":"hello-world","date":"2015-12-31T17:02:03.000Z","updated":"2017-08-05T15:33:01.000Z","comments":true,"path":"2016/01/01/hello-world/","link":"","permalink":"http://fuxkdb.com/2016/01/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}